Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:53, 20.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:37<03:56, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:54<03:32, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:10<03:07, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:25<02:45, 16.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:41<02:26, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:57<02:08, 16.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:13<01:53, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:29<01:36, 16.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:44<01:17, 15.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:01<01:04, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:16<00:47, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:32<00:31, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:49<00:16, 16.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:06<00:00, 16.39s/it]Generating: 100%|██████████| 15/15 [04:06<00:00, 16.42s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')"}}
['Relation : instrument . Context : Later in the year ( 1141–1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz Inácio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : Geniçiu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.64s/it]Extractor Estimating: 2it [00:18,  8.19s/it]Extractor Estimating: 3it [00:20,  5.00s/it]Extractor Estimating: 4it [00:20,  3.29s/it]Extractor Estimating: 5it [00:21,  2.31s/it]Extractor Estimating: 6it [00:21,  1.73s/it]Extractor Estimating: 7it [00:22,  1.36s/it]Extractor Estimating: 8it [00:23,  1.13s/it]Extractor Estimating: 9it [00:23,  1.03it/s]Extractor Estimating: 10it [00:24,  1.16it/s]Extractor Estimating: 11it [00:25,  1.26it/s]Extractor Estimating: 12it [00:26,  1.05it/s]Extractor Estimating: 13it [00:27,  1.15it/s]Extractor Estimating: 14it [00:27,  1.25it/s]Extractor Estimating: 15it [00:28,  1.32it/s]Extractor Estimating: 16it [00:29,  1.12it/s]Extractor Estimating: 17it [00:30,  1.22it/s]Extractor Estimating: 18it [00:32,  1.11s/it]Extractor Estimating: 19it [00:32,  1.04it/s]Extractor Estimating: 20it [00:33,  1.16it/s]Extractor Estimating: 21it [00:33,  1.26it/s]Extractor Estimating: 22it [00:34,  1.29it/s]Extractor Estimating: 23it [00:35,  1.37it/s]Extractor Estimating: 24it [00:35,  1.41it/s]Extractor Estimating: 25it [00:36,  1.46it/s]Extractor Estimating: 26it [00:37,  1.50it/s]Extractor Estimating: 27it [00:37,  1.40it/s]Extractor Estimating: 28it [00:38,  1.45it/s]Extractor Estimating: 29it [00:39,  1.47it/s]Extractor Estimating: 30it [00:39,  1.45it/s]Extractor Estimating: 31it [00:40,  1.54it/s]Extractor Estimating: 32it [00:41,  1.58it/s]Extractor Estimating: 33it [00:41,  1.52it/s]Extractor Estimating: 34it [00:42,  1.54it/s]Extractor Estimating: 35it [00:43,  1.54it/s]Extractor Estimating: 36it [00:43,  1.54it/s]Extractor Estimating: 37it [00:44,  1.52it/s]Extractor Estimating: 38it [00:45,  1.53it/s]Extractor Estimating: 39it [00:45,  1.53it/s]Extractor Estimating: 40it [00:46,  1.59it/s]Extractor Estimating: 41it [00:47,  1.54it/s]Extractor Estimating: 42it [00:47,  1.50it/s]Extractor Estimating: 43it [00:48,  1.50it/s]Extractor Estimating: 44it [00:49,  1.47it/s]Extractor Estimating: 45it [00:49,  1.43it/s]Extractor Estimating: 46it [00:50,  1.47it/s]Extractor Estimating: 47it [00:51,  1.46it/s]Extractor Estimating: 48it [00:51,  1.41it/s]Extractor Estimating: 49it [00:53,  1.04s/it]Extractor Estimating: 50it [00:54,  1.08it/s]Extractor Estimating: 51it [00:55,  1.18it/s]Extractor Estimating: 52it [00:55,  1.26it/s]Extractor Estimating: 53it [00:56,  1.36it/s]Extractor Estimating: 54it [00:57,  1.30it/s]Extractor Estimating: 55it [00:57,  1.38it/s]Extractor Estimating: 56it [00:58,  1.43it/s]Extractor Estimating: 57it [00:59,  1.45it/s]Extractor Estimating: 58it [00:59,  1.48it/s]Extractor Estimating: 59it [01:00,  1.52it/s]Extractor Estimating: 60it [01:01,  1.50it/s]Extractor Estimating: 61it [01:01,  1.50it/s]Extractor Estimating: 62it [01:03,  1.04s/it]Extractor Estimating: 63it [01:04,  1.10it/s]Extractor Estimating: 64it [01:04,  1.19it/s]Extractor Estimating: 65it [01:05,  1.29it/s]Extractor Estimating: 66it [01:06,  1.37it/s]Extractor Estimating: 67it [01:06,  1.42it/s]Extractor Estimating: 68it [01:07,  1.44it/s]Extractor Estimating: 69it [01:08,  1.44it/s]Extractor Estimating: 70it [01:08,  1.47it/s]Extractor Estimating: 71it [01:09,  1.49it/s]Extractor Estimating: 72it [01:10,  1.51it/s]Extractor Estimating: 73it [01:10,  1.49it/s]Extractor Estimating: 74it [01:11,  1.53it/s]Extractor Estimating: 75it [01:12,  1.53it/s]Extractor Estimating: 76it [01:12,  1.57it/s]Extractor Estimating: 77it [01:13,  1.57it/s]Extractor Estimating: 78it [01:14,  1.33it/s]Extractor Estimating: 79it [01:15,  1.38it/s]Extractor Estimating: 80it [01:15,  1.46it/s]Extractor Estimating: 81it [01:16,  1.49it/s]Extractor Estimating: 82it [01:16,  1.49it/s]Extractor Estimating: 83it [01:17,  1.54it/s]Extractor Estimating: 84it [01:18,  1.56it/s]Extractor Estimating: 85it [01:18,  1.56it/s]Extractor Estimating: 86it [01:19,  1.51it/s]Extractor Estimating: 87it [01:20,  1.50it/s]Extractor Estimating: 88it [01:20,  1.51it/s]Extractor Estimating: 89it [01:21,  1.54it/s]Extractor Estimating: 90it [01:22,  1.53it/s]Extractor Estimating: 91it [01:22,  1.57it/s]Extractor Estimating: 92it [01:23,  1.57it/s]Extractor Estimating: 93it [01:23,  1.60it/s]Extractor Estimating: 94it [01:24,  1.49it/s]Extractor Estimating: 95it [01:25,  1.54it/s]Extractor Estimating: 96it [01:25,  1.56it/s]Extractor Estimating: 97it [01:26,  1.55it/s]Extractor Estimating: 98it [01:27,  1.55it/s]Extractor Estimating: 99it [01:27,  1.58it/s]Extractor Estimating: 100it [01:28,  1.57it/s]Extractor Estimating: 101it [01:29,  1.50it/s]Extractor Estimating: 102it [01:30,  1.33it/s]Extractor Estimating: 103it [01:30,  1.43it/s]Extractor Estimating: 104it [01:31,  1.48it/s]Extractor Estimating: 105it [01:32,  1.46it/s]Extractor Estimating: 106it [01:32,  1.48it/s]Extractor Estimating: 107it [01:33,  1.48it/s]Extractor Estimating: 108it [01:34,  1.52it/s]Extractor Estimating: 109it [01:34,  1.54it/s]Extractor Estimating: 110it [01:35,  1.50it/s]Extractor Estimating: 111it [01:35,  1.56it/s]Extractor Estimating: 112it [01:36,  1.49it/s]Extractor Estimating: 113it [01:37,  1.48it/s]Extractor Estimating: 114it [01:38,  1.49it/s]Extractor Estimating: 115it [01:38,  1.52it/s]Extractor Estimating: 116it [01:39,  1.58it/s]Extractor Estimating: 117it [01:39,  1.57it/s]Extractor Estimating: 118it [01:40,  1.60it/s]Extractor Estimating: 119it [01:41,  1.45it/s]Extractor Estimating: 120it [01:41,  1.49it/s]Extractor Estimating: 121it [01:42,  1.56it/s]Extractor Estimating: 122it [01:43,  1.51it/s]Extractor Estimating: 123it [01:43,  1.59it/s]Extractor Estimating: 124it [01:44,  1.57it/s]Extractor Estimating: 125it [01:45,  1.54it/s]Extractor Estimating: 126it [01:45,  1.61it/s]Extractor Estimating: 127it [01:46,  1.59it/s]Extractor Estimating: 128it [01:46,  1.62it/s]Extractor Estimating: 129it [01:47,  1.62it/s]Extractor Estimating: 130it [01:48,  1.70it/s]Extractor Estimating: 131it [01:48,  1.67it/s]Extractor Estimating: 132it [01:49,  1.67it/s]Extractor Estimating: 133it [01:49,  1.67it/s]Extractor Estimating: 134it [01:50,  1.64it/s]Extractor Estimating: 135it [01:51,  1.68it/s]Extractor Estimating: 136it [01:51,  1.68it/s]Extractor Estimating: 137it [01:52,  1.56it/s]Extractor Estimating: 138it [01:53,  1.58it/s]Extractor Estimating: 139it [01:53,  1.64it/s]Extractor Estimating: 140it [01:54,  1.65it/s]Extractor Estimating: 141it [01:54,  1.63it/s]Extractor Estimating: 142it [01:55,  1.63it/s]Extractor Estimating: 143it [01:56,  1.65it/s]Extractor Estimating: 144it [01:56,  1.64it/s]Extractor Estimating: 145it [01:57,  1.59it/s]Extractor Estimating: 146it [01:57,  1.64it/s]Extractor Estimating: 147it [01:58,  1.62it/s]Extractor Estimating: 148it [01:59,  1.59it/s]Extractor Estimating: 149it [01:59,  1.54it/s]Extractor Estimating: 150it [02:00,  1.53it/s]Extractor Estimating: 151it [02:01,  1.44it/s]Extractor Estimating: 152it [02:01,  1.47it/s]Extractor Estimating: 153it [02:02,  1.51it/s]Extractor Estimating: 154it [02:03,  1.53it/s]Extractor Estimating: 155it [02:03,  1.54it/s]Extractor Estimating: 156it [02:04,  1.56it/s]Extractor Estimating: 157it [02:05,  1.51it/s]Extractor Estimating: 158it [02:05,  1.54it/s]Extractor Estimating: 159it [02:06,  1.53it/s]Extractor Estimating: 160it [02:07,  1.55it/s]Extractor Estimating: 161it [02:07,  1.56it/s]Extractor Estimating: 162it [02:08,  1.56it/s]Extractor Estimating: 163it [02:08,  1.57it/s]Extractor Estimating: 164it [02:09,  1.60it/s]Extractor Estimating: 165it [02:10,  1.63it/s]Extractor Estimating: 166it [02:10,  1.61it/s]Extractor Estimating: 167it [02:11,  1.52it/s]Extractor Estimating: 168it [02:12,  1.54it/s]Extractor Estimating: 169it [02:12,  1.56it/s]Extractor Estimating: 170it [02:13,  1.64it/s]Extractor Estimating: 171it [02:13,  1.63it/s]Extractor Estimating: 172it [02:14,  1.56it/s]Extractor Estimating: 173it [02:15,  1.53it/s]Extractor Estimating: 174it [02:16,  1.52it/s]Extractor Estimating: 175it [02:16,  1.41it/s]Extractor Estimating: 176it [02:17,  1.42it/s]Extractor Estimating: 177it [02:18,  1.46it/s]Extractor Estimating: 178it [02:18,  1.50it/s]Extractor Estimating: 179it [02:19,  1.53it/s]Extractor Estimating: 180it [02:20,  1.54it/s]Extractor Estimating: 181it [02:20,  1.59it/s]Extractor Estimating: 182it [02:21,  1.57it/s]Extractor Estimating: 183it [02:22,  1.26it/s]Extractor Estimating: 184it [02:23,  1.34it/s]Extractor Estimating: 185it [02:23,  1.41it/s]Extractor Estimating: 186it [02:24,  1.47it/s]Extractor Estimating: 187it [02:25,  1.47it/s]Extractor Estimating: 188it [02:25,  1.50it/s]Extractor Estimating: 189it [02:27,  1.04s/it]Extractor Estimating: 190it [02:28,  1.09it/s]Extractor Estimating: 191it [02:28,  1.23it/s]Extractor Estimating: 192it [02:29,  1.29it/s]Extractor Estimating: 193it [02:30,  1.36it/s]Extractor Estimating: 194it [02:30,  1.40it/s]Extractor Estimating: 195it [02:31,  1.46it/s]Extractor Estimating: 196it [02:31,  1.51it/s]Extractor Estimating: 197it [02:33,  1.27it/s]Extractor Estimating: 198it [02:33,  1.35it/s]Extractor Estimating: 199it [02:34,  1.33it/s]Extractor Estimating: 200it [02:35,  1.38it/s]Extractor Estimating: 201it [02:35,  1.39it/s]Extractor Estimating: 202it [02:36,  1.44it/s]Extractor Estimating: 203it [02:37,  1.46it/s]Extractor Estimating: 204it [02:38,  1.16it/s]Extractor Estimating: 205it [02:39,  1.27it/s]Extractor Estimating: 206it [02:39,  1.32it/s]Extractor Estimating: 207it [02:40,  1.42it/s]Extractor Estimating: 208it [02:41,  1.39it/s]Extractor Estimating: 209it [02:41,  1.40it/s]Extractor Estimating: 210it [02:42,  1.45it/s]Extractor Estimating: 211it [02:43,  1.45it/s]Extractor Estimating: 212it [02:43,  1.46it/s]Extractor Estimating: 213it [02:44,  1.43it/s]Extractor Estimating: 214it [02:45,  1.46it/s]Extractor Estimating: 215it [02:45,  1.49it/s]Extractor Estimating: 216it [02:46,  1.44it/s]Extractor Estimating: 217it [02:47,  1.44it/s]Extractor Estimating: 218it [02:47,  1.48it/s]Extractor Estimating: 219it [02:48,  1.29it/s]Extractor Estimating: 220it [02:49,  1.36it/s]Extractor Estimating: 221it [02:50,  1.35it/s]Extractor Estimating: 222it [02:50,  1.37it/s]Extractor Estimating: 223it [02:51,  1.40it/s]Extractor Estimating: 224it [02:52,  1.43it/s]Extractor Estimating: 225it [02:52,  1.46it/s]Extractor Estimating: 226it [02:53,  1.44it/s]Extractor Estimating: 227it [02:54,  1.52it/s]Extractor Estimating: 228it [02:54,  1.58it/s]Extractor Estimating: 229it [02:55,  1.61it/s]Extractor Estimating: 230it [02:56,  1.61it/s]Extractor Estimating: 231it [02:56,  1.65it/s]Extractor Estimating: 232it [02:57,  1.70it/s]Extractor Estimating: 233it [02:57,  1.74it/s]Extractor Estimating: 234it [02:58,  1.72it/s]Extractor Estimating: 235it [02:58,  1.70it/s]Extractor Estimating: 236it [02:59,  1.67it/s]Extractor Estimating: 237it [03:00,  1.66it/s]Extractor Estimating: 238it [03:00,  1.65it/s]Extractor Estimating: 239it [03:01,  1.64it/s]Extractor Estimating: 240it [03:01,  1.65it/s]Extractor Estimating: 241it [03:02,  1.68it/s]Extractor Estimating: 242it [03:03,  1.67it/s]Extractor Estimating: 243it [03:03,  1.62it/s]Extractor Estimating: 244it [03:04,  1.53it/s]Extractor Estimating: 245it [03:05,  1.58it/s]Extractor Estimating: 246it [03:05,  1.62it/s]Extractor Estimating: 247it [03:06,  1.62it/s]Extractor Estimating: 248it [03:06,  1.60it/s]Extractor Estimating: 249it [03:07,  1.61it/s]Extractor Estimating: 250it [03:08,  1.64it/s]Extractor Estimating: 251it [03:08,  1.53it/s]Extractor Estimating: 252it [03:09,  1.53it/s]Extractor Estimating: 253it [03:10,  1.55it/s]Extractor Estimating: 254it [03:10,  1.49it/s]Extractor Estimating: 255it [03:11,  1.55it/s]Extractor Estimating: 256it [03:12,  1.64it/s]Extractor Estimating: 257it [03:12,  1.63it/s]Extractor Estimating: 258it [03:13,  1.52it/s]Extractor Estimating: 259it [03:14,  1.57it/s]Extractor Estimating: 260it [03:15,  1.03it/s]Extractor Estimating: 261it [03:16,  1.13it/s]Extractor Estimating: 262it [03:17,  1.21it/s]Extractor Estimating: 263it [03:17,  1.31it/s]Extractor Estimating: 264it [03:18,  1.39it/s]Extractor Estimating: 265it [03:19,  1.40it/s]Extractor Estimating: 266it [03:19,  1.45it/s]Extractor Estimating: 267it [03:21,  1.11s/it]Extractor Estimating: 268it [03:22,  1.06it/s]Extractor Estimating: 269it [03:22,  1.18it/s]Extractor Estimating: 270it [03:23,  1.25it/s]Extractor Estimating: 271it [03:24,  1.32it/s]Extractor Estimating: 272it [03:24,  1.36it/s]Extractor Estimating: 273it [03:25,  1.42it/s]Extractor Estimating: 274it [03:27,  1.10it/s]Extractor Estimating: 275it [03:27,  1.22it/s]Extractor Estimating: 276it [03:28,  1.35it/s]Extractor Estimating: 277it [03:28,  1.41it/s]Extractor Estimating: 278it [03:29,  1.53it/s]Extractor Estimating: 279it [03:29,  1.55it/s]Extractor Estimating: 280it [03:30,  1.58it/s]Extractor Estimating: 281it [03:31,  1.62it/s]Extractor Estimating: 282it [03:33,  1.15s/it]Extractor Estimating: 283it [03:34,  1.00it/s]Extractor Estimating: 284it [03:34,  1.13it/s]Extractor Estimating: 285it [03:35,  1.26it/s]Extractor Estimating: 286it [03:36,  1.28it/s]Extractor Estimating: 287it [03:36,  1.38it/s]Extractor Estimating: 288it [03:37,  1.42it/s]Extractor Estimating: 289it [03:38,  1.48it/s]Extractor Estimating: 290it [03:38,  1.51it/s]Extractor Estimating: 291it [03:39,  1.60it/s]Extractor Estimating: 292it [03:39,  1.58it/s]Extractor Estimating: 293it [03:40,  1.63it/s]Extractor Estimating: 294it [03:41,  1.62it/s]Extractor Estimating: 295it [03:41,  1.62it/s]Extractor Estimating: 296it [03:42,  1.67it/s]Extractor Estimating: 297it [03:42,  1.69it/s]Extractor Estimating: 298it [03:43,  1.70it/s]Extractor Estimating: 299it [03:43,  1.74it/s]Extractor Estimating: 300it [03:44,  1.76it/s]Extractor Estimating: 301it [03:45,  1.66it/s]Extractor Estimating: 302it [03:45,  1.59it/s]Extractor Estimating: 303it [03:46,  1.60it/s]Extractor Estimating: 304it [03:46,  1.67it/s]Extractor Estimating: 305it [03:47,  1.65it/s]Extractor Estimating: 306it [03:48,  1.67it/s]Extractor Estimating: 307it [03:48,  1.69it/s]Extractor Estimating: 308it [03:49,  1.77it/s]Extractor Estimating: 309it [03:49,  1.77it/s]Extractor Estimating: 310it [03:50,  1.71it/s]Extractor Estimating: 311it [03:51,  1.62it/s]Extractor Estimating: 312it [03:51,  1.68it/s]Extractor Estimating: 313it [03:52,  1.64it/s]Extractor Estimating: 314it [03:52,  1.61it/s]Extractor Estimating: 315it [03:54,  1.29it/s]Extractor Estimating: 316it [03:54,  1.39it/s]Extractor Estimating: 317it [03:55,  1.47it/s]Extractor Estimating: 318it [03:55,  1.49it/s]Extractor Estimating: 319it [03:56,  1.55it/s]Extractor Estimating: 320it [03:57,  1.52it/s]Extractor Estimating: 321it [03:57,  1.60it/s]Extractor Estimating: 322it [03:58,  1.59it/s]Extractor Estimating: 323it [03:59,  1.46it/s]Extractor Estimating: 324it [03:59,  1.52it/s]Extractor Estimating: 325it [04:00,  1.56it/s]Extractor Estimating: 326it [04:01,  1.45it/s]Extractor Estimating: 327it [04:01,  1.50it/s]Extractor Estimating: 328it [04:02,  1.49it/s]Extractor Estimating: 329it [04:03,  1.54it/s]Extractor Estimating: 330it [04:03,  1.54it/s]Extractor Estimating: 331it [04:04,  1.34it/s]Extractor Estimating: 332it [04:05,  1.39it/s]Extractor Estimating: 333it [04:06,  1.43it/s]Extractor Estimating: 334it [04:06,  1.49it/s]Extractor Estimating: 335it [04:07,  1.48it/s]Extractor Estimating: 336it [04:07,  1.52it/s]Extractor Estimating: 337it [04:08,  1.56it/s]Extractor Estimating: 338it [04:09,  1.55it/s]Extractor Estimating: 339it [04:10,  1.41it/s]Extractor Estimating: 340it [04:10,  1.48it/s]Extractor Estimating: 341it [04:11,  1.44it/s]Extractor Estimating: 342it [04:12,  1.48it/s]Extractor Estimating: 343it [04:12,  1.50it/s]Extractor Estimating: 344it [04:13,  1.51it/s]Extractor Estimating: 345it [04:13,  1.55it/s]Extractor Estimating: 346it [04:14,  1.50it/s]Extractor Estimating: 347it [04:15,  1.44it/s]Extractor Estimating: 348it [04:16,  1.43it/s]Extractor Estimating: 349it [04:16,  1.44it/s]Extractor Estimating: 350it [04:17,  1.48it/s]Extractor Estimating: 351it [04:18,  1.52it/s]Extractor Estimating: 352it [04:18,  1.57it/s]Extractor Estimating: 353it [04:19,  1.60it/s]Extractor Estimating: 354it [04:19,  1.65it/s]Extractor Estimating: 355it [04:20,  1.65it/s]Extractor Estimating: 356it [04:21,  1.60it/s]Extractor Estimating: 357it [04:21,  1.62it/s]Extractor Estimating: 358it [04:22,  1.61it/s]Extractor Estimating: 359it [04:22,  1.70it/s]Extractor Estimating: 360it [04:23,  1.55it/s]Extractor Estimating: 361it [04:24,  1.57it/s]Extractor Estimating: 362it [04:24,  1.57it/s]Extractor Estimating: 363it [04:25,  1.65it/s]Extractor Estimating: 364it [04:26,  1.61it/s]Extractor Estimating: 365it [04:26,  1.55it/s]Extractor Estimating: 366it [04:27,  1.57it/s]Extractor Estimating: 367it [04:27,  1.59it/s]Extractor Estimating: 368it [04:28,  1.57it/s]Extractor Estimating: 369it [04:29,  1.58it/s]Extractor Estimating: 370it [04:29,  1.62it/s]Extractor Estimating: 371it [04:30,  1.60it/s]Extractor Estimating: 372it [04:31,  1.49it/s]Extractor Estimating: 373it [04:31,  1.50it/s]Extractor Estimating: 374it [04:32,  1.50it/s]Extractor Estimating: 375it [04:33,  1.61it/s]Extractor Estimating: 375it [04:33,  1.37it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1513 mean pseudo reward: 0.951858053246523
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 15288
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15388, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15388, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 36, avg_time 1.302, loss:388.8173
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 8, avg_time 0.956, loss:281.3416
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 44, avg_time 0.968, loss:245.9796
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 16, avg_time 0.945, loss:203.8068
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 52, avg_time 0.969, loss:183.3560
>> valid entity prec:0.4768, rec:0.5129, f1:0.4942
>> valid relation prec:0.1038, rec:0.0678, f1:0.0821
>> valid relation with NER prec:0.1038, rec:0.0678, f1:0.0821
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 24, avg_time 2.214, loss:164.9108
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 60, avg_time 0.980, loss:161.0122
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 32, avg_time 0.967, loss:147.0991
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 4, avg_time 0.977, loss:147.0201
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 40, avg_time 1.053, loss:141.7801
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4574, rec:0.4853, f1:0.4709
>> valid relation prec:0.0874, rec:0.0598, f1:0.0710
>> valid relation with NER prec:0.0874, rec:0.0598, f1:0.0710
g_step 1100, step 12, avg_time 2.214, loss:131.3494
g_step 1200, step 48, avg_time 0.969, loss:125.4552
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:21:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:21:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-21-12_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:21:16 - WARNING - datasets.builder -   Using custom data configuration default-863db69ebbd6390c
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-863db69ebbd6390c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:10, 10.89s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:21:38,774 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:21:38,775 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:21:38,776 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:21:38,777 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:21:39,050 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:21:39,202 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:21:40,335 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:21:43,777 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:21:43,777 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-863db69ebbd6390c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 18:21:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14f6cc753950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:04<00:04,  4.69s/ba]100%|██████████| 2/2 [00:05<00:00,  2.20s/ba]100%|██████████| 2/2 [00:05<00:00,  2.58s/ba]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.94ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.94ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.54ba/s]100%|██████████| 4/4 [00:01<00:00,  4.71ba/s]100%|██████████| 4/4 [00:01<00:00,  3.80ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  2.19ba/s]100%|██████████| 2/2 [00:00<00:00,  3.86ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.30s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.66ba/s]100%|██████████| 4/4 [00:01<00:00,  3.73ba/s]100%|██████████| 4/4 [00:01<00:00,  2.57ba/s]
[INFO|trainer.py:414] 2023-08-28 18:21:59,729 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:22:01,072 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:22:01,239 >>   Num examples = 1513
[INFO|trainer.py:1149] 2023-08-28 18:22:01,240 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:22:01,240 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:22:01,240 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:22:01,240 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:22:01,240 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:05<11:33,  5.83s/it]  2%|▏         | 2/120 [00:06<05:13,  2.66s/it]  2%|▎         | 3/120 [00:06<03:04,  1.58s/it]  3%|▎         | 4/120 [00:06<02:03,  1.07s/it]  4%|▍         | 5/120 [00:07<01:30,  1.27it/s]  5%|▌         | 6/120 [00:07<01:10,  1.62it/s]  6%|▌         | 7/120 [00:07<00:57,  1.96it/s]  7%|▋         | 8/120 [00:08<00:49,  2.27it/s]  8%|▊         | 9/120 [00:08<00:43,  2.54it/s]  8%|▊         | 10/120 [00:08<00:39,  2.77it/s]  9%|▉         | 11/120 [00:08<00:36,  2.95it/s] 10%|█         | 12/120 [00:09<00:38,  2.79it/s] 11%|█         | 13/120 [00:09<00:36,  2.96it/s] 12%|█▏        | 14/120 [00:09<00:34,  3.09it/s] 12%|█▎        | 15/120 [00:10<00:32,  3.19it/s] 13%|█▎        | 16/120 [00:10<00:31,  3.27it/s] 14%|█▍        | 17/120 [00:10<00:31,  3.32it/s] 15%|█▌        | 18/120 [00:11<00:30,  3.36it/s] 16%|█▌        | 19/120 [00:12<01:09,  1.45it/s] 17%|█▋        | 20/120 [00:12<00:59,  1.69it/s] 18%|█▊        | 21/120 [00:13<00:49,  1.99it/s] 18%|█▊        | 22/120 [00:13<00:42,  2.28it/s] 19%|█▉        | 23/120 [00:13<00:38,  2.54it/s] 20%|██        | 24/120 [00:14<00:35,  2.68it/s][INFO|trainer.py:2140] 2023-08-28 18:22:15,420 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:22:15,421 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:22:15,421 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.37it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.71it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.18it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.47it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.17it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.93it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.56it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.92it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.58it/s][A
 12%|█▏        | 53/435 [00:01<00:13, 28.06it/s][A
 13%|█▎        | 57/435 [00:01<00:14, 26.06it/s][A
 14%|█▍        | 61/435 [00:01<00:15, 24.23it/s][A
 15%|█▌        | 66/435 [00:01<00:12, 28.65it/s][A
 16%|█▋        | 71/435 [00:01<00:11, 32.50it/s][A
 17%|█▋        | 76/435 [00:02<00:10, 35.82it/s][A
 19%|█▊        | 81/435 [00:02<00:09, 38.51it/s][A
 20%|█▉        | 86/435 [00:02<00:08, 40.63it/s][A
 21%|██        | 91/435 [00:02<00:08, 42.27it/s][A
 22%|██▏       | 96/435 [00:02<00:07, 43.42it/s][A
 23%|██▎       | 101/435 [00:02<00:07, 43.79it/s][A
 24%|██▍       | 106/435 [00:02<00:07, 44.08it/s][A
 26%|██▌       | 111/435 [00:02<00:07, 44.16it/s][A
 27%|██▋       | 116/435 [00:02<00:07, 44.47it/s][A
 28%|██▊       | 121/435 [00:03<00:07, 44.77it/s][A
 29%|██▉       | 126/435 [00:03<00:06, 45.23it/s][A
 30%|███       | 131/435 [00:03<00:06, 45.59it/s][A
 31%|███▏      | 136/435 [00:03<00:06, 45.72it/s][A
 32%|███▏      | 141/435 [00:03<00:09, 32.37it/s][A
 34%|███▎      | 146/435 [00:03<00:08, 35.54it/s][A
 35%|███▍      | 151/435 [00:03<00:07, 38.17it/s][A
 36%|███▌      | 156/435 [00:03<00:06, 40.20it/s][A
 37%|███▋      | 161/435 [00:04<00:06, 41.83it/s][A
 38%|███▊      | 166/435 [00:04<00:06, 43.04it/s][A
 39%|███▉      | 171/435 [00:04<00:06, 43.93it/s][A
 40%|████      | 176/435 [00:04<00:08, 29.36it/s][A
 42%|████▏     | 181/435 [00:04<00:07, 33.05it/s][A
 43%|████▎     | 185/435 [00:04<00:09, 27.14it/s][A
 44%|████▎     | 190/435 [00:05<00:07, 31.15it/s][A
 45%|████▍     | 195/435 [00:05<00:06, 34.63it/s][A
 46%|████▌     | 200/435 [00:05<00:06, 37.50it/s][A
 47%|████▋     | 205/435 [00:05<00:05, 39.68it/s][A
 48%|████▊     | 210/435 [00:05<00:05, 41.45it/s][A
 49%|████▉     | 215/435 [00:05<00:05, 42.68it/s][A
 51%|█████     | 220/435 [00:05<00:04, 43.66it/s][A
 52%|█████▏    | 225/435 [00:05<00:04, 43.93it/s][A
 53%|█████▎    | 230/435 [00:05<00:04, 44.25it/s][A
 54%|█████▍    | 235/435 [00:06<00:04, 44.65it/s][A
 55%|█████▌    | 240/435 [00:06<00:04, 44.99it/s][A
 56%|█████▋    | 245/435 [00:06<00:04, 45.25it/s][A
 57%|█████▋    | 250/435 [00:06<00:04, 45.49it/s][A
 59%|█████▊    | 255/435 [00:06<00:06, 26.94it/s][A
 60%|█████▉    | 260/435 [00:06<00:05, 30.76it/s][A
 61%|██████    | 265/435 [00:06<00:04, 34.18it/s][A
 62%|██████▏   | 270/435 [00:07<00:04, 37.01it/s][A
 63%|██████▎   | 275/435 [00:07<00:04, 39.32it/s][A
 64%|██████▍   | 280/435 [00:07<00:03, 41.13it/s][A
 66%|██████▌   | 285/435 [00:07<00:03, 42.43it/s][A
 67%|██████▋   | 290/435 [00:07<00:03, 43.43it/s][A
 68%|██████▊   | 295/435 [00:07<00:03, 43.67it/s][A
 69%|██████▉   | 300/435 [00:07<00:04, 27.19it/s][A
 70%|███████   | 305/435 [00:08<00:04, 31.05it/s][A
 71%|███████   | 309/435 [00:08<00:05, 23.02it/s][A
 72%|███████▏  | 314/435 [00:08<00:04, 27.29it/s][A
 73%|███████▎  | 319/435 [00:08<00:03, 31.23it/s][A
 74%|███████▍  | 324/435 [00:08<00:03, 34.58it/s][A
 76%|███████▌  | 329/435 [00:08<00:02, 37.37it/s][A
 77%|███████▋  | 334/435 [00:08<00:02, 39.66it/s][A
 78%|███████▊  | 339/435 [00:08<00:02, 41.38it/s][A
 79%|███████▉  | 344/435 [00:09<00:02, 42.69it/s][A
 80%|████████  | 349/435 [00:09<00:01, 43.24it/s][A
 81%|████████▏ | 354/435 [00:09<00:01, 43.74it/s][A
 83%|████████▎ | 359/435 [00:09<00:01, 44.24it/s][A
 84%|████████▎ | 364/435 [00:09<00:01, 41.65it/s][A
 85%|████████▍ | 369/435 [00:09<00:01, 42.90it/s][A
 86%|████████▌ | 374/435 [00:09<00:01, 43.92it/s][A
 87%|████████▋ | 379/435 [00:09<00:01, 44.51it/s][A
 88%|████████▊ | 384/435 [00:09<00:01, 45.06it/s][A
 89%|████████▉ | 389/435 [00:10<00:01, 45.25it/s][A
 91%|█████████ | 394/435 [00:10<00:00, 45.24it/s][A
 92%|█████████▏| 399/435 [00:10<00:00, 45.08it/s][A
 93%|█████████▎| 404/435 [00:10<00:00, 45.01it/s][A
 94%|█████████▍| 409/435 [00:10<00:00, 45.06it/s][A
 95%|█████████▌| 414/435 [00:10<00:00, 45.19it/s][A
 96%|█████████▋| 419/435 [00:10<00:00, 45.38it/s][A
 97%|█████████▋| 424/435 [00:10<00:00, 45.63it/s][A
 99%|█████████▊| 429/435 [00:10<00:00, 45.79it/s][A
100%|█████████▉| 434/435 [00:11<00:00, 45.89it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:25<00:35,  2.68it/s]
100%|██████████| 435/435 [00:11<00:00, 45.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:22:27,383 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 18:22:27,737 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:22:36,078 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:22:37,767 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:22:37,926 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [00:58<21:21, 13.49s/it] 22%|██▏       | 26/120 [00:58<14:58,  9.56s/it] 22%|██▎       | 27/120 [00:58<10:30,  6.78s/it] 23%|██▎       | 28/120 [00:59<07:24,  4.83s/it] 24%|██▍       | 29/120 [00:59<05:15,  3.47s/it] 25%|██▌       | 30/120 [00:59<03:46,  2.52s/it] 26%|██▌       | 31/120 [01:00<02:44,  1.85s/it] 27%|██▋       | 32/120 [01:00<02:01,  1.38s/it] 28%|██▊       | 33/120 [01:00<01:31,  1.05s/it] 28%|██▊       | 34/120 [01:00<01:10,  1.21it/s] 29%|██▉       | 35/120 [01:01<00:56,  1.51it/s] 30%|███       | 36/120 [01:01<00:48,  1.72it/s] 31%|███       | 37/120 [01:01<00:41,  2.02it/s] 32%|███▏      | 38/120 [01:02<00:35,  2.31it/s] 32%|███▎      | 39/120 [01:02<00:31,  2.56it/s] 33%|███▎      | 40/120 [01:02<00:28,  2.77it/s] 34%|███▍      | 41/120 [01:03<00:26,  2.94it/s] 35%|███▌      | 42/120 [01:03<00:25,  3.07it/s] 36%|███▌      | 43/120 [01:03<00:24,  3.18it/s] 37%|███▋      | 44/120 [01:03<00:23,  3.25it/s] 38%|███▊      | 45/120 [01:04<00:22,  3.31it/s] 38%|███▊      | 46/120 [01:04<00:23,  3.17it/s] 39%|███▉      | 47/120 [01:04<00:22,  3.25it/s] 40%|████      | 48/120 [01:05<00:19,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 18:23:06,353 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:23:06,353 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:23:06,353 >>   Batch size = 8
{'eval_loss': 0.99981290102005, 'eval_runtime': 11.1421, 'eval_samples_per_second': 312.24, 'eval_steps_per_second': 39.041, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.77it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.74it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.05it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.06it/s][A
  6%|▋         | 28/435 [00:00<00:10, 39.84it/s][A
  8%|▊         | 33/435 [00:00<00:09, 41.84it/s][A
  9%|▊         | 38/435 [00:00<00:09, 43.06it/s][A
 10%|▉         | 43/435 [00:00<00:08, 43.90it/s][A
 11%|█         | 48/435 [00:01<00:08, 44.61it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.03it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.39it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.58it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.35it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 45.00it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.85it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 44.99it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.20it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.30it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.59it/s][A
 24%|██▎       | 103/435 [00:02<00:09, 34.88it/s][A
 25%|██▍       | 108/435 [00:02<00:08, 37.62it/s][A
 26%|██▌       | 113/435 [00:02<00:08, 39.74it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 41.38it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 42.49it/s][A
 29%|██▉       | 128/435 [00:02<00:07, 43.35it/s][A
 31%|███       | 133/435 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 44.57it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 44.59it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 44.74it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.06it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.25it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 45.30it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.34it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.28it/s][A
 41%|████      | 178/435 [00:04<00:05, 45.35it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.36it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.13it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.19it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.26it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.36it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 45.53it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.63it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.57it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 45.56it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.43it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.34it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 41.18it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 42.59it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 43.57it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 44.30it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.80it/s][A
 60%|██████    | 263/435 [00:05<00:03, 45.09it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.19it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.17it/s][A
 64%|██████▍   | 278/435 [00:07<00:11, 14.26it/s][A
 65%|██████▌   | 283/435 [00:07<00:08, 18.00it/s][A
 66%|██████▌   | 287/435 [00:07<00:07, 19.32it/s][A
 67%|██████▋   | 291/435 [00:07<00:07, 20.17it/s][A
 68%|██████▊   | 296/435 [00:07<00:05, 24.85it/s][A
 69%|██████▉   | 301/435 [00:07<00:04, 29.10it/s][A
 70%|███████   | 306/435 [00:07<00:03, 32.85it/s][A
 71%|███████▏  | 311/435 [00:07<00:03, 35.97it/s][A
 73%|███████▎  | 316/435 [00:08<00:03, 38.53it/s][A
 74%|███████▍  | 321/435 [00:08<00:02, 40.51it/s][A
 75%|███████▍  | 326/435 [00:08<00:02, 41.95it/s][A
 76%|███████▌  | 331/435 [00:08<00:03, 28.00it/s][A
 77%|███████▋  | 336/435 [00:08<00:03, 31.73it/s][A
 78%|███████▊  | 341/435 [00:08<00:02, 34.96it/s][A
 80%|███████▉  | 346/435 [00:08<00:02, 37.67it/s][A
 81%|████████  | 351/435 [00:09<00:02, 39.78it/s][A
 82%|████████▏ | 356/435 [00:09<00:01, 41.42it/s][A
 83%|████████▎ | 361/435 [00:09<00:01, 42.69it/s][A
 84%|████████▍ | 366/435 [00:09<00:01, 43.51it/s][A
 85%|████████▌ | 371/435 [00:09<00:01, 43.74it/s][A
 86%|████████▋ | 376/435 [00:09<00:01, 43.94it/s][A
 88%|████████▊ | 381/435 [00:09<00:01, 44.30it/s][A
 89%|████████▊ | 386/435 [00:09<00:01, 44.63it/s][A
 90%|████████▉ | 391/435 [00:09<00:00, 44.87it/s][A
 91%|█████████ | 396/435 [00:10<00:00, 45.12it/s][A
 92%|█████████▏| 401/435 [00:10<00:00, 45.37it/s][A
 93%|█████████▎| 406/435 [00:10<00:00, 45.52it/s][A
 94%|█████████▍| 411/435 [00:10<00:00, 45.54it/s][A
 96%|█████████▌| 416/435 [00:10<00:00, 45.29it/s][A
 97%|█████████▋| 421/435 [00:10<00:00, 45.07it/s][A
 98%|█████████▊| 426/435 [00:10<00:00, 45.13it/s][A
 99%|█████████▉| 431/435 [00:10<00:00, 45.15it/s][A                                                
                                                 [A 40%|████      | 48/120 [01:16<00:19,  3.62it/s]
100%|██████████| 435/435 [00:10<00:00, 45.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:23:17,775 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 18:23:18,530 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:23:29,793 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:23:30,102 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:23:30,225 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [02:03<21:06, 17.84s/it] 42%|████▏     | 50/120 [02:04<14:42, 12.60s/it] 42%|████▎     | 51/120 [02:04<10:14,  8.91s/it] 43%|████▎     | 52/120 [02:04<07:09,  6.32s/it] 44%|████▍     | 53/120 [02:05<05:02,  4.51s/it] 45%|████▌     | 54/120 [02:05<03:34,  3.25s/it] 46%|████▌     | 55/120 [02:06<02:44,  2.53s/it] 47%|████▋     | 56/120 [02:06<01:59,  1.86s/it] 48%|████▊     | 57/120 [02:06<01:27,  1.39s/it] 48%|████▊     | 58/120 [02:07<01:08,  1.11s/it] 49%|████▉     | 59/120 [02:07<00:52,  1.16it/s] 50%|█████     | 60/120 [02:07<00:41,  1.44it/s] 51%|█████     | 61/120 [02:08<00:33,  1.75it/s] 52%|█████▏    | 62/120 [02:08<00:28,  2.05it/s] 52%|█████▎    | 63/120 [02:08<00:24,  2.34it/s] 53%|█████▎    | 64/120 [02:09<00:21,  2.59it/s] 54%|█████▍    | 65/120 [02:09<00:19,  2.80it/s] 55%|█████▌    | 66/120 [02:09<00:18,  2.96it/s] 56%|█████▌    | 67/120 [02:09<00:17,  3.10it/s] 57%|█████▋    | 68/120 [02:10<00:22,  2.35it/s] 57%|█████▊    | 69/120 [02:10<00:19,  2.60it/s] 58%|█████▊    | 70/120 [02:11<00:17,  2.80it/s] 59%|█████▉    | 71/120 [02:14<00:52,  1.08s/it] 60%|██████    | 72/120 [02:14<00:41,  1.15it/s][INFO|trainer.py:2140] 2023-08-28 18:24:15,622 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:24:15,622 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:24:15,622 >>   Batch size = 8
{'eval_loss': 1.0054714679718018, 'eval_runtime': 10.9089, 'eval_samples_per_second': 318.914, 'eval_steps_per_second': 39.876, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.80it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.43it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.26it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.59it/s][A
  6%|▋         | 28/435 [00:00<00:08, 47.10it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.78it/s][A
  9%|▊         | 38/435 [00:00<00:08, 46.17it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.53it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.41it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.50it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.73it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.81it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 45.92it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.97it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.97it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.58it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.34it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.22it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.30it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.40it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.52it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.65it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 45.83it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.92it/s][A
 29%|██▉       | 128/435 [00:02<00:10, 29.15it/s][A
 31%|███       | 133/435 [00:03<00:09, 32.80it/s][A
 32%|███▏      | 138/435 [00:03<00:08, 35.93it/s][A
 33%|███▎      | 143/435 [00:03<00:07, 38.46it/s][A
 34%|███▍      | 148/435 [00:03<00:07, 40.42it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 41.98it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 43.18it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 43.88it/s][A
 39%|███▊      | 168/435 [00:03<00:06, 44.02it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.16it/s][A
 41%|████      | 178/435 [00:04<00:05, 44.47it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.26it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.45it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.69it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.83it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 45.67it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.38it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.16it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 45.14it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.20it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.43it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.50it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.67it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.77it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 45.73it/s][A
 59%|█████▉    | 258/435 [00:06<00:05, 29.64it/s][A
 60%|██████    | 263/435 [00:06<00:05, 33.19it/s][A
 62%|██████▏   | 268/435 [00:06<00:04, 36.20it/s][A
 63%|██████▎   | 273/435 [00:06<00:04, 38.60it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 40.51it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 41.95it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 43.06it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 43.73it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 44.00it/s][A
 70%|██████▉   | 303/435 [00:07<00:02, 44.40it/s][A
 71%|███████   | 308/435 [00:07<00:02, 44.78it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 45.03it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.25it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.37it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.46it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.52it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.46it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.27it/s][A
 80%|████████  | 348/435 [00:08<00:01, 45.19it/s][A
 81%|████████  | 353/435 [00:08<00:01, 45.26it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 45.50it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.51it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.59it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.51it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.56it/s][A
 88%|████████▊ | 383/435 [00:09<00:01, 45.27it/s][A
 89%|████████▉ | 388/435 [00:09<00:01, 27.13it/s][A
 90%|█████████ | 393/435 [00:09<00:01, 30.89it/s][A
 91%|█████████▏| 398/435 [00:09<00:01, 34.20it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 37.03it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 39.30it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 41.02it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 42.34it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 43.30it/s][A
 98%|█████████▊| 428/435 [00:10<00:00, 43.65it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 44.20it/s][A                                                
                                                 [A 60%|██████    | 72/120 [02:24<00:41,  1.15it/s]
100%|██████████| 435/435 [00:10<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:24:26,185 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 18:24:26,798 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:24:39,152 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:24:39,417 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:24:39,494 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [03:07<12:57, 16.53s/it] 62%|██████▏   | 74/120 [03:08<09:02, 11.80s/it] 62%|██████▎   | 75/120 [03:08<06:15,  8.35s/it] 63%|██████▎   | 76/120 [03:08<04:20,  5.93s/it] 64%|██████▍   | 77/120 [03:09<03:02,  4.24s/it] 65%|██████▌   | 78/120 [03:09<02:08,  3.05s/it] 66%|██████▌   | 79/120 [03:09<01:31,  2.22s/it] 67%|██████▋   | 80/120 [03:09<01:05,  1.64s/it] 68%|██████▊   | 81/120 [03:10<00:48,  1.24s/it] 68%|██████▊   | 82/120 [03:10<00:36,  1.05it/s] 69%|██████▉   | 83/120 [03:10<00:29,  1.24it/s] 70%|███████   | 84/120 [03:11<00:23,  1.54it/s] 71%|███████   | 85/120 [03:11<00:23,  1.51it/s] 72%|███████▏  | 86/120 [03:13<00:27,  1.24it/s] 72%|███████▎  | 87/120 [03:13<00:23,  1.42it/s] 73%|███████▎  | 88/120 [03:13<00:19,  1.62it/s] 74%|███████▍  | 89/120 [03:14<00:16,  1.93it/s] 75%|███████▌  | 90/120 [03:14<00:13,  2.22it/s] 76%|███████▌  | 91/120 [03:14<00:11,  2.49it/s] 77%|███████▋  | 92/120 [03:15<00:10,  2.71it/s] 78%|███████▊  | 93/120 [03:15<00:09,  2.90it/s] 78%|███████▊  | 94/120 [03:15<00:08,  3.04it/s] 79%|███████▉  | 95/120 [03:16<00:07,  3.15it/s] 80%|████████  | 96/120 [03:16<00:06,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 18:25:17,471 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:25:17,472 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:25:17,472 >>   Batch size = 8
{'eval_loss': 1.0177528858184814, 'eval_runtime': 10.1852, 'eval_samples_per_second': 341.575, 'eval_steps_per_second': 42.709, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.97it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.89it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.01it/s][A
  5%|▌         | 23/435 [00:00<00:13, 31.08it/s][A
  6%|▋         | 28/435 [00:00<00:11, 34.88it/s][A
  8%|▊         | 33/435 [00:00<00:10, 37.87it/s][A
  9%|▊         | 38/435 [00:00<00:09, 40.09it/s][A
 10%|▉         | 43/435 [00:01<00:09, 41.83it/s][A
 11%|█         | 48/435 [00:01<00:08, 43.04it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 43.86it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 44.47it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 44.45it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.43it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.67it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.94it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.15it/s][A
 20%|██        | 88/435 [00:02<00:07, 45.51it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.70it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.74it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.78it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.53it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.28it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 45.24it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.21it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.35it/s][A
 31%|███       | 133/435 [00:03<00:06, 45.40it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 45.66it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.79it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.81it/s][A
 35%|███▌      | 153/435 [00:03<00:08, 33.07it/s][A
 36%|███▋      | 158/435 [00:03<00:07, 36.13it/s][A
 37%|███▋      | 163/435 [00:03<00:07, 38.65it/s][A
 39%|███▊      | 168/435 [00:03<00:06, 40.58it/s][A
 40%|███▉      | 173/435 [00:04<00:06, 42.01it/s][A
 41%|████      | 178/435 [00:04<00:05, 43.16it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 43.99it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 44.57it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 44.50it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 44.46it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.66it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.96it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.23it/s][A
 50%|█████     | 218/435 [00:05<00:04, 45.43it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 45.68it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.85it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.79it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.28it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.18it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.12it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 45.25it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 45.42it/s][A
 60%|██████    | 263/435 [00:06<00:03, 45.59it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.63it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.62it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.63it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.52it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 40.83it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 42.17it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 43.19it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 44.07it/s][A
 71%|███████   | 308/435 [00:07<00:02, 44.72it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 45.03it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.23it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.21it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 44.86it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 44.88it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 44.99it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.19it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.31it/s][A
 81%|████████  | 353/435 [00:08<00:01, 45.54it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 45.75it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.78it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.64it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.20it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.09it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.15it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.24it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 45.36it/s][A
 91%|█████████▏| 398/435 [00:09<00:00, 45.46it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 45.65it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 45.81it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.65it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.34it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 30.46it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 33.91it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 36.84it/s][A                                                
                                                 [A 80%|████████  | 96/120 [03:26<00:06,  3.54it/s]
100%|██████████| 435/435 [00:10<00:00, 36.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:25:28,748 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 18:25:30,165 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:25:40,283 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:25:41,104 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:25:41,266 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [04:15<06:52, 17.95s/it] 82%|████████▏ | 98/120 [04:15<04:38, 12.66s/it] 82%|████████▎ | 99/120 [04:16<03:07,  8.95s/it] 83%|████████▎ | 100/120 [04:16<02:07,  6.35s/it] 84%|████████▍ | 101/120 [04:16<01:26,  4.53s/it] 85%|████████▌ | 102/120 [04:16<00:58,  3.26s/it] 86%|████████▌ | 103/120 [04:17<00:40,  2.37s/it] 87%|████████▋ | 104/120 [04:17<00:27,  1.74s/it] 88%|████████▊ | 105/120 [04:17<00:19,  1.31s/it] 88%|████████▊ | 106/120 [04:18<00:13,  1.00it/s] 89%|████████▉ | 107/120 [04:18<00:10,  1.27it/s] 90%|█████████ | 108/120 [04:18<00:07,  1.57it/s] 91%|█████████ | 109/120 [04:18<00:06,  1.78it/s] 92%|█████████▏| 110/120 [04:19<00:04,  2.09it/s] 92%|█████████▎| 111/120 [04:19<00:03,  2.38it/s] 93%|█████████▎| 112/120 [04:19<00:03,  2.63it/s] 94%|█████████▍| 113/120 [04:20<00:02,  2.84it/s] 95%|█████████▌| 114/120 [04:20<00:01,  3.01it/s] 96%|█████████▌| 115/120 [04:20<00:01,  3.14it/s] 97%|█████████▋| 116/120 [04:20<00:01,  3.24it/s] 98%|█████████▊| 117/120 [04:21<00:00,  3.31it/s] 98%|█████████▊| 118/120 [04:21<00:00,  3.36it/s] 99%|█████████▉| 119/120 [04:21<00:00,  3.40it/s]100%|██████████| 120/120 [04:22<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 18:26:23,329 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:26:23,329 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:26:23,329 >>   Batch size = 8
{'eval_loss': 1.0220082998275757, 'eval_runtime': 10.0565, 'eval_samples_per_second': 345.944, 'eval_steps_per_second': 43.255, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.30it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.12it/s][A
  4%|▍         | 18/435 [00:00<00:08, 48.10it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.28it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.89it/s][A
  8%|▊         | 33/435 [00:00<00:08, 46.41it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.77it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.27it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.33it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.61it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.61it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.82it/s][A
 16%|█▌        | 68/435 [00:01<00:07, 45.91it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.99it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.77it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.46it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.25it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.29it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.41it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.47it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.58it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.70it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 45.86it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.77it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.49it/s][A
 31%|███       | 133/435 [00:02<00:07, 42.61it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 43.55it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 44.19it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 44.73it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.04it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.26it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 45.45it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.51it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.21it/s][A
 41%|████      | 178/435 [00:03<00:05, 45.12it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.30it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.49it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.51it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.67it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.71it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 45.80it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.65it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.35it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.12it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.24it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.39it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.54it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.60it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.73it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 45.76it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 45.67it/s][A
 60%|██████    | 263/435 [00:05<00:03, 45.55it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.47it/s][A
 63%|██████▎   | 273/435 [00:06<00:05, 27.20it/s][A
 64%|██████▍   | 278/435 [00:06<00:05, 30.99it/s][A
 65%|██████▍   | 282/435 [00:06<00:05, 28.87it/s][A
 66%|██████▌   | 287/435 [00:06<00:04, 32.78it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 35.97it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 38.62it/s][A
 69%|██████▉   | 302/435 [00:06<00:03, 40.57it/s][A
 71%|███████   | 307/435 [00:07<00:03, 42.05it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 43.08it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 43.83it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.05it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.10it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.46it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.78it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 45.13it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 45.27it/s][A
 81%|████████  | 352/435 [00:08<00:01, 45.52it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 45.61it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 45.59it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 45.36it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.13it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.07it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.32it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.35it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 45.55it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 45.63it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 45.67it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 45.65it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 32.83it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 35.93it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 38.50it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 40.52it/s][A                                                 
                                                 [A100%|██████████| 120/120 [04:32<00:00,  3.60it/s]
100%|██████████| 435/435 [00:10<00:00, 40.52it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:26:33,655 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 18:26:33,883 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:26:40,266 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:26:40,518 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:26:41,031 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:27:05,784 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:27:05,941 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24 (score: 0.99981290102005).
                                                 100%|██████████| 120/120 [05:36<00:00,  3.60it/s]100%|██████████| 120/120 [05:36<00:00,  2.81s/it]
[INFO|trainer.py:1894] 2023-08-28 18:27:38,279 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 18:27:38,495 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:27:48,717 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:50,123 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:50,343 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:27:53,512 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,623 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,623 >>   train_loss               =      0.555
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,623 >>   train_runtime            = 0:05:36.46
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,624 >>   train_samples            =       1513
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,624 >>   train_samples_per_second =     22.484
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:27:53,624 >>   train_steps_per_second   =      0.357
{'eval_loss': 1.0250831842422485, 'eval_runtime': 10.0283, 'eval_samples_per_second': 346.917, 'eval_steps_per_second': 43.377, 'epoch': 5.0}
{'train_runtime': 336.4649, 'train_samples_per_second': 22.484, 'train_steps_per_second': 0.357, 'train_loss': 0.555035400390625, 'epoch': 5.0}
08/28/2023 18:27:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:27:55,610 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:27:55,610 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 18:27:55,610 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 56.75it/s]  3%|▎         | 12/435 [00:00<00:08, 49.93it/s]  4%|▍         | 18/435 [00:00<00:08, 48.05it/s]  5%|▌         | 23/435 [00:00<00:08, 47.34it/s]  6%|▋         | 28/435 [00:00<00:08, 46.89it/s]  8%|▊         | 33/435 [00:00<00:08, 46.61it/s]  9%|▊         | 38/435 [00:00<00:08, 46.48it/s] 10%|▉         | 43/435 [00:00<00:08, 45.93it/s] 11%|█         | 48/435 [00:01<00:09, 39.10it/s] 12%|█▏        | 53/435 [00:01<00:09, 40.99it/s] 13%|█▎        | 58/435 [00:01<00:08, 42.44it/s] 14%|█▍        | 63/435 [00:01<00:08, 43.45it/s] 16%|█▌        | 68/435 [00:01<00:08, 44.17it/s] 17%|█▋        | 73/435 [00:01<00:08, 44.72it/s] 18%|█▊        | 78/435 [00:01<00:07, 45.09it/s] 19%|█▉        | 83/435 [00:01<00:07, 45.29it/s] 20%|██        | 88/435 [00:01<00:07, 45.00it/s] 21%|██▏       | 93/435 [00:02<00:07, 44.66it/s] 23%|██▎       | 98/435 [00:02<00:07, 44.76it/s] 24%|██▎       | 103/435 [00:02<00:07, 44.87it/s] 25%|██▍       | 108/435 [00:02<00:07, 45.12it/s] 26%|██▌       | 113/435 [00:02<00:07, 45.32it/s] 27%|██▋       | 118/435 [00:02<00:06, 45.59it/s] 28%|██▊       | 123/435 [00:02<00:06, 45.63it/s] 29%|██▉       | 128/435 [00:02<00:06, 45.68it/s] 31%|███       | 133/435 [00:02<00:06, 45.32it/s] 32%|███▏      | 138/435 [00:03<00:06, 45.18it/s] 33%|███▎      | 143/435 [00:03<00:06, 45.06it/s] 34%|███▍      | 148/435 [00:03<00:06, 45.07it/s] 35%|███▌      | 153/435 [00:03<00:06, 45.12it/s] 36%|███▋      | 158/435 [00:03<00:06, 45.33it/s] 37%|███▋      | 163/435 [00:03<00:05, 45.41it/s] 39%|███▊      | 168/435 [00:03<00:05, 45.49it/s] 40%|███▉      | 173/435 [00:03<00:05, 45.60it/s] 41%|████      | 178/435 [00:03<00:05, 45.55it/s] 42%|████▏     | 183/435 [00:04<00:06, 40.58it/s] 43%|████▎     | 188/435 [00:04<00:05, 42.05it/s] 44%|████▍     | 193/435 [00:04<00:05, 42.99it/s] 46%|████▌     | 198/435 [00:04<00:05, 43.85it/s] 47%|████▋     | 203/435 [00:04<00:05, 44.48it/s] 48%|████▊     | 208/435 [00:04<00:05, 44.97it/s] 49%|████▉     | 213/435 [00:04<00:04, 45.29it/s] 50%|█████     | 218/435 [00:04<00:04, 45.44it/s] 51%|█████▏    | 223/435 [00:04<00:04, 45.25it/s] 52%|█████▏    | 228/435 [00:05<00:04, 45.37it/s] 54%|█████▎    | 233/435 [00:05<00:04, 45.47it/s] 55%|█████▍    | 238/435 [00:05<00:04, 45.64it/s] 56%|█████▌    | 243/435 [00:05<00:04, 45.80it/s] 57%|█████▋    | 248/435 [00:05<00:04, 45.76it/s] 58%|█████▊    | 253/435 [00:05<00:03, 45.78it/s] 59%|█████▉    | 258/435 [00:05<00:03, 45.79it/s] 60%|██████    | 263/435 [00:05<00:03, 45.78it/s] 62%|██████▏   | 268/435 [00:05<00:03, 45.65it/s] 63%|██████▎   | 273/435 [00:06<00:03, 45.64it/s] 64%|██████▍   | 278/435 [00:06<00:03, 45.54it/s] 65%|██████▌   | 283/435 [00:06<00:04, 33.02it/s] 66%|██████▌   | 288/435 [00:06<00:04, 36.10it/s] 67%|██████▋   | 293/435 [00:06<00:03, 38.63it/s] 69%|██████▊   | 298/435 [00:06<00:03, 40.55it/s] 70%|██████▉   | 303/435 [00:06<00:03, 42.07it/s] 71%|███████   | 308/435 [00:06<00:02, 43.19it/s] 72%|███████▏  | 313/435 [00:07<00:02, 44.02it/s] 73%|███████▎  | 318/435 [00:07<00:02, 44.61it/s] 74%|███████▍  | 323/435 [00:07<00:02, 44.57it/s] 75%|███████▌  | 328/435 [00:07<00:02, 44.81it/s] 77%|███████▋  | 333/435 [00:07<00:02, 45.09it/s] 78%|███████▊  | 338/435 [00:07<00:02, 45.35it/s] 79%|███████▉  | 343/435 [00:07<00:02, 45.55it/s] 80%|████████  | 348/435 [00:07<00:01, 45.68it/s] 81%|████████  | 353/435 [00:07<00:01, 45.74it/s] 82%|████████▏ | 358/435 [00:08<00:01, 45.88it/s] 83%|████████▎ | 363/435 [00:08<00:01, 45.65it/s] 85%|████████▍ | 368/435 [00:08<00:01, 45.50it/s] 86%|████████▌ | 373/435 [00:08<00:01, 45.33it/s] 87%|████████▋ | 378/435 [00:08<00:01, 45.47it/s] 88%|████████▊ | 383/435 [00:08<00:01, 45.61it/s] 89%|████████▉ | 388/435 [00:08<00:01, 45.70it/s] 90%|█████████ | 393/435 [00:08<00:00, 45.71it/s] 91%|█████████▏| 398/435 [00:08<00:00, 45.77it/s] 93%|█████████▎| 403/435 [00:09<00:00, 45.89it/s] 94%|█████████▍| 408/435 [00:09<00:00, 45.74it/s] 95%|█████████▍| 413/435 [00:09<00:00, 45.57it/s] 96%|█████████▌| 418/435 [00:09<00:00, 30.04it/s] 97%|█████████▋| 423/435 [00:09<00:00, 33.55it/s] 98%|█████████▊| 428/435 [00:09<00:00, 36.56it/s]100%|█████████▉| 433/435 [00:09<00:00, 38.96it/s]100%|██████████| 435/435 [00:09<00:00, 43.75it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:28:05,572 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   eval_loss               =     0.9998
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   eval_runtime            = 0:00:09.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   eval_samples_per_second =    349.215
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,572 >>   eval_steps_per_second   =     43.664
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:28:05,573 >>   perplexity              =     2.7178
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:32,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:32,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:32,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:32,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:32,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:28:34,060 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:28:34,061 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:28:34,819 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:28:36,059 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:28:36,127 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:40,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:40,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:40,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:40,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:28:40,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:28:41,400 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:28:41,401 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:28:42,333 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:28:42,565 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:28:42,565 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-48
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-72
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-24
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.13it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.35it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:08,  1.41it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.42it/s]Extractor Predicting: 14it [00:10,  1.43it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:12,  1.47it/s]Extractor Predicting: 18it [00:12,  1.53it/s]Extractor Predicting: 19it [00:13,  1.51it/s]Extractor Predicting: 20it [00:14,  1.52it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:15,  1.55it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.44it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:21,  1.42it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.46it/s]Extractor Predicting: 33it [00:23,  1.40it/s]Extractor Predicting: 34it [00:23,  1.45it/s]Extractor Predicting: 35it [00:24,  1.48it/s]Extractor Predicting: 36it [00:25,  1.50it/s]Extractor Predicting: 37it [00:25,  1.51it/s]Extractor Predicting: 38it [00:26,  1.40it/s]Extractor Predicting: 39it [00:27,  1.45it/s]Extractor Predicting: 40it [00:27,  1.46it/s]Extractor Predicting: 41it [00:28,  1.49it/s]Extractor Predicting: 42it [00:29,  1.51it/s]Extractor Predicting: 43it [00:29,  1.49it/s]Extractor Predicting: 44it [00:30,  1.52it/s]Extractor Predicting: 45it [00:31,  1.52it/s]Extractor Predicting: 46it [00:31,  1.52it/s]Extractor Predicting: 47it [00:32,  1.52it/s]Extractor Predicting: 48it [00:33,  1.52it/s]Extractor Predicting: 49it [00:33,  1.52it/s]Extractor Predicting: 50it [00:34,  1.53it/s]Extractor Predicting: 51it [00:35,  1.54it/s]Extractor Predicting: 52it [00:35,  1.55it/s]Extractor Predicting: 53it [00:36,  1.47it/s]Extractor Predicting: 54it [00:37,  1.48it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:38,  1.51it/s]Extractor Predicting: 57it [00:39,  1.50it/s]Extractor Predicting: 58it [00:40,  1.35it/s]Extractor Predicting: 59it [00:40,  1.43it/s]Extractor Predicting: 60it [00:41,  1.49it/s]Extractor Predicting: 61it [00:41,  1.54it/s]Extractor Predicting: 62it [00:42,  1.54it/s]Extractor Predicting: 63it [00:43,  1.48it/s]Extractor Predicting: 64it [00:43,  1.52it/s]Extractor Predicting: 65it [00:44,  1.54it/s]Extractor Predicting: 66it [00:45,  1.54it/s]Extractor Predicting: 67it [00:45,  1.56it/s]Extractor Predicting: 68it [00:46,  1.57it/s]Extractor Predicting: 69it [00:47,  1.43it/s]Extractor Predicting: 70it [00:47,  1.48it/s]Extractor Predicting: 71it [00:48,  1.53it/s]Extractor Predicting: 72it [00:49,  1.52it/s]Extractor Predicting: 73it [00:49,  1.55it/s]Extractor Predicting: 74it [00:50,  1.47it/s]Extractor Predicting: 75it [00:51,  1.47it/s]Extractor Predicting: 76it [00:51,  1.48it/s]Extractor Predicting: 77it [00:52,  1.53it/s]Extractor Predicting: 78it [00:53,  1.56it/s]Extractor Predicting: 79it [00:53,  1.49it/s]Extractor Predicting: 80it [00:54,  1.51it/s]Extractor Predicting: 81it [00:55,  1.52it/s]Extractor Predicting: 82it [00:55,  1.53it/s]Extractor Predicting: 83it [00:56,  1.55it/s]Extractor Predicting: 84it [00:57,  1.55it/s]Extractor Predicting: 85it [00:57,  1.58it/s]Extractor Predicting: 86it [00:58,  1.60it/s]Extractor Predicting: 87it [00:59,  1.49it/s]Extractor Predicting: 88it [00:59,  1.51it/s]Extractor Predicting: 89it [01:00,  1.30it/s]Extractor Predicting: 90it [01:01,  1.39it/s]Extractor Predicting: 91it [01:01,  1.48it/s]Extractor Predicting: 92it [01:02,  1.56it/s]Extractor Predicting: 93it [01:03,  1.56it/s]Extractor Predicting: 94it [01:03,  1.43it/s]Extractor Predicting: 95it [01:04,  1.49it/s]Extractor Predicting: 96it [01:05,  1.53it/s]Extractor Predicting: 97it [01:05,  1.56it/s]Extractor Predicting: 98it [01:06,  1.57it/s]Extractor Predicting: 99it [01:07,  1.29it/s]Extractor Predicting: 100it [01:08,  1.34it/s]Extractor Predicting: 101it [01:08,  1.40it/s]Extractor Predicting: 102it [01:09,  1.52it/s]Extractor Predicting: 103it [01:09,  1.57it/s]Extractor Predicting: 104it [01:10,  1.35it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.48it/s]Extractor Predicting: 107it [01:12,  1.51it/s]Extractor Predicting: 108it [01:13,  1.54it/s]Extractor Predicting: 109it [01:14,  1.43it/s]Extractor Predicting: 110it [01:14,  1.47it/s]Extractor Predicting: 111it [01:15,  1.52it/s]Extractor Predicting: 112it [01:16,  1.56it/s]Extractor Predicting: 113it [01:16,  1.63it/s]Extractor Predicting: 114it [01:17,  1.64it/s]Extractor Predicting: 115it [01:18,  1.39it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.47it/s]Extractor Predicting: 118it [01:20,  1.52it/s]Extractor Predicting: 119it [01:20,  1.51it/s]Extractor Predicting: 120it [01:21,  1.43it/s]Extractor Predicting: 121it [01:22,  1.47it/s]Extractor Predicting: 122it [01:22,  1.48it/s]Extractor Predicting: 123it [01:23,  1.52it/s]Extractor Predicting: 124it [01:24,  1.55it/s]Extractor Predicting: 125it [01:24,  1.52it/s]Extractor Predicting: 126it [01:25,  1.52it/s]Extractor Predicting: 127it [01:25,  1.57it/s]Extractor Predicting: 128it [01:26,  1.58it/s]Extractor Predicting: 129it [01:27,  1.56it/s]Extractor Predicting: 130it [01:27,  1.48it/s]Extractor Predicting: 131it [01:28,  1.53it/s]Extractor Predicting: 132it [01:29,  1.54it/s]Extractor Predicting: 133it [01:29,  1.53it/s]Extractor Predicting: 134it [01:30,  1.52it/s]Extractor Predicting: 135it [01:31,  1.52it/s]Extractor Predicting: 136it [01:31,  1.52it/s]Extractor Predicting: 137it [01:32,  1.54it/s]Extractor Predicting: 138it [01:33,  1.51it/s]Extractor Predicting: 139it [01:33,  1.53it/s]Extractor Predicting: 140it [01:34,  1.40it/s]Extractor Predicting: 141it [01:35,  1.45it/s]Extractor Predicting: 142it [01:35,  1.47it/s]Extractor Predicting: 143it [01:36,  1.50it/s]Extractor Predicting: 144it [01:36,  1.87it/s]Extractor Predicting: 144it [01:37,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:41,531 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:42,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:42,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:42,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:42,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:30:42,539 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:30:42,540 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:30:43,379 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:30:44,397 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:30:44,397 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:46,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:47,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:47,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:47,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:30:47,031 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:30:47,414 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:30:47,415 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:30:47,733 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:30:47,904 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:30:47,904 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19493844049247605,
  "recall": 0.08192009198045415,
  "score": 0.1153612629022465,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.01it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:05,  1.49it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:06,  1.57it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.62it/s]Extractor Predicting: 12it [00:08,  1.60it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:10,  1.60it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:11,  1.59it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.61it/s]Extractor Predicting: 23it [00:15,  1.34it/s]Extractor Predicting: 24it [00:16,  1.41it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.54it/s]Extractor Predicting: 28it [00:19,  1.27it/s]Extractor Predicting: 29it [00:19,  1.37it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:21,  1.48it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:22,  1.37it/s]Extractor Predicting: 34it [00:23,  1.42it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.53it/s]Extractor Predicting: 37it [00:24,  1.58it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.56it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.58it/s]Extractor Predicting: 42it [00:28,  1.64it/s]Extractor Predicting: 43it [00:28,  1.59it/s]Extractor Predicting: 44it [00:29,  1.59it/s]Extractor Predicting: 45it [00:30,  1.60it/s]Extractor Predicting: 46it [00:30,  1.58it/s]Extractor Predicting: 47it [00:31,  1.62it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.62it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:35,  1.56it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:37,  1.57it/s]Extractor Predicting: 57it [00:37,  1.56it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.36it/s]Extractor Predicting: 60it [00:39,  1.41it/s]Extractor Predicting: 61it [00:40,  1.45it/s]Extractor Predicting: 62it [00:41,  1.51it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.53it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.52it/s]Extractor Predicting: 71it [00:47,  1.54it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:48,  1.57it/s]Extractor Predicting: 74it [00:49,  1.44it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.57it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:52,  1.57it/s]Extractor Predicting: 80it [00:52,  1.60it/s]Extractor Predicting: 81it [00:53,  1.62it/s]Extractor Predicting: 82it [00:54,  1.62it/s]Extractor Predicting: 83it [00:54,  1.60it/s]Extractor Predicting: 84it [00:55,  1.57it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:58,  1.54it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:00,  1.53it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.40it/s]Extractor Predicting: 95it [01:02,  1.45it/s]Extractor Predicting: 96it [01:03,  1.40it/s]Extractor Predicting: 97it [01:04,  1.42it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:05,  1.48it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:06,  1.51it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:08,  1.53it/s]Extractor Predicting: 105it [01:09,  1.52it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:12,  1.51it/s]Extractor Predicting: 110it [01:12,  1.52it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.50it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:15,  1.54it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.41it/s]Extractor Predicting: 117it [01:17,  1.44it/s]Extractor Predicting: 118it [01:18,  1.47it/s]Extractor Predicting: 119it [01:18,  1.49it/s]Extractor Predicting: 120it [01:19,  1.53it/s]Extractor Predicting: 121it [01:20,  1.45it/s]Extractor Predicting: 122it [01:20,  1.50it/s]Extractor Predicting: 123it [01:21,  1.51it/s]Extractor Predicting: 124it [01:22,  1.55it/s]Extractor Predicting: 125it [01:22,  1.59it/s]Extractor Predicting: 126it [01:23,  1.49it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:24,  1.57it/s]Extractor Predicting: 129it [01:25,  1.59it/s]Extractor Predicting: 130it [01:26,  1.57it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:28,  1.57it/s]Extractor Predicting: 134it [01:28,  1.55it/s]Extractor Predicting: 135it [01:29,  1.60it/s]Extractor Predicting: 136it [01:29,  1.62it/s]Extractor Predicting: 137it [01:30,  1.65it/s]Extractor Predicting: 138it [01:31,  1.64it/s]Extractor Predicting: 139it [01:31,  1.63it/s]Extractor Predicting: 140it [01:32,  1.62it/s]Extractor Predicting: 141it [01:33,  1.60it/s]Extractor Predicting: 142it [01:33,  1.59it/s]Extractor Predicting: 143it [01:34,  1.62it/s]Extractor Predicting: 144it [01:34,  1.60it/s]Extractor Predicting: 145it [01:35,  1.57it/s]Extractor Predicting: 146it [01:36,  1.61it/s]Extractor Predicting: 147it [01:36,  1.64it/s]Extractor Predicting: 148it [01:37,  1.68it/s]Extractor Predicting: 149it [01:37,  1.68it/s]Extractor Predicting: 150it [01:38,  1.70it/s]Extractor Predicting: 151it [01:39,  1.60it/s]Extractor Predicting: 152it [01:39,  1.65it/s]Extractor Predicting: 153it [01:40,  1.65it/s]Extractor Predicting: 154it [01:40,  1.67it/s]Extractor Predicting: 155it [01:41,  1.73it/s]Extractor Predicting: 156it [01:41,  1.72it/s]Extractor Predicting: 157it [01:42,  1.75it/s]Extractor Predicting: 158it [01:43,  1.78it/s]Extractor Predicting: 159it [01:43,  1.76it/s]Extractor Predicting: 160it [01:44,  1.71it/s]Extractor Predicting: 161it [01:44,  1.71it/s]Extractor Predicting: 162it [01:45,  1.74it/s]Extractor Predicting: 163it [01:46,  1.74it/s]Extractor Predicting: 164it [01:46,  1.75it/s]Extractor Predicting: 165it [01:47,  1.76it/s]Extractor Predicting: 166it [01:47,  1.73it/s]Extractor Predicting: 167it [01:48,  1.76it/s]Extractor Predicting: 168it [01:48,  1.74it/s]Extractor Predicting: 169it [01:49,  1.74it/s]Extractor Predicting: 170it [01:50,  1.74it/s]Extractor Predicting: 171it [01:50,  1.75it/s]Extractor Predicting: 172it [01:51,  1.69it/s]Extractor Predicting: 173it [01:51,  1.67it/s]Extractor Predicting: 174it [01:52,  1.63it/s]Extractor Predicting: 175it [01:53,  1.31it/s]Extractor Predicting: 176it [01:54,  1.38it/s]Extractor Predicting: 177it [01:54,  1.44it/s]Extractor Predicting: 178it [01:55,  1.47it/s]Extractor Predicting: 179it [01:56,  1.28it/s]Extractor Predicting: 180it [01:57,  1.36it/s]Extractor Predicting: 181it [01:57,  1.42it/s]Extractor Predicting: 182it [01:58,  1.45it/s]Extractor Predicting: 183it [01:59,  1.49it/s]Extractor Predicting: 184it [02:00,  1.22it/s]Extractor Predicting: 185it [02:00,  1.29it/s]Extractor Predicting: 186it [02:01,  1.35it/s]Extractor Predicting: 187it [02:02,  1.41it/s]Extractor Predicting: 188it [02:03,  1.30it/s]Extractor Predicting: 189it [02:03,  1.36it/s]Extractor Predicting: 190it [02:04,  1.42it/s]Extractor Predicting: 191it [02:05,  1.43it/s]Extractor Predicting: 192it [02:05,  1.44it/s]Extractor Predicting: 193it [02:06,  1.43it/s]Extractor Predicting: 194it [02:07,  1.45it/s]Extractor Predicting: 195it [02:07,  1.48it/s]Extractor Predicting: 196it [02:08,  1.51it/s]Extractor Predicting: 197it [02:09,  1.51it/s]Extractor Predicting: 198it [02:09,  1.45it/s]Extractor Predicting: 199it [02:10,  1.50it/s]Extractor Predicting: 200it [02:11,  1.49it/s]Extractor Predicting: 201it [02:11,  1.49it/s]Extractor Predicting: 202it [02:12,  1.42it/s]Extractor Predicting: 203it [02:13,  1.44it/s]Extractor Predicting: 204it [02:13,  1.49it/s]Extractor Predicting: 205it [02:14,  1.50it/s]Extractor Predicting: 206it [02:15,  1.51it/s]Extractor Predicting: 207it [02:15,  1.51it/s]Extractor Predicting: 208it [02:16,  1.39it/s]Extractor Predicting: 209it [02:17,  1.39it/s]Extractor Predicting: 210it [02:18,  1.43it/s]Extractor Predicting: 211it [02:18,  1.45it/s]Extractor Predicting: 212it [02:19,  1.46it/s]Extractor Predicting: 213it [02:20,  1.32it/s]Extractor Predicting: 214it [02:21,  1.36it/s]Extractor Predicting: 215it [02:21,  1.43it/s]Extractor Predicting: 216it [02:22,  1.43it/s]Extractor Predicting: 217it [02:22,  1.46it/s]Extractor Predicting: 218it [02:23,  1.42it/s]Extractor Predicting: 219it [02:24,  1.43it/s]Extractor Predicting: 220it [02:25,  1.47it/s]Extractor Predicting: 221it [02:25,  1.45it/s]Extractor Predicting: 222it [02:26,  1.46it/s]Extractor Predicting: 223it [02:27,  1.44it/s]Extractor Predicting: 224it [02:27,  1.47it/s]Extractor Predicting: 225it [02:28,  1.49it/s]Extractor Predicting: 226it [02:29,  1.49it/s]Extractor Predicting: 227it [02:29,  1.55it/s]Extractor Predicting: 228it [02:30,  1.32it/s]Extractor Predicting: 229it [02:31,  1.38it/s]Extractor Predicting: 230it [02:31,  1.47it/s]Extractor Predicting: 231it [02:32,  1.51it/s]Extractor Predicting: 232it [02:33,  1.57it/s]Extractor Predicting: 233it [02:33,  1.48it/s]Extractor Predicting: 234it [02:34,  1.52it/s]Extractor Predicting: 235it [02:35,  1.53it/s]Extractor Predicting: 236it [02:35,  1.53it/s]Extractor Predicting: 237it [02:36,  1.52it/s]Extractor Predicting: 238it [02:37,  1.42it/s]Extractor Predicting: 239it [02:37,  1.44it/s]Extractor Predicting: 240it [02:38,  1.48it/s]Extractor Predicting: 241it [02:39,  1.45it/s]Extractor Predicting: 242it [02:39,  1.50it/s]Extractor Predicting: 243it [02:40,  1.44it/s]Extractor Predicting: 244it [02:41,  1.49it/s]Extractor Predicting: 245it [02:41,  1.53it/s]Extractor Predicting: 246it [02:42,  1.53it/s]Extractor Predicting: 247it [02:43,  1.54it/s]Extractor Predicting: 248it [02:44,  1.46it/s]Extractor Predicting: 249it [02:44,  1.48it/s]Extractor Predicting: 250it [02:45,  1.52it/s]Extractor Predicting: 251it [02:45,  1.54it/s]Extractor Predicting: 252it [02:46,  1.55it/s]Extractor Predicting: 253it [02:47,  1.47it/s]Extractor Predicting: 254it [02:47,  1.51it/s]Extractor Predicting: 255it [02:48,  1.48it/s]Extractor Predicting: 256it [02:49,  1.45it/s]Extractor Predicting: 257it [02:50,  1.47it/s]Extractor Predicting: 258it [02:50,  1.44it/s]Extractor Predicting: 259it [02:51,  1.48it/s]Extractor Predicting: 260it [02:52,  1.47it/s]Extractor Predicting: 261it [02:52,  1.52it/s]Extractor Predicting: 262it [02:53,  1.50it/s]Extractor Predicting: 263it [02:54,  1.45it/s]Extractor Predicting: 264it [02:54,  1.46it/s]Extractor Predicting: 265it [02:55,  1.42it/s]Extractor Predicting: 266it [02:56,  1.44it/s]Extractor Predicting: 267it [02:56,  1.45it/s]Extractor Predicting: 268it [02:57,  1.35it/s]Extractor Predicting: 269it [02:58,  1.40it/s]Extractor Predicting: 270it [02:59,  1.44it/s]Extractor Predicting: 271it [02:59,  1.43it/s]Extractor Predicting: 272it [03:00,  1.46it/s]Extractor Predicting: 273it [03:01,  1.46it/s]Extractor Predicting: 274it [03:01,  1.50it/s]Extractor Predicting: 275it [03:02,  1.52it/s]Extractor Predicting: 276it [03:02,  1.57it/s]Extractor Predicting: 277it [03:03,  1.56it/s]Extractor Predicting: 278it [03:04,  1.39it/s]Extractor Predicting: 279it [03:05,  1.43it/s]Extractor Predicting: 280it [03:05,  1.43it/s]Extractor Predicting: 281it [03:06,  1.45it/s]Extractor Predicting: 282it [03:07,  1.56it/s]Extractor Predicting: 282it [03:07,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:29,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:30,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:30,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:30,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:30,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:34:31,821 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:34:31,823 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:34:32,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:34:34,081 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:34:34,081 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:38,100 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:38,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:38,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:38,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:34:38,102 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:34:39,497 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:34:39,533 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:34:40,461 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:34:41,001 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:34:41,001 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22327529204320035,
  "recall": 0.14985207100591716,
  "score": 0.17933964769407806,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 5it [00:03,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:34:50,255 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:50,256 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:34:50,352 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:50,353 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:34:50,387 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:35:34,631 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:35:34,677 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:35:35,825 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:35:35,826 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:35:36,212 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,496 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,496 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,496 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,496 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,497 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:35:36,497 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.296875,
  "recall": 0.07916666666666666,
  "score": 0.125,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:35:37,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:38,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:39,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:40,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:41,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:41,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:42,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:43,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:43,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:44,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:44,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:45,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:45,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:46,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:47,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:47,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:48,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:48,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:49,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:49,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:50,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:51,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:51,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:52,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:32, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:35:52,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:53,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:53,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:54,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:55,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:56,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:57,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:58,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:58,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:59,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:35:59,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:00,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:01,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:01,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:02,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:03,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:03,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:04,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:05,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:06,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:06,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:14, 15.00s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:08,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:08,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:09,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:09,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:10,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:10,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:11,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:12,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:12,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:13,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:13,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:14,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:14,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:15,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:16,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:16,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:17,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:18,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:18,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:20,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:21,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:21,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:22,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:05, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:23,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:24,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:24,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:25,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:26,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:26,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:27,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:27,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:28,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:29,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:29,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:30,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:30,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:31,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:32,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:32,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:33,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:34,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:34,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:35,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:36,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:36,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:37,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:45, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:37,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:38,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:39,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:39,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:40,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:41,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:41,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:42,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:43,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:43,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:44,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:44,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:45,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:46,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:46,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:47,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:48,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:48,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:49,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:49,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:50,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:13<02:23, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-28 18:36:51,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:51,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:52,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:53,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:53,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:54,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:54,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:55,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:55,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:56,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:57,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:57,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:58,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:59,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:36:59,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:00,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:01,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:01,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:02,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:03,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:03,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:06, 14.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:04,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:05,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:05,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:06,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:06,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:07,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:07,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:08,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:09,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:10,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:11,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:12,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:12,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:13,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:13,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:14,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:15,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:15,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:16,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:16,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:17,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:18,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:41<01:53, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:19,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:19,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:20,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:20,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:21,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:22,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:23,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:23,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:24,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:25,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:25,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:26,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:26,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:27,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:28,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:28,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:29,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:29,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:30,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:31,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:32,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:55<01:38, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:32,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:33,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:34,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:34,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:35,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:35,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:36,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:37,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:38,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:38,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:39,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:40,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:40,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:41,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:41,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:42,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:43,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:43,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:44,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:45,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:46,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:09<01:24, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:46,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:47,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:48,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:48,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:49,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:49,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:51,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:51,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:52,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:53,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:53,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:54,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:55,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:55,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:56,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:56,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:57,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:58,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:37:59,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:22<01:08, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-28 18:37:59,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:00,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:00,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:01,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:02,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:03,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:03,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:04,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:05,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:05,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:06,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:06,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:07,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:08,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:08,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:09,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:10,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:11,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:11,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:12,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:12,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:36<00:55, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:13,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:14,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:14,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:15,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:16,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:16,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:17,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:17,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:18,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:19,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:20,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:20,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:21,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:21,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:22,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:22,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:23,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:24,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:24,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:25,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:26,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:49<00:40, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:26,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:27,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:27,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:28,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:29,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:29,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:30,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:30,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:31,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:32,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:32,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:33,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:34,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:34,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:35,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:35,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:36,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:36,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:37,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:38,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:38,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:39,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:02<00:27, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:40,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:40,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:41,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:42,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:42,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:43,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:43,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:44,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:45,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:45,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:46,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:47,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:48,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:49,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:50,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:51,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:52,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:52,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:53,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:54,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:17<00:13, 13.84s/it][WARNING|generation_utils.py:914] 2023-08-28 18:38:54,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:38:55,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:02,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:04,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:04,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:05,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:06,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:07,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:08,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:09,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:10,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:11,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:12,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:12,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:13,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:14,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:15,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:15,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:39:16,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:39<00:00, 16.46s/it]Generating: 100%|██████████| 15/15 [03:39<00:00, 14.65s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:42,281 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:42,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:42,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:42,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:42,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:39:43,975 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:39:43,976 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:39:44,915 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:39:46,321 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:39:46,321 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:50,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:50,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:50,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:50,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:39:50,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:39:51,271 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:39:51,272 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:39:52,539 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:39:53,075 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:39:53,075 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : On 31 March 2014 , the band announced that they would be releasing their fourth studio album , " I Am The One " , on iTunes in May . Head Entity : I Am The One , Tail Entity : United States .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 247, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 297, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 472, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 519, 'raw': 704}
{'target': 600, 'success': 546, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : country .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : Following his work with the band The Last Man on Earth , he wrote other music at different times , including " Amadeus " ( 1987 ) and " The Last Man on Earth " ( 1989 ) . Head Entity : Amadeus , Tail Entity : bands .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : genre .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.845108695652174, 'errors': {'', "('Parliamentary Labor Party', 'member of', '', 'It is a British political party , the Parliamentary Labor Party , which is elected on the basis of the House of Commons .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.875, 'errors': {''}}
['Relation : platform . Context : The game is a single - player title , developed by Rare Software and released on November 2011 in Japan . Head Entity : Rare Software , Tail Entity : Sony PlayStation .\n']
['Relation : platform . Context : The game is a single - player title , developed by Rare Software and released on November 2011 in Japan . Head Entity : Rare Software , Tail Entity : Sony PlayStation .\n', 'Relation : platform . Context : The first two of the three game titles , " Fates of Being Human " and " Fates of Being Human II " , both from the same publisher , were released on October 8 , 2008 . Head Entity : Fates of Being Human , Tail Entity : Windows .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : platform .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : publisher . Context : The book was nominated for the 1939 Academy Award for Best Picture and received a nomination at the 1939 Academy Awards for Best Foreign Language Film . Head Entity : 1939 Academy Award for Best Picture , Tail Entity : Academy Award for Best Foreign Language Film .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8650568181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : spouse . Context : Later in life , he married his second wife , a young princess of the family at the age of eleven , Princesses Aimee , Princesses Elizabeth , Elizabeth 's second wife , and Princesses William , Margaret , Catherine , and Mary . Head Entity : Princesses Elizabeth , Princesses Elizabeth , Princesses William , Margaret , Catherine , Mary , and Mary , Tail Entity : Princesses Charles .\n"]
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 10340
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10440, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.58it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:04,  1.39it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.55it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:07,  1.56it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:10,  1.42it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.40it/s]Extractor Estimating: 22it [00:14,  1.53it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.65it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:16,  1.70it/s]Extractor Estimating: 27it [00:17,  1.72it/s]Extractor Estimating: 28it [00:17,  1.65it/s]Extractor Estimating: 29it [00:18,  1.55it/s]Extractor Estimating: 30it [00:19,  1.52it/s]Extractor Estimating: 31it [00:19,  1.42it/s]Extractor Estimating: 32it [00:20,  1.49it/s]Extractor Estimating: 33it [00:21,  1.58it/s]Extractor Estimating: 34it [00:21,  1.42it/s]Extractor Estimating: 35it [00:22,  1.50it/s]Extractor Estimating: 36it [00:23,  1.56it/s]Extractor Estimating: 37it [00:23,  1.63it/s]Extractor Estimating: 38it [00:24,  1.67it/s]Extractor Estimating: 39it [00:25,  1.54it/s]Extractor Estimating: 40it [00:25,  1.54it/s]Extractor Estimating: 41it [00:26,  1.56it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:27,  1.59it/s]Extractor Estimating: 44it [00:28,  1.38it/s]Extractor Estimating: 45it [00:29,  1.49it/s]Extractor Estimating: 46it [00:29,  1.50it/s]Extractor Estimating: 47it [00:30,  1.53it/s]Extractor Estimating: 48it [00:30,  1.55it/s]Extractor Estimating: 49it [00:32,  1.26it/s]Extractor Estimating: 50it [00:32,  1.36it/s]Extractor Estimating: 51it [00:33,  1.44it/s]Extractor Estimating: 52it [00:33,  1.47it/s]Extractor Estimating: 53it [00:34,  1.53it/s]Extractor Estimating: 54it [00:35,  1.51it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:36,  1.57it/s]Extractor Estimating: 57it [00:37,  1.61it/s]Extractor Estimating: 58it [00:37,  1.66it/s]Extractor Estimating: 59it [00:38,  1.60it/s]Extractor Estimating: 60it [00:38,  1.62it/s]Extractor Estimating: 61it [00:39,  1.65it/s]Extractor Estimating: 62it [00:39,  1.71it/s]Extractor Estimating: 63it [00:40,  1.69it/s]Extractor Estimating: 64it [00:41,  1.40it/s]Extractor Estimating: 65it [00:42,  1.46it/s]Extractor Estimating: 66it [00:42,  1.51it/s]Extractor Estimating: 67it [00:43,  1.53it/s]Extractor Estimating: 68it [00:44,  1.57it/s]Extractor Estimating: 69it [00:44,  1.57it/s]Extractor Estimating: 70it [00:45,  1.61it/s]Extractor Estimating: 71it [00:45,  1.66it/s]Extractor Estimating: 72it [00:46,  1.61it/s]Extractor Estimating: 73it [00:47,  1.60it/s]Extractor Estimating: 74it [00:47,  1.59it/s]Extractor Estimating: 75it [00:48,  1.61it/s]Extractor Estimating: 76it [00:48,  1.61it/s]Extractor Estimating: 77it [00:49,  1.64it/s]Extractor Estimating: 78it [00:50,  1.67it/s]Extractor Estimating: 79it [00:50,  1.66it/s]Extractor Estimating: 80it [00:51,  1.58it/s]Extractor Estimating: 81it [00:52,  1.59it/s]Extractor Estimating: 82it [00:52,  1.68it/s]Extractor Estimating: 83it [00:53,  1.68it/s]Extractor Estimating: 84it [00:53,  1.65it/s]Extractor Estimating: 85it [00:54,  1.67it/s]Extractor Estimating: 86it [00:54,  1.71it/s]Extractor Estimating: 87it [00:55,  1.68it/s]Extractor Estimating: 88it [00:56,  1.72it/s]Extractor Estimating: 89it [00:56,  1.67it/s]Extractor Estimating: 90it [00:57,  1.59it/s]Extractor Estimating: 91it [00:58,  1.63it/s]Extractor Estimating: 92it [00:58,  1.48it/s]Extractor Estimating: 93it [00:59,  1.50it/s]Extractor Estimating: 94it [01:00,  1.57it/s]Extractor Estimating: 95it [01:00,  1.59it/s]Extractor Estimating: 96it [01:01,  1.61it/s]Extractor Estimating: 97it [01:02,  1.47it/s]Extractor Estimating: 98it [01:02,  1.52it/s]Extractor Estimating: 99it [01:03,  1.57it/s]Extractor Estimating: 100it [01:03,  1.61it/s]Extractor Estimating: 101it [01:04,  1.59it/s]Extractor Estimating: 102it [01:05,  1.37it/s]Extractor Estimating: 103it [01:05,  1.50it/s]Extractor Estimating: 104it [01:06,  1.55it/s]Extractor Estimating: 105it [01:07,  1.58it/s]Extractor Estimating: 106it [01:07,  1.63it/s]Extractor Estimating: 107it [01:08,  1.52it/s]Extractor Estimating: 108it [01:09,  1.57it/s]Extractor Estimating: 109it [01:09,  1.56it/s]Extractor Estimating: 110it [01:10,  1.56it/s]Extractor Estimating: 111it [01:10,  1.60it/s]Extractor Estimating: 112it [01:11,  1.40it/s]Extractor Estimating: 113it [01:12,  1.48it/s]Extractor Estimating: 114it [01:13,  1.53it/s]Extractor Estimating: 115it [01:13,  1.54it/s]Extractor Estimating: 116it [01:14,  1.54it/s]Extractor Estimating: 117it [01:15,  1.46it/s]Extractor Estimating: 118it [01:15,  1.52it/s]Extractor Estimating: 119it [01:16,  1.53it/s]Extractor Estimating: 120it [01:17,  1.25it/s]Extractor Estimating: 121it [01:18,  1.33it/s]Extractor Estimating: 122it [01:18,  1.39it/s]Extractor Estimating: 123it [01:19,  1.49it/s]Extractor Estimating: 124it [01:20,  1.48it/s]Extractor Estimating: 125it [01:20,  1.58it/s]Extractor Estimating: 126it [01:21,  1.68it/s]Extractor Estimating: 127it [01:21,  1.74it/s]Extractor Estimating: 128it [01:22,  1.77it/s]Extractor Estimating: 129it [01:22,  1.83it/s]Extractor Estimating: 130it [01:23,  1.78it/s]Extractor Estimating: 131it [01:23,  1.80it/s]Extractor Estimating: 132it [01:24,  1.85it/s]Extractor Estimating: 133it [01:24,  1.87it/s]Extractor Estimating: 134it [01:25,  1.86it/s]Extractor Estimating: 135it [01:25,  1.95it/s]Extractor Estimating: 136it [01:26,  1.73it/s]Extractor Estimating: 137it [01:27,  1.77it/s]Extractor Estimating: 138it [01:27,  1.83it/s]Extractor Estimating: 139it [01:28,  1.82it/s]Extractor Estimating: 140it [01:28,  1.88it/s]Extractor Estimating: 141it [01:29,  1.83it/s]Extractor Estimating: 142it [01:30,  1.53it/s]Extractor Estimating: 143it [01:30,  1.66it/s]Extractor Estimating: 144it [01:31,  1.68it/s]Extractor Estimating: 145it [01:31,  1.73it/s]Extractor Estimating: 146it [01:32,  1.79it/s]Extractor Estimating: 147it [01:32,  1.64it/s]Extractor Estimating: 148it [01:33,  1.72it/s]Extractor Estimating: 149it [01:34,  1.74it/s]Extractor Estimating: 150it [01:34,  1.81it/s]Extractor Estimating: 151it [01:35,  1.79it/s]Extractor Estimating: 152it [01:35,  1.77it/s]Extractor Estimating: 153it [01:36,  1.63it/s]Extractor Estimating: 154it [01:36,  1.69it/s]Extractor Estimating: 155it [01:37,  1.68it/s]Extractor Estimating: 156it [01:38,  1.70it/s]Extractor Estimating: 157it [01:38,  1.69it/s]Extractor Estimating: 158it [01:39,  1.42it/s]Extractor Estimating: 159it [01:40,  1.49it/s]Extractor Estimating: 160it [01:40,  1.52it/s]Extractor Estimating: 161it [01:41,  1.56it/s]Extractor Estimating: 162it [01:42,  1.61it/s]Extractor Estimating: 163it [01:42,  1.56it/s]Extractor Estimating: 164it [01:43,  1.58it/s]Extractor Estimating: 165it [01:43,  1.63it/s]Extractor Estimating: 166it [01:44,  1.67it/s]Extractor Estimating: 167it [01:45,  1.68it/s]Extractor Estimating: 168it [01:45,  1.65it/s]Extractor Estimating: 169it [01:46,  1.72it/s]Extractor Estimating: 170it [01:46,  1.73it/s]Extractor Estimating: 171it [01:47,  1.69it/s]Extractor Estimating: 172it [01:48,  1.53it/s]Extractor Estimating: 173it [01:48,  1.60it/s]Extractor Estimating: 174it [01:49,  1.56it/s]Extractor Estimating: 175it [01:50,  1.63it/s]Extractor Estimating: 176it [01:50,  1.63it/s]Extractor Estimating: 177it [01:51,  1.42it/s]Extractor Estimating: 178it [01:52,  1.50it/s]Extractor Estimating: 179it [01:52,  1.56it/s]Extractor Estimating: 180it [01:53,  1.43it/s]Extractor Estimating: 181it [01:54,  1.48it/s]Extractor Estimating: 182it [01:54,  1.45it/s]Extractor Estimating: 183it [01:55,  1.49it/s]Extractor Estimating: 184it [01:56,  1.55it/s]Extractor Estimating: 185it [01:56,  1.59it/s]Extractor Estimating: 186it [01:57,  1.63it/s]Extractor Estimating: 187it [01:57,  1.58it/s]Extractor Estimating: 188it [01:58,  1.61it/s]Extractor Estimating: 189it [01:59,  1.64it/s]Extractor Estimating: 190it [01:59,  1.68it/s]Extractor Estimating: 191it [02:00,  1.71it/s]Extractor Estimating: 192it [02:00,  1.65it/s]Extractor Estimating: 193it [02:01,  1.58it/s]Extractor Estimating: 194it [02:02,  1.61it/s]Extractor Estimating: 195it [02:02,  1.65it/s]Extractor Estimating: 196it [02:03,  1.58it/s]Extractor Estimating: 197it [02:04,  1.64it/s]Extractor Estimating: 198it [02:04,  1.62it/s]Extractor Estimating: 199it [02:05,  1.70it/s]Extractor Estimating: 200it [02:05,  1.68it/s]Extractor Estimating: 201it [02:06,  1.62it/s]Extractor Estimating: 202it [02:07,  1.68it/s]Extractor Estimating: 203it [02:07,  1.64it/s]Extractor Estimating: 204it [02:08,  1.46it/s]Extractor Estimating: 205it [02:09,  1.50it/s]Extractor Estimating: 206it [02:09,  1.53it/s]Extractor Estimating: 207it [02:10,  1.57it/s]Extractor Estimating: 208it [02:11,  1.59it/s]Extractor Estimating: 209it [02:11,  1.41it/s]Extractor Estimating: 210it [02:12,  1.53it/s]Extractor Estimating: 211it [02:12,  1.58it/s]Extractor Estimating: 212it [02:13,  1.63it/s]Extractor Estimating: 213it [02:14,  1.51it/s]Extractor Estimating: 214it [02:15,  1.48it/s]Extractor Estimating: 215it [02:15,  1.56it/s]Extractor Estimating: 216it [02:16,  1.62it/s]Extractor Estimating: 217it [02:16,  1.61it/s]Extractor Estimating: 218it [02:17,  1.64it/s]Extractor Estimating: 219it [02:18,  1.64it/s]Extractor Estimating: 220it [02:18,  1.50it/s]Extractor Estimating: 221it [02:19,  1.49it/s]Extractor Estimating: 222it [02:20,  1.54it/s]Extractor Estimating: 223it [02:20,  1.59it/s]Extractor Estimating: 224it [02:21,  1.63it/s]Extractor Estimating: 225it [02:22,  1.42it/s]Extractor Estimating: 226it [02:22,  1.55it/s]Extractor Estimating: 227it [02:23,  1.57it/s]Extractor Estimating: 228it [02:23,  1.62it/s]Extractor Estimating: 229it [02:24,  1.69it/s]Extractor Estimating: 230it [02:25,  1.55it/s]Extractor Estimating: 231it [02:25,  1.60it/s]Extractor Estimating: 232it [02:26,  1.69it/s]Extractor Estimating: 233it [02:26,  1.75it/s]Extractor Estimating: 234it [02:27,  1.79it/s]Extractor Estimating: 235it [02:27,  1.76it/s]Extractor Estimating: 236it [02:28,  1.80it/s]Extractor Estimating: 237it [02:29,  1.77it/s]Extractor Estimating: 238it [02:29,  1.81it/s]Extractor Estimating: 239it [02:30,  1.85it/s]Extractor Estimating: 240it [02:30,  1.85it/s]Extractor Estimating: 241it [02:31,  1.83it/s]Extractor Estimating: 242it [02:32,  1.33it/s]Extractor Estimating: 243it [02:32,  1.47it/s]Extractor Estimating: 244it [02:33,  1.53it/s]Extractor Estimating: 245it [02:33,  1.64it/s]Extractor Estimating: 246it [02:34,  1.69it/s]Extractor Estimating: 247it [02:35,  1.69it/s]Extractor Estimating: 248it [02:35,  1.74it/s]Extractor Estimating: 249it [02:36,  1.73it/s]Extractor Estimating: 250it [02:36,  1.73it/s]Extractor Estimating: 251it [02:37,  1.74it/s]Extractor Estimating: 252it [02:37,  1.76it/s]Extractor Estimating: 253it [02:38,  1.64it/s]Extractor Estimating: 254it [02:39,  1.64it/s]Extractor Estimating: 255it [02:39,  1.64it/s]Extractor Estimating: 256it [02:40,  1.65it/s]Extractor Estimating: 257it [02:41,  1.69it/s]Extractor Estimating: 258it [02:41,  1.42it/s]Extractor Estimating: 259it [02:42,  1.48it/s]Extractor Estimating: 260it [02:43,  1.57it/s]Extractor Estimating: 261it [02:43,  1.57it/s]Extractor Estimating: 262it [02:44,  1.60it/s]Extractor Estimating: 263it [02:45,  1.55it/s]Extractor Estimating: 264it [02:45,  1.61it/s]Extractor Estimating: 265it [02:46,  1.59it/s]Extractor Estimating: 266it [02:46,  1.62it/s]Extractor Estimating: 267it [02:47,  1.69it/s]Extractor Estimating: 268it [02:48,  1.44it/s]Extractor Estimating: 269it [02:48,  1.48it/s]Extractor Estimating: 270it [02:49,  1.49it/s]Extractor Estimating: 271it [02:50,  1.58it/s]Extractor Estimating: 272it [02:50,  1.63it/s]Extractor Estimating: 273it [02:51,  1.71it/s]Extractor Estimating: 274it [02:51,  1.71it/s]Extractor Estimating: 275it [02:52,  1.64it/s]Extractor Estimating: 276it [02:53,  1.68it/s]Extractor Estimating: 277it [02:53,  1.63it/s]Extractor Estimating: 278it [02:54,  1.67it/s]Extractor Estimating: 279it [02:54,  1.74it/s]Extractor Estimating: 280it [02:55,  1.76it/s]Extractor Estimating: 281it [02:56,  1.61it/s]Extractor Estimating: 282it [02:56,  1.62it/s]Extractor Estimating: 283it [02:57,  1.65it/s]Extractor Estimating: 284it [02:57,  1.65it/s]Extractor Estimating: 285it [02:58,  1.48it/s]Extractor Estimating: 286it [02:59,  1.31it/s]Extractor Estimating: 287it [03:00,  1.45it/s]Extractor Estimating: 288it [03:00,  1.49it/s]Extractor Estimating: 289it [03:01,  1.55it/s]Extractor Estimating: 290it [03:02,  1.61it/s]Extractor Estimating: 291it [03:02,  1.62it/s]Extractor Estimating: 292it [03:03,  1.68it/s]Extractor Estimating: 293it [03:03,  1.73it/s]Extractor Estimating: 294it [03:04,  1.75it/s]Extractor Estimating: 295it [03:04,  1.72it/s]Extractor Estimating: 296it [03:05,  1.67it/s]Extractor Estimating: 297it [03:06,  1.44it/s]Extractor Estimating: 298it [03:07,  1.50it/s]Extractor Estimating: 299it [03:07,  1.54it/s]Extractor Estimating: 300it [03:08,  1.57it/s]Extractor Estimating: 301it [03:08,  1.67it/s]Extractor Estimating: 302it [03:10,  1.18it/s]Extractor Estimating: 303it [03:10,  1.31it/s]Extractor Estimating: 304it [03:11,  1.37it/s]Extractor Estimating: 305it [03:11,  1.48it/s]Extractor Estimating: 306it [03:12,  1.46it/s]Extractor Estimating: 307it [03:13,  1.54it/s]Extractor Estimating: 308it [03:13,  1.58it/s]Extractor Estimating: 309it [03:14,  1.67it/s]Extractor Estimating: 310it [03:14,  1.67it/s]Extractor Estimating: 311it [03:15,  1.60it/s]Extractor Estimating: 312it [03:16,  1.44it/s]Extractor Estimating: 313it [03:17,  1.49it/s]Extractor Estimating: 314it [03:17,  1.55it/s]Extractor Estimating: 315it [03:18,  1.64it/s]Extractor Estimating: 316it [03:18,  1.68it/s]Extractor Estimating: 317it [03:19,  1.59it/s]Extractor Estimating: 318it [03:20,  1.45it/s]Extractor Estimating: 319it [03:20,  1.53it/s]Extractor Estimating: 320it [03:21,  1.61it/s]Extractor Estimating: 321it [03:21,  1.68it/s]Extractor Estimating: 322it [03:22,  1.72it/s]Extractor Estimating: 323it [03:23,  1.59it/s]Extractor Estimating: 324it [03:23,  1.65it/s]Extractor Estimating: 325it [03:24,  1.73it/s]Extractor Estimating: 326it [03:24,  1.67it/s]Extractor Estimating: 327it [03:25,  1.69it/s]Extractor Estimating: 328it [03:26,  1.62it/s]Extractor Estimating: 329it [03:26,  1.64it/s]Extractor Estimating: 330it [03:27,  1.65it/s]Extractor Estimating: 331it [03:27,  1.65it/s]Extractor Estimating: 332it [03:28,  1.66it/s]Extractor Estimating: 333it [03:29,  1.60it/s]Extractor Estimating: 334it [03:29,  1.63it/s]Extractor Estimating: 335it [03:30,  1.62it/s]Extractor Estimating: 336it [03:31,  1.60it/s]Extractor Estimating: 337it [03:31,  1.64it/s]Extractor Estimating: 338it [03:32,  1.60it/s]Extractor Estimating: 339it [03:32,  1.62it/s]Extractor Estimating: 340it [03:33,  1.61it/s]Extractor Estimating: 341it [03:34,  1.63it/s]Extractor Estimating: 342it [03:34,  1.60it/s]Extractor Estimating: 343it [03:35,  1.46it/s]Extractor Estimating: 344it [03:36,  1.48it/s]Extractor Estimating: 345it [03:36,  1.53it/s]Extractor Estimating: 346it [03:37,  1.56it/s]Extractor Estimating: 347it [03:38,  1.62it/s]Extractor Estimating: 348it [03:38,  1.51it/s]Extractor Estimating: 349it [03:39,  1.54it/s]Extractor Estimating: 350it [03:40,  1.58it/s]Extractor Estimating: 351it [03:40,  1.67it/s]Extractor Estimating: 352it [03:41,  1.68it/s]Extractor Estimating: 353it [03:42,  1.32it/s]Extractor Estimating: 354it [03:42,  1.42it/s]Extractor Estimating: 355it [03:43,  1.51it/s]Extractor Estimating: 356it [03:43,  1.60it/s]Extractor Estimating: 357it [03:44,  1.63it/s]Extractor Estimating: 358it [03:45,  1.62it/s]Extractor Estimating: 359it [03:45,  1.64it/s]Extractor Estimating: 360it [03:46,  1.60it/s]Extractor Estimating: 361it [03:47,  1.65it/s]Extractor Estimating: 362it [03:47,  1.61it/s]Extractor Estimating: 363it [03:48,  1.37it/s]Extractor Estimating: 364it [03:49,  1.44it/s]Extractor Estimating: 365it [03:49,  1.51it/s]Extractor Estimating: 366it [03:50,  1.59it/s]Extractor Estimating: 367it [03:51,  1.48it/s]Extractor Estimating: 368it [03:51,  1.53it/s]Extractor Estimating: 369it [03:52,  1.59it/s]Extractor Estimating: 370it [03:52,  1.60it/s]Extractor Estimating: 371it [03:53,  1.63it/s]Extractor Estimating: 372it [03:54,  1.48it/s]Extractor Estimating: 373it [03:54,  1.54it/s]Extractor Estimating: 374it [03:55,  1.61it/s]Extractor Estimating: 375it [03:56,  1.72it/s]Extractor Estimating: 375it [03:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:29,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:29,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:29,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:29,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:29,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:44:31,670 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:44:31,672 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:44:32,630 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:44:34,529 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:44:34,529 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:38,461 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:38,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:38,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:38,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:44:38,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:44:40,130 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:44:40,131 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:44:40,996 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:44:41,328 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:44:41,328 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:35:39,606 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:35:39,638 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 2999 mean pseudo reward: 0.974486243432582
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 15789
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15889, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15889, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.953, loss:426.8844
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 0.957, loss:382.4096
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.960, loss:380.5645
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 0.950, loss:323.4465
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 0.954, loss:341.0641
>> valid entity prec:0.4924, rec:0.4731, f1:0.4826
>> valid relation prec:0.0944, rec:0.0428, f1:0.0589
>> valid relation with NER prec:0.0944, rec:0.0428, f1:0.0589
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.958, loss:310.4718
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 0.957, loss:294.2249
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.957, loss:296.4324
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 0.965, loss:278.9629
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 0.963, loss:297.7354
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4495, rec:0.5260, f1:0.4848
>> valid relation prec:0.0779, rec:0.0532, f1:0.0632
>> valid relation with NER prec:0.0779, rec:0.0532, f1:0.0632
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 0.955, loss:261.1788
g_step 1200, step 75, avg_time 0.974, loss:266.1339
g_step 1300, step 50, avg_time 0.948, loss:249.7931
g_step 1400, step 25, avg_time 0.971, loss:234.4550
g_step 1500, step 125, avg_time 0.959, loss:237.9041
>> valid entity prec:0.4648, rec:0.4247, f1:0.4439
>> valid relation prec:0.0938, rec:0.0517, f1:0.0667
>> valid relation with NER prec:0.0938, rec:0.0517, f1:0.0667
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 100, avg_time 0.965, loss:212.9382
g_step 1700, step 75, avg_time 0.967, loss:219.4512
g_step 1800, step 50, avg_time 0.962, loss:188.8902
g_step 1900, step 25, avg_time 0.955, loss:191.7991
g_step 2000, step 125, avg_time 0.963, loss:186.4089
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4677, rec:0.4370, f1:0.4518
>> valid relation prec:0.0627, rec:0.0379, f1:0.0473
>> valid relation with NER prec:0.0627, rec:0.0379, f1:0.0473
g_step 2100, step 100, avg_time 0.955, loss:166.5030
g_step 2200, step 75, avg_time 0.972, loss:168.2604
g_step 2300, step 50, avg_time 0.954, loss:161.5706
g_step 2400, step 25, avg_time 0.962, loss:158.9146
g_step 2500, step 125, avg_time 0.970, loss:151.9373
>> valid entity prec:0.4457, rec:0.4013, f1:0.4224
>> valid relation prec:0.0640, rec:0.0339, f1:0.0443
>> valid relation with NER prec:0.0640, rec:0.0339, f1:0.0443
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:35:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:35:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-35-39_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:35:40 - WARNING - datasets.builder -   Using custom data configuration default-2d2eada261ecfe62
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2d2eada261ecfe62/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:35:48,428 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:48,495 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:35:48,496 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:48,497 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:35:48,730 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,831 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,832 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,832 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,832 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,832 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:48,832 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:35:50,646 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:35:53,955 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:35:54,118 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2d2eada261ecfe62/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:01<00:02,  1.17s/ba] 67%|██████▋   | 2/3 [00:01<00:00,  1.67ba/s]100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]100%|██████████| 3/3 [00:01<00:00,  1.91ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.24s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.29ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  1.93ba/s]100%|██████████| 4/4 [00:02<00:00,  2.81ba/s]100%|██████████| 4/4 [00:02<00:00,  1.99ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.87ba/s]100%|██████████| 3/3 [00:00<00:00,  6.31ba/s]100%|██████████| 3/3 [00:00<00:00,  5.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.25ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.56ba/s]100%|██████████| 4/4 [00:01<00:00,  5.24ba/s]100%|██████████| 4/4 [00:01<00:00,  3.82ba/s]
[INFO|trainer.py:414] 2023-08-28 19:36:07,526 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:36:07,591 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:36:07,591 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-28 19:36:07,591 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:36:07,591 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:36:07,591 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:36:07,591 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:36:07,591 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:02<10:34,  2.71s/it]  1%|          | 2/235 [00:03<05:43,  1.48s/it]  1%|▏         | 3/235 [00:04<05:23,  1.39s/it]  2%|▏         | 4/235 [00:04<03:40,  1.05it/s]  2%|▏         | 5/235 [00:05<02:44,  1.40it/s]  3%|▎         | 6/235 [00:05<02:10,  1.76it/s]  3%|▎         | 7/235 [00:06<02:11,  1.73it/s]  3%|▎         | 8/235 [00:06<01:57,  1.94it/s]  4%|▍         | 9/235 [00:06<01:40,  2.25it/s]  4%|▍         | 10/235 [00:07<01:28,  2.53it/s]  5%|▍         | 11/235 [00:07<01:20,  2.77it/s]  5%|▌         | 12/235 [00:07<01:15,  2.95it/s]  6%|▌         | 13/235 [00:07<01:11,  3.10it/s]  6%|▌         | 14/235 [00:08<01:08,  3.21it/s]  6%|▋         | 15/235 [00:08<01:06,  3.29it/s]  7%|▋         | 16/235 [00:08<01:05,  3.35it/s]  7%|▋         | 17/235 [00:09<01:04,  3.39it/s]  8%|▊         | 18/235 [00:09<01:03,  3.42it/s]  8%|▊         | 19/235 [00:09<01:14,  2.90it/s]  9%|▊         | 20/235 [00:10<01:10,  3.06it/s]  9%|▉         | 21/235 [00:10<01:07,  3.18it/s]  9%|▉         | 22/235 [00:10<01:05,  3.26it/s] 10%|▉         | 23/235 [00:10<01:03,  3.33it/s] 10%|█         | 24/235 [00:11<01:02,  3.37it/s] 11%|█         | 25/235 [00:11<01:01,  3.41it/s] 11%|█         | 26/235 [00:11<01:00,  3.43it/s] 11%|█▏        | 27/235 [00:12<01:00,  3.45it/s] 12%|█▏        | 28/235 [00:12<00:59,  3.46it/s] 12%|█▏        | 29/235 [00:13<01:38,  2.08it/s] 13%|█▎        | 30/235 [00:13<01:26,  2.37it/s] 13%|█▎        | 31/235 [00:13<01:17,  2.62it/s] 14%|█▎        | 32/235 [00:14<01:11,  2.83it/s] 14%|█▍        | 33/235 [00:14<01:07,  3.00it/s] 14%|█▍        | 34/235 [00:14<01:04,  3.13it/s] 15%|█▍        | 35/235 [00:15<01:01,  3.23it/s] 15%|█▌        | 36/235 [00:15<01:00,  3.30it/s] 16%|█▌        | 37/235 [00:15<00:59,  3.35it/s] 16%|█▌        | 38/235 [00:16<01:23,  2.35it/s] 17%|█▋        | 39/235 [00:16<01:15,  2.60it/s] 17%|█▋        | 40/235 [00:16<01:09,  2.82it/s] 17%|█▋        | 41/235 [00:17<01:04,  2.99it/s] 18%|█▊        | 42/235 [00:17<01:01,  3.12it/s] 18%|█▊        | 43/235 [00:17<00:59,  3.22it/s] 19%|█▊        | 44/235 [00:18<00:57,  3.30it/s] 19%|█▉        | 45/235 [00:18<00:56,  3.35it/s] 20%|█▉        | 46/235 [00:18<00:55,  3.39it/s] 20%|██        | 47/235 [00:19<01:03,  2.95it/s][INFO|trainer.py:2140] 2023-08-28 19:36:26,637 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:36:26,637 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:36:26,637 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.80it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.89it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.98it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.06it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.62it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.91it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.57it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.46it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.40it/s][A
 12%|█▏        | 53/435 [00:01<00:09, 39.11it/s][A
 13%|█▎        | 58/435 [00:01<00:09, 41.03it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 42.35it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 43.44it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.19it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.78it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.14it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.18it/s][A
 21%|██▏       | 93/435 [00:02<00:13, 25.62it/s][A
 23%|██▎       | 98/435 [00:02<00:11, 29.54it/s][A
 24%|██▎       | 103/435 [00:02<00:10, 33.08it/s][A
 25%|██▍       | 108/435 [00:02<00:12, 26.44it/s][A
 26%|██▌       | 113/435 [00:02<00:10, 30.27it/s][A
 27%|██▋       | 118/435 [00:03<00:09, 33.72it/s][A
 28%|██▊       | 123/435 [00:03<00:08, 36.64it/s][A
 29%|██▉       | 128/435 [00:03<00:07, 39.00it/s][A
 31%|███       | 133/435 [00:03<00:07, 40.79it/s][A
 32%|███▏      | 138/435 [00:03<00:07, 42.15it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 43.16it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 43.55it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 44.01it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 44.40it/s][A
 37%|███▋      | 163/435 [00:04<00:06, 44.76it/s][A
 39%|███▊      | 168/435 [00:04<00:05, 45.07it/s][A
 40%|███▉      | 173/435 [00:04<00:05, 45.21it/s][A
 41%|████      | 178/435 [00:05<00:21, 12.10it/s][A
 42%|████▏     | 182/435 [00:06<00:19, 13.17it/s][A
 43%|████▎     | 185/435 [00:06<00:23, 10.61it/s][A
 44%|████▎     | 190/435 [00:06<00:17, 14.36it/s][A
 45%|████▍     | 195/435 [00:06<00:12, 18.49it/s][A
 46%|████▌     | 200/435 [00:06<00:10, 22.80it/s][A
 47%|████▋     | 205/435 [00:06<00:08, 27.05it/s][A
 48%|████▊     | 210/435 [00:06<00:07, 30.95it/s][A
 49%|████▉     | 215/435 [00:06<00:06, 34.29it/s][A
 51%|█████     | 220/435 [00:06<00:05, 37.11it/s][A
 52%|█████▏    | 225/435 [00:06<00:05, 39.18it/s][A
 53%|█████▎    | 230/435 [00:07<00:05, 40.82it/s][A
 54%|█████▍    | 235/435 [00:07<00:04, 42.10it/s][A
 55%|█████▌    | 240/435 [00:07<00:04, 43.12it/s][A
 56%|█████▋    | 245/435 [00:07<00:04, 43.80it/s][A
 57%|█████▋    | 250/435 [00:07<00:04, 44.36it/s][A
 59%|█████▊    | 255/435 [00:07<00:04, 44.69it/s][A
 60%|█████▉    | 260/435 [00:07<00:03, 45.03it/s][A
 61%|██████    | 265/435 [00:07<00:03, 45.09it/s][A
 62%|██████▏   | 270/435 [00:07<00:03, 45.08it/s][A
 63%|██████▎   | 275/435 [00:08<00:03, 45.11it/s][A
 64%|██████▍   | 280/435 [00:08<00:03, 45.14it/s][A
 66%|██████▌   | 285/435 [00:08<00:03, 45.29it/s][A
 67%|██████▋   | 290/435 [00:08<00:03, 45.41it/s][A
 68%|██████▊   | 295/435 [00:08<00:03, 45.36it/s][A
 69%|██████▉   | 300/435 [00:09<00:05, 23.72it/s][A
 70%|██████▉   | 304/435 [00:09<00:07, 18.33it/s][A
 71%|███████   | 307/435 [00:09<00:07, 18.18it/s][A
 72%|███████▏  | 312/435 [00:09<00:05, 22.94it/s][A
 73%|███████▎  | 317/435 [00:09<00:04, 27.37it/s][A
 74%|███████▍  | 322/435 [00:09<00:03, 31.39it/s][A
 75%|███████▌  | 327/435 [00:09<00:03, 34.80it/s][A
 76%|███████▋  | 332/435 [00:10<00:02, 37.53it/s][A
 77%|███████▋  | 337/435 [00:10<00:02, 39.66it/s][A
 79%|███████▊  | 342/435 [00:10<00:02, 41.36it/s][A
 80%|███████▉  | 347/435 [00:10<00:02, 42.42it/s][A
 81%|████████  | 352/435 [00:10<00:01, 43.03it/s][A
 82%|████████▏ | 357/435 [00:10<00:01, 43.67it/s][A
 83%|████████▎ | 362/435 [00:10<00:01, 44.23it/s][A
 84%|████████▍ | 367/435 [00:10<00:01, 44.70it/s][A
 86%|████████▌ | 372/435 [00:10<00:01, 45.00it/s][A
 87%|████████▋ | 377/435 [00:11<00:01, 45.15it/s][A
 88%|████████▊ | 382/435 [00:11<00:01, 45.24it/s][A
 89%|████████▉ | 387/435 [00:11<00:01, 45.27it/s][A
 90%|█████████ | 392/435 [00:11<00:00, 45.08it/s][A
 91%|█████████▏| 397/435 [00:11<00:00, 44.87it/s][A
 92%|█████████▏| 402/435 [00:11<00:00, 44.94it/s][A
 94%|█████████▎| 407/435 [00:11<00:00, 45.20it/s][A
 95%|█████████▍| 412/435 [00:11<00:00, 45.42it/s][A
 96%|█████████▌| 417/435 [00:11<00:00, 45.55it/s][A
 97%|█████████▋| 422/435 [00:12<00:00, 45.63it/s][A
 98%|█████████▊| 427/435 [00:12<00:00, 35.76it/s][A
 99%|█████████▉| 432/435 [00:12<00:00, 38.30it/s][A
                                                 [A                                                
100%|██████████| 435/435 [00:12<00:00, 38.30it/s][A 20%|██        | 47/235 [00:31<01:03,  2.95it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:36:39,810 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 19:36:40,698 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:36:46,128 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:36:46,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:36:46,454 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [01:04<43:03, 13.81s/it] 21%|██        | 49/235 [01:05<30:58,  9.99s/it] 21%|██▏       | 50/235 [01:05<21:50,  7.08s/it] 22%|██▏       | 51/235 [01:05<15:28,  5.05s/it] 22%|██▏       | 52/235 [01:06<11:02,  3.62s/it] 23%|██▎       | 53/235 [01:06<07:56,  2.62s/it] 23%|██▎       | 54/235 [01:06<05:47,  1.92s/it] 23%|██▎       | 55/235 [01:07<04:17,  1.43s/it] 24%|██▍       | 56/235 [01:07<03:15,  1.09s/it] 24%|██▍       | 57/235 [01:08<02:53,  1.03it/s] 25%|██▍       | 58/235 [01:08<02:16,  1.30it/s] 25%|██▌       | 59/235 [01:08<01:50,  1.60it/s] 26%|██▌       | 60/235 [01:08<01:31,  1.91it/s] 26%|██▌       | 61/235 [01:09<01:19,  2.20it/s] 26%|██▋       | 62/235 [01:09<01:10,  2.47it/s] 27%|██▋       | 63/235 [01:09<01:03,  2.69it/s] 27%|██▋       | 64/235 [01:10<00:59,  2.88it/s] 28%|██▊       | 65/235 [01:10<00:56,  3.03it/s] 28%|██▊       | 66/235 [01:10<00:57,  2.95it/s] 29%|██▊       | 67/235 [01:11<00:54,  3.08it/s] 29%|██▉       | 68/235 [01:11<00:52,  3.18it/s] 29%|██▉       | 69/235 [01:11<00:51,  3.25it/s] 30%|██▉       | 70/235 [01:11<00:49,  3.31it/s] 30%|███       | 71/235 [01:12<00:49,  3.34it/s] 31%|███       | 72/235 [01:12<00:48,  3.37it/s] 31%|███       | 73/235 [01:12<00:47,  3.39it/s] 31%|███▏      | 74/235 [01:13<00:47,  3.41it/s] 32%|███▏      | 75/235 [01:13<00:46,  3.41it/s] 32%|███▏      | 76/235 [01:13<00:46,  3.42it/s] 33%|███▎      | 77/235 [01:14<00:49,  3.18it/s] 33%|███▎      | 78/235 [01:14<00:48,  3.25it/s] 34%|███▎      | 79/235 [01:14<00:47,  3.30it/s] 34%|███▍      | 80/235 [01:14<00:46,  3.34it/s] 34%|███▍      | 81/235 [01:15<00:45,  3.37it/s] 35%|███▍      | 82/235 [01:15<00:45,  3.39it/s] 35%|███▌      | 83/235 [01:15<00:44,  3.40it/s] 36%|███▌      | 84/235 [01:16<00:44,  3.41it/s] 36%|███▌      | 85/235 [01:16<00:43,  3.42it/s] 37%|███▋      | 86/235 [01:16<00:43,  3.43it/s] 37%|███▋      | 87/235 [01:16<00:43,  3.43it/s] 37%|███▋      | 88/235 [01:17<00:54,  2.68it/s] 38%|███▊      | 89/235 [01:17<00:50,  2.88it/s] 38%|███▊      | 90/235 [01:18<00:47,  3.04it/s] 39%|███▊      | 91/235 [01:18<00:45,  3.16it/s] 39%|███▉      | 92/235 [01:18<00:43,  3.25it/s] 40%|███▉      | 93/235 [01:18<00:42,  3.32it/s] 40%|████      | 94/235 [01:19<00:40,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 19:37:26,838 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:37:26,838 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:37:26,838 >>   Batch size = 8
{'eval_loss': 1.040338397026062, 'eval_runtime': 12.433, 'eval_samples_per_second': 279.819, 'eval_steps_per_second': 34.987, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.83it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.57it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.66it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.83it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.37it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.81it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.69it/s][A
 10%|▉         | 43/435 [00:01<00:10, 36.12it/s][A
 11%|█         | 48/435 [00:01<00:09, 38.78it/s][A
 12%|█▏        | 53/435 [00:01<00:09, 40.74it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 42.21it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 43.38it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.18it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.71it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 44.96it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 44.86it/s][A
 20%|██        | 88/435 [00:01<00:07, 44.73it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.89it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.14it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.46it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.67it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.85it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 45.84it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.78it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.40it/s][A
 31%|███       | 133/435 [00:02<00:06, 45.22it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 45.14it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.21it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.39it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.68it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.81it/s][A
 37%|███▋      | 163/435 [00:03<00:05, 45.88it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.79it/s][A
 40%|███▉      | 173/435 [00:04<00:05, 45.57it/s][A
 41%|████      | 178/435 [00:04<00:11, 23.27it/s][A
 42%|████▏     | 183/435 [00:04<00:09, 27.35it/s][A
 43%|████▎     | 188/435 [00:04<00:07, 31.14it/s][A
 44%|████▍     | 193/435 [00:04<00:07, 34.51it/s][A
 46%|████▌     | 198/435 [00:04<00:06, 37.28it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 39.56it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 41.30it/s][A
 49%|████▉     | 213/435 [00:05<00:05, 42.56it/s][A
 50%|█████     | 218/435 [00:05<00:05, 42.96it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 43.46it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 43.96it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 44.45it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 44.78it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.22it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.45it/s][A
 58%|█████▊    | 253/435 [00:05<00:03, 45.68it/s][A
 59%|█████▉    | 258/435 [00:06<00:03, 45.62it/s][A
 60%|██████    | 263/435 [00:06<00:03, 45.42it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.20it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.17it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.12it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.29it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.57it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 45.69it/s][A
 69%|██████▊   | 298/435 [00:07<00:02, 45.81it/s][A
 70%|██████▉   | 303/435 [00:07<00:05, 24.36it/s][A
 71%|███████   | 308/435 [00:07<00:04, 28.40it/s][A
 72%|███████▏  | 313/435 [00:07<00:03, 32.06it/s][A
 73%|███████▎  | 318/435 [00:07<00:03, 35.31it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 37.97it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 40.07it/s][A
 77%|███████▋  | 333/435 [00:08<00:02, 41.69it/s][A
 78%|███████▊  | 338/435 [00:08<00:02, 42.83it/s][A
 79%|███████▉  | 343/435 [00:08<00:02, 43.27it/s][A
 80%|████████  | 348/435 [00:08<00:01, 43.51it/s][A
 81%|████████  | 353/435 [00:08<00:01, 44.09it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 44.53it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 44.96it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.26it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.46it/s][A
 87%|████████▋ | 378/435 [00:09<00:01, 45.54it/s][A
 88%|████████▊ | 383/435 [00:09<00:01, 45.46it/s][A
 89%|████████▉ | 388/435 [00:09<00:01, 45.16it/s][A
 90%|█████████ | 393/435 [00:09<00:00, 45.02it/s][A
 91%|█████████▏| 398/435 [00:09<00:00, 45.00it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 45.21it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 45.42it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.58it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.71it/s][A
 97%|█████████▋| 423/435 [00:10<00:00, 45.83it/s][A
 98%|█████████▊| 428/435 [00:10<00:00, 24.62it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 28.65it/s][A
                                                 [A                                                
100%|██████████| 435/435 [00:10<00:00, 28.65it/s][A 40%|████      | 94/235 [01:29<00:40,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:37:37,687 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 19:37:38,254 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:37:53,537 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:37:55,090 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:37:55,278 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [02:30<50:21, 21.58s/it] 41%|████      | 96/235 [02:30<35:16, 15.22s/it] 41%|████▏     | 97/235 [02:31<24:42, 10.74s/it] 42%|████▏     | 98/235 [02:31<17:22,  7.61s/it] 42%|████▏     | 99/235 [02:31<12:16,  5.41s/it] 43%|████▎     | 100/235 [02:32<08:48,  3.92s/it] 43%|████▎     | 101/235 [02:32<06:19,  2.83s/it] 43%|████▎     | 102/235 [02:32<04:35,  2.07s/it] 44%|████▍     | 103/235 [02:33<03:22,  1.53s/it] 44%|████▍     | 104/235 [02:33<02:32,  1.16s/it] 45%|████▍     | 105/235 [02:33<01:57,  1.11it/s] 45%|████▌     | 106/235 [02:34<01:36,  1.34it/s] 46%|████▌     | 107/235 [02:34<01:17,  1.64it/s] 46%|████▌     | 108/235 [02:34<01:05,  1.95it/s] 46%|████▋     | 109/235 [02:34<00:56,  2.24it/s] 47%|████▋     | 110/235 [02:35<00:49,  2.50it/s] 47%|████▋     | 111/235 [02:35<00:45,  2.73it/s] 48%|████▊     | 112/235 [02:35<00:42,  2.91it/s] 48%|████▊     | 113/235 [02:36<00:39,  3.05it/s] 49%|████▊     | 114/235 [02:36<00:38,  3.16it/s] 49%|████▉     | 115/235 [02:36<00:37,  3.24it/s] 49%|████▉     | 116/235 [02:36<00:36,  3.29it/s] 50%|████▉     | 117/235 [02:37<00:35,  3.33it/s] 50%|█████     | 118/235 [02:37<00:34,  3.36it/s] 51%|█████     | 119/235 [02:37<00:34,  3.38it/s] 51%|█████     | 120/235 [02:38<00:51,  2.25it/s] 51%|█████▏    | 121/235 [02:38<00:45,  2.51it/s] 52%|█████▏    | 122/235 [02:39<00:41,  2.73it/s] 52%|█████▏    | 123/235 [02:39<00:38,  2.91it/s] 53%|█████▎    | 124/235 [02:39<00:38,  2.89it/s] 53%|█████▎    | 125/235 [02:40<00:36,  3.03it/s] 54%|█████▎    | 126/235 [02:40<00:34,  3.14it/s] 54%|█████▍    | 127/235 [02:40<00:33,  3.23it/s] 54%|█████▍    | 128/235 [02:40<00:32,  3.29it/s] 55%|█████▍    | 129/235 [02:41<00:31,  3.33it/s] 55%|█████▌    | 130/235 [02:41<00:43,  2.42it/s] 56%|█████▌    | 131/235 [02:42<00:41,  2.54it/s] 56%|█████▌    | 132/235 [02:42<00:37,  2.75it/s] 57%|█████▋    | 133/235 [02:43<01:00,  1.68it/s] 57%|█████▋    | 134/235 [02:44<01:06,  1.52it/s] 57%|█████▋    | 135/235 [02:45<01:05,  1.52it/s] 58%|█████▊    | 136/235 [02:45<00:54,  1.82it/s] 58%|█████▊    | 137/235 [02:45<00:46,  2.12it/s] 59%|█████▊    | 138/235 [02:46<00:51,  1.87it/s] 59%|█████▉    | 139/235 [02:46<00:44,  2.17it/s] 60%|█████▉    | 140/235 [02:47<00:38,  2.44it/s] 60%|██████    | 141/235 [02:47<00:34,  2.74it/s][INFO|trainer.py:2140] 2023-08-28 19:38:54,910 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:38:54,910 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:38:54,910 >>   Batch size = 8
{'eval_loss': 1.0511248111724854, 'eval_runtime': 10.6106, 'eval_samples_per_second': 327.881, 'eval_steps_per_second': 40.997, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.95it/s][A
  3%|▎         | 12/435 [00:00<00:08, 50.02it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.88it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.02it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.58it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.94it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.68it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.50it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.57it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.68it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.74it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.71it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.77it/s][A
 17%|█▋        | 73/435 [00:01<00:12, 29.65it/s][A
 18%|█▊        | 78/435 [00:01<00:10, 33.20it/s][A
 19%|█▉        | 83/435 [00:01<00:09, 36.25it/s][A
 20%|██        | 88/435 [00:02<00:08, 38.69it/s][A
 21%|██▏       | 93/435 [00:02<00:08, 40.48it/s][A
 23%|██▎       | 98/435 [00:02<00:08, 41.90it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 43.03it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 43.82it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 43.99it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 44.40it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 44.88it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.03it/s][A
 31%|███       | 133/435 [00:03<00:06, 45.20it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 45.11it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.21it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.24it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.26it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.12it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 45.18it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.36it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.53it/s][A
 41%|████      | 178/435 [00:04<00:05, 45.61it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.51it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.53it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.48it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.37it/s][A
 47%|████▋     | 203/435 [00:04<00:08, 28.98it/s][A
 48%|████▊     | 208/435 [00:04<00:06, 32.62it/s][A
 49%|████▉     | 213/435 [00:05<00:06, 35.69it/s][A
 50%|█████     | 218/435 [00:05<00:05, 38.26it/s][A
 51%|█████▏    | 223/435 [00:05<00:05, 40.32it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 41.85it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 42.97it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 43.67it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 43.78it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 44.37it/s][A
 59%|█████▉    | 258/435 [00:06<00:03, 44.66it/s][A
 60%|██████    | 263/435 [00:06<00:03, 44.81it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.21it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.36it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.56it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.50it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.29it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 45.10it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 45.05it/s][A
 70%|██████▉   | 303/435 [00:07<00:02, 45.16it/s][A
 71%|███████   | 308/435 [00:07<00:02, 45.24it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 45.43it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.51it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.62it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.56it/s][A
 77%|███████▋  | 333/435 [00:08<00:04, 24.82it/s][A
 78%|███████▊  | 338/435 [00:08<00:03, 28.73it/s][A
 79%|███████▉  | 343/435 [00:08<00:02, 32.34it/s][A
 80%|████████  | 348/435 [00:08<00:02, 35.48it/s][A
 81%|████████  | 353/435 [00:08<00:02, 38.06it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 40.05it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 41.61it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 42.75it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 43.13it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 43.67it/s][A
 88%|████████▊ | 383/435 [00:09<00:01, 44.27it/s][A
 89%|████████▉ | 388/435 [00:09<00:01, 44.67it/s][A
 90%|█████████ | 393/435 [00:09<00:00, 45.01it/s][A
 91%|█████████▏| 398/435 [00:09<00:00, 45.22it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 45.26it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 45.47it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.43it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.28it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 45.08it/s][A
 98%|█████████▊| 428/435 [00:10<00:00, 45.09it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 45.22it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 45.22it/s][A 60%|██████    | 141/235 [02:57<00:34,  2.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:39:05,996 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 19:39:06,670 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:39:18,697 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:39:19,298 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:39:19,475 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [03:56<32:20, 20.86s/it] 61%|██████    | 143/235 [03:57<22:52, 14.92s/it] 61%|██████▏   | 144/235 [03:57<15:58, 10.53s/it] 62%|██████▏   | 145/235 [03:57<11:11,  7.46s/it] 62%|██████▏   | 146/235 [03:57<07:52,  5.31s/it] 63%|██████▎   | 147/235 [03:58<05:34,  3.80s/it] 63%|██████▎   | 148/235 [03:58<03:59,  2.75s/it] 63%|██████▎   | 149/235 [03:58<02:52,  2.01s/it] 64%|██████▍   | 150/235 [03:59<02:07,  1.49s/it] 64%|██████▍   | 151/235 [03:59<01:36,  1.15s/it] 65%|██████▍   | 152/235 [03:59<01:14,  1.12it/s] 65%|██████▌   | 153/235 [04:00<00:58,  1.41it/s] 66%|██████▌   | 154/235 [04:00<00:47,  1.71it/s] 66%|██████▌   | 155/235 [04:00<00:39,  2.02it/s] 66%|██████▋   | 156/235 [04:00<00:34,  2.31it/s] 67%|██████▋   | 157/235 [04:01<00:30,  2.57it/s] 67%|██████▋   | 158/235 [04:01<00:27,  2.79it/s] 68%|██████▊   | 159/235 [04:01<00:25,  2.97it/s] 68%|██████▊   | 160/235 [04:02<00:24,  3.11it/s] 69%|██████▊   | 161/235 [04:02<00:23,  3.21it/s] 69%|██████▉   | 162/235 [04:02<00:26,  2.81it/s] 69%|██████▉   | 163/235 [04:03<00:24,  2.98it/s] 70%|██████▉   | 164/235 [04:03<00:22,  3.11it/s] 70%|███████   | 165/235 [04:03<00:21,  3.20it/s] 71%|███████   | 166/235 [04:03<00:21,  3.27it/s] 71%|███████   | 167/235 [04:04<00:20,  3.32it/s] 71%|███████▏  | 168/235 [04:04<00:19,  3.35it/s] 72%|███████▏  | 169/235 [04:04<00:19,  3.38it/s] 72%|███████▏  | 170/235 [04:05<00:19,  3.40it/s] 73%|███████▎  | 171/235 [04:05<00:18,  3.41it/s] 73%|███████▎  | 172/235 [04:06<00:25,  2.48it/s] 74%|███████▎  | 173/235 [04:06<00:22,  2.70it/s] 74%|███████▍  | 174/235 [04:06<00:21,  2.89it/s] 74%|███████▍  | 175/235 [04:06<00:19,  3.03it/s] 75%|███████▍  | 176/235 [04:07<00:18,  3.14it/s] 75%|███████▌  | 177/235 [04:07<00:17,  3.23it/s] 76%|███████▌  | 178/235 [04:07<00:17,  3.29it/s] 76%|███████▌  | 179/235 [04:08<00:16,  3.33it/s] 77%|███████▋  | 180/235 [04:08<00:16,  3.36it/s] 77%|███████▋  | 181/235 [04:08<00:15,  3.38it/s] 77%|███████▋  | 182/235 [04:08<00:15,  3.37it/s] 78%|███████▊  | 183/235 [04:09<00:15,  3.39it/s] 78%|███████▊  | 184/235 [04:09<00:14,  3.41it/s] 79%|███████▊  | 185/235 [04:09<00:14,  3.41it/s] 79%|███████▉  | 186/235 [04:10<00:14,  3.42it/s] 80%|███████▉  | 187/235 [04:10<00:14,  3.42it/s] 80%|████████  | 188/235 [04:10<00:13,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 19:40:18,261 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:40:18,261 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:40:18,262 >>   Batch size = 8
{'eval_loss': 1.068317174911499, 'eval_runtime': 10.2721, 'eval_samples_per_second': 338.684, 'eval_steps_per_second': 42.348, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.71it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.58it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.79it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.96it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.45it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.89it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.55it/s][A
 10%|▉         | 43/435 [00:01<00:08, 45.50it/s][A
 11%|█         | 48/435 [00:01<00:21, 18.35it/s][A
 12%|█▏        | 53/435 [00:01<00:16, 22.49it/s][A
 13%|█▎        | 58/435 [00:01<00:14, 26.59it/s][A
 14%|█▍        | 63/435 [00:01<00:12, 30.49it/s][A
 16%|█▌        | 68/435 [00:01<00:10, 33.94it/s][A
 17%|█▋        | 73/435 [00:02<00:09, 36.82it/s][A
 18%|█▊        | 78/435 [00:02<00:09, 39.18it/s][A
 19%|█▉        | 83/435 [00:02<00:08, 40.98it/s][A
 20%|██        | 88/435 [00:02<00:08, 41.88it/s][A
 21%|██▏       | 93/435 [00:02<00:08, 42.74it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 43.52it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 44.05it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 44.96it/s][A
 27%|██▋       | 118/435 [00:03<00:07, 45.21it/s][A
 28%|██▊       | 123/435 [00:03<00:06, 45.43it/s][A
 29%|██▉       | 128/435 [00:03<00:06, 45.32it/s][A
 31%|███       | 133/435 [00:03<00:06, 45.21it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 45.18it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.09it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.23it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.44it/s][A
 36%|███▋      | 158/435 [00:04<00:06, 45.48it/s][A
 37%|███▋      | 163/435 [00:04<00:12, 21.23it/s][A
 39%|███▊      | 168/435 [00:04<00:10, 25.34it/s][A
 40%|███▉      | 173/435 [00:04<00:08, 29.25it/s][A
 41%|████      | 178/435 [00:04<00:07, 32.81it/s][A
 42%|████▏     | 183/435 [00:04<00:07, 35.82it/s][A
 43%|████▎     | 188/435 [00:05<00:06, 38.32it/s][A
 44%|████▍     | 193/435 [00:05<00:06, 40.30it/s][A
 46%|████▌     | 198/435 [00:05<00:05, 41.76it/s][A
 47%|████▋     | 203/435 [00:05<00:05, 42.39it/s][A
 48%|████▊     | 208/435 [00:05<00:05, 43.26it/s][A
 49%|████▉     | 213/435 [00:05<00:05, 44.02it/s][A
 50%|█████     | 218/435 [00:05<00:04, 44.50it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 44.83it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 44.84it/s][A
 54%|█████▎    | 233/435 [00:06<00:04, 45.06it/s][A
 55%|█████▍    | 238/435 [00:06<00:04, 45.20it/s][A
 56%|█████▌    | 243/435 [00:06<00:04, 45.02it/s][A
 57%|█████▋    | 248/435 [00:06<00:04, 44.85it/s][A
 58%|█████▊    | 253/435 [00:06<00:04, 44.99it/s][A
 59%|█████▉    | 258/435 [00:06<00:03, 45.11it/s][A
 60%|██████    | 263/435 [00:06<00:03, 45.37it/s][A
 62%|██████▏   | 268/435 [00:06<00:03, 45.48it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.60it/s][A
 64%|██████▍   | 278/435 [00:07<00:03, 45.65it/s][A
 65%|██████▌   | 283/435 [00:07<00:06, 23.49it/s][A
 66%|██████▌   | 288/435 [00:07<00:05, 27.52it/s][A
 67%|██████▋   | 293/435 [00:07<00:04, 31.24it/s][A
 69%|██████▊   | 298/435 [00:07<00:03, 34.55it/s][A
 70%|██████▉   | 303/435 [00:07<00:03, 37.28it/s][A
 71%|███████   | 308/435 [00:08<00:03, 39.46it/s][A
 72%|███████▏  | 313/435 [00:08<00:02, 41.18it/s][A
 73%|███████▎  | 318/435 [00:08<00:02, 42.42it/s][A
 74%|███████▍  | 323/435 [00:08<00:02, 42.90it/s][A
 75%|███████▌  | 328/435 [00:08<00:02, 43.57it/s][A
 77%|███████▋  | 333/435 [00:08<00:02, 44.19it/s][A
 78%|███████▊  | 338/435 [00:08<00:02, 44.61it/s][A
 79%|███████▉  | 343/435 [00:08<00:02, 44.84it/s][A
 80%|████████  | 348/435 [00:08<00:01, 44.98it/s][A
 81%|████████  | 353/435 [00:09<00:01, 45.13it/s][A
 82%|████████▏ | 358/435 [00:09<00:01, 45.25it/s][A
 83%|████████▎ | 363/435 [00:09<00:01, 45.19it/s][A
 85%|████████▍ | 368/435 [00:09<00:01, 45.02it/s][A
 86%|████████▌ | 373/435 [00:09<00:01, 45.02it/s][A
 87%|████████▋ | 378/435 [00:09<00:01, 45.21it/s][A
 88%|████████▊ | 383/435 [00:09<00:01, 45.36it/s][A
 89%|████████▉ | 388/435 [00:09<00:01, 45.48it/s][A
 90%|█████████ | 393/435 [00:09<00:00, 45.56it/s][A
 91%|█████████▏| 398/435 [00:10<00:00, 45.64it/s][A
 93%|█████████▎| 403/435 [00:10<00:00, 45.68it/s][A
 94%|█████████▍| 408/435 [00:10<00:00, 40.79it/s][A
 95%|█████████▍| 413/435 [00:10<00:00, 42.27it/s][A
 96%|█████████▌| 418/435 [00:10<00:00, 43.33it/s][A
 97%|█████████▋| 423/435 [00:10<00:00, 44.05it/s][A
 98%|█████████▊| 428/435 [00:10<00:00, 44.57it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 44.98it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 44.98it/s][A 80%|████████  | 188/235 [04:21<00:13,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:40:30,207 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 19:40:32,153 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:40:43,836 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:40:44,381 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:40:44,983 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [05:12<14:27, 18.85s/it] 81%|████████  | 190/235 [05:13<10:04, 13.42s/it] 81%|████████▏ | 191/235 [05:13<06:57,  9.48s/it] 82%|████████▏ | 192/235 [05:14<04:49,  6.73s/it] 82%|████████▏ | 193/235 [05:14<03:21,  4.80s/it] 83%|████████▎ | 194/235 [05:14<02:21,  3.44s/it] 83%|████████▎ | 195/235 [05:15<01:39,  2.50s/it] 83%|████████▎ | 196/235 [05:15<01:11,  1.84s/it] 84%|████████▍ | 197/235 [05:15<00:52,  1.37s/it] 84%|████████▍ | 198/235 [05:15<00:38,  1.05s/it] 85%|████████▍ | 199/235 [05:16<00:30,  1.16it/s] 85%|████████▌ | 200/235 [05:16<00:24,  1.45it/s] 86%|████████▌ | 201/235 [05:16<00:19,  1.76it/s] 86%|████████▌ | 202/235 [05:17<00:16,  2.06it/s] 86%|████████▋ | 203/235 [05:17<00:13,  2.34it/s] 87%|████████▋ | 204/235 [05:17<00:11,  2.59it/s] 87%|████████▋ | 205/235 [05:18<00:10,  2.80it/s] 88%|████████▊ | 206/235 [05:18<00:09,  2.98it/s] 88%|████████▊ | 207/235 [05:18<00:09,  3.11it/s] 89%|████████▊ | 208/235 [05:18<00:08,  3.22it/s] 89%|████████▉ | 209/235 [05:19<00:09,  2.60it/s] 89%|████████▉ | 210/235 [05:19<00:08,  2.82it/s] 90%|████████▉ | 211/235 [05:20<00:08,  2.99it/s] 90%|█████████ | 212/235 [05:20<00:07,  3.13it/s] 91%|█████████ | 213/235 [05:20<00:06,  3.23it/s] 91%|█████████ | 214/235 [05:20<00:06,  3.31it/s] 91%|█████████▏| 215/235 [05:21<00:05,  3.36it/s] 92%|█████████▏| 216/235 [05:21<00:05,  3.40it/s] 92%|█████████▏| 217/235 [05:21<00:05,  3.43it/s] 93%|█████████▎| 218/235 [05:22<00:04,  3.45it/s] 93%|█████████▎| 219/235 [05:22<00:05,  3.17it/s] 94%|█████████▎| 220/235 [05:22<00:04,  3.26it/s] 94%|█████████▍| 221/235 [05:23<00:04,  3.33it/s] 94%|█████████▍| 222/235 [05:23<00:03,  3.38it/s] 95%|█████████▍| 223/235 [05:23<00:03,  3.41it/s] 95%|█████████▌| 224/235 [05:23<00:03,  3.43it/s] 96%|█████████▌| 225/235 [05:24<00:02,  3.45it/s] 96%|█████████▌| 226/235 [05:24<00:02,  3.46it/s] 97%|█████████▋| 227/235 [05:24<00:02,  3.47it/s] 97%|█████████▋| 228/235 [05:25<00:02,  3.48it/s] 97%|█████████▋| 229/235 [05:25<00:01,  3.48it/s] 98%|█████████▊| 230/235 [05:25<00:01,  3.04it/s] 98%|█████████▊| 231/235 [05:26<00:01,  3.17it/s] 99%|█████████▊| 232/235 [05:26<00:00,  3.25it/s] 99%|█████████▉| 233/235 [05:26<00:00,  3.32it/s]100%|█████████▉| 234/235 [05:26<00:00,  3.37it/s]100%|██████████| 235/235 [05:27<00:00,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 19:41:34,742 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:41:34,742 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:41:34,743 >>   Batch size = 8
{'eval_loss': 1.0744421482086182, 'eval_runtime': 10.8903, 'eval_samples_per_second': 319.458, 'eval_steps_per_second': 39.944, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.31it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.44it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.60it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.11it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.91it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.65it/s][A
 10%|▉         | 42/435 [00:00<00:08, 45.59it/s][A
 11%|█         | 47/435 [00:01<00:08, 45.44it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 45.63it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 45.67it/s][A
 14%|█▍        | 62/435 [00:01<00:21, 17.19it/s][A
 15%|█▌        | 67/435 [00:02<00:17, 21.22it/s][A
 17%|█▋        | 72/435 [00:02<00:14, 25.34it/s][A
 18%|█▊        | 77/435 [00:02<00:12, 29.30it/s][A
 19%|█▉        | 82/435 [00:02<00:10, 32.88it/s][A
 20%|██        | 87/435 [00:02<00:09, 36.02it/s][A
 21%|██        | 92/435 [00:02<00:08, 38.55it/s][A
 22%|██▏       | 97/435 [00:02<00:08, 40.47it/s][A
 23%|██▎       | 102/435 [00:02<00:08, 41.52it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 42.30it/s][A
 26%|██▌       | 112/435 [00:03<00:07, 42.98it/s][A
 27%|██▋       | 117/435 [00:03<00:07, 43.58it/s][A
 28%|██▊       | 122/435 [00:03<00:07, 44.18it/s][A
 29%|██▉       | 127/435 [00:03<00:06, 44.64it/s][A
 30%|███       | 132/435 [00:03<00:06, 44.99it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.35it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 45.52it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.44it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.06it/s][A
 36%|███▌      | 157/435 [00:04<00:06, 45.06it/s][A
 37%|███▋      | 162/435 [00:04<00:06, 45.03it/s][A
 38%|███▊      | 167/435 [00:04<00:05, 45.10it/s][A
 40%|███▉      | 172/435 [00:04<00:07, 37.46it/s][A
 41%|████      | 177/435 [00:04<00:06, 39.71it/s][A
 42%|████▏     | 182/435 [00:04<00:06, 41.40it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 42.58it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 43.51it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 44.31it/s][A
 46%|████▋     | 202/435 [00:05<00:05, 44.82it/s][A
 48%|████▊     | 207/435 [00:05<00:05, 45.08it/s][A
 49%|████▊     | 212/435 [00:05<00:04, 44.63it/s][A
 50%|████▉     | 217/435 [00:05<00:04, 44.61it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.58it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.77it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 45.01it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.32it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.52it/s][A
 57%|█████▋    | 247/435 [00:06<00:05, 37.51it/s][A
 58%|█████▊    | 252/435 [00:06<00:04, 40.39it/s][A
 59%|█████▉    | 257/435 [00:06<00:04, 41.93it/s][A
 60%|██████    | 262/435 [00:06<00:04, 43.14it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 43.79it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.28it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.65it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 45.02it/s][A
 66%|██████▌   | 287/435 [00:07<00:03, 45.10it/s][A
 67%|██████▋   | 292/435 [00:07<00:03, 44.72it/s][A
 68%|██████▊   | 297/435 [00:07<00:03, 44.61it/s][A
 69%|██████▉   | 302/435 [00:07<00:07, 16.89it/s][A
 71%|███████   | 307/435 [00:08<00:06, 20.83it/s][A
 72%|███████▏  | 312/435 [00:08<00:04, 24.92it/s][A
 73%|███████▎  | 317/435 [00:08<00:04, 28.86it/s][A
 74%|███████▍  | 322/435 [00:08<00:03, 32.48it/s][A
 75%|███████▌  | 327/435 [00:08<00:03, 35.57it/s][A
 76%|███████▋  | 332/435 [00:08<00:02, 38.10it/s][A
 77%|███████▋  | 337/435 [00:08<00:02, 40.08it/s][A
 79%|███████▊  | 342/435 [00:08<00:02, 41.20it/s][A
 80%|███████▉  | 347/435 [00:08<00:02, 42.07it/s][A
 81%|████████  | 352/435 [00:09<00:01, 43.00it/s][A
 82%|████████▏ | 357/435 [00:09<00:01, 43.70it/s][A
 83%|████████▎ | 362/435 [00:09<00:01, 44.33it/s][A
 84%|████████▍ | 367/435 [00:09<00:01, 44.77it/s][A
 86%|████████▌ | 372/435 [00:09<00:01, 45.11it/s][A
 87%|████████▋ | 377/435 [00:09<00:01, 45.30it/s][A
 88%|████████▊ | 382/435 [00:09<00:01, 45.26it/s][A
 89%|████████▉ | 387/435 [00:09<00:01, 44.96it/s][A
 90%|█████████ | 392/435 [00:09<00:00, 44.80it/s][A
 91%|█████████▏| 397/435 [00:10<00:00, 44.82it/s][A
 92%|█████████▏| 402/435 [00:10<00:00, 44.99it/s][A
 94%|█████████▎| 407/435 [00:10<00:00, 45.16it/s][A
 95%|█████████▍| 412/435 [00:10<00:00, 29.31it/s][A
 96%|█████████▌| 417/435 [00:10<00:00, 32.88it/s][A
 97%|█████████▋| 422/435 [00:10<00:00, 35.91it/s][A
 98%|█████████▊| 427/435 [00:10<00:00, 38.36it/s][A
 99%|█████████▉| 432/435 [00:11<00:00, 40.30it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:11<00:00, 40.30it/s][A100%|██████████| 235/235 [05:38<00:00,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:41:46,071 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 19:41:46,878 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:41:58,871 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:41:59,915 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:00,267 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:42:19,728 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:42:19,883 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47 (score: 1.040338397026062).
                                                 100%|██████████| 235/235 [06:54<00:00,  3.52it/s]100%|██████████| 235/235 [06:54<00:00,  1.76s/it]
[INFO|trainer.py:1894] 2023-08-28 19:43:01,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 19:43:02,610 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:43:15,138 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:43:16,173 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:43:16,330 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:43:18,136 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   train_loss               =      0.446
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   train_runtime            = 0:06:53.94
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   train_samples_per_second =     36.237
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:18,136 >>   train_steps_per_second   =      0.568
{'eval_loss': 1.0787032842636108, 'eval_runtime': 11.1329, 'eval_samples_per_second': 312.498, 'eval_steps_per_second': 39.073, 'epoch': 5.0}
{'train_runtime': 413.9422, 'train_samples_per_second': 36.237, 'train_steps_per_second': 0.568, 'train_loss': 0.445966030688996, 'epoch': 5.0}
08/28/2023 19:43:19 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:43:19,813 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:43:19,813 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:43:19,813 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 56.08it/s]  3%|▎         | 12/435 [00:00<00:08, 50.07it/s]  4%|▍         | 18/435 [00:00<00:08, 47.99it/s]  5%|▌         | 23/435 [00:00<00:08, 47.37it/s]  6%|▋         | 28/435 [00:00<00:08, 46.85it/s]  8%|▊         | 33/435 [00:00<00:08, 46.64it/s]  9%|▊         | 38/435 [00:00<00:08, 46.47it/s] 10%|▉         | 43/435 [00:00<00:08, 45.95it/s] 11%|█         | 48/435 [00:01<00:08, 45.36it/s] 12%|█▏        | 53/435 [00:01<00:08, 45.07it/s] 13%|█▎        | 58/435 [00:01<00:08, 45.14it/s] 14%|█▍        | 63/435 [00:01<00:08, 45.32it/s] 16%|█▌        | 68/435 [00:01<00:08, 45.35it/s] 17%|█▋        | 73/435 [00:01<00:07, 45.52it/s] 18%|█▊        | 78/435 [00:01<00:07, 45.64it/s] 19%|█▉        | 83/435 [00:01<00:07, 45.70it/s] 20%|██        | 88/435 [00:01<00:07, 45.74it/s] 21%|██▏       | 93/435 [00:02<00:07, 45.50it/s] 23%|██▎       | 98/435 [00:02<00:07, 45.34it/s] 24%|██▎       | 103/435 [00:02<00:07, 45.39it/s] 25%|██▍       | 108/435 [00:02<00:07, 45.44it/s] 26%|██▌       | 113/435 [00:02<00:07, 45.63it/s] 27%|██▋       | 118/435 [00:02<00:06, 45.79it/s] 28%|██▊       | 123/435 [00:02<00:06, 45.83it/s] 29%|██▉       | 128/435 [00:02<00:06, 45.93it/s] 31%|███       | 133/435 [00:02<00:06, 45.88it/s] 32%|███▏      | 138/435 [00:02<00:06, 45.84it/s] 33%|███▎      | 143/435 [00:03<00:09, 32.19it/s] 34%|███▍      | 148/435 [00:03<00:08, 35.41it/s] 35%|███▌      | 153/435 [00:03<00:07, 38.07it/s] 36%|███▋      | 158/435 [00:03<00:06, 40.13it/s] 37%|███▋      | 163/435 [00:03<00:06, 41.78it/s] 39%|███▊      | 168/435 [00:03<00:06, 42.92it/s] 40%|███▉      | 173/435 [00:03<00:05, 43.78it/s] 41%|████      | 178/435 [00:04<00:05, 44.44it/s] 42%|████▏     | 183/435 [00:04<00:05, 44.50it/s] 43%|████▎     | 188/435 [00:04<00:05, 44.71it/s] 44%|████▍     | 193/435 [00:04<00:05, 44.97it/s] 46%|████▌     | 198/435 [00:04<00:05, 45.29it/s] 47%|████▋     | 203/435 [00:04<00:05, 45.43it/s] 48%|████▊     | 208/435 [00:04<00:04, 45.65it/s] 49%|████▉     | 213/435 [00:04<00:04, 45.64it/s] 50%|█████     | 218/435 [00:04<00:04, 45.79it/s] 51%|█████▏    | 223/435 [00:05<00:04, 45.74it/s] 52%|█████▏    | 228/435 [00:05<00:04, 45.50it/s] 54%|█████▎    | 233/435 [00:05<00:04, 45.45it/s] 55%|█████▍    | 238/435 [00:05<00:04, 45.41it/s] 56%|█████▌    | 243/435 [00:05<00:04, 45.64it/s] 57%|█████▋    | 248/435 [00:05<00:04, 45.76it/s] 58%|█████▊    | 253/435 [00:05<00:03, 45.84it/s] 59%|█████▉    | 258/435 [00:05<00:03, 45.75it/s] 60%|██████    | 263/435 [00:05<00:03, 45.76it/s] 62%|██████▏   | 268/435 [00:05<00:03, 45.75it/s] 63%|██████▎   | 273/435 [00:06<00:03, 45.58it/s] 64%|██████▍   | 278/435 [00:06<00:05, 29.34it/s] 65%|██████▌   | 283/435 [00:06<00:04, 32.95it/s] 66%|██████▌   | 288/435 [00:06<00:04, 35.99it/s] 67%|██████▋   | 293/435 [00:06<00:03, 38.50it/s] 69%|██████▊   | 298/435 [00:06<00:03, 40.52it/s] 70%|██████▉   | 303/435 [00:06<00:03, 41.99it/s] 71%|███████   | 308/435 [00:07<00:02, 43.12it/s] 72%|███████▏  | 313/435 [00:07<00:02, 43.99it/s] 73%|███████▎  | 318/435 [00:07<00:02, 44.08it/s] 74%|███████▍  | 323/435 [00:07<00:02, 44.43it/s] 75%|███████▌  | 328/435 [00:07<00:02, 44.85it/s] 77%|███████▋  | 333/435 [00:07<00:02, 45.12it/s] 78%|███████▊  | 338/435 [00:07<00:02, 45.38it/s] 79%|███████▉  | 343/435 [00:07<00:02, 45.57it/s] 80%|████████  | 348/435 [00:07<00:01, 45.65it/s] 81%|████████  | 353/435 [00:08<00:01, 45.74it/s] 82%|████████▏ | 358/435 [00:08<00:01, 45.60it/s] 83%|████████▎ | 363/435 [00:08<00:01, 45.45it/s] 85%|████████▍ | 368/435 [00:08<00:01, 45.39it/s] 86%|████████▌ | 373/435 [00:08<00:01, 45.36it/s] 87%|████████▋ | 378/435 [00:08<00:01, 45.54it/s] 88%|████████▊ | 383/435 [00:08<00:01, 45.65it/s] 89%|████████▉ | 388/435 [00:08<00:01, 45.40it/s] 90%|█████████ | 393/435 [00:08<00:00, 45.63it/s] 91%|█████████▏| 398/435 [00:09<00:00, 45.65it/s] 93%|█████████▎| 403/435 [00:09<00:00, 45.66it/s] 94%|█████████▍| 408/435 [00:09<00:00, 36.88it/s] 95%|█████████▍| 413/435 [00:09<00:00, 39.27it/s] 96%|█████████▌| 418/435 [00:09<00:00, 41.11it/s] 97%|█████████▋| 423/435 [00:09<00:00, 42.46it/s] 98%|█████████▊| 428/435 [00:09<00:00, 43.50it/s]100%|█████████▉| 433/435 [00:09<00:00, 44.24it/s]100%|██████████| 435/435 [00:09<00:00, 43.74it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:43:29,779 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   eval_loss               =     1.0403
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   eval_runtime            = 0:00:09.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   eval_samples_per_second =    349.089
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,779 >>   eval_steps_per_second   =     43.649
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:29,780 >>   perplexity              =     2.8302
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:53,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:53,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:53,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:53,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:53,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:43:54,225 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:43:54,226 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:55,223 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:56,278 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:56,278 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:59,033 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:59,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:59,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:59,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:59,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:44:00,379 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:44:00,380 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:44:00,978 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:44:01,151 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:44:01,151 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-141
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-235
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-47
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.33it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:09,  1.38it/s]Extractor Predicting: 15it [00:10,  1.32it/s]Extractor Predicting: 16it [00:11,  1.35it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:12,  1.41it/s]Extractor Predicting: 19it [00:13,  1.34it/s]Extractor Predicting: 20it [00:14,  1.39it/s]Extractor Predicting: 21it [00:14,  1.42it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:16,  1.36it/s]Extractor Predicting: 24it [00:17,  1.39it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.40it/s]Extractor Predicting: 27it [00:19,  1.23it/s]Extractor Predicting: 28it [00:20,  1.32it/s]Extractor Predicting: 29it [00:20,  1.35it/s]Extractor Predicting: 30it [00:21,  1.29it/s]Extractor Predicting: 31it [00:22,  1.34it/s]Extractor Predicting: 32it [00:23,  1.38it/s]Extractor Predicting: 33it [00:23,  1.40it/s]Extractor Predicting: 34it [00:24,  1.40it/s]Extractor Predicting: 35it [00:25,  1.44it/s]Extractor Predicting: 36it [00:25,  1.46it/s]Extractor Predicting: 37it [00:26,  1.46it/s]Extractor Predicting: 38it [00:27,  1.49it/s]Extractor Predicting: 39it [00:27,  1.51it/s]Extractor Predicting: 40it [00:28,  1.50it/s]Extractor Predicting: 41it [00:29,  1.51it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:30,  1.49it/s]Extractor Predicting: 44it [00:31,  1.52it/s]Extractor Predicting: 45it [00:31,  1.43it/s]Extractor Predicting: 46it [00:32,  1.44it/s]Extractor Predicting: 47it [00:33,  1.46it/s]Extractor Predicting: 48it [00:33,  1.49it/s]Extractor Predicting: 49it [00:34,  1.37it/s]Extractor Predicting: 50it [00:35,  1.41it/s]Extractor Predicting: 51it [00:36,  1.44it/s]Extractor Predicting: 52it [00:36,  1.47it/s]Extractor Predicting: 53it [00:37,  1.49it/s]Extractor Predicting: 54it [00:38,  1.49it/s]Extractor Predicting: 55it [00:38,  1.46it/s]Extractor Predicting: 56it [00:39,  1.50it/s]Extractor Predicting: 57it [00:40,  1.15it/s]Extractor Predicting: 58it [00:41,  1.23it/s]Extractor Predicting: 59it [00:42,  1.00it/s]Extractor Predicting: 60it [00:43,  1.14it/s]Extractor Predicting: 61it [00:44,  1.25it/s]Extractor Predicting: 62it [00:44,  1.32it/s]Extractor Predicting: 63it [00:45,  1.30it/s]Extractor Predicting: 64it [00:46,  1.37it/s]Extractor Predicting: 65it [00:46,  1.43it/s]Extractor Predicting: 66it [00:47,  1.46it/s]Extractor Predicting: 67it [00:48,  1.49it/s]Extractor Predicting: 68it [00:48,  1.51it/s]Extractor Predicting: 69it [00:49,  1.57it/s]Extractor Predicting: 70it [00:49,  1.57it/s]Extractor Predicting: 71it [00:51,  1.31it/s]Extractor Predicting: 72it [00:51,  1.36it/s]Extractor Predicting: 73it [00:52,  1.42it/s]Extractor Predicting: 74it [00:53,  1.39it/s]Extractor Predicting: 75it [00:53,  1.41it/s]Extractor Predicting: 76it [00:54,  1.43it/s]Extractor Predicting: 77it [00:55,  1.48it/s]Extractor Predicting: 78it [00:55,  1.50it/s]Extractor Predicting: 79it [00:56,  1.54it/s]Extractor Predicting: 80it [00:56,  1.54it/s]Extractor Predicting: 81it [00:57,  1.54it/s]Extractor Predicting: 82it [00:58,  1.54it/s]Extractor Predicting: 83it [00:59,  1.46it/s]Extractor Predicting: 84it [00:59,  1.49it/s]Extractor Predicting: 85it [01:00,  1.53it/s]Extractor Predicting: 86it [01:00,  1.55it/s]Extractor Predicting: 87it [01:01,  1.59it/s]Extractor Predicting: 88it [01:02,  1.48it/s]Extractor Predicting: 89it [01:02,  1.52it/s]Extractor Predicting: 90it [01:03,  1.55it/s]Extractor Predicting: 91it [01:04,  1.59it/s]Extractor Predicting: 92it [01:04,  1.63it/s]Extractor Predicting: 93it [01:05,  1.56it/s]Extractor Predicting: 94it [01:06,  1.57it/s]Extractor Predicting: 95it [01:06,  1.59it/s]Extractor Predicting: 96it [01:07,  1.60it/s]Extractor Predicting: 97it [01:07,  1.60it/s]Extractor Predicting: 98it [01:08,  1.51it/s]Extractor Predicting: 99it [01:09,  1.52it/s]Extractor Predicting: 100it [01:09,  1.50it/s]Extractor Predicting: 101it [01:10,  1.39it/s]Extractor Predicting: 102it [01:11,  1.51it/s]Extractor Predicting: 103it [01:12,  1.46it/s]Extractor Predicting: 104it [01:12,  1.49it/s]Extractor Predicting: 105it [01:13,  1.52it/s]Extractor Predicting: 106it [01:13,  1.54it/s]Extractor Predicting: 107it [01:14,  1.56it/s]Extractor Predicting: 108it [01:15,  1.29it/s]Extractor Predicting: 109it [01:16,  1.37it/s]Extractor Predicting: 110it [01:16,  1.44it/s]Extractor Predicting: 111it [01:17,  1.48it/s]Extractor Predicting: 112it [01:18,  1.53it/s]Extractor Predicting: 113it [01:19,  1.23it/s]Extractor Predicting: 114it [01:19,  1.34it/s]Extractor Predicting: 115it [01:20,  1.43it/s]Extractor Predicting: 116it [01:21,  1.46it/s]Extractor Predicting: 117it [01:22,  1.34it/s]Extractor Predicting: 118it [01:22,  1.41it/s]Extractor Predicting: 119it [01:23,  1.43it/s]Extractor Predicting: 120it [01:23,  1.48it/s]Extractor Predicting: 121it [01:24,  1.50it/s]Extractor Predicting: 122it [01:25,  1.49it/s]Extractor Predicting: 123it [01:25,  1.52it/s]Extractor Predicting: 124it [01:26,  1.50it/s]Extractor Predicting: 125it [01:27,  1.55it/s]Extractor Predicting: 126it [01:27,  1.52it/s]Extractor Predicting: 127it [01:28,  1.56it/s]Extractor Predicting: 128it [01:29,  1.56it/s]Extractor Predicting: 129it [01:29,  1.53it/s]Extractor Predicting: 130it [01:30,  1.52it/s]Extractor Predicting: 131it [01:31,  1.56it/s]Extractor Predicting: 132it [01:31,  1.54it/s]Extractor Predicting: 133it [01:32,  1.52it/s]Extractor Predicting: 134it [01:33,  1.44it/s]Extractor Predicting: 135it [01:33,  1.49it/s]Extractor Predicting: 136it [01:34,  1.48it/s]Extractor Predicting: 137it [01:35,  1.50it/s]Extractor Predicting: 138it [01:35,  1.47it/s]Extractor Predicting: 139it [01:36,  1.37it/s]Extractor Predicting: 140it [01:37,  1.39it/s]Extractor Predicting: 141it [01:38,  1.44it/s]Extractor Predicting: 142it [01:38,  1.45it/s]Extractor Predicting: 143it [01:39,  1.48it/s]Extractor Predicting: 144it [01:39,  1.80it/s]Extractor Predicting: 144it [01:39,  1.44it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:16,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:16,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:16,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:16,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:16,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:46:17,931 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:46:17,932 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:18,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:20,152 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:20,211 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,121 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:46:25,467 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:46:25,469 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:26,858 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:27,675 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:27,723 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.16905660377358492,
  "recall": 0.06438631790744467,
  "score": 0.093255620316403,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.62it/s]Extractor Predicting: 12it [00:08,  1.30it/s]Extractor Predicting: 13it [00:08,  1.37it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.41it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.57it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:19,  1.59it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:20,  1.60it/s]Extractor Predicting: 32it [00:21,  1.29it/s]Extractor Predicting: 33it [00:22,  1.37it/s]Extractor Predicting: 34it [00:22,  1.42it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.43it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.56it/s]Extractor Predicting: 48it [00:32,  1.23it/s]Extractor Predicting: 49it [00:32,  1.35it/s]Extractor Predicting: 50it [00:33,  1.41it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:34,  1.42it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.56it/s]Extractor Predicting: 57it [00:38,  1.43it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:40,  1.51it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.56it/s]Extractor Predicting: 64it [00:42,  1.59it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:44,  1.55it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:46,  1.49it/s]Extractor Predicting: 70it [00:46,  1.52it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.36it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:49,  1.47it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.51it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:52,  1.57it/s]Extractor Predicting: 80it [00:53,  1.61it/s]Extractor Predicting: 81it [00:53,  1.61it/s]Extractor Predicting: 82it [00:54,  1.51it/s]Extractor Predicting: 83it [00:55,  1.52it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.28it/s]Extractor Predicting: 88it [00:59,  1.35it/s]Extractor Predicting: 89it [00:59,  1.38it/s]Extractor Predicting: 90it [01:00,  1.42it/s]Extractor Predicting: 91it [01:01,  1.31it/s]Extractor Predicting: 92it [01:01,  1.39it/s]Extractor Predicting: 93it [01:02,  1.44it/s]Extractor Predicting: 94it [01:03,  1.39it/s]Extractor Predicting: 95it [01:03,  1.45it/s]Extractor Predicting: 96it [01:04,  1.44it/s]Extractor Predicting: 97it [01:05,  1.45it/s]Extractor Predicting: 98it [01:06,  1.26it/s]Extractor Predicting: 99it [01:06,  1.34it/s]Extractor Predicting: 100it [01:07,  1.40it/s]Extractor Predicting: 101it [01:08,  1.47it/s]Extractor Predicting: 102it [01:08,  1.47it/s]Extractor Predicting: 103it [01:09,  1.34it/s]Extractor Predicting: 104it [01:10,  1.43it/s]Extractor Predicting: 105it [01:11,  1.32it/s]Extractor Predicting: 106it [01:11,  1.38it/s]Extractor Predicting: 107it [01:12,  1.42it/s]Extractor Predicting: 108it [01:13,  1.44it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:14,  1.47it/s]Extractor Predicting: 111it [01:15,  1.50it/s]Extractor Predicting: 112it [01:15,  1.46it/s]Extractor Predicting: 113it [01:16,  1.50it/s]Extractor Predicting: 114it [01:17,  1.52it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:18,  1.51it/s]Extractor Predicting: 117it [01:19,  1.37it/s]Extractor Predicting: 118it [01:20,  1.40it/s]Extractor Predicting: 119it [01:20,  1.44it/s]Extractor Predicting: 120it [01:21,  1.49it/s]Extractor Predicting: 121it [01:22,  1.52it/s]Extractor Predicting: 122it [01:22,  1.48it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:24,  1.53it/s]Extractor Predicting: 125it [01:24,  1.56it/s]Extractor Predicting: 126it [01:25,  1.57it/s]Extractor Predicting: 127it [01:26,  1.51it/s]Extractor Predicting: 128it [01:26,  1.56it/s]Extractor Predicting: 129it [01:27,  1.58it/s]Extractor Predicting: 130it [01:27,  1.56it/s]Extractor Predicting: 131it [01:28,  1.61it/s]Extractor Predicting: 132it [01:29,  1.51it/s]Extractor Predicting: 133it [01:29,  1.56it/s]Extractor Predicting: 134it [01:30,  1.54it/s]Extractor Predicting: 135it [01:31,  1.59it/s]Extractor Predicting: 136it [01:31,  1.61it/s]Extractor Predicting: 137it [01:32,  1.47it/s]Extractor Predicting: 138it [01:33,  1.50it/s]Extractor Predicting: 139it [01:34,  1.27it/s]Extractor Predicting: 140it [01:34,  1.36it/s]Extractor Predicting: 141it [01:35,  1.40it/s]Extractor Predicting: 142it [01:36,  1.45it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:37,  1.49it/s]Extractor Predicting: 145it [01:37,  1.57it/s]Extractor Predicting: 146it [01:38,  1.61it/s]Extractor Predicting: 147it [01:39,  1.63it/s]Extractor Predicting: 148it [01:39,  1.60it/s]Extractor Predicting: 149it [01:40,  1.62it/s]Extractor Predicting: 150it [01:40,  1.65it/s]Extractor Predicting: 151it [01:41,  1.66it/s]Extractor Predicting: 152it [01:42,  1.69it/s]Extractor Predicting: 153it [01:42,  1.67it/s]Extractor Predicting: 154it [01:43,  1.48it/s]Extractor Predicting: 155it [01:44,  1.58it/s]Extractor Predicting: 156it [01:44,  1.61it/s]Extractor Predicting: 157it [01:45,  1.72it/s]Extractor Predicting: 158it [01:45,  1.76it/s]Extractor Predicting: 159it [01:46,  1.65it/s]Extractor Predicting: 160it [01:47,  1.64it/s]Extractor Predicting: 161it [01:47,  1.64it/s]Extractor Predicting: 162it [01:48,  1.68it/s]Extractor Predicting: 163it [01:48,  1.72it/s]Extractor Predicting: 164it [01:49,  1.66it/s]Extractor Predicting: 165it [01:49,  1.69it/s]Extractor Predicting: 166it [01:50,  1.67it/s]Extractor Predicting: 167it [01:51,  1.72it/s]Extractor Predicting: 168it [01:51,  1.70it/s]Extractor Predicting: 169it [01:52,  1.77it/s]Extractor Predicting: 170it [01:52,  1.65it/s]Extractor Predicting: 171it [01:53,  1.68it/s]Extractor Predicting: 172it [01:54,  1.64it/s]Extractor Predicting: 173it [01:54,  1.63it/s]Extractor Predicting: 174it [01:55,  1.60it/s]Extractor Predicting: 175it [01:56,  1.49it/s]Extractor Predicting: 176it [01:56,  1.52it/s]Extractor Predicting: 177it [01:57,  1.54it/s]Extractor Predicting: 178it [01:58,  1.53it/s]Extractor Predicting: 179it [01:58,  1.53it/s]Extractor Predicting: 180it [01:59,  1.49it/s]Extractor Predicting: 181it [02:00,  1.52it/s]Extractor Predicting: 182it [02:00,  1.52it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:02,  1.53it/s]Extractor Predicting: 185it [02:03,  1.25it/s]Extractor Predicting: 186it [02:03,  1.32it/s]Extractor Predicting: 187it [02:04,  1.38it/s]Extractor Predicting: 188it [02:05,  1.43it/s]Extractor Predicting: 189it [02:05,  1.46it/s]Extractor Predicting: 190it [02:06,  1.50it/s]Extractor Predicting: 191it [02:07,  1.48it/s]Extractor Predicting: 192it [02:07,  1.44it/s]Extractor Predicting: 193it [02:08,  1.46it/s]Extractor Predicting: 194it [02:09,  1.47it/s]Extractor Predicting: 195it [02:09,  1.50it/s]Extractor Predicting: 196it [02:10,  1.51it/s]Extractor Predicting: 197it [02:11,  1.38it/s]Extractor Predicting: 198it [02:11,  1.44it/s]Extractor Predicting: 199it [02:12,  1.49it/s]Extractor Predicting: 200it [02:13,  1.48it/s]Extractor Predicting: 201it [02:13,  1.47it/s]Extractor Predicting: 202it [02:14,  1.49it/s]Extractor Predicting: 203it [02:15,  1.50it/s]Extractor Predicting: 204it [02:15,  1.54it/s]Extractor Predicting: 205it [02:16,  1.54it/s]Extractor Predicting: 206it [02:17,  1.55it/s]Extractor Predicting: 207it [02:17,  1.45it/s]Extractor Predicting: 208it [02:18,  1.47it/s]Extractor Predicting: 209it [02:19,  1.44it/s]Extractor Predicting: 210it [02:20,  1.33it/s]Extractor Predicting: 211it [02:20,  1.36it/s]Extractor Predicting: 212it [02:21,  1.39it/s]Extractor Predicting: 213it [02:22,  1.43it/s]Extractor Predicting: 214it [02:22,  1.43it/s]Extractor Predicting: 215it [02:23,  1.48it/s]Extractor Predicting: 216it [02:24,  1.42it/s]Extractor Predicting: 217it [02:25,  1.45it/s]Extractor Predicting: 218it [02:25,  1.48it/s]Extractor Predicting: 219it [02:26,  1.47it/s]Extractor Predicting: 220it [02:27,  1.49it/s]Extractor Predicting: 221it [02:27,  1.31it/s]Extractor Predicting: 222it [02:28,  1.35it/s]Extractor Predicting: 223it [02:29,  1.42it/s]Extractor Predicting: 224it [02:29,  1.44it/s]Extractor Predicting: 225it [02:30,  1.47it/s]Extractor Predicting: 226it [02:31,  1.40it/s]Extractor Predicting: 227it [02:32,  1.47it/s]Extractor Predicting: 228it [02:32,  1.48it/s]Extractor Predicting: 229it [02:33,  1.49it/s]Extractor Predicting: 230it [02:33,  1.56it/s]Extractor Predicting: 231it [02:34,  1.52it/s]Extractor Predicting: 232it [02:35,  1.57it/s]Extractor Predicting: 233it [02:35,  1.54it/s]Extractor Predicting: 234it [02:36,  1.56it/s]Extractor Predicting: 235it [02:37,  1.56it/s]Extractor Predicting: 236it [02:37,  1.55it/s]Extractor Predicting: 237it [02:38,  1.46it/s]Extractor Predicting: 238it [02:39,  1.50it/s]Extractor Predicting: 239it [02:39,  1.49it/s]Extractor Predicting: 240it [02:40,  1.52it/s]Extractor Predicting: 241it [02:41,  1.47it/s]Extractor Predicting: 242it [02:41,  1.43it/s]Extractor Predicting: 243it [02:42,  1.45it/s]Extractor Predicting: 244it [02:43,  1.48it/s]Extractor Predicting: 245it [02:43,  1.52it/s]Extractor Predicting: 246it [02:44,  1.52it/s]Extractor Predicting: 247it [02:45,  1.36it/s]Extractor Predicting: 248it [02:46,  1.41it/s]Extractor Predicting: 249it [02:46,  1.44it/s]Extractor Predicting: 250it [02:47,  1.49it/s]Extractor Predicting: 251it [02:48,  1.51it/s]Extractor Predicting: 252it [02:48,  1.43it/s]Extractor Predicting: 253it [02:49,  1.44it/s]Extractor Predicting: 254it [02:50,  1.49it/s]Extractor Predicting: 255it [02:50,  1.47it/s]Extractor Predicting: 256it [02:51,  1.44it/s]Extractor Predicting: 257it [02:52,  1.29it/s]Extractor Predicting: 258it [02:53,  1.34it/s]Extractor Predicting: 259it [02:53,  1.40it/s]Extractor Predicting: 260it [02:54,  1.41it/s]Extractor Predicting: 261it [02:55,  1.47it/s]Extractor Predicting: 262it [02:55,  1.40it/s]Extractor Predicting: 263it [02:56,  1.42it/s]Extractor Predicting: 264it [02:57,  1.44it/s]Extractor Predicting: 265it [02:58,  1.41it/s]Extractor Predicting: 266it [02:58,  1.42it/s]Extractor Predicting: 267it [02:59,  1.16it/s]Extractor Predicting: 268it [03:00,  1.22it/s]Extractor Predicting: 269it [03:01,  1.31it/s]Extractor Predicting: 270it [03:01,  1.37it/s]Extractor Predicting: 271it [03:02,  1.33it/s]Extractor Predicting: 272it [03:03,  1.38it/s]Extractor Predicting: 273it [03:04,  1.42it/s]Extractor Predicting: 274it [03:04,  1.46it/s]Extractor Predicting: 275it [03:05,  1.49it/s]Extractor Predicting: 276it [03:06,  1.17it/s]Extractor Predicting: 277it [03:07,  1.27it/s]Extractor Predicting: 278it [03:07,  1.34it/s]Extractor Predicting: 279it [03:08,  1.40it/s]Extractor Predicting: 280it [03:09,  1.33it/s]Extractor Predicting: 281it [03:10,  1.38it/s]Extractor Predicting: 282it [03:10,  1.50it/s]Extractor Predicting: 282it [03:10,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:10,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:10,313 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:10,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:10,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:10,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:50:12,133 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:50:12,134 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:50:13,240 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:50:14,590 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:50:14,590 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:19,358 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:19,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:19,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:19,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:19,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:50:20,919 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:50:21,116 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:50:22,020 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:50:22,392 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:50:22,393 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20069629326233873,
  "recall": 0.14497041420118342,
  "score": 0.16834149274242036,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 5it [00:03,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:50:33,985 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:50:33,986 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:50:34,311 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:50:34,312 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:50:34,409 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:51:23,889 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:51:24,226 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:51:25,194 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:51:25,195 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:51:25,483 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,577 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,578 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:51:25,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.21333333333333335,
  "recall": 0.06666666666666667,
  "score": 0.10158730158730159,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:51:26,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:27,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:28,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:28,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:29,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:29,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:30,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:31,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:31,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:32,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:32,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:33,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:33,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:34,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:35,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:35,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:36,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:36,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:37,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:38,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:38,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:39,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:39,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:08, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:40,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:41,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:41,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:42,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:42,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:43,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:44,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:44,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:45,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:46,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:46,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:47,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:48,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:48,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:49,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:49,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:50,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:51,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:52,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:52,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:53,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:56, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 19:51:54,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:54,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:55,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:55,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:56,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:57,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:57,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:58,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:58,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:59,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:51:59,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:00,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:01,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:01,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:02,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:03,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:03,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:04,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:04,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:05,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:06,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:06,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:40, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:07,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:08,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:09,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:09,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:10,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:10,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:11,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:12,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:12,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:13,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:14,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:14,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:15,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:15,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:16,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:17,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:17,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:18,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:18,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:23, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:19,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:20,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:21,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:21,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:22,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:22,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:23,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:23,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:24,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:25,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:26,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:27,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:28,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:28,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:29,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:30,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:30,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:31,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:32,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:32,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:33,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:15, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:34,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:35,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:35,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:36,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:37,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:38,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:38,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:39,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:40,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:41,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:42,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:42,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:43,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:44,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:44,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:45,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:46,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:46,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:47,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:21<02:03, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 19:52:48,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:48,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:49,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:50,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:50,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:51,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:52,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:53,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:53,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:54,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:55,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:55,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:56,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:57,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:57,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:58,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:58,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:59,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:52:59,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:00,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:01,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:34<01:49, 13.63s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:01,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:02,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:03,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:03,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:04,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:04,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:05,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:06,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:07,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:08,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:08,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:09,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:10,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:10,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:11,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:12,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:12,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:13,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:37, 13.87s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:16,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:16,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:23,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:24,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:24,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:25,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:26,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:26,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:27,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:27,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:01<01:20, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:28,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:30,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:36,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:36,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:38,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:39,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:06, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:41,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:51,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:51,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:52,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:54,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:29<00:54, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:56,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:57,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:00,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:03,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:42<00:40, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:09,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:11,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:11,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:13,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:13,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:15,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:18,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:18,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:21,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:55<00:26, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:22,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:27,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:31,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:35,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:09<00:13, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:36,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:39,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:42,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:42,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:44,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:49,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:49,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:23<00:00, 13.90s/it]Generating: 100%|██████████| 15/15 [03:23<00:00, 13.60s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:13,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:13,237 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:13,237 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:13,237 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:13,237 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:55:15,285 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:55:15,287 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:55:16,108 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:55:17,429 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:55:17,429 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:21,064 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:55:21,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:55:22,713 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:55:22,715 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:55:23,633 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:55:23,941 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:55:23,941 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : On 31 March 2014 , the band announced that they would be releasing their fourth studio album , " I Am the World " , on 21 March 2014 . Head Entity : I Am the World , Tail Entity : United States .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : country .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 8674
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8774, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:04,  1.38it/s]Extractor Estimating: 7it [00:04,  1.44it/s]Extractor Estimating: 8it [00:05,  1.48it/s]Extractor Estimating: 9it [00:06,  1.52it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:07,  1.61it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.66it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:09,  1.53it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:11,  1.42it/s]Extractor Estimating: 18it [00:11,  1.49it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.53it/s]Extractor Estimating: 21it [00:14,  1.41it/s]Extractor Estimating: 22it [00:14,  1.50it/s]Extractor Estimating: 23it [00:15,  1.55it/s]Extractor Estimating: 24it [00:15,  1.58it/s]Extractor Estimating: 25it [00:16,  1.46it/s]Extractor Estimating: 26it [00:17,  1.44it/s]Extractor Estimating: 27it [00:17,  1.45it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:19,  1.33it/s]Extractor Estimating: 30it [00:20,  1.38it/s]Extractor Estimating: 31it [00:20,  1.49it/s]Extractor Estimating: 32it [00:22,  1.07it/s]Extractor Estimating: 33it [00:22,  1.19it/s]Extractor Estimating: 34it [00:23,  1.28it/s]Extractor Estimating: 35it [00:24,  1.37it/s]Extractor Estimating: 36it [00:25,  1.26it/s]Extractor Estimating: 37it [00:25,  1.35it/s]Extractor Estimating: 38it [00:26,  1.37it/s]Extractor Estimating: 39it [00:27,  1.38it/s]Extractor Estimating: 40it [00:28,  1.28it/s]Extractor Estimating: 41it [00:28,  1.39it/s]Extractor Estimating: 42it [00:29,  1.43it/s]Extractor Estimating: 43it [00:29,  1.45it/s]Extractor Estimating: 44it [00:30,  1.43it/s]Extractor Estimating: 45it [00:31,  1.47it/s]Extractor Estimating: 46it [00:31,  1.52it/s]Extractor Estimating: 47it [00:32,  1.42it/s]Extractor Estimating: 48it [00:33,  1.44it/s]Extractor Estimating: 49it [00:34,  1.46it/s]Extractor Estimating: 50it [00:34,  1.51it/s]Extractor Estimating: 51it [00:35,  1.54it/s]Extractor Estimating: 52it [00:36,  1.26it/s]Extractor Estimating: 53it [00:37,  1.37it/s]Extractor Estimating: 54it [00:37,  1.43it/s]Extractor Estimating: 55it [00:38,  1.49it/s]Extractor Estimating: 56it [00:38,  1.55it/s]Extractor Estimating: 57it [00:39,  1.58it/s]Extractor Estimating: 58it [00:40,  1.56it/s]Extractor Estimating: 59it [00:40,  1.63it/s]Extractor Estimating: 60it [00:41,  1.57it/s]Extractor Estimating: 61it [00:41,  1.62it/s]Extractor Estimating: 62it [00:42,  1.67it/s]Extractor Estimating: 63it [00:43,  1.64it/s]Extractor Estimating: 64it [00:43,  1.61it/s]Extractor Estimating: 65it [00:44,  1.64it/s]Extractor Estimating: 66it [00:44,  1.61it/s]Extractor Estimating: 67it [00:45,  1.61it/s]Extractor Estimating: 68it [00:46,  1.62it/s]Extractor Estimating: 69it [00:46,  1.50it/s]Extractor Estimating: 70it [00:47,  1.55it/s]Extractor Estimating: 71it [00:48,  1.54it/s]Extractor Estimating: 72it [00:48,  1.53it/s]Extractor Estimating: 73it [00:49,  1.59it/s]Extractor Estimating: 74it [00:50,  1.64it/s]Extractor Estimating: 75it [00:50,  1.64it/s]Extractor Estimating: 76it [00:51,  1.67it/s]Extractor Estimating: 77it [00:51,  1.63it/s]Extractor Estimating: 78it [00:52,  1.60it/s]Extractor Estimating: 79it [00:53,  1.59it/s]Extractor Estimating: 80it [00:53,  1.62it/s]Extractor Estimating: 81it [00:54,  1.57it/s]Extractor Estimating: 82it [00:55,  1.61it/s]Extractor Estimating: 83it [00:55,  1.63it/s]Extractor Estimating: 84it [00:56,  1.60it/s]Extractor Estimating: 85it [00:56,  1.63it/s]Extractor Estimating: 86it [00:57,  1.58it/s]Extractor Estimating: 87it [00:58,  1.63it/s]Extractor Estimating: 88it [00:58,  1.64it/s]Extractor Estimating: 89it [00:59,  1.65it/s]Extractor Estimating: 90it [00:59,  1.58it/s]Extractor Estimating: 91it [01:00,  1.57it/s]Extractor Estimating: 92it [01:01,  1.59it/s]Extractor Estimating: 93it [01:01,  1.62it/s]Extractor Estimating: 94it [01:02,  1.61it/s]Extractor Estimating: 95it [01:03,  1.61it/s]Extractor Estimating: 96it [01:03,  1.59it/s]Extractor Estimating: 97it [01:04,  1.55it/s]Extractor Estimating: 98it [01:04,  1.61it/s]Extractor Estimating: 99it [01:05,  1.58it/s]Extractor Estimating: 100it [01:06,  1.60it/s]Extractor Estimating: 101it [01:06,  1.64it/s]Extractor Estimating: 102it [01:07,  1.63it/s]Extractor Estimating: 103it [01:08,  1.61it/s]Extractor Estimating: 104it [01:08,  1.48it/s]Extractor Estimating: 105it [01:09,  1.57it/s]Extractor Estimating: 106it [01:10,  1.61it/s]Extractor Estimating: 107it [01:10,  1.65it/s]Extractor Estimating: 108it [01:11,  1.67it/s]Extractor Estimating: 109it [01:11,  1.64it/s]Extractor Estimating: 110it [01:12,  1.68it/s]Extractor Estimating: 111it [01:13,  1.41it/s]Extractor Estimating: 112it [01:13,  1.45it/s]Extractor Estimating: 113it [01:14,  1.55it/s]Extractor Estimating: 114it [01:15,  1.55it/s]Extractor Estimating: 115it [01:15,  1.53it/s]Extractor Estimating: 116it [01:16,  1.59it/s]Extractor Estimating: 117it [01:17,  1.62it/s]Extractor Estimating: 118it [01:17,  1.67it/s]Extractor Estimating: 119it [01:18,  1.51it/s]Extractor Estimating: 120it [01:18,  1.59it/s]Extractor Estimating: 121it [01:19,  1.58it/s]Extractor Estimating: 122it [01:20,  1.62it/s]Extractor Estimating: 123it [01:20,  1.68it/s]Extractor Estimating: 124it [01:22,  1.08it/s]Extractor Estimating: 125it [01:22,  1.25it/s]Extractor Estimating: 126it [01:23,  1.42it/s]Extractor Estimating: 127it [01:23,  1.55it/s]Extractor Estimating: 128it [01:24,  1.46it/s]Extractor Estimating: 129it [01:25,  1.61it/s]Extractor Estimating: 130it [01:25,  1.69it/s]Extractor Estimating: 131it [01:26,  1.69it/s]Extractor Estimating: 132it [01:26,  1.76it/s]Extractor Estimating: 133it [01:27,  1.79it/s]Extractor Estimating: 134it [01:28,  1.33it/s]Extractor Estimating: 135it [01:29,  1.45it/s]Extractor Estimating: 136it [01:29,  1.49it/s]Extractor Estimating: 137it [01:30,  1.58it/s]Extractor Estimating: 138it [01:30,  1.69it/s]Extractor Estimating: 139it [01:31,  1.74it/s]Extractor Estimating: 140it [01:31,  1.77it/s]Extractor Estimating: 141it [01:32,  1.80it/s]Extractor Estimating: 142it [01:32,  1.81it/s]Extractor Estimating: 143it [01:33,  1.77it/s]Extractor Estimating: 144it [01:33,  1.79it/s]Extractor Estimating: 145it [01:34,  1.78it/s]Extractor Estimating: 146it [01:35,  1.77it/s]Extractor Estimating: 147it [01:35,  1.81it/s]Extractor Estimating: 148it [01:36,  1.81it/s]Extractor Estimating: 149it [01:37,  1.55it/s]Extractor Estimating: 150it [01:37,  1.63it/s]Extractor Estimating: 151it [01:38,  1.65it/s]Extractor Estimating: 152it [01:38,  1.72it/s]Extractor Estimating: 153it [01:39,  1.66it/s]Extractor Estimating: 154it [01:40,  1.56it/s]Extractor Estimating: 155it [01:40,  1.61it/s]Extractor Estimating: 156it [01:41,  1.62it/s]Extractor Estimating: 157it [01:41,  1.69it/s]Extractor Estimating: 158it [01:42,  1.75it/s]Extractor Estimating: 159it [01:42,  1.70it/s]Extractor Estimating: 160it [01:43,  1.69it/s]Extractor Estimating: 161it [01:44,  1.36it/s]Extractor Estimating: 162it [01:45,  1.44it/s]Extractor Estimating: 163it [01:46,  1.11it/s]Extractor Estimating: 164it [01:47,  1.25it/s]Extractor Estimating: 165it [01:47,  1.38it/s]Extractor Estimating: 166it [01:48,  1.46it/s]Extractor Estimating: 167it [01:49,  1.38it/s]Extractor Estimating: 168it [01:49,  1.49it/s]Extractor Estimating: 169it [01:50,  1.57it/s]Extractor Estimating: 170it [01:50,  1.61it/s]Extractor Estimating: 171it [01:51,  1.68it/s]Extractor Estimating: 172it [01:51,  1.72it/s]Extractor Estimating: 173it [01:52,  1.67it/s]Extractor Estimating: 174it [01:53,  1.67it/s]Extractor Estimating: 175it [01:53,  1.73it/s]Extractor Estimating: 176it [01:54,  1.71it/s]Extractor Estimating: 177it [01:55,  1.51it/s]Extractor Estimating: 178it [01:55,  1.58it/s]Extractor Estimating: 179it [01:56,  1.64it/s]Extractor Estimating: 180it [01:56,  1.67it/s]Extractor Estimating: 181it [01:57,  1.48it/s]Extractor Estimating: 182it [01:58,  1.54it/s]Extractor Estimating: 183it [01:58,  1.58it/s]Extractor Estimating: 184it [01:59,  1.65it/s]Extractor Estimating: 185it [02:00,  1.56it/s]Extractor Estimating: 186it [02:00,  1.62it/s]Extractor Estimating: 187it [02:01,  1.55it/s]Extractor Estimating: 188it [02:01,  1.66it/s]Extractor Estimating: 189it [02:02,  1.62it/s]Extractor Estimating: 190it [02:03,  1.64it/s]Extractor Estimating: 191it [02:03,  1.65it/s]Extractor Estimating: 192it [02:04,  1.66it/s]Extractor Estimating: 193it [02:04,  1.64it/s]Extractor Estimating: 194it [02:05,  1.47it/s]Extractor Estimating: 195it [02:06,  1.52it/s]Extractor Estimating: 196it [02:06,  1.58it/s]Extractor Estimating: 197it [02:07,  1.36it/s]Extractor Estimating: 198it [02:08,  1.42it/s]Extractor Estimating: 199it [02:09,  1.49it/s]Extractor Estimating: 200it [02:09,  1.54it/s]Extractor Estimating: 201it [02:10,  1.38it/s]Extractor Estimating: 202it [02:11,  1.39it/s]Extractor Estimating: 203it [02:12,  1.43it/s]Extractor Estimating: 204it [02:12,  1.46it/s]Extractor Estimating: 205it [02:13,  1.48it/s]Extractor Estimating: 206it [02:14,  1.48it/s]Extractor Estimating: 207it [02:14,  1.52it/s]Extractor Estimating: 208it [02:15,  1.54it/s]Extractor Estimating: 209it [02:16,  1.43it/s]Extractor Estimating: 210it [02:16,  1.50it/s]Extractor Estimating: 211it [02:17,  1.54it/s]Extractor Estimating: 212it [02:17,  1.57it/s]Extractor Estimating: 213it [02:18,  1.47it/s]Extractor Estimating: 214it [02:19,  1.50it/s]Extractor Estimating: 215it [02:19,  1.53it/s]Extractor Estimating: 216it [02:20,  1.52it/s]Extractor Estimating: 217it [02:21,  1.48it/s]Extractor Estimating: 218it [02:22,  1.47it/s]Extractor Estimating: 219it [02:22,  1.52it/s]Extractor Estimating: 220it [02:23,  1.54it/s]Extractor Estimating: 221it [02:24,  1.15it/s]Extractor Estimating: 222it [02:25,  1.28it/s]Extractor Estimating: 223it [02:25,  1.37it/s]Extractor Estimating: 224it [02:26,  1.37it/s]Extractor Estimating: 225it [02:27,  1.41it/s]Extractor Estimating: 226it [02:27,  1.52it/s]Extractor Estimating: 227it [02:28,  1.58it/s]Extractor Estimating: 228it [02:28,  1.64it/s]Extractor Estimating: 229it [02:30,  1.23it/s]Extractor Estimating: 230it [02:30,  1.37it/s]Extractor Estimating: 231it [02:31,  1.50it/s]Extractor Estimating: 232it [02:31,  1.59it/s]Extractor Estimating: 233it [02:32,  1.65it/s]Extractor Estimating: 234it [02:32,  1.70it/s]Extractor Estimating: 235it [02:33,  1.72it/s]Extractor Estimating: 236it [02:33,  1.77it/s]Extractor Estimating: 237it [02:34,  1.82it/s]Extractor Estimating: 238it [02:35,  1.55it/s]Extractor Estimating: 239it [02:35,  1.63it/s]Extractor Estimating: 240it [02:36,  1.69it/s]Extractor Estimating: 241it [02:36,  1.76it/s]Extractor Estimating: 242it [02:37,  1.79it/s]Extractor Estimating: 243it [02:38,  1.36it/s]Extractor Estimating: 244it [02:39,  1.46it/s]Extractor Estimating: 245it [02:39,  1.58it/s]Extractor Estimating: 246it [02:40,  1.30it/s]Extractor Estimating: 247it [02:41,  1.45it/s]Extractor Estimating: 248it [02:41,  1.55it/s]Extractor Estimating: 249it [02:42,  1.63it/s]Extractor Estimating: 250it [02:42,  1.61it/s]Extractor Estimating: 251it [02:43,  1.44it/s]Extractor Estimating: 252it [02:44,  1.52it/s]Extractor Estimating: 253it [02:45,  1.58it/s]Extractor Estimating: 254it [02:46,  1.26it/s]Extractor Estimating: 255it [02:46,  1.36it/s]Extractor Estimating: 256it [02:47,  1.48it/s]Extractor Estimating: 257it [02:47,  1.55it/s]Extractor Estimating: 258it [02:48,  1.57it/s]Extractor Estimating: 259it [02:49,  1.59it/s]Extractor Estimating: 260it [02:49,  1.62it/s]Extractor Estimating: 261it [02:50,  1.63it/s]Extractor Estimating: 262it [02:50,  1.67it/s]Extractor Estimating: 263it [02:51,  1.69it/s]Extractor Estimating: 264it [02:51,  1.74it/s]Extractor Estimating: 265it [02:52,  1.66it/s]Extractor Estimating: 266it [02:53,  1.63it/s]Extractor Estimating: 267it [02:53,  1.66it/s]Extractor Estimating: 268it [02:54,  1.61it/s]Extractor Estimating: 269it [02:55,  1.66it/s]Extractor Estimating: 270it [02:55,  1.65it/s]Extractor Estimating: 271it [02:56,  1.63it/s]Extractor Estimating: 272it [02:56,  1.64it/s]Extractor Estimating: 273it [02:57,  1.64it/s]Extractor Estimating: 274it [02:58,  1.49it/s]Extractor Estimating: 275it [02:58,  1.58it/s]Extractor Estimating: 276it [02:59,  1.54it/s]Extractor Estimating: 277it [03:00,  1.58it/s]Extractor Estimating: 278it [03:00,  1.60it/s]Extractor Estimating: 279it [03:01,  1.63it/s]Extractor Estimating: 280it [03:01,  1.68it/s]Extractor Estimating: 281it [03:02,  1.66it/s]Extractor Estimating: 282it [03:03,  1.71it/s]Extractor Estimating: 283it [03:03,  1.64it/s]Extractor Estimating: 284it [03:04,  1.66it/s]Extractor Estimating: 285it [03:05,  1.43it/s]Extractor Estimating: 286it [03:05,  1.52it/s]Extractor Estimating: 287it [03:06,  1.60it/s]Extractor Estimating: 288it [03:06,  1.65it/s]Extractor Estimating: 289it [03:07,  1.65it/s]Extractor Estimating: 290it [03:08,  1.63it/s]Extractor Estimating: 291it [03:08,  1.70it/s]Extractor Estimating: 292it [03:09,  1.68it/s]Extractor Estimating: 293it [03:10,  1.57it/s]Extractor Estimating: 294it [03:10,  1.64it/s]Extractor Estimating: 295it [03:11,  1.61it/s]Extractor Estimating: 296it [03:11,  1.64it/s]Extractor Estimating: 297it [03:12,  1.67it/s]Extractor Estimating: 298it [03:13,  1.66it/s]Extractor Estimating: 299it [03:13,  1.71it/s]Extractor Estimating: 300it [03:14,  1.74it/s]Extractor Estimating: 301it [03:14,  1.69it/s]Extractor Estimating: 302it [03:15,  1.70it/s]Extractor Estimating: 303it [03:15,  1.71it/s]Extractor Estimating: 304it [03:16,  1.67it/s]Extractor Estimating: 305it [03:17,  1.66it/s]Extractor Estimating: 306it [03:17,  1.74it/s]Extractor Estimating: 307it [03:18,  1.81it/s]Extractor Estimating: 308it [03:18,  1.83it/s]Extractor Estimating: 309it [03:19,  1.88it/s]Extractor Estimating: 310it [03:19,  1.84it/s]Extractor Estimating: 311it [03:20,  1.73it/s]Extractor Estimating: 312it [03:20,  1.74it/s]Extractor Estimating: 313it [03:21,  1.79it/s]Extractor Estimating: 314it [03:22,  1.76it/s]Extractor Estimating: 315it [03:22,  1.66it/s]Extractor Estimating: 316it [03:23,  1.75it/s]Extractor Estimating: 317it [03:23,  1.82it/s]Extractor Estimating: 318it [03:24,  1.76it/s]Extractor Estimating: 319it [03:24,  1.77it/s]Extractor Estimating: 320it [03:25,  1.43it/s]Extractor Estimating: 321it [03:26,  1.42it/s]Extractor Estimating: 322it [03:27,  1.50it/s]Extractor Estimating: 323it [03:27,  1.47it/s]Extractor Estimating: 324it [03:28,  1.49it/s]Extractor Estimating: 325it [03:29,  1.54it/s]Extractor Estimating: 326it [03:29,  1.59it/s]Extractor Estimating: 327it [03:30,  1.53it/s]Extractor Estimating: 328it [03:31,  1.55it/s]Extractor Estimating: 329it [03:31,  1.60it/s]Extractor Estimating: 330it [03:32,  1.54it/s]Extractor Estimating: 331it [03:33,  1.58it/s]Extractor Estimating: 332it [03:33,  1.59it/s]Extractor Estimating: 333it [03:34,  1.64it/s]Extractor Estimating: 334it [03:34,  1.61it/s]Extractor Estimating: 335it [03:35,  1.60it/s]Extractor Estimating: 336it [03:36,  1.63it/s]Extractor Estimating: 337it [03:36,  1.62it/s]Extractor Estimating: 338it [03:37,  1.65it/s]Extractor Estimating: 339it [03:37,  1.69it/s]Extractor Estimating: 340it [03:38,  1.67it/s]Extractor Estimating: 341it [03:39,  1.68it/s]Extractor Estimating: 342it [03:39,  1.73it/s]Extractor Estimating: 343it [03:40,  1.70it/s]Extractor Estimating: 344it [03:40,  1.68it/s]Extractor Estimating: 345it [03:41,  1.34it/s]Extractor Estimating: 346it [03:42,  1.43it/s]Extractor Estimating: 347it [03:43,  1.50it/s]Extractor Estimating: 348it [03:44,  1.29it/s]Extractor Estimating: 349it [03:45,  1.22it/s]Extractor Estimating: 350it [03:45,  1.29it/s]Extractor Estimating: 351it [03:46,  1.42it/s]Extractor Estimating: 352it [03:47,  1.29it/s]Extractor Estimating: 353it [03:47,  1.42it/s]Extractor Estimating: 354it [03:48,  1.48it/s]Extractor Estimating: 355it [03:49,  1.06it/s]Extractor Estimating: 356it [03:50,  1.21it/s]Extractor Estimating: 357it [03:51,  1.32it/s]Extractor Estimating: 358it [03:51,  1.46it/s]Extractor Estimating: 359it [03:52,  1.47it/s]Extractor Estimating: 360it [03:52,  1.55it/s]Extractor Estimating: 361it [03:53,  1.65it/s]Extractor Estimating: 362it [03:53,  1.69it/s]Extractor Estimating: 363it [03:54,  1.72it/s]Extractor Estimating: 364it [03:55,  1.65it/s]Extractor Estimating: 365it [03:55,  1.64it/s]Extractor Estimating: 366it [03:56,  1.64it/s]Extractor Estimating: 367it [03:56,  1.67it/s]Extractor Estimating: 368it [03:57,  1.64it/s]Extractor Estimating: 369it [03:58,  1.40it/s]Extractor Estimating: 370it [03:59,  1.51it/s]Extractor Estimating: 371it [03:59,  1.60it/s]Extractor Estimating: 372it [04:00,  1.24it/s]Extractor Estimating: 373it [04:01,  1.34it/s]Extractor Estimating: 374it [04:01,  1.45it/s]Extractor Estimating: 375it [04:02,  1.64it/s]Extractor Estimating: 375it [04:02,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:00,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:00,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:00,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:00,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:00,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:00:03,162 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:00:03,163 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:00:03,898 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:00:05,603 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:00:05,740 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:10,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:10,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:10,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:10,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:00:10,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:00:11,672 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:00:11,739 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:00:12,721 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:00:13,439 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:00:13,879 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:16:46,305 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:16:46,794 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4500 mean pseudo reward: 0.9699073236976514
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16016
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16116, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16116, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.980, loss:439.7953
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 0.973, loss:388.7736
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 0.970, loss:374.5011
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 0.978, loss:370.7941
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 0.985, loss:354.7384
>> valid entity prec:0.4640, rec:0.4568, f1:0.4604
>> valid relation prec:0.0690, rec:0.0336, f1:0.0452
>> valid relation with NER prec:0.0690, rec:0.0336, f1:0.0452
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 2.280, loss:350.8712
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 0.968, loss:350.4812
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 0.975, loss:349.5502
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 0.987, loss:338.8301
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 0.972, loss:326.1213
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4684, rec:0.4387, f1:0.4531
>> valid relation prec:0.0761, rec:0.0362, f1:0.0491
>> valid relation with NER prec:0.0761, rec:0.0362, f1:0.0491
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 160, avg_time 2.253, loss:343.2054
g_step 1200, step 72, avg_time 0.966, loss:317.6632
g_step 1300, step 172, avg_time 0.985, loss:333.6308
g_step 1400, step 84, avg_time 0.972, loss:305.4042
g_step 1500, step 184, avg_time 0.982, loss:308.1807
>> valid entity prec:0.4586, rec:0.4367, f1:0.4474
>> valid relation prec:0.0705, rec:0.0339, f1:0.0458
>> valid relation with NER prec:0.0705, rec:0.0339, f1:0.0458
g_step 1600, step 96, avg_time 2.241, loss:277.0244
g_step 1700, step 8, avg_time 0.979, loss:287.5337
g_step 1800, step 108, avg_time 0.987, loss:275.8586
g_step 1900, step 20, avg_time 0.974, loss:273.3920
g_step 2000, step 120, avg_time 0.969, loss:270.8295
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4431, rec:0.4728, f1:0.4575
>> valid relation prec:0.0611, rec:0.0305, f1:0.0407
>> valid relation with NER prec:0.0611, rec:0.0305, f1:0.0407
g_step 2100, step 32, avg_time 2.263, loss:266.1097
g_step 2200, step 132, avg_time 0.977, loss:243.9189
g_step 2300, step 44, avg_time 0.970, loss:238.1399
g_step 2400, step 144, avg_time 0.988, loss:237.5590
g_step 2500, step 56, avg_time 0.964, loss:235.5057
>> valid entity prec:0.4770, rec:0.4198, f1:0.4466
>> valid relation prec:0.0806, rec:0.0356, f1:0.0494
>> valid relation with NER prec:0.0806, rec:0.0356, f1:0.0494
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 156, avg_time 2.251, loss:224.8180
g_step 2700, step 68, avg_time 0.976, loss:198.3978
g_step 2800, step 168, avg_time 0.985, loss:237.6662
g_step 2900, step 80, avg_time 0.976, loss:213.8078
g_step 3000, step 180, avg_time 0.969, loss:210.6635
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4617, rec:0.4753, f1:0.4684
>> valid relation prec:0.0693, rec:0.0379, f1:0.0490
>> valid relation with NER prec:0.0693, rec:0.0379, f1:0.0490
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 92, avg_time 2.296, loss:194.0684
g_step 3200, step 4, avg_time 0.968, loss:203.4516
g_step 3300, step 104, avg_time 0.973, loss:188.8928
g_step 3400, step 16, avg_time 0.986, loss:193.9254
g_step 3500, step 116, avg_time 0.987, loss:186.1353
>> valid entity prec:0.4632, rec:0.4277, f1:0.4447
>> valid relation prec:0.0874, rec:0.0535, f1:0.0663
>> valid relation with NER prec:0.0874, rec:0.0535, f1:0.0663
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 28, avg_time 2.241, loss:186.3037
g_step 3700, step 128, avg_time 0.981, loss:168.5858
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:16:46 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:16:46 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-16-46_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:16:48 - WARNING - datasets.builder -   Using custom data configuration default-977dc0d9a428b675
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-977dc0d9a428b675/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:16:55,962 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:16:56,109 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:16:56,110 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:16:56,111 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:16:56,395 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:16:56,474 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:16:58,388 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:17:01,922 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:17:02,062 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-977dc0d9a428b675/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:04,  1.22s/ba] 40%|████      | 2/5 [00:01<00:02,  1.46ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.16ba/s] 80%|████████  | 4/5 [00:01<00:00,  2.79ba/s]100%|██████████| 5/5 [00:02<00:00,  2.46ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.13ba/s] 50%|█████     | 2/4 [00:01<00:00,  2.03ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.72ba/s]100%|██████████| 4/4 [00:01<00:00,  3.78ba/s]100%|██████████| 4/4 [00:01<00:00,  2.80ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:04,  1.06s/ba] 40%|████      | 2/5 [00:01<00:01,  1.55ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.51ba/s]100%|██████████| 5/5 [00:01<00:00,  4.69ba/s]100%|██████████| 5/5 [00:01<00:00,  3.01ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.12s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.62ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.61ba/s]100%|██████████| 4/4 [00:01<00:00,  2.60ba/s]
[INFO|trainer.py:414] 2023-08-28 21:17:15,775 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:17:16,212 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:17:16,276 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 21:17:16,276 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:17:16,276 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:17:16,276 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:17:16,276 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:17:16,276 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:02<11:54,  2.05s/it]  1%|          | 2/350 [00:03<09:28,  1.63s/it]  1%|          | 3/350 [00:03<06:26,  1.11s/it]  1%|          | 4/350 [00:04<05:16,  1.09it/s]  1%|▏         | 5/350 [00:04<04:21,  1.32it/s]  2%|▏         | 6/350 [00:05<04:18,  1.33it/s]  2%|▏         | 7/350 [00:06<03:26,  1.66it/s]  2%|▏         | 8/350 [00:06<02:52,  1.99it/s]  3%|▎         | 9/350 [00:06<02:29,  2.29it/s]  3%|▎         | 10/350 [00:06<02:13,  2.55it/s]  3%|▎         | 11/350 [00:07<02:28,  2.28it/s]  3%|▎         | 12/350 [00:07<02:13,  2.54it/s]  4%|▎         | 13/350 [00:08<02:02,  2.75it/s]  4%|▍         | 14/350 [00:08<01:54,  2.93it/s]  4%|▍         | 15/350 [00:08<01:49,  3.06it/s]  5%|▍         | 16/350 [00:08<01:45,  3.16it/s]  5%|▍         | 17/350 [00:09<01:42,  3.23it/s]  5%|▌         | 18/350 [00:09<01:40,  3.30it/s]  5%|▌         | 19/350 [00:09<01:38,  3.35it/s]  6%|▌         | 20/350 [00:10<01:37,  3.38it/s]  6%|▌         | 21/350 [00:10<01:41,  3.25it/s]  6%|▋         | 22/350 [00:10<01:39,  3.31it/s]  7%|▋         | 23/350 [00:10<01:37,  3.36it/s]  7%|▋         | 24/350 [00:11<01:36,  3.39it/s]  7%|▋         | 25/350 [00:11<01:35,  3.41it/s]  7%|▋         | 26/350 [00:11<01:34,  3.43it/s]  8%|▊         | 27/350 [00:12<01:33,  3.44it/s]  8%|▊         | 28/350 [00:12<01:33,  3.45it/s]  8%|▊         | 29/350 [00:12<01:33,  3.45it/s]  9%|▊         | 30/350 [00:12<01:32,  3.46it/s]  9%|▉         | 31/350 [00:13<01:32,  3.46it/s]  9%|▉         | 32/350 [00:13<01:34,  3.37it/s]  9%|▉         | 33/350 [00:13<01:33,  3.39it/s] 10%|▉         | 34/350 [00:14<01:32,  3.42it/s] 10%|█         | 35/350 [00:14<01:31,  3.43it/s] 10%|█         | 36/350 [00:14<01:31,  3.44it/s] 11%|█         | 37/350 [00:15<01:30,  3.45it/s] 11%|█         | 38/350 [00:15<01:30,  3.46it/s] 11%|█         | 39/350 [00:15<01:29,  3.46it/s] 11%|█▏        | 40/350 [00:15<01:29,  3.46it/s] 12%|█▏        | 41/350 [00:16<01:29,  3.46it/s] 12%|█▏        | 42/350 [00:16<01:28,  3.46it/s] 12%|█▏        | 43/350 [00:16<01:50,  2.78it/s] 13%|█▎        | 44/350 [00:17<01:43,  2.96it/s] 13%|█▎        | 45/350 [00:17<01:38,  3.09it/s] 13%|█▎        | 46/350 [00:18<02:13,  2.28it/s] 13%|█▎        | 47/350 [00:18<01:59,  2.55it/s] 14%|█▎        | 48/350 [00:18<01:49,  2.76it/s] 14%|█▍        | 49/350 [00:19<01:42,  2.94it/s] 14%|█▍        | 50/350 [00:19<01:37,  3.08it/s] 15%|█▍        | 51/350 [00:19<01:33,  3.19it/s] 15%|█▍        | 52/350 [00:20<01:39,  2.99it/s] 15%|█▌        | 53/350 [00:20<01:35,  3.12it/s] 15%|█▌        | 54/350 [00:20<01:32,  3.21it/s] 16%|█▌        | 55/350 [00:20<01:29,  3.28it/s] 16%|█▌        | 56/350 [00:21<01:28,  3.33it/s] 16%|█▋        | 57/350 [00:21<01:26,  3.37it/s] 17%|█▋        | 58/350 [00:21<01:25,  3.40it/s] 17%|█▋        | 59/350 [00:22<01:25,  3.42it/s] 17%|█▋        | 60/350 [00:22<01:24,  3.43it/s] 17%|█▋        | 61/350 [00:22<01:23,  3.44it/s] 18%|█▊        | 62/350 [00:22<01:23,  3.45it/s] 18%|█▊        | 63/350 [00:23<01:45,  2.71it/s] 18%|█▊        | 64/350 [00:23<01:38,  2.90it/s] 19%|█▊        | 65/350 [00:24<01:33,  3.05it/s] 19%|█▉        | 66/350 [00:24<01:29,  3.17it/s] 19%|█▉        | 67/350 [00:24<01:27,  3.25it/s] 19%|█▉        | 68/350 [00:24<01:25,  3.30it/s] 20%|█▉        | 69/350 [00:25<01:24,  3.34it/s] 20%|██        | 70/350 [00:25<01:22,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:17:41,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:41,889 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:17:41,889 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.85it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.44it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.72it/s][A
  5%|▌         | 23/435 [00:00<00:14, 29.33it/s][A
  6%|▋         | 28/435 [00:00<00:12, 33.34it/s][A
  8%|▊         | 33/435 [00:00<00:11, 36.54it/s][A
  9%|▊         | 38/435 [00:00<00:10, 39.04it/s][A
 10%|▉         | 43/435 [00:01<00:09, 40.90it/s][A
 11%|█         | 48/435 [00:01<00:09, 42.36it/s][A
 12%|█▏        | 53/435 [00:01<00:09, 38.83it/s][A
 13%|█▎        | 58/435 [00:01<00:09, 40.58it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 41.97it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 43.07it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 43.90it/s][A
 18%|█▊        | 78/435 [00:01<00:08, 44.54it/s][A
 19%|█▉        | 83/435 [00:02<00:07, 44.97it/s][A
 20%|██        | 88/435 [00:02<00:07, 44.97it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.79it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 44.63it/s][A
 24%|██▎       | 103/435 [00:03<00:27, 12.15it/s][A
 25%|██▍       | 107/435 [00:03<00:31, 10.56it/s][A
 26%|██▌       | 112/435 [00:04<00:23, 13.90it/s][A
 27%|██▋       | 117/435 [00:04<00:17, 17.74it/s][A
 28%|██▊       | 122/435 [00:04<00:14, 21.83it/s][A
 29%|██▉       | 127/435 [00:04<00:12, 23.89it/s][A
 30%|███       | 131/435 [00:04<00:11, 25.96it/s][A
 31%|███       | 135/435 [00:05<00:18, 16.20it/s][A
 32%|███▏      | 140/435 [00:05<00:14, 20.65it/s][A
 33%|███▎      | 145/435 [00:05<00:11, 25.01it/s][A
 34%|███▍      | 150/435 [00:05<00:09, 29.13it/s][A
 36%|███▌      | 155/435 [00:05<00:08, 32.80it/s][A
 37%|███▋      | 160/435 [00:05<00:07, 35.93it/s][A
 38%|███▊      | 165/435 [00:05<00:07, 38.39it/s][A
 39%|███▉      | 170/435 [00:05<00:06, 40.39it/s][A
 40%|████      | 175/435 [00:05<00:06, 39.06it/s][A
 41%|████▏     | 180/435 [00:06<00:06, 41.09it/s][A
 43%|████▎     | 185/435 [00:06<00:05, 42.39it/s][A
 44%|████▎     | 190/435 [00:06<00:05, 43.31it/s][A
 45%|████▍     | 195/435 [00:06<00:05, 44.00it/s][A
 46%|████▌     | 200/435 [00:06<00:05, 43.34it/s][A
 47%|████▋     | 205/435 [00:06<00:09, 23.43it/s][A
 48%|████▊     | 210/435 [00:07<00:08, 27.49it/s][A
 49%|████▉     | 215/435 [00:07<00:07, 31.24it/s][A
 51%|█████     | 220/435 [00:07<00:06, 34.50it/s][A
 52%|█████▏    | 225/435 [00:07<00:05, 37.07it/s][A
 53%|█████▎    | 230/435 [00:07<00:05, 39.27it/s][A
 54%|█████▍    | 235/435 [00:07<00:04, 41.01it/s][A
 55%|█████▌    | 240/435 [00:07<00:04, 42.20it/s][A
 56%|█████▋    | 245/435 [00:07<00:04, 42.93it/s][A
 57%|█████▋    | 250/435 [00:07<00:04, 43.58it/s][A
 59%|█████▊    | 255/435 [00:08<00:04, 44.23it/s][A
 60%|█████▉    | 260/435 [00:08<00:03, 44.65it/s][A
 61%|██████    | 265/435 [00:08<00:03, 44.86it/s][A
 62%|██████▏   | 270/435 [00:08<00:03, 45.16it/s][A
 63%|██████▎   | 275/435 [00:08<00:03, 45.26it/s][A
 64%|██████▍   | 280/435 [00:08<00:03, 45.22it/s][A
 66%|██████▌   | 285/435 [00:08<00:03, 45.19it/s][A
 67%|██████▋   | 290/435 [00:08<00:03, 45.15it/s][A
 68%|██████▊   | 295/435 [00:09<00:03, 45.22it/s][A
 69%|██████▉   | 300/435 [00:09<00:08, 15.15it/s][A
 70%|███████   | 305/435 [00:09<00:06, 18.98it/s][A
 71%|███████   | 309/435 [00:10<00:06, 18.60it/s][A
 72%|███████▏  | 314/435 [00:10<00:05, 22.86it/s][A
 73%|███████▎  | 319/435 [00:10<00:04, 27.04it/s][A
 74%|███████▍  | 324/435 [00:10<00:03, 30.93it/s][A
 76%|███████▌  | 329/435 [00:10<00:03, 34.28it/s][A
 77%|███████▋  | 334/435 [00:10<00:02, 37.07it/s][A
 78%|███████▊  | 339/435 [00:10<00:02, 39.34it/s][A
 79%|███████▉  | 344/435 [00:10<00:02, 41.03it/s][A
 80%|████████  | 349/435 [00:11<00:02, 42.05it/s][A
 81%|████████▏ | 354/435 [00:11<00:01, 42.60it/s][A
 83%|████████▎ | 359/435 [00:11<00:01, 43.56it/s][A
 84%|████████▎ | 364/435 [00:11<00:01, 44.15it/s][A
 85%|████████▍ | 369/435 [00:11<00:01, 44.57it/s][A
 86%|████████▌ | 374/435 [00:11<00:01, 44.85it/s][A
 87%|████████▋ | 379/435 [00:11<00:01, 45.15it/s][A
 88%|████████▊ | 384/435 [00:11<00:01, 45.17it/s][A
 89%|████████▉ | 389/435 [00:12<00:01, 45.27it/s][A
 91%|█████████ | 394/435 [00:12<00:01, 24.92it/s][A
 92%|█████████▏| 399/435 [00:12<00:01, 28.89it/s][A
 93%|█████████▎| 404/435 [00:12<00:00, 32.40it/s][A
 94%|█████████▍| 409/435 [00:12<00:00, 35.58it/s][A
 95%|█████████▌| 414/435 [00:12<00:00, 38.07it/s][A
 96%|█████████▋| 419/435 [00:12<00:00, 40.15it/s][A
 97%|█████████▋| 424/435 [00:12<00:00, 41.65it/s][A
 99%|█████████▊| 429/435 [00:13<00:00, 42.72it/s][A
100%|█████████▉| 434/435 [00:13<00:00, 43.19it/s][A                                                
                                                 [A 20%|██        | 70/350 [00:38<01:22,  3.37it/s]
100%|██████████| 435/435 [00:13<00:00, 43.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:17:56,628 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 21:17:57,646 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:09,433 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:10,513 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:10,635 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [01:16<1:11:57, 15.47s/it] 21%|██        | 72/350 [01:16<50:51, 10.98s/it]   21%|██        | 73/350 [01:17<35:52,  7.77s/it] 21%|██        | 74/350 [01:17<25:25,  5.53s/it] 21%|██▏       | 75/350 [01:17<18:08,  3.96s/it] 22%|██▏       | 76/350 [01:18<13:16,  2.91s/it] 22%|██▏       | 77/350 [01:18<09:39,  2.12s/it] 22%|██▏       | 78/350 [01:18<07:07,  1.57s/it] 23%|██▎       | 79/350 [01:19<05:22,  1.19s/it] 23%|██▎       | 80/350 [01:19<04:08,  1.09it/s] 23%|██▎       | 81/350 [01:19<03:16,  1.37it/s] 23%|██▎       | 82/350 [01:20<03:12,  1.39it/s] 24%|██▎       | 83/350 [01:20<02:37,  1.70it/s] 24%|██▍       | 84/350 [01:21<02:13,  2.00it/s] 24%|██▍       | 85/350 [01:21<01:56,  2.28it/s] 25%|██▍       | 86/350 [01:21<01:43,  2.54it/s] 25%|██▍       | 87/350 [01:21<01:35,  2.75it/s] 25%|██▌       | 88/350 [01:22<01:29,  2.92it/s] 25%|██▌       | 89/350 [01:22<01:25,  3.06it/s] 26%|██▌       | 90/350 [01:22<01:22,  3.16it/s] 26%|██▌       | 91/350 [01:23<01:22,  3.13it/s] 26%|██▋       | 92/350 [01:23<01:20,  3.22it/s] 27%|██▋       | 93/350 [01:23<01:18,  3.28it/s] 27%|██▋       | 94/350 [01:23<01:17,  3.32it/s] 27%|██▋       | 95/350 [01:24<01:16,  3.35it/s] 27%|██▋       | 96/350 [01:24<01:15,  3.37it/s] 28%|██▊       | 97/350 [01:24<01:14,  3.39it/s] 28%|██▊       | 98/350 [01:25<01:14,  3.40it/s] 28%|██▊       | 99/350 [01:25<01:13,  3.41it/s] 29%|██▊       | 100/350 [01:25<01:13,  3.41it/s] 29%|██▉       | 101/350 [01:25<01:12,  3.42it/s] 29%|██▉       | 102/350 [01:26<01:28,  2.81it/s] 29%|██▉       | 103/350 [01:26<01:23,  2.97it/s] 30%|██▉       | 104/350 [01:27<01:19,  3.10it/s] 30%|███       | 105/350 [01:27<01:16,  3.19it/s] 30%|███       | 106/350 [01:27<01:14,  3.26it/s] 31%|███       | 107/350 [01:27<01:13,  3.30it/s] 31%|███       | 108/350 [01:29<02:18,  1.75it/s] 31%|███       | 109/350 [01:29<02:11,  1.84it/s] 31%|███▏      | 110/350 [01:29<01:52,  2.14it/s] 32%|███▏      | 111/350 [01:31<02:55,  1.36it/s] 32%|███▏      | 112/350 [01:31<02:22,  1.66it/s] 32%|███▏      | 113/350 [01:31<02:00,  1.97it/s] 33%|███▎      | 114/350 [01:32<01:44,  2.26it/s] 33%|███▎      | 115/350 [01:32<01:33,  2.51it/s] 33%|███▎      | 116/350 [01:33<01:50,  2.11it/s] 33%|███▎      | 117/350 [01:33<01:37,  2.39it/s] 34%|███▎      | 118/350 [01:33<01:28,  2.63it/s] 34%|███▍      | 119/350 [01:33<01:21,  2.82it/s] 34%|███▍      | 120/350 [01:34<01:17,  2.98it/s] 35%|███▍      | 121/350 [01:34<01:13,  3.10it/s] 35%|███▍      | 122/350 [01:34<01:11,  3.19it/s] 35%|███▌      | 123/350 [01:35<01:09,  3.26it/s] 35%|███▌      | 124/350 [01:35<01:08,  3.30it/s] 36%|███▌      | 125/350 [01:35<01:07,  3.34it/s] 36%|███▌      | 126/350 [01:36<01:11,  3.14it/s] 36%|███▋      | 127/350 [01:36<01:22,  2.72it/s] 37%|███▋      | 128/350 [01:36<01:16,  2.91it/s] 37%|███▋      | 129/350 [01:37<01:12,  3.06it/s] 37%|███▋      | 130/350 [01:37<01:09,  3.17it/s] 37%|███▋      | 131/350 [01:37<01:07,  3.26it/s] 38%|███▊      | 132/350 [01:38<01:05,  3.32it/s] 38%|███▊      | 133/350 [01:38<01:04,  3.37it/s] 38%|███▊      | 134/350 [01:38<01:03,  3.39it/s] 39%|███▊      | 135/350 [01:38<01:02,  3.42it/s] 39%|███▉      | 136/350 [01:39<01:02,  3.43it/s] 39%|███▉      | 137/350 [01:39<01:29,  2.39it/s] 39%|███▉      | 138/350 [01:40<01:20,  2.63it/s] 40%|███▉      | 139/350 [01:40<01:14,  2.84it/s] 40%|████      | 140/350 [01:40<01:09,  3.00it/s][INFO|trainer.py:2140] 2023-08-28 21:18:57,065 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:18:57,065 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:18:57,065 >>   Batch size = 8
{'eval_loss': 1.0753414630889893, 'eval_runtime': 13.2846, 'eval_samples_per_second': 261.882, 'eval_steps_per_second': 32.745, 'epoch': 0.99}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.67it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.43it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.52it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.06it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.91it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.63it/s][A
 10%|▉         | 42/435 [00:00<00:10, 38.59it/s][A
 11%|█         | 47/435 [00:01<00:09, 40.61it/s][A
 12%|█▏        | 52/435 [00:01<00:09, 42.20it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 43.43it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.20it/s][A
 15%|█▌        | 67/435 [00:02<00:21, 17.21it/s][A
 17%|█▋        | 72/435 [00:02<00:17, 21.22it/s][A
 18%|█▊        | 77/435 [00:02<00:14, 25.33it/s][A
 19%|█▉        | 82/435 [00:02<00:12, 29.30it/s][A
 20%|██        | 87/435 [00:02<00:10, 32.91it/s][A
 21%|██        | 92/435 [00:02<00:09, 35.98it/s][A
 22%|██▏       | 97/435 [00:02<00:08, 38.53it/s][A
 23%|██▎       | 102/435 [00:02<00:08, 40.41it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 41.58it/s][A
 26%|██▌       | 112/435 [00:03<00:07, 42.34it/s][A
 27%|██▋       | 117/435 [00:03<00:07, 43.05it/s][A
 28%|██▊       | 122/435 [00:03<00:07, 43.87it/s][A
 29%|██▉       | 127/435 [00:03<00:06, 44.43it/s][A
 30%|███       | 132/435 [00:03<00:06, 44.87it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.30it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 45.49it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.43it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.24it/s][A
 36%|███▌      | 157/435 [00:04<00:06, 44.96it/s][A
 37%|███▋      | 162/435 [00:04<00:06, 44.92it/s][A
 38%|███▊      | 167/435 [00:04<00:05, 45.13it/s][A
 40%|███▉      | 172/435 [00:04<00:05, 45.33it/s][A
 41%|████      | 177/435 [00:04<00:07, 33.86it/s][A
 42%|████▏     | 182/435 [00:04<00:06, 36.67it/s][A
 43%|████▎     | 187/435 [00:04<00:06, 39.00it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 40.87it/s][A
 45%|████▌     | 197/435 [00:05<00:05, 42.11it/s][A
 46%|████▋     | 202/435 [00:05<00:05, 43.22it/s][A
 48%|████▊     | 207/435 [00:05<00:05, 43.92it/s][A
 49%|████▊     | 212/435 [00:05<00:05, 44.40it/s][A
 50%|████▉     | 217/435 [00:05<00:04, 44.38it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.60it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.99it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 45.20it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.37it/s][A
 56%|█████▌    | 242/435 [00:06<00:04, 45.43it/s][A
 57%|█████▋    | 247/435 [00:06<00:04, 45.41it/s][A
 58%|█████▊    | 252/435 [00:06<00:04, 45.53it/s][A
 59%|█████▉    | 257/435 [00:06<00:03, 45.36it/s][A
 60%|██████    | 262/435 [00:06<00:03, 45.05it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 45.18it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 45.30it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 45.38it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 45.57it/s][A
 66%|██████▌   | 287/435 [00:07<00:03, 45.55it/s][A
 67%|██████▋   | 292/435 [00:07<00:03, 45.51it/s][A
 68%|██████▊   | 297/435 [00:07<00:03, 45.38it/s][A
 69%|██████▉   | 302/435 [00:07<00:02, 45.19it/s][A
 71%|███████   | 307/435 [00:07<00:02, 45.10it/s][A
 72%|███████▏  | 312/435 [00:07<00:03, 40.78it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 42.19it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 43.33it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.07it/s][A
 76%|███████▋  | 332/435 [00:08<00:02, 44.53it/s][A
 77%|███████▋  | 337/435 [00:08<00:02, 45.08it/s][A
 79%|███████▊  | 342/435 [00:08<00:02, 45.17it/s][A
 80%|███████▉  | 347/435 [00:08<00:01, 45.17it/s][A
 81%|████████  | 352/435 [00:08<00:01, 44.77it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.78it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.91it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 45.21it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 45.45it/s][A
 87%|████████▋ | 377/435 [00:09<00:01, 45.63it/s][A
 88%|████████▊ | 382/435 [00:09<00:01, 45.65it/s][A
 89%|████████▉ | 387/435 [00:09<00:01, 45.64it/s][A
 90%|█████████ | 392/435 [00:09<00:00, 45.44it/s][A
 91%|█████████▏| 397/435 [00:09<00:00, 45.15it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 45.00it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 45.07it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 45.15it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 45.38it/s][A
 97%|█████████▋| 422/435 [00:10<00:00, 45.60it/s][A
 98%|█████████▊| 427/435 [00:10<00:00, 45.77it/s][A
 99%|█████████▉| 432/435 [00:10<00:00, 45.57it/s][A                                                 
                                                 [A 40%|████      | 140/350 [01:51<01:09,  3.00it/s]
100%|██████████| 435/435 [00:10<00:00, 45.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:08,343 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 21:19:08,641 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:20,030 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:20,400 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:20,497 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [02:25<48:02, 13.79s/it] 41%|████      | 142/350 [02:26<33:52,  9.77s/it] 41%|████      | 143/350 [02:26<23:53,  6.93s/it] 41%|████      | 144/350 [02:26<16:56,  4.94s/it] 41%|████▏     | 145/350 [02:27<12:06,  3.54s/it] 42%|████▏     | 146/350 [02:27<08:43,  2.57s/it] 42%|████▏     | 147/350 [02:27<06:22,  1.88s/it] 42%|████▏     | 148/350 [02:28<04:44,  1.41s/it] 43%|████▎     | 149/350 [02:28<03:47,  1.13s/it] 43%|████▎     | 150/350 [02:29<03:59,  1.20s/it] 43%|████▎     | 151/350 [02:30<03:09,  1.05it/s] 43%|████▎     | 152/350 [02:30<02:41,  1.23it/s] 44%|████▎     | 153/350 [02:31<02:13,  1.48it/s] 44%|████▍     | 154/350 [02:32<02:32,  1.28it/s] 44%|████▍     | 155/350 [02:32<02:03,  1.58it/s] 45%|████▍     | 156/350 [02:32<01:42,  1.88it/s] 45%|████▍     | 157/350 [02:33<01:28,  2.18it/s] 45%|████▌     | 158/350 [02:33<01:23,  2.31it/s] 45%|████▌     | 159/350 [02:33<01:14,  2.56it/s] 46%|████▌     | 160/350 [02:34<01:08,  2.77it/s] 46%|████▌     | 161/350 [02:34<01:04,  2.94it/s] 46%|████▋     | 162/350 [02:34<01:01,  3.07it/s] 47%|████▋     | 163/350 [02:34<00:58,  3.17it/s] 47%|████▋     | 164/350 [02:35<00:57,  3.25it/s] 47%|████▋     | 165/350 [02:35<00:56,  3.30it/s] 47%|████▋     | 166/350 [02:35<00:55,  3.34it/s] 48%|████▊     | 167/350 [02:36<00:54,  3.37it/s] 48%|████▊     | 168/350 [02:36<00:53,  3.38it/s] 48%|████▊     | 169/350 [02:37<01:15,  2.40it/s] 49%|████▊     | 170/350 [02:37<01:08,  2.64it/s] 49%|████▉     | 171/350 [02:37<01:03,  2.83it/s] 49%|████▉     | 172/350 [02:38<01:10,  2.54it/s] 49%|████▉     | 173/350 [02:38<01:04,  2.75it/s] 50%|████▉     | 174/350 [02:38<01:00,  2.93it/s] 50%|█████     | 175/350 [02:38<00:57,  3.06it/s] 50%|█████     | 176/350 [02:39<00:55,  3.16it/s] 51%|█████     | 177/350 [02:39<00:53,  3.24it/s] 51%|█████     | 178/350 [02:39<00:52,  3.29it/s] 51%|█████     | 179/350 [02:40<00:51,  3.33it/s] 51%|█████▏    | 180/350 [02:40<00:50,  3.36it/s] 52%|█████▏    | 181/350 [02:40<00:50,  3.37it/s] 52%|█████▏    | 182/350 [02:41<00:58,  2.85it/s] 52%|█████▏    | 183/350 [02:41<00:55,  3.00it/s] 53%|█████▎    | 184/350 [02:42<01:03,  2.60it/s] 53%|█████▎    | 185/350 [02:42<00:58,  2.81it/s] 53%|█████▎    | 186/350 [02:42<00:55,  2.96it/s] 53%|█████▎    | 187/350 [02:42<00:52,  3.09it/s] 54%|█████▎    | 188/350 [02:43<00:50,  3.18it/s] 54%|█████▍    | 189/350 [02:43<00:49,  3.25it/s] 54%|█████▍    | 190/350 [02:43<00:48,  3.30it/s] 55%|█████▍    | 191/350 [02:44<00:51,  3.06it/s] 55%|█████▍    | 192/350 [02:44<00:49,  3.16it/s] 55%|█████▌    | 193/350 [02:44<00:48,  3.23it/s] 55%|█████▌    | 194/350 [02:45<00:47,  3.29it/s] 56%|█████▌    | 195/350 [02:45<00:46,  3.32it/s] 56%|█████▌    | 196/350 [02:45<00:45,  3.36it/s] 56%|█████▋    | 197/350 [02:45<00:45,  3.37it/s] 57%|█████▋    | 198/350 [02:46<00:44,  3.39it/s] 57%|█████▋    | 199/350 [02:46<00:44,  3.40it/s] 57%|█████▋    | 200/350 [02:46<00:44,  3.40it/s] 57%|█████▋    | 201/350 [02:47<01:11,  2.09it/s] 58%|█████▊    | 202/350 [02:47<01:02,  2.36it/s] 58%|█████▊    | 203/350 [02:48<00:56,  2.61it/s] 58%|█████▊    | 204/350 [02:48<00:51,  2.81it/s] 59%|█████▊    | 205/350 [02:48<00:48,  2.97it/s] 59%|█████▉    | 206/350 [02:49<00:46,  3.09it/s] 59%|█████▉    | 207/350 [02:49<00:44,  3.18it/s] 59%|█████▉    | 208/350 [02:49<00:43,  3.25it/s] 60%|█████▉    | 209/350 [02:50<00:42,  3.30it/s] 60%|██████    | 210/350 [02:50<00:51,  2.72it/s][INFO|trainer.py:2140] 2023-08-28 21:20:06,861 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:20:06,861 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:20:06,861 >>   Batch size = 8
{'eval_loss': 1.093613862991333, 'eval_runtime': 10.4251, 'eval_samples_per_second': 333.714, 'eval_steps_per_second': 41.726, 'epoch': 1.99}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.64it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.50it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.71it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.81it/s][A
  6%|▋         | 28/435 [00:00<00:08, 45.84it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.44it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.25it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.02it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.04it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.22it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.47it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.48it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.59it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.37it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.08it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 44.93it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.03it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.06it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.13it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 44.97it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.40it/s][A
 26%|██▌       | 113/435 [00:03<00:07, 45.59it/s][A
 27%|██▋       | 118/435 [00:03<00:17, 18.50it/s][A
 28%|██▊       | 123/435 [00:03<00:13, 22.53it/s][A
 29%|██▉       | 128/435 [00:03<00:11, 26.57it/s][A
 31%|███       | 133/435 [00:03<00:09, 30.36it/s][A
 32%|███▏      | 138/435 [00:03<00:08, 33.72it/s][A
 33%|███▎      | 143/435 [00:03<00:07, 36.65it/s][A
 34%|███▍      | 148/435 [00:03<00:07, 39.02it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 40.75it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 41.66it/s][A
 37%|███▋      | 163/435 [00:04<00:06, 42.31it/s][A
 39%|███▊      | 168/435 [00:04<00:06, 43.15it/s][A
 40%|███▉      | 173/435 [00:04<00:05, 43.75it/s][A
 41%|████      | 178/435 [00:04<00:05, 44.30it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 44.69it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 44.96it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.21it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.18it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.95it/s][A
 48%|████▊     | 208/435 [00:05<00:05, 44.86it/s][A
 49%|████▉     | 213/435 [00:05<00:04, 44.82it/s][A
 50%|█████     | 218/435 [00:05<00:04, 45.02it/s][A
 51%|█████▏    | 223/435 [00:05<00:04, 45.13it/s][A
 52%|█████▏    | 228/435 [00:06<00:04, 45.26it/s][A
 54%|█████▎    | 233/435 [00:06<00:10, 18.70it/s][A
 55%|█████▍    | 238/435 [00:06<00:08, 22.75it/s][A
 56%|█████▌    | 243/435 [00:06<00:07, 26.78it/s][A
 57%|█████▋    | 248/435 [00:06<00:06, 30.58it/s][A
 58%|█████▊    | 253/435 [00:06<00:05, 33.93it/s][A
 59%|█████▉    | 258/435 [00:06<00:04, 36.81it/s][A
 60%|██████    | 263/435 [00:06<00:04, 39.15it/s][A
 62%|██████▏   | 268/435 [00:06<00:04, 40.72it/s][A
 63%|██████▎   | 273/435 [00:07<00:03, 41.62it/s][A
 64%|██████▍   | 278/435 [00:07<00:03, 42.31it/s][A
 65%|██████▌   | 283/435 [00:07<00:03, 43.21it/s][A
 66%|██████▌   | 288/435 [00:07<00:03, 43.88it/s][A
 67%|██████▋   | 293/435 [00:07<00:03, 44.44it/s][A
 69%|██████▊   | 298/435 [00:07<00:03, 44.82it/s][A
 70%|██████▉   | 303/435 [00:07<00:02, 45.09it/s][A
 71%|███████   | 308/435 [00:07<00:02, 45.21it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 45.11it/s][A
 73%|███████▎  | 318/435 [00:08<00:02, 44.86it/s][A
 74%|███████▍  | 323/435 [00:08<00:02, 44.72it/s][A
 75%|███████▌  | 328/435 [00:08<00:02, 44.80it/s][A
 77%|███████▋  | 333/435 [00:08<00:02, 44.96it/s][A
 78%|███████▊  | 338/435 [00:08<00:02, 45.13it/s][A
 79%|███████▉  | 343/435 [00:08<00:02, 45.38it/s][A
 80%|████████  | 348/435 [00:08<00:03, 28.88it/s][A
 81%|████████  | 353/435 [00:09<00:02, 32.49it/s][A
 82%|████████▏ | 358/435 [00:09<00:02, 35.62it/s][A
 83%|████████▎ | 363/435 [00:09<00:01, 38.10it/s][A
 85%|████████▍ | 368/435 [00:09<00:01, 40.13it/s][A
 86%|████████▌ | 373/435 [00:09<00:01, 41.64it/s][A
 87%|████████▋ | 378/435 [00:09<00:01, 42.83it/s][A
 88%|████████▊ | 383/435 [00:09<00:01, 43.57it/s][A
 89%|████████▉ | 388/435 [00:09<00:01, 43.51it/s][A
 90%|█████████ | 393/435 [00:09<00:00, 43.75it/s][A
 91%|█████████▏| 398/435 [00:10<00:00, 44.13it/s][A
 93%|█████████▎| 403/435 [00:10<00:00, 44.44it/s][A
 94%|█████████▍| 408/435 [00:10<00:00, 44.80it/s][A
 95%|█████████▍| 413/435 [00:10<00:00, 44.99it/s][A
 96%|█████████▌| 418/435 [00:10<00:00, 45.28it/s][A
 97%|█████████▋| 423/435 [00:10<00:00, 45.45it/s][A
 98%|█████████▊| 428/435 [00:10<00:00, 45.26it/s][A
100%|█████████▉| 433/435 [00:10<00:00, 45.02it/s][A                                                 
                                                 [A 60%|██████    | 210/350 [03:01<00:51,  2.72it/s]
100%|██████████| 435/435 [00:10<00:00, 45.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:20:17,874 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 21:20:18,016 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:20:25,967 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:20:26,812 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:20:27,050 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [03:25<24:34, 10.61s/it] 61%|██████    | 212/350 [03:25<17:17,  7.52s/it] 61%|██████    | 213/350 [03:25<12:13,  5.35s/it] 61%|██████    | 214/350 [03:25<08:41,  3.83s/it] 61%|██████▏   | 215/350 [03:26<06:14,  2.77s/it] 62%|██████▏   | 216/350 [03:26<04:31,  2.03s/it] 62%|██████▏   | 217/350 [03:26<03:20,  1.51s/it] 62%|██████▏   | 218/350 [03:27<02:30,  1.14s/it] 63%|██████▎   | 219/350 [03:27<01:56,  1.13it/s] 63%|██████▎   | 220/350 [03:27<01:32,  1.41it/s] 63%|██████▎   | 221/350 [03:27<01:15,  1.72it/s] 63%|██████▎   | 222/350 [03:28<01:03,  2.02it/s] 64%|██████▎   | 223/350 [03:28<01:01,  2.06it/s] 64%|██████▍   | 224/350 [03:29<00:54,  2.32it/s] 64%|██████▍   | 225/350 [03:29<00:50,  2.47it/s] 65%|██████▍   | 226/350 [03:30<01:15,  1.64it/s] 65%|██████▍   | 227/350 [03:30<01:06,  1.85it/s] 65%|██████▌   | 228/350 [03:31<01:02,  1.94it/s] 65%|██████▌   | 229/350 [03:31<00:59,  2.02it/s] 66%|██████▌   | 230/350 [03:32<00:52,  2.31it/s] 66%|██████▌   | 231/350 [03:32<00:46,  2.55it/s] 66%|██████▋   | 232/350 [03:32<00:42,  2.77it/s] 67%|██████▋   | 233/350 [03:32<00:39,  2.93it/s] 67%|██████▋   | 234/350 [03:33<00:37,  3.06it/s] 67%|██████▋   | 235/350 [03:33<00:36,  3.16it/s] 67%|██████▋   | 236/350 [03:33<00:35,  3.23it/s] 68%|██████▊   | 237/350 [03:34<00:34,  3.28it/s] 68%|██████▊   | 238/350 [03:34<00:33,  3.32it/s] 68%|██████▊   | 239/350 [03:34<00:34,  3.26it/s] 69%|██████▊   | 240/350 [03:34<00:33,  3.30it/s] 69%|██████▉   | 241/350 [03:35<00:32,  3.33it/s] 69%|██████▉   | 242/350 [03:35<00:32,  3.36it/s] 69%|██████▉   | 243/350 [03:35<00:31,  3.38it/s] 70%|██████▉   | 244/350 [03:36<00:31,  3.39it/s] 70%|███████   | 245/350 [03:36<00:30,  3.40it/s] 70%|███████   | 246/350 [03:36<00:30,  3.40it/s] 71%|███████   | 247/350 [03:37<00:30,  3.41it/s] 71%|███████   | 248/350 [03:37<00:29,  3.41it/s] 71%|███████   | 249/350 [03:37<00:29,  3.41it/s] 71%|███████▏  | 250/350 [03:37<00:29,  3.36it/s] 72%|███████▏  | 251/350 [03:38<00:29,  3.37it/s] 72%|███████▏  | 252/350 [03:38<00:28,  3.38it/s] 72%|███████▏  | 253/350 [03:38<00:28,  3.39it/s] 73%|███████▎  | 254/350 [03:39<00:28,  3.40it/s] 73%|███████▎  | 255/350 [03:39<00:27,  3.40it/s] 73%|███████▎  | 256/350 [03:39<00:27,  3.41it/s] 73%|███████▎  | 257/350 [03:39<00:27,  3.41it/s] 74%|███████▎  | 258/350 [03:40<00:26,  3.41it/s] 74%|███████▍  | 259/350 [03:40<00:26,  3.41it/s] 74%|███████▍  | 260/350 [03:40<00:26,  3.41it/s] 75%|███████▍  | 261/350 [03:41<00:26,  3.41it/s] 75%|███████▍  | 262/350 [03:41<00:25,  3.41it/s] 75%|███████▌  | 263/350 [03:41<00:25,  3.41it/s] 75%|███████▌  | 264/350 [03:42<00:25,  3.41it/s] 76%|███████▌  | 265/350 [03:42<00:25,  3.27it/s] 76%|███████▌  | 266/350 [03:42<00:25,  3.31it/s] 76%|███████▋  | 267/350 [03:42<00:24,  3.34it/s] 77%|███████▋  | 268/350 [03:43<00:24,  3.36it/s] 77%|███████▋  | 269/350 [03:43<00:23,  3.38it/s] 77%|███████▋  | 270/350 [03:43<00:23,  3.39it/s] 77%|███████▋  | 271/350 [03:44<00:23,  3.40it/s] 78%|███████▊  | 272/350 [03:44<00:22,  3.40it/s] 78%|███████▊  | 273/350 [03:44<00:22,  3.41it/s] 78%|███████▊  | 274/350 [03:45<00:22,  3.41it/s] 79%|███████▊  | 275/350 [03:45<00:21,  3.41it/s] 79%|███████▉  | 276/350 [03:45<00:23,  3.21it/s] 79%|███████▉  | 277/350 [03:45<00:22,  3.27it/s] 79%|███████▉  | 278/350 [03:46<00:21,  3.31it/s] 80%|███████▉  | 279/350 [03:46<00:21,  3.34it/s] 80%|████████  | 280/350 [03:46<00:20,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 21:21:03,156 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:21:03,156 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:21:03,156 >>   Batch size = 8
{'eval_loss': 1.111430048942566, 'eval_runtime': 10.9184, 'eval_samples_per_second': 318.637, 'eval_steps_per_second': 39.841, 'epoch': 2.99}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.03it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.23it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.51it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.64it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.03it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.67it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.37it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.98it/s][A
 11%|█         | 47/435 [00:01<00:08, 45.17it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 45.48it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 45.36it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 45.43it/s][A
 17%|█▋        | 72/435 [00:01<00:10, 34.16it/s][A
 18%|█▊        | 77/435 [00:01<00:09, 36.96it/s][A
 19%|█▉        | 82/435 [00:01<00:09, 39.13it/s][A
 20%|██        | 87/435 [00:02<00:08, 40.99it/s][A
 21%|██        | 92/435 [00:02<00:08, 42.35it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 43.24it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 43.93it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 44.37it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 44.24it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 44.28it/s][A
 28%|██▊       | 122/435 [00:02<00:07, 44.38it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.59it/s][A
 30%|███       | 132/435 [00:03<00:06, 44.96it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.25it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 45.39it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.40it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.38it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 45.10it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 44.83it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 44.68it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.85it/s][A
 41%|████      | 177/435 [00:04<00:05, 45.07it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 45.32it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 45.33it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.38it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.27it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.07it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 41.95it/s][A
 49%|████▊     | 212/435 [00:04<00:05, 42.95it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 43.69it/s][A
 51%|█████     | 222/435 [00:05<00:04, 44.18it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.57it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.82it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.02it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.09it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.79it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 44.67it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 44.77it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.92it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 45.00it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 45.18it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 45.35it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 45.37it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 45.30it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.98it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.85it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.88it/s][A
 71%|███████   | 307/435 [00:06<00:02, 45.05it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 45.16it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 45.19it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 45.23it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 45.32it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 45.14it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.98it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 42.58it/s][A
 80%|███████▉  | 347/435 [00:07<00:02, 43.45it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.03it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 44.45it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.77it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.94it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 45.07it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 44.94it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 44.65it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 44.75it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 44.94it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 45.00it/s][A
 92%|█████████▏| 402/435 [00:09<00:00, 45.06it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 45.11it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 45.14it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 45.14it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 44.91it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.68it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.81it/s][A                                                 
                                                 [A 80%|████████  | 280/350 [03:56<00:20,  3.36it/s]
100%|██████████| 435/435 [00:09<00:00, 44.81it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:21:13,608 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 21:21:14,030 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:21,077 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:21,741 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:22,004 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [04:25<13:26, 11.68s/it] 81%|████████  | 282/350 [04:25<09:23,  8.29s/it] 81%|████████  | 283/350 [04:25<06:34,  5.89s/it] 81%|████████  | 284/350 [04:26<04:37,  4.21s/it] 81%|████████▏ | 285/350 [04:26<03:17,  3.03s/it] 82%|████████▏ | 286/350 [04:26<02:21,  2.21s/it] 82%|████████▏ | 287/350 [04:26<01:43,  1.64s/it] 82%|████████▏ | 288/350 [04:27<01:16,  1.23s/it] 83%|████████▎ | 289/350 [04:27<00:57,  1.05it/s] 83%|████████▎ | 290/350 [04:27<00:45,  1.33it/s] 83%|████████▎ | 291/350 [04:28<00:36,  1.63it/s] 83%|████████▎ | 292/350 [04:28<00:29,  1.93it/s] 84%|████████▎ | 293/350 [04:28<00:29,  1.93it/s] 84%|████████▍ | 294/350 [04:29<00:25,  2.17it/s] 84%|████████▍ | 295/350 [04:29<00:22,  2.42it/s] 85%|████████▍ | 296/350 [04:29<00:21,  2.49it/s] 85%|████████▍ | 297/350 [04:30<00:19,  2.68it/s] 85%|████████▌ | 298/350 [04:30<00:18,  2.87it/s] 85%|████████▌ | 299/350 [04:30<00:18,  2.83it/s] 86%|████████▌ | 300/350 [04:31<00:19,  2.60it/s] 86%|████████▌ | 301/350 [04:31<00:19,  2.52it/s] 86%|████████▋ | 302/350 [04:32<00:19,  2.43it/s] 87%|████████▋ | 303/350 [04:32<00:17,  2.66it/s] 87%|████████▋ | 304/350 [04:32<00:16,  2.85it/s] 87%|████████▋ | 305/350 [04:33<00:14,  3.00it/s] 87%|████████▋ | 306/350 [04:33<00:14,  3.12it/s] 88%|████████▊ | 307/350 [04:33<00:13,  3.20it/s] 88%|████████▊ | 308/350 [04:33<00:12,  3.26it/s] 88%|████████▊ | 309/350 [04:34<00:12,  3.31it/s] 89%|████████▊ | 310/350 [04:34<00:11,  3.34it/s] 89%|████████▉ | 311/350 [04:34<00:11,  3.36it/s] 89%|████████▉ | 312/350 [04:35<00:11,  3.25it/s] 89%|████████▉ | 313/350 [04:35<00:11,  3.30it/s] 90%|████████▉ | 314/350 [04:35<00:10,  3.33it/s] 90%|█████████ | 315/350 [04:36<00:10,  3.36it/s] 90%|█████████ | 316/350 [04:36<00:10,  3.38it/s] 91%|█████████ | 317/350 [04:36<00:09,  3.39it/s] 91%|█████████ | 318/350 [04:36<00:09,  3.40it/s] 91%|█████████ | 319/350 [04:37<00:09,  3.40it/s] 91%|█████████▏| 320/350 [04:37<00:08,  3.41it/s] 92%|█████████▏| 321/350 [04:37<00:08,  3.41it/s] 92%|█████████▏| 322/350 [04:38<00:08,  3.41it/s] 92%|█████████▏| 323/350 [04:38<00:07,  3.41it/s] 93%|█████████▎| 324/350 [04:38<00:07,  3.43it/s] 93%|█████████▎| 325/350 [04:38<00:07,  3.44it/s] 93%|█████████▎| 326/350 [04:39<00:06,  3.45it/s] 93%|█████████▎| 327/350 [04:39<00:06,  3.45it/s] 94%|█████████▎| 328/350 [04:39<00:06,  3.46it/s] 94%|█████████▍| 329/350 [04:40<00:06,  3.46it/s] 94%|█████████▍| 330/350 [04:40<00:05,  3.46it/s] 95%|█████████▍| 331/350 [04:40<00:05,  3.46it/s] 95%|█████████▍| 332/350 [04:40<00:05,  3.46it/s] 95%|█████████▌| 333/350 [04:41<00:04,  3.47it/s] 95%|█████████▌| 334/350 [04:41<00:04,  3.46it/s] 96%|█████████▌| 335/350 [04:41<00:04,  3.47it/s] 96%|█████████▌| 336/350 [04:42<00:04,  3.46it/s] 96%|█████████▋| 337/350 [04:42<00:03,  3.46it/s] 97%|█████████▋| 338/350 [04:42<00:03,  3.46it/s] 97%|█████████▋| 339/350 [04:42<00:03,  3.46it/s] 97%|█████████▋| 340/350 [04:43<00:02,  3.46it/s] 97%|█████████▋| 341/350 [04:43<00:02,  3.46it/s] 98%|█████████▊| 342/350 [04:43<00:02,  3.31it/s] 98%|█████████▊| 343/350 [04:44<00:02,  3.35it/s] 98%|█████████▊| 344/350 [04:44<00:01,  3.38it/s] 99%|█████████▊| 345/350 [04:44<00:01,  3.41it/s] 99%|█████████▉| 346/350 [04:45<00:01,  3.42it/s] 99%|█████████▉| 347/350 [04:45<00:00,  3.43it/s] 99%|█████████▉| 348/350 [04:45<00:00,  3.44it/s]100%|█████████▉| 349/350 [04:45<00:00,  3.45it/s]100%|██████████| 350/350 [04:46<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 21:22:02,483 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:22:02,483 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:22:02,483 >>   Batch size = 8
{'eval_loss': 1.120902419090271, 'eval_runtime': 9.8428, 'eval_samples_per_second': 353.455, 'eval_steps_per_second': 44.195, 'epoch': 3.99}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.73it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.52it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.52it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.67it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.16it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.56it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.25it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.05it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.08it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.20it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.34it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.49it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.53it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.35it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.12it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.07it/s][A
 20%|██        | 88/435 [00:01<00:07, 44.98it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.94it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.04it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.22it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.43it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.45it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 45.28it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.18it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.09it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.96it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 44.95it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 44.98it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.16it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.22it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.34it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 42.87it/s][A
 39%|███▊      | 168/435 [00:03<00:06, 43.70it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.09it/s][A
 41%|████      | 178/435 [00:03<00:05, 44.35it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 44.46it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 44.71it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 44.92it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.00it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.92it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.94it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.13it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.17it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.18it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.05it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.16it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.06it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.12it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.10it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 45.12it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 45.20it/s][A
 60%|██████    | 263/435 [00:05<00:03, 45.20it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 45.24it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.13it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.19it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.00it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.19it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 45.12it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 45.16it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 45.07it/s][A
 71%|███████   | 308/435 [00:06<00:02, 45.16it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 45.19it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.16it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.08it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.00it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.09it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.15it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.19it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.18it/s][A
 81%|████████  | 353/435 [00:07<00:01, 45.20it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 45.24it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.15it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.12it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.07it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.04it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.07it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.15it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 45.20it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 45.23it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 45.15it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 45.11it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.12it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.09it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 45.04it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 45.00it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 45.11it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 45.11it/s][A100%|██████████| 350/350 [04:55<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:22:12,389 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 21:22:12,696 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:22:18,611 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:22:18,838 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:22:18,868 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:22:28,965 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:22:28,965 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70 (score: 1.0753414630889893).
                                                 100%|██████████| 350/350 [05:18<00:00,  3.45it/s]100%|██████████| 350/350 [05:18<00:00,  1.10it/s]
[INFO|trainer.py:1894] 2023-08-28 21:22:34,619 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 21:22:34,636 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:22:38,432 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:22:38,491 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:22:38,555 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:22:39,072 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,072 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,073 >>   train_loss               =     0.4062
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,073 >>   train_runtime            = 0:05:18.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,073 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,073 >>   train_samples_per_second =      70.68
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:39,073 >>   train_steps_per_second   =      1.099
{'eval_loss': 1.1269029378890991, 'eval_runtime': 9.7278, 'eval_samples_per_second': 357.634, 'eval_steps_per_second': 44.717, 'epoch': 4.99}
{'train_runtime': 318.3356, 'train_samples_per_second': 70.68, 'train_steps_per_second': 1.099, 'train_loss': 0.40622467041015625, 'epoch': 4.99}
08/28/2023 21:22:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:22:39,217 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:22:39,217 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 21:22:39,217 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 56.99it/s]  3%|▎         | 12/435 [00:00<00:08, 50.05it/s]  4%|▍         | 18/435 [00:00<00:08, 48.28it/s]  5%|▌         | 23/435 [00:00<00:08, 47.46it/s]  6%|▋         | 28/435 [00:00<00:08, 46.99it/s]  8%|▊         | 33/435 [00:00<00:08, 46.54it/s]  9%|▊         | 38/435 [00:00<00:08, 46.49it/s] 10%|▉         | 43/435 [00:00<00:08, 46.02it/s] 11%|█         | 48/435 [00:01<00:08, 45.44it/s] 12%|█▏        | 53/435 [00:01<00:08, 45.20it/s] 13%|█▎        | 58/435 [00:01<00:08, 45.29it/s] 14%|█▍        | 63/435 [00:01<00:08, 45.38it/s] 16%|█▌        | 68/435 [00:01<00:08, 45.54it/s] 17%|█▋        | 73/435 [00:01<00:07, 45.70it/s] 18%|█▊        | 78/435 [00:01<00:07, 45.86it/s] 19%|█▉        | 83/435 [00:01<00:07, 45.82it/s] 20%|██        | 88/435 [00:01<00:07, 45.69it/s] 21%|██▏       | 93/435 [00:02<00:07, 45.21it/s] 23%|██▎       | 98/435 [00:02<00:07, 45.10it/s] 24%|██▎       | 103/435 [00:02<00:07, 45.07it/s] 25%|██▍       | 108/435 [00:02<00:07, 41.78it/s] 26%|██▌       | 113/435 [00:02<00:07, 42.97it/s] 27%|██▋       | 118/435 [00:02<00:07, 43.83it/s] 28%|██▊       | 123/435 [00:02<00:07, 44.38it/s] 29%|██▉       | 128/435 [00:02<00:06, 44.74it/s] 31%|███       | 133/435 [00:02<00:06, 45.02it/s] 32%|███▏      | 138/435 [00:03<00:06, 45.08it/s] 33%|███▎      | 143/435 [00:03<00:06, 45.16it/s] 34%|███▍      | 148/435 [00:03<00:06, 44.84it/s] 35%|███▌      | 153/435 [00:03<00:06, 44.90it/s] 36%|███▋      | 158/435 [00:03<00:06, 45.02it/s] 37%|███▋      | 163/435 [00:03<00:06, 45.31it/s] 39%|███▊      | 168/435 [00:03<00:05, 45.41it/s] 40%|███▉      | 173/435 [00:03<00:05, 45.62it/s] 41%|████      | 178/435 [00:03<00:05, 45.74it/s] 42%|████▏     | 183/435 [00:04<00:05, 45.73it/s] 43%|████▎     | 188/435 [00:04<00:05, 45.48it/s] 44%|████▍     | 193/435 [00:04<00:05, 45.12it/s] 46%|████▌     | 198/435 [00:04<00:05, 45.09it/s] 47%|████▋     | 203/435 [00:04<00:05, 45.15it/s] 48%|████▊     | 208/435 [00:04<00:05, 45.29it/s] 49%|████▉     | 213/435 [00:04<00:04, 45.46it/s] 50%|█████     | 218/435 [00:04<00:04, 45.62it/s] 51%|█████▏    | 223/435 [00:04<00:04, 45.76it/s] 52%|█████▏    | 228/435 [00:05<00:04, 45.68it/s] 54%|█████▎    | 233/435 [00:05<00:04, 45.53it/s] 55%|█████▍    | 238/435 [00:05<00:04, 45.29it/s] 56%|█████▌    | 243/435 [00:05<00:04, 41.44it/s] 57%|█████▋    | 248/435 [00:05<00:04, 42.59it/s] 58%|█████▊    | 253/435 [00:05<00:04, 43.53it/s] 59%|█████▉    | 258/435 [00:05<00:04, 44.15it/s] 60%|██████    | 263/435 [00:05<00:03, 44.52it/s] 62%|██████▏   | 268/435 [00:06<00:05, 32.81it/s] 63%|██████▎   | 273/435 [00:06<00:04, 35.91it/s] 64%|██████▍   | 278/435 [00:06<00:05, 26.83it/s] 65%|██████▍   | 282/435 [00:06<00:05, 27.42it/s] 66%|██████▌   | 287/435 [00:06<00:04, 31.79it/s] 67%|██████▋   | 292/435 [00:06<00:04, 35.08it/s] 68%|██████▊   | 296/435 [00:07<00:11, 11.99it/s] 69%|██████▊   | 299/435 [00:07<00:09, 13.66it/s] 70%|███████   | 305/435 [00:08<00:06, 18.98it/s] 71%|███████▏  | 310/435 [00:08<00:05, 23.25it/s] 72%|███████▏  | 314/435 [00:08<00:06, 17.53it/s] 73%|███████▎  | 317/435 [00:08<00:09, 12.39it/s] 74%|███████▍  | 322/435 [00:09<00:06, 16.59it/s] 75%|███████▌  | 327/435 [00:09<00:05, 21.01it/s] 76%|███████▋  | 332/435 [00:09<00:04, 25.40it/s] 77%|███████▋  | 337/435 [00:09<00:03, 29.55it/s] 79%|███████▊  | 342/435 [00:09<00:02, 33.21it/s] 80%|███████▉  | 347/435 [00:09<00:02, 36.29it/s] 81%|████████  | 352/435 [00:09<00:02, 38.72it/s] 82%|████████▏ | 357/435 [00:09<00:01, 40.43it/s] 83%|████████▎ | 362/435 [00:09<00:01, 41.54it/s] 84%|████████▍ | 367/435 [00:10<00:01, 42.70it/s] 86%|████████▌ | 372/435 [00:10<00:01, 43.46it/s] 87%|████████▋ | 377/435 [00:10<00:01, 44.08it/s] 88%|████████▊ | 382/435 [00:10<00:01, 44.56it/s] 89%|████████▉ | 387/435 [00:10<00:01, 44.91it/s] 90%|█████████ | 392/435 [00:10<00:00, 45.09it/s] 91%|█████████▏| 397/435 [00:10<00:00, 45.20it/s] 92%|█████████▏| 402/435 [00:10<00:00, 44.89it/s] 94%|█████████▎| 407/435 [00:10<00:00, 44.89it/s] 95%|█████████▍| 412/435 [00:11<00:00, 44.96it/s] 96%|█████████▌| 417/435 [00:11<00:00, 45.16it/s] 97%|█████████▋| 422/435 [00:11<00:00, 45.34it/s] 98%|█████████▊| 427/435 [00:11<00:00, 45.55it/s] 99%|█████████▉| 432/435 [00:11<00:00, 45.62it/s]100%|██████████| 435/435 [00:11<00:00, 37.18it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:22:50,936 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   eval_loss               =     1.0753
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   eval_runtime            = 0:00:11.71
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   eval_samples_per_second =     296.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   eval_steps_per_second   =     37.121
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:22:50,936 >>   perplexity              =      2.931
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:21,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:21,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:21,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:21,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:21,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:23:23,836 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:23:23,837 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:24,581 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:26,094 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:26,125 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:31,441 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:31,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:31,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:31,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:31,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:23:33,162 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:23:33,228 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:34,045 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:34,551 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:34,551 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-280
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-350
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-210
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-70
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/checkpoint-140
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.12it/s]Extractor Predicting: 7it [00:05,  1.21it/s]Extractor Predicting: 8it [00:06,  1.30it/s]Extractor Predicting: 9it [00:06,  1.34it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.32it/s]Extractor Predicting: 12it [00:09,  1.35it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:11,  1.34it/s]Extractor Predicting: 17it [00:12,  1.40it/s]Extractor Predicting: 18it [00:13,  1.47it/s]Extractor Predicting: 19it [00:13,  1.47it/s]Extractor Predicting: 20it [00:14,  1.30it/s]Extractor Predicting: 21it [00:15,  1.36it/s]Extractor Predicting: 22it [00:16,  1.43it/s]Extractor Predicting: 23it [00:16,  1.45it/s]Extractor Predicting: 24it [00:17,  1.45it/s]Extractor Predicting: 25it [00:18,  1.22it/s]Extractor Predicting: 26it [00:19,  1.27it/s]Extractor Predicting: 27it [00:20,  1.32it/s]Extractor Predicting: 28it [00:20,  1.39it/s]Extractor Predicting: 29it [00:21,  1.26it/s]Extractor Predicting: 30it [00:22,  1.30it/s]Extractor Predicting: 31it [00:23,  1.35it/s]Extractor Predicting: 32it [00:23,  1.39it/s]Extractor Predicting: 33it [00:24,  1.33it/s]Extractor Predicting: 34it [00:25,  1.40it/s]Extractor Predicting: 35it [00:25,  1.44it/s]Extractor Predicting: 36it [00:26,  1.46it/s]Extractor Predicting: 37it [00:27,  1.48it/s]Extractor Predicting: 38it [00:28,  1.16it/s]Extractor Predicting: 39it [00:29,  1.26it/s]Extractor Predicting: 40it [00:29,  1.32it/s]Extractor Predicting: 41it [00:30,  1.38it/s]Extractor Predicting: 42it [00:31,  1.25it/s]Extractor Predicting: 43it [00:31,  1.34it/s]Extractor Predicting: 44it [00:32,  1.41it/s]Extractor Predicting: 45it [00:33,  1.43it/s]Extractor Predicting: 46it [00:33,  1.46it/s]Extractor Predicting: 47it [00:34,  1.35it/s]Extractor Predicting: 48it [00:35,  1.40it/s]Extractor Predicting: 49it [00:36,  1.44it/s]Extractor Predicting: 50it [00:36,  1.46it/s]Extractor Predicting: 51it [00:37,  1.50it/s]Extractor Predicting: 52it [00:38,  1.43it/s]Extractor Predicting: 53it [00:38,  1.47it/s]Extractor Predicting: 54it [00:39,  1.48it/s]Extractor Predicting: 55it [00:40,  1.46it/s]Extractor Predicting: 56it [00:40,  1.51it/s]Extractor Predicting: 57it [00:41,  1.36it/s]Extractor Predicting: 58it [00:42,  1.40it/s]Extractor Predicting: 59it [00:42,  1.46it/s]Extractor Predicting: 60it [00:43,  1.52it/s]Extractor Predicting: 61it [00:44,  1.56it/s]Extractor Predicting: 62it [00:45,  1.39it/s]Extractor Predicting: 63it [00:45,  1.42it/s]Extractor Predicting: 64it [00:46,  1.47it/s]Extractor Predicting: 65it [00:47,  1.50it/s]Extractor Predicting: 66it [00:47,  1.51it/s]Extractor Predicting: 67it [00:48,  1.49it/s]Extractor Predicting: 68it [00:48,  1.51it/s]Extractor Predicting: 69it [00:49,  1.44it/s]Extractor Predicting: 70it [00:50,  1.48it/s]Extractor Predicting: 71it [00:51,  1.52it/s]Extractor Predicting: 72it [00:52,  1.28it/s]Extractor Predicting: 73it [00:52,  1.36it/s]Extractor Predicting: 74it [00:53,  1.41it/s]Extractor Predicting: 75it [00:54,  1.42it/s]Extractor Predicting: 76it [00:54,  1.40it/s]Extractor Predicting: 77it [00:55,  1.45it/s]Extractor Predicting: 78it [00:56,  1.49it/s]Extractor Predicting: 79it [00:56,  1.53it/s]Extractor Predicting: 80it [00:57,  1.53it/s]Extractor Predicting: 81it [00:58,  1.42it/s]Extractor Predicting: 82it [00:58,  1.46it/s]Extractor Predicting: 83it [00:59,  1.50it/s]Extractor Predicting: 84it [01:00,  1.53it/s]Extractor Predicting: 85it [01:00,  1.55it/s]Extractor Predicting: 86it [01:01,  1.49it/s]Extractor Predicting: 87it [01:01,  1.55it/s]Extractor Predicting: 88it [01:02,  1.55it/s]Extractor Predicting: 89it [01:03,  1.57it/s]Extractor Predicting: 90it [01:03,  1.59it/s]Extractor Predicting: 91it [01:04,  1.52it/s]Extractor Predicting: 92it [01:05,  1.59it/s]Extractor Predicting: 93it [01:05,  1.58it/s]Extractor Predicting: 94it [01:06,  1.59it/s]Extractor Predicting: 95it [01:06,  1.61it/s]Extractor Predicting: 96it [01:07,  1.55it/s]Extractor Predicting: 97it [01:08,  1.57it/s]Extractor Predicting: 98it [01:08,  1.58it/s]Extractor Predicting: 99it [01:09,  1.57it/s]Extractor Predicting: 100it [01:10,  1.53it/s]Extractor Predicting: 101it [01:11,  1.41it/s]Extractor Predicting: 102it [01:11,  1.53it/s]Extractor Predicting: 103it [01:12,  1.57it/s]Extractor Predicting: 104it [01:12,  1.58it/s]Extractor Predicting: 105it [01:13,  1.59it/s]Extractor Predicting: 106it [01:14,  1.53it/s]Extractor Predicting: 107it [01:14,  1.55it/s]Extractor Predicting: 108it [01:15,  1.56it/s]Extractor Predicting: 109it [01:16,  1.58it/s]Extractor Predicting: 110it [01:16,  1.60it/s]Extractor Predicting: 111it [01:17,  1.47it/s]Extractor Predicting: 112it [01:18,  1.52it/s]Extractor Predicting: 113it [01:18,  1.59it/s]Extractor Predicting: 114it [01:19,  1.53it/s]Extractor Predicting: 115it [01:19,  1.59it/s]Extractor Predicting: 116it [01:20,  1.57it/s]Extractor Predicting: 117it [01:21,  1.55it/s]Extractor Predicting: 118it [01:21,  1.57it/s]Extractor Predicting: 119it [01:22,  1.52it/s]Extractor Predicting: 120it [01:23,  1.54it/s]Extractor Predicting: 121it [01:23,  1.54it/s]Extractor Predicting: 122it [01:24,  1.52it/s]Extractor Predicting: 123it [01:25,  1.55it/s]Extractor Predicting: 124it [01:25,  1.48it/s]Extractor Predicting: 125it [01:26,  1.53it/s]Extractor Predicting: 126it [01:27,  1.52it/s]Extractor Predicting: 127it [01:27,  1.56it/s]Extractor Predicting: 128it [01:28,  1.57it/s]Extractor Predicting: 129it [01:29,  1.36it/s]Extractor Predicting: 130it [01:30,  1.40it/s]Extractor Predicting: 131it [01:30,  1.46it/s]Extractor Predicting: 132it [01:31,  1.47it/s]Extractor Predicting: 133it [01:31,  1.47it/s]Extractor Predicting: 134it [01:32,  1.45it/s]Extractor Predicting: 135it [01:33,  1.48it/s]Extractor Predicting: 136it [01:34,  1.48it/s]Extractor Predicting: 137it [01:34,  1.51it/s]Extractor Predicting: 138it [01:35,  1.49it/s]Extractor Predicting: 139it [01:36,  1.41it/s]Extractor Predicting: 140it [01:36,  1.33it/s]Extractor Predicting: 141it [01:37,  1.39it/s]Extractor Predicting: 142it [01:38,  1.41it/s]Extractor Predicting: 143it [01:38,  1.45it/s]Extractor Predicting: 144it [01:39,  1.75it/s]Extractor Predicting: 144it [01:39,  1.45it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:51,228 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:51,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:51,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:51,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:25:51,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:25:53,620 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:25:53,622 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:25:54,375 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:25:56,148 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:25:56,148 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:01,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:02,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:02,166 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:02,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:02,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:26:04,138 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:26:04,139 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:26:05,492 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:26:06,868 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:26:06,868 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.15789473684210525,
  "recall": 0.052601322219028454,
  "score": 0.07891332470892626,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:04,  1.35it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:13,  1.58it/s]Extractor Predicting: 21it [00:13,  1.42it/s]Extractor Predicting: 22it [00:14,  1.36it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.58it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:20,  1.25it/s]Extractor Predicting: 32it [00:21,  1.37it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:22,  1.38it/s]Extractor Predicting: 35it [00:24,  1.11it/s]Extractor Predicting: 36it [00:24,  1.23it/s]Extractor Predicting: 37it [00:25,  1.34it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:26,  1.42it/s]Extractor Predicting: 40it [00:27,  1.46it/s]Extractor Predicting: 41it [00:28,  1.50it/s]Extractor Predicting: 42it [00:28,  1.58it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:30,  1.41it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:32,  1.54it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.53it/s]Extractor Predicting: 52it [00:35,  1.53it/s]Extractor Predicting: 53it [00:35,  1.56it/s]Extractor Predicting: 54it [00:36,  1.32it/s]Extractor Predicting: 55it [00:37,  1.42it/s]Extractor Predicting: 56it [00:38,  1.46it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.51it/s]Extractor Predicting: 59it [00:40,  1.45it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:41,  1.49it/s]Extractor Predicting: 62it [00:42,  1.55it/s]Extractor Predicting: 63it [00:42,  1.56it/s]Extractor Predicting: 64it [00:43,  1.53it/s]Extractor Predicting: 65it [00:44,  1.53it/s]Extractor Predicting: 66it [00:44,  1.52it/s]Extractor Predicting: 67it [00:45,  1.53it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.55it/s]Extractor Predicting: 71it [00:47,  1.55it/s]Extractor Predicting: 72it [00:48,  1.36it/s]Extractor Predicting: 73it [00:49,  1.43it/s]Extractor Predicting: 74it [00:50,  1.47it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 76it [00:51,  1.51it/s]Extractor Predicting: 77it [00:52,  1.47it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:53,  1.57it/s]Extractor Predicting: 80it [00:53,  1.61it/s]Extractor Predicting: 81it [00:54,  1.62it/s]Extractor Predicting: 82it [00:55,  1.57it/s]Extractor Predicting: 83it [00:55,  1.57it/s]Extractor Predicting: 84it [00:56,  1.56it/s]Extractor Predicting: 85it [00:57,  1.54it/s]Extractor Predicting: 86it [00:57,  1.51it/s]Extractor Predicting: 87it [00:58,  1.44it/s]Extractor Predicting: 88it [00:59,  1.47it/s]Extractor Predicting: 89it [00:59,  1.46it/s]Extractor Predicting: 90it [01:00,  1.48it/s]Extractor Predicting: 91it [01:01,  1.47it/s]Extractor Predicting: 92it [01:02,  1.15it/s]Extractor Predicting: 93it [01:03,  1.25it/s]Extractor Predicting: 94it [01:03,  1.33it/s]Extractor Predicting: 95it [01:04,  1.40it/s]Extractor Predicting: 96it [01:05,  1.14it/s]Extractor Predicting: 97it [01:06,  1.23it/s]Extractor Predicting: 98it [01:07,  1.30it/s]Extractor Predicting: 99it [01:07,  1.38it/s]Extractor Predicting: 100it [01:08,  1.30it/s]Extractor Predicting: 101it [01:09,  1.39it/s]Extractor Predicting: 102it [01:09,  1.42it/s]Extractor Predicting: 103it [01:10,  1.43it/s]Extractor Predicting: 104it [01:11,  1.50it/s]Extractor Predicting: 105it [01:11,  1.44it/s]Extractor Predicting: 106it [01:12,  1.47it/s]Extractor Predicting: 107it [01:13,  1.52it/s]Extractor Predicting: 108it [01:13,  1.52it/s]Extractor Predicting: 109it [01:14,  1.52it/s]Extractor Predicting: 110it [01:15,  1.46it/s]Extractor Predicting: 111it [01:16,  1.12it/s]Extractor Predicting: 112it [01:17,  1.21it/s]Extractor Predicting: 113it [01:17,  1.30it/s]Extractor Predicting: 114it [01:18,  1.38it/s]Extractor Predicting: 115it [01:19,  1.26it/s]Extractor Predicting: 116it [01:20,  1.36it/s]Extractor Predicting: 117it [01:20,  1.40it/s]Extractor Predicting: 118it [01:21,  1.44it/s]Extractor Predicting: 119it [01:22,  1.47it/s]Extractor Predicting: 120it [01:23,  1.06it/s]Extractor Predicting: 121it [01:24,  1.19it/s]Extractor Predicting: 122it [01:24,  1.28it/s]Extractor Predicting: 123it [01:25,  1.36it/s]Extractor Predicting: 124it [01:26,  1.08it/s]Extractor Predicting: 125it [01:27,  1.21it/s]Extractor Predicting: 126it [01:28,  1.31it/s]Extractor Predicting: 127it [01:28,  1.38it/s]Extractor Predicting: 128it [01:29,  1.17it/s]Extractor Predicting: 129it [01:30,  1.28it/s]Extractor Predicting: 130it [01:31,  1.35it/s]Extractor Predicting: 131it [01:31,  1.44it/s]Extractor Predicting: 132it [01:33,  1.10it/s]Extractor Predicting: 133it [01:33,  1.24it/s]Extractor Predicting: 134it [01:34,  1.31it/s]Extractor Predicting: 135it [01:34,  1.42it/s]Extractor Predicting: 136it [01:35,  1.35it/s]Extractor Predicting: 137it [01:36,  1.44it/s]Extractor Predicting: 138it [01:36,  1.49it/s]Extractor Predicting: 139it [01:37,  1.53it/s]Extractor Predicting: 140it [01:38,  1.57it/s]Extractor Predicting: 141it [01:39,  1.22it/s]Extractor Predicting: 142it [01:40,  1.20it/s]Extractor Predicting: 143it [01:40,  1.31it/s]Extractor Predicting: 144it [01:41,  1.38it/s]Extractor Predicting: 145it [01:42,  1.30it/s]Extractor Predicting: 146it [01:42,  1.40it/s]Extractor Predicting: 147it [01:43,  1.48it/s]Extractor Predicting: 148it [01:44,  1.56it/s]Extractor Predicting: 149it [01:44,  1.59it/s]Extractor Predicting: 150it [01:45,  1.48it/s]Extractor Predicting: 151it [01:46,  1.55it/s]Extractor Predicting: 152it [01:46,  1.61it/s]Extractor Predicting: 153it [01:47,  1.62it/s]Extractor Predicting: 154it [01:47,  1.64it/s]Extractor Predicting: 155it [01:48,  1.50it/s]Extractor Predicting: 156it [01:49,  1.55it/s]Extractor Predicting: 157it [01:49,  1.67it/s]Extractor Predicting: 158it [01:50,  1.72it/s]Extractor Predicting: 159it [01:50,  1.70it/s]Extractor Predicting: 160it [01:51,  1.56it/s]Extractor Predicting: 161it [01:52,  1.59it/s]Extractor Predicting: 162it [01:52,  1.64it/s]Extractor Predicting: 163it [01:53,  1.69it/s]Extractor Predicting: 164it [01:53,  1.71it/s]Extractor Predicting: 165it [01:54,  1.73it/s]Extractor Predicting: 166it [01:55,  1.61it/s]Extractor Predicting: 167it [01:55,  1.68it/s]Extractor Predicting: 168it [01:56,  1.68it/s]Extractor Predicting: 169it [01:56,  1.75it/s]Extractor Predicting: 170it [01:57,  1.75it/s]Extractor Predicting: 171it [01:57,  1.76it/s]Extractor Predicting: 172it [01:58,  1.39it/s]Extractor Predicting: 173it [01:59,  1.45it/s]Extractor Predicting: 174it [02:00,  1.48it/s]Extractor Predicting: 175it [02:00,  1.47it/s]Extractor Predicting: 176it [02:01,  1.45it/s]Extractor Predicting: 177it [02:02,  1.49it/s]Extractor Predicting: 178it [02:02,  1.50it/s]Extractor Predicting: 179it [02:03,  1.52it/s]Extractor Predicting: 180it [02:04,  1.54it/s]Extractor Predicting: 181it [02:04,  1.46it/s]Extractor Predicting: 182it [02:05,  1.48it/s]Extractor Predicting: 183it [02:06,  1.51it/s]Extractor Predicting: 184it [02:06,  1.51it/s]Extractor Predicting: 185it [02:07,  1.50it/s]Extractor Predicting: 186it [02:08,  1.48it/s]Extractor Predicting: 187it [02:08,  1.50it/s]Extractor Predicting: 188it [02:09,  1.51it/s]Extractor Predicting: 189it [02:10,  1.52it/s]Extractor Predicting: 190it [02:10,  1.55it/s]Extractor Predicting: 191it [02:11,  1.44it/s]Extractor Predicting: 192it [02:12,  1.45it/s]Extractor Predicting: 193it [02:13,  1.47it/s]Extractor Predicting: 194it [02:13,  1.48it/s]Extractor Predicting: 195it [02:14,  1.50it/s]Extractor Predicting: 196it [02:15,  1.48it/s]Extractor Predicting: 197it [02:15,  1.49it/s]Extractor Predicting: 198it [02:16,  1.52it/s]Extractor Predicting: 199it [02:16,  1.55it/s]Extractor Predicting: 200it [02:17,  1.53it/s]Extractor Predicting: 201it [02:18,  1.44it/s]Extractor Predicting: 202it [02:18,  1.53it/s]Extractor Predicting: 203it [02:19,  1.53it/s]Extractor Predicting: 204it [02:20,  1.56it/s]Extractor Predicting: 205it [02:20,  1.56it/s]Extractor Predicting: 206it [02:21,  1.49it/s]Extractor Predicting: 207it [02:22,  1.41it/s]Extractor Predicting: 208it [02:23,  1.45it/s]Extractor Predicting: 209it [02:23,  1.43it/s]Extractor Predicting: 210it [02:24,  1.47it/s]Extractor Predicting: 211it [02:25,  1.47it/s]Extractor Predicting: 212it [02:26,  1.32it/s]Extractor Predicting: 213it [02:26,  1.37it/s]Extractor Predicting: 214it [02:27,  1.40it/s]Extractor Predicting: 215it [02:27,  1.46it/s]Extractor Predicting: 216it [02:28,  1.45it/s]Extractor Predicting: 217it [02:29,  1.42it/s]Extractor Predicting: 218it [02:30,  1.45it/s]Extractor Predicting: 219it [02:30,  1.46it/s]Extractor Predicting: 220it [02:31,  1.49it/s]Extractor Predicting: 221it [02:32,  1.47it/s]Extractor Predicting: 222it [02:32,  1.38it/s]Extractor Predicting: 223it [02:33,  1.45it/s]Extractor Predicting: 224it [02:34,  1.47it/s]Extractor Predicting: 225it [02:34,  1.49it/s]Extractor Predicting: 226it [02:35,  1.49it/s]Extractor Predicting: 227it [02:36,  1.40it/s]Extractor Predicting: 228it [02:36,  1.44it/s]Extractor Predicting: 229it [02:37,  1.47it/s]Extractor Predicting: 230it [02:38,  1.54it/s]Extractor Predicting: 231it [02:38,  1.57it/s]Extractor Predicting: 232it [02:39,  1.55it/s]Extractor Predicting: 233it [02:40,  1.55it/s]Extractor Predicting: 234it [02:40,  1.57it/s]Extractor Predicting: 235it [02:41,  1.57it/s]Extractor Predicting: 236it [02:42,  1.56it/s]Extractor Predicting: 237it [02:43,  1.15it/s]Extractor Predicting: 238it [02:44,  1.26it/s]Extractor Predicting: 239it [02:44,  1.32it/s]Extractor Predicting: 240it [02:45,  1.38it/s]Extractor Predicting: 241it [02:46,  1.29it/s]Extractor Predicting: 242it [02:46,  1.38it/s]Extractor Predicting: 243it [02:47,  1.41it/s]Extractor Predicting: 244it [02:48,  1.46it/s]Extractor Predicting: 245it [02:48,  1.51it/s]Extractor Predicting: 246it [02:49,  1.31it/s]Extractor Predicting: 247it [02:50,  1.38it/s]Extractor Predicting: 248it [02:51,  1.43it/s]Extractor Predicting: 249it [02:51,  1.44it/s]Extractor Predicting: 250it [02:52,  1.48it/s]Extractor Predicting: 251it [02:53,  1.21it/s]Extractor Predicting: 252it [02:54,  1.31it/s]Extractor Predicting: 253it [02:54,  1.36it/s]Extractor Predicting: 254it [02:55,  1.43it/s]Extractor Predicting: 255it [02:56,  1.34it/s]Extractor Predicting: 256it [02:56,  1.36it/s]Extractor Predicting: 257it [02:57,  1.40it/s]Extractor Predicting: 258it [02:58,  1.42it/s]Extractor Predicting: 259it [02:58,  1.46it/s]Extractor Predicting: 260it [02:59,  1.39it/s]Extractor Predicting: 261it [03:00,  1.46it/s]Extractor Predicting: 262it [03:01,  1.46it/s]Extractor Predicting: 263it [03:01,  1.46it/s]Extractor Predicting: 264it [03:02,  1.48it/s]Extractor Predicting: 265it [03:03,  1.31it/s]Extractor Predicting: 266it [03:04,  1.36it/s]Extractor Predicting: 267it [03:04,  1.39it/s]Extractor Predicting: 268it [03:05,  1.39it/s]Extractor Predicting: 269it [03:06,  1.21it/s]Extractor Predicting: 270it [03:07,  1.30it/s]Extractor Predicting: 271it [03:07,  1.33it/s]Extractor Predicting: 272it [03:08,  1.38it/s]Extractor Predicting: 273it [03:09,  1.33it/s]Extractor Predicting: 274it [03:09,  1.40it/s]Extractor Predicting: 275it [03:10,  1.45it/s]Extractor Predicting: 276it [03:11,  1.51it/s]Extractor Predicting: 277it [03:11,  1.53it/s]Extractor Predicting: 278it [03:12,  1.51it/s]Extractor Predicting: 279it [03:13,  1.52it/s]Extractor Predicting: 280it [03:13,  1.48it/s]Extractor Predicting: 281it [03:14,  1.49it/s]Extractor Predicting: 282it [03:15,  1.59it/s]Extractor Predicting: 282it [03:15,  1.44it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:59,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:59,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:59,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:59,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:59,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:30:02,189 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:30:02,190 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:03,498 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:06,037 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:06,037 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:15,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:15,882 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:15,882 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:15,882 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:15,882 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:30:16,907 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:30:16,908 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:18,227 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:18,388 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:19,166 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20996613449443638,
  "recall": 0.12840236686390533,
  "score": 0.15935377271892784,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 5it [00:03,  1.29it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:30:34,001 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:30:34,136 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:30:34,279 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:30:34,280 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:30:34,341 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:31:11,602 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:31:11,653 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:31:12,178 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:31:12,179 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:31:12,480 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:31:13,133 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.36486486486486486,
  "recall": 0.1125,
  "score": 0.17197452229299362,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:31:14,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:14,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:15,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:16,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:17,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:18,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:19,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:20,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:21,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:22,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:23,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:23,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:24,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:25,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:26,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:03, 13.08s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:27,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:27,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:28,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:29,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:29,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:30,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:30,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:31,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:32,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:32,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:33,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:34,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:35,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:36,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:37,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:38,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:38,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:43, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:39,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:40,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:42,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:42,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:43,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:44,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:45,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:47,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:47,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:48,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:48,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:49,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:50,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:51,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:51,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:52,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:53,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:54,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:55,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:50, 14.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:31:55,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:56,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:57,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:58,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:59,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:59,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:00,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:01,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:01,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:02,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:03,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:04,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:04,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:05,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:06,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:06,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:07,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:07,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:29, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:08,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:08,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:09,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:10,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:11,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:11,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:12,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:13,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:14,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:15,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:15,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:16,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:17,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:17,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:18,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:19,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:19,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:20,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:20,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:21,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:16, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:22,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:22,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:23,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:24,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:24,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:25,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:25,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:26,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:27,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:27,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:28,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:29,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:30,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:30,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:31,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:32,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:32,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:33,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:33,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:20<01:58, 13.21s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:34,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:35,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:35,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:36,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:36,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:37,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:38,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:39,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:40,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:41,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:42,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:43,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:44,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:44,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:45,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:46,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:47,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:33<01:46, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:32:48,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:48,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:49,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:50,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:51,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:52,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:52,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:53,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:53,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:54,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:55,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:56,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:56,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:57,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:57,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:58,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:59,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:32:59,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:00,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:46<01:32, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:01,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:02,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:02,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:03,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:04,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:04,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:05,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:05,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:06,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:07,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:08,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:08,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:09,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:09,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:10,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:11,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:11,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:13,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:13,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:59<01:19, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:14,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:15,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:15,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:16,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:16,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:17,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:18,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:18,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:19,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:20,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:20,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:21,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:22,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:22,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:23,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:23,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:24,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:25,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:25,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:26,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:12<01:05, 13.08s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:27,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:27,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:28,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:28,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:29,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:29,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:30,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:31,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:31,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:32,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:32,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:33,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:33,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:34,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:35,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:35,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:36,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:36,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:37,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:38,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:38,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:39,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:25<00:52, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:39,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:40,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:40,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:41,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:42,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:42,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:44,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:44,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:45,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:45,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:46,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:47,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:47,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:48,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:49,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:49,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:50,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:50,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:51,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:37<00:38, 12.74s/it][WARNING|generation_utils.py:914] 2023-08-28 21:33:52,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:52,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:53,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:53,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:54,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:54,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:55,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:55,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:56,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:56,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:57,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:57,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:58,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:59,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:33:59,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:00,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:00,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:01,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:02,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:02,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:48<00:24, 12.29s/it][WARNING|generation_utils.py:914] 2023-08-28 21:34:03,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:03,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:04,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:05,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:06,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:06,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:07,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:07,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:08,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:09,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:09,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:10,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:11,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:11,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:13,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:14,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:14,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:16,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:16,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:17,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:03<00:13, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-28 21:34:17,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:18,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:19,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:20,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:21,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:21,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:22,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:22,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:24,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:25,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:26,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:27,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:27,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:28,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:30,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:31,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:32,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:32,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:34:34,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:20<00:00, 14.12s/it]Generating: 100%|██████████| 15/15 [03:20<00:00, 13.36s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:55,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:55,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:55,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:55,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:34:55,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:34:57,308 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:34:57,309 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:34:58,273 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:34:59,355 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:34:59,533 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:04,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:35:06,148 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:35:06,149 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:07,550 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:07,723 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:08,085 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 7570
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7670, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.48it/s]Extractor Estimating: 2it [00:01,  1.36it/s]Extractor Estimating: 3it [00:02,  1.52it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.59it/s]Extractor Estimating: 12it [00:07,  1.51it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.61it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:10,  1.66it/s]Extractor Estimating: 17it [00:10,  1.61it/s]Extractor Estimating: 18it [00:11,  1.65it/s]Extractor Estimating: 19it [00:11,  1.68it/s]Extractor Estimating: 20it [00:12,  1.70it/s]Extractor Estimating: 21it [00:13,  1.69it/s]Extractor Estimating: 22it [00:13,  1.70it/s]Extractor Estimating: 23it [00:14,  1.61it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:15,  1.61it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:16,  1.55it/s]Extractor Estimating: 28it [00:17,  1.43it/s]Extractor Estimating: 29it [00:18,  1.51it/s]Extractor Estimating: 30it [00:18,  1.56it/s]Extractor Estimating: 31it [00:19,  1.56it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:21,  1.38it/s]Extractor Estimating: 34it [00:21,  1.46it/s]Extractor Estimating: 35it [00:22,  1.50it/s]Extractor Estimating: 36it [00:22,  1.52it/s]Extractor Estimating: 37it [00:23,  1.54it/s]Extractor Estimating: 38it [00:24,  1.54it/s]Extractor Estimating: 39it [00:24,  1.58it/s]Extractor Estimating: 40it [00:25,  1.51it/s]Extractor Estimating: 41it [00:26,  1.55it/s]Extractor Estimating: 42it [00:26,  1.55it/s]Extractor Estimating: 43it [00:27,  1.32it/s]Extractor Estimating: 44it [00:28,  1.38it/s]Extractor Estimating: 45it [00:29,  1.29it/s]Extractor Estimating: 46it [00:29,  1.40it/s]Extractor Estimating: 47it [00:30,  1.46it/s]Extractor Estimating: 48it [00:31,  1.37it/s]Extractor Estimating: 49it [00:32,  1.35it/s]Extractor Estimating: 50it [00:32,  1.37it/s]Extractor Estimating: 51it [00:33,  1.47it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:34,  1.58it/s]Extractor Estimating: 54it [00:35,  1.40it/s]Extractor Estimating: 55it [00:36,  1.47it/s]Extractor Estimating: 56it [00:36,  1.52it/s]Extractor Estimating: 57it [00:37,  1.54it/s]Extractor Estimating: 58it [00:37,  1.57it/s]Extractor Estimating: 59it [00:38,  1.53it/s]Extractor Estimating: 60it [00:39,  1.61it/s]Extractor Estimating: 61it [00:39,  1.59it/s]Extractor Estimating: 62it [00:40,  1.67it/s]Extractor Estimating: 63it [00:41,  1.65it/s]Extractor Estimating: 64it [00:42,  1.21it/s]Extractor Estimating: 65it [00:42,  1.34it/s]Extractor Estimating: 66it [00:43,  1.41it/s]Extractor Estimating: 67it [00:44,  1.50it/s]Extractor Estimating: 68it [00:44,  1.38it/s]Extractor Estimating: 69it [00:45,  1.50it/s]Extractor Estimating: 70it [00:46,  1.54it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:47,  1.58it/s]Extractor Estimating: 73it [00:48,  1.51it/s]Extractor Estimating: 74it [00:48,  1.53it/s]Extractor Estimating: 75it [00:49,  1.56it/s]Extractor Estimating: 76it [00:49,  1.64it/s]Extractor Estimating: 77it [00:50,  1.73it/s]Extractor Estimating: 78it [00:51,  1.67it/s]Extractor Estimating: 79it [00:51,  1.73it/s]Extractor Estimating: 80it [00:52,  1.82it/s]Extractor Estimating: 81it [00:52,  1.78it/s]Extractor Estimating: 82it [00:53,  1.78it/s]Extractor Estimating: 83it [00:53,  1.79it/s]Extractor Estimating: 84it [00:54,  1.68it/s]Extractor Estimating: 85it [00:54,  1.72it/s]Extractor Estimating: 86it [00:55,  1.74it/s]Extractor Estimating: 87it [00:56,  1.71it/s]Extractor Estimating: 88it [00:56,  1.76it/s]Extractor Estimating: 89it [00:57,  1.75it/s]Extractor Estimating: 90it [00:57,  1.75it/s]Extractor Estimating: 91it [00:58,  1.81it/s]Extractor Estimating: 92it [00:58,  1.82it/s]Extractor Estimating: 93it [00:59,  1.78it/s]Extractor Estimating: 94it [01:00,  1.77it/s]Extractor Estimating: 95it [01:00,  1.79it/s]Extractor Estimating: 96it [01:01,  1.77it/s]Extractor Estimating: 97it [01:01,  1.77it/s]Extractor Estimating: 98it [01:02,  1.72it/s]Extractor Estimating: 99it [01:03,  1.51it/s]Extractor Estimating: 100it [01:03,  1.56it/s]Extractor Estimating: 101it [01:04,  1.60it/s]Extractor Estimating: 102it [01:04,  1.67it/s]Extractor Estimating: 103it [01:05,  1.67it/s]Extractor Estimating: 104it [01:06,  1.63it/s]Extractor Estimating: 105it [01:06,  1.70it/s]Extractor Estimating: 106it [01:07,  1.70it/s]Extractor Estimating: 107it [01:07,  1.76it/s]Extractor Estimating: 108it [01:08,  1.82it/s]Extractor Estimating: 109it [01:08,  1.78it/s]Extractor Estimating: 110it [01:09,  1.77it/s]Extractor Estimating: 111it [01:09,  1.79it/s]Extractor Estimating: 112it [01:10,  1.81it/s]Extractor Estimating: 113it [01:11,  1.83it/s]Extractor Estimating: 114it [01:11,  1.84it/s]Extractor Estimating: 115it [01:12,  1.82it/s]Extractor Estimating: 116it [01:12,  1.71it/s]Extractor Estimating: 117it [01:13,  1.66it/s]Extractor Estimating: 118it [01:14,  1.67it/s]Extractor Estimating: 119it [01:14,  1.71it/s]Extractor Estimating: 120it [01:15,  1.80it/s]Extractor Estimating: 121it [01:15,  1.81it/s]Extractor Estimating: 122it [01:16,  1.75it/s]Extractor Estimating: 123it [01:16,  1.77it/s]Extractor Estimating: 124it [01:17,  1.74it/s]Extractor Estimating: 125it [01:17,  1.78it/s]Extractor Estimating: 126it [01:18,  1.82it/s]Extractor Estimating: 127it [01:18,  1.91it/s]Extractor Estimating: 128it [01:19,  1.93it/s]Extractor Estimating: 129it [01:19,  1.94it/s]Extractor Estimating: 130it [01:20,  2.01it/s]Extractor Estimating: 131it [01:20,  2.02it/s]Extractor Estimating: 132it [01:21,  2.02it/s]Extractor Estimating: 133it [01:21,  1.99it/s]Extractor Estimating: 134it [01:22,  1.94it/s]Extractor Estimating: 135it [01:23,  1.88it/s]Extractor Estimating: 136it [01:23,  1.92it/s]Extractor Estimating: 137it [01:23,  1.96it/s]Extractor Estimating: 138it [01:24,  1.95it/s]Extractor Estimating: 139it [01:25,  1.92it/s]Extractor Estimating: 140it [01:25,  1.92it/s]Extractor Estimating: 141it [01:26,  1.73it/s]Extractor Estimating: 142it [01:26,  1.79it/s]Extractor Estimating: 143it [01:27,  1.91it/s]Extractor Estimating: 144it [01:27,  1.89it/s]Extractor Estimating: 145it [01:28,  1.94it/s]Extractor Estimating: 146it [01:28,  1.94it/s]Extractor Estimating: 147it [01:29,  1.88it/s]Extractor Estimating: 148it [01:29,  1.98it/s]Extractor Estimating: 149it [01:30,  2.07it/s]Extractor Estimating: 150it [01:30,  1.97it/s]Extractor Estimating: 151it [01:31,  1.93it/s]Extractor Estimating: 152it [01:32,  1.76it/s]Extractor Estimating: 153it [01:32,  1.74it/s]Extractor Estimating: 154it [01:33,  1.80it/s]Extractor Estimating: 155it [01:33,  1.68it/s]Extractor Estimating: 156it [01:34,  1.69it/s]Extractor Estimating: 157it [01:34,  1.75it/s]Extractor Estimating: 158it [01:35,  1.79it/s]Extractor Estimating: 159it [01:36,  1.75it/s]Extractor Estimating: 160it [01:36,  1.74it/s]Extractor Estimating: 161it [01:37,  1.68it/s]Extractor Estimating: 162it [01:37,  1.72it/s]Extractor Estimating: 163it [01:38,  1.67it/s]Extractor Estimating: 164it [01:38,  1.74it/s]Extractor Estimating: 165it [01:39,  1.79it/s]Extractor Estimating: 166it [01:40,  1.71it/s]Extractor Estimating: 167it [01:41,  1.43it/s]Extractor Estimating: 168it [01:41,  1.52it/s]Extractor Estimating: 169it [01:42,  1.61it/s]Extractor Estimating: 170it [01:42,  1.57it/s]Extractor Estimating: 171it [01:43,  1.59it/s]Extractor Estimating: 172it [01:44,  1.51it/s]Extractor Estimating: 173it [01:44,  1.56it/s]Extractor Estimating: 174it [01:45,  1.60it/s]Extractor Estimating: 175it [01:46,  1.50it/s]Extractor Estimating: 176it [01:46,  1.56it/s]Extractor Estimating: 177it [01:47,  1.58it/s]Extractor Estimating: 178it [01:47,  1.63it/s]Extractor Estimating: 179it [01:48,  1.67it/s]Extractor Estimating: 180it [01:49,  1.67it/s]Extractor Estimating: 181it [01:49,  1.72it/s]Extractor Estimating: 182it [01:50,  1.68it/s]Extractor Estimating: 183it [01:50,  1.59it/s]Extractor Estimating: 184it [01:51,  1.65it/s]Extractor Estimating: 185it [01:52,  1.71it/s]Extractor Estimating: 186it [01:52,  1.75it/s]Extractor Estimating: 187it [01:53,  1.76it/s]Extractor Estimating: 188it [01:53,  1.66it/s]Extractor Estimating: 189it [01:54,  1.61it/s]Extractor Estimating: 190it [01:55,  1.65it/s]Extractor Estimating: 191it [01:55,  1.62it/s]Extractor Estimating: 192it [01:56,  1.68it/s]Extractor Estimating: 193it [01:56,  1.67it/s]Extractor Estimating: 194it [01:57,  1.56it/s]Extractor Estimating: 195it [01:58,  1.58it/s]Extractor Estimating: 196it [01:58,  1.63it/s]Extractor Estimating: 197it [01:59,  1.69it/s]Extractor Estimating: 198it [01:59,  1.70it/s]Extractor Estimating: 199it [02:01,  1.20it/s]Extractor Estimating: 200it [02:02,  1.26it/s]Extractor Estimating: 201it [02:02,  1.33it/s]Extractor Estimating: 202it [02:03,  1.40it/s]Extractor Estimating: 203it [02:03,  1.47it/s]Extractor Estimating: 204it [02:04,  1.44it/s]Extractor Estimating: 205it [02:05,  1.54it/s]Extractor Estimating: 206it [02:05,  1.55it/s]Extractor Estimating: 207it [02:06,  1.58it/s]Extractor Estimating: 208it [02:07,  1.57it/s]Extractor Estimating: 209it [02:07,  1.42it/s]Extractor Estimating: 210it [02:08,  1.45it/s]Extractor Estimating: 211it [02:09,  1.51it/s]Extractor Estimating: 212it [02:09,  1.57it/s]Extractor Estimating: 213it [02:10,  1.56it/s]Extractor Estimating: 214it [02:11,  1.46it/s]Extractor Estimating: 215it [02:11,  1.49it/s]Extractor Estimating: 216it [02:12,  1.53it/s]Extractor Estimating: 217it [02:13,  1.56it/s]Extractor Estimating: 218it [02:13,  1.60it/s]Extractor Estimating: 219it [02:14,  1.53it/s]Extractor Estimating: 220it [02:15,  1.54it/s]Extractor Estimating: 221it [02:15,  1.57it/s]Extractor Estimating: 222it [02:16,  1.62it/s]Extractor Estimating: 223it [02:16,  1.61it/s]Extractor Estimating: 224it [02:17,  1.53it/s]Extractor Estimating: 225it [02:18,  1.63it/s]Extractor Estimating: 226it [02:18,  1.70it/s]Extractor Estimating: 227it [02:19,  1.73it/s]Extractor Estimating: 228it [02:19,  1.75it/s]Extractor Estimating: 229it [02:20,  1.84it/s]Extractor Estimating: 230it [02:20,  1.69it/s]Extractor Estimating: 231it [02:21,  1.59it/s]Extractor Estimating: 232it [02:22,  1.66it/s]Extractor Estimating: 233it [02:22,  1.76it/s]Extractor Estimating: 234it [02:23,  1.81it/s]Extractor Estimating: 235it [02:23,  1.85it/s]Extractor Estimating: 236it [02:24,  1.76it/s]Extractor Estimating: 237it [02:24,  1.82it/s]Extractor Estimating: 238it [02:25,  1.85it/s]Extractor Estimating: 239it [02:25,  1.87it/s]Extractor Estimating: 240it [02:26,  1.89it/s]Extractor Estimating: 241it [02:26,  1.93it/s]Extractor Estimating: 242it [02:27,  1.86it/s]Extractor Estimating: 243it [02:27,  1.92it/s]Extractor Estimating: 244it [02:28,  1.89it/s]Extractor Estimating: 245it [02:28,  1.95it/s]Extractor Estimating: 246it [02:29,  1.98it/s]Extractor Estimating: 247it [02:29,  1.97it/s]Extractor Estimating: 248it [02:30,  1.73it/s]Extractor Estimating: 249it [02:31,  1.80it/s]Extractor Estimating: 250it [02:31,  1.79it/s]Extractor Estimating: 251it [02:32,  1.80it/s]Extractor Estimating: 252it [02:32,  1.81it/s]Extractor Estimating: 253it [02:33,  1.81it/s]Extractor Estimating: 254it [02:33,  1.79it/s]Extractor Estimating: 255it [02:34,  1.80it/s]Extractor Estimating: 256it [02:35,  1.69it/s]Extractor Estimating: 257it [02:35,  1.73it/s]Extractor Estimating: 258it [02:36,  1.73it/s]Extractor Estimating: 259it [02:36,  1.75it/s]Extractor Estimating: 260it [02:37,  1.78it/s]Extractor Estimating: 261it [02:37,  1.76it/s]Extractor Estimating: 262it [02:38,  1.59it/s]Extractor Estimating: 263it [02:39,  1.66it/s]Extractor Estimating: 264it [02:39,  1.74it/s]Extractor Estimating: 265it [02:40,  1.76it/s]Extractor Estimating: 266it [02:40,  1.81it/s]Extractor Estimating: 267it [02:41,  1.75it/s]Extractor Estimating: 268it [02:42,  1.49it/s]Extractor Estimating: 269it [02:42,  1.62it/s]Extractor Estimating: 270it [02:43,  1.72it/s]Extractor Estimating: 271it [02:43,  1.76it/s]Extractor Estimating: 272it [02:44,  1.77it/s]Extractor Estimating: 273it [02:44,  1.83it/s]Extractor Estimating: 274it [02:45,  1.73it/s]Extractor Estimating: 275it [02:46,  1.76it/s]Extractor Estimating: 276it [02:46,  1.72it/s]Extractor Estimating: 277it [02:47,  1.74it/s]Extractor Estimating: 278it [02:47,  1.73it/s]Extractor Estimating: 279it [02:48,  1.75it/s]Extractor Estimating: 280it [02:49,  1.65it/s]Extractor Estimating: 281it [02:49,  1.72it/s]Extractor Estimating: 282it [02:50,  1.63it/s]Extractor Estimating: 283it [02:50,  1.64it/s]Extractor Estimating: 284it [02:51,  1.66it/s]Extractor Estimating: 285it [02:52,  1.60it/s]Extractor Estimating: 286it [02:52,  1.62it/s]Extractor Estimating: 287it [02:53,  1.68it/s]Extractor Estimating: 288it [02:53,  1.71it/s]Extractor Estimating: 289it [02:54,  1.77it/s]Extractor Estimating: 290it [02:55,  1.73it/s]Extractor Estimating: 291it [02:55,  1.69it/s]Extractor Estimating: 292it [02:56,  1.72it/s]Extractor Estimating: 293it [02:56,  1.71it/s]Extractor Estimating: 294it [02:57,  1.72it/s]Extractor Estimating: 295it [02:58,  1.73it/s]Extractor Estimating: 296it [02:58,  1.74it/s]Extractor Estimating: 297it [02:59,  1.60it/s]Extractor Estimating: 298it [02:59,  1.62it/s]Extractor Estimating: 299it [03:00,  1.64it/s]Extractor Estimating: 300it [03:01,  1.69it/s]Extractor Estimating: 301it [03:01,  1.72it/s]Extractor Estimating: 302it [03:02,  1.65it/s]Extractor Estimating: 303it [03:02,  1.60it/s]Extractor Estimating: 304it [03:03,  1.65it/s]Extractor Estimating: 305it [03:03,  1.76it/s]Extractor Estimating: 306it [03:04,  1.84it/s]Extractor Estimating: 307it [03:05,  1.86it/s]Extractor Estimating: 308it [03:05,  1.69it/s]Extractor Estimating: 309it [03:06,  1.76it/s]Extractor Estimating: 310it [03:06,  1.79it/s]Extractor Estimating: 311it [03:07,  1.84it/s]Extractor Estimating: 312it [03:08,  1.65it/s]Extractor Estimating: 313it [03:08,  1.71it/s]Extractor Estimating: 314it [03:09,  1.71it/s]Extractor Estimating: 315it [03:09,  1.76it/s]Extractor Estimating: 316it [03:10,  1.74it/s]Extractor Estimating: 317it [03:10,  1.80it/s]Extractor Estimating: 318it [03:11,  1.83it/s]Extractor Estimating: 319it [03:12,  1.46it/s]Extractor Estimating: 320it [03:12,  1.59it/s]Extractor Estimating: 321it [03:13,  1.62it/s]Extractor Estimating: 322it [03:13,  1.73it/s]Extractor Estimating: 323it [03:14,  1.78it/s]Extractor Estimating: 324it [03:15,  1.70it/s]Extractor Estimating: 325it [03:15,  1.74it/s]Extractor Estimating: 326it [03:16,  1.68it/s]Extractor Estimating: 327it [03:16,  1.72it/s]Extractor Estimating: 328it [03:17,  1.71it/s]Extractor Estimating: 329it [03:17,  1.79it/s]Extractor Estimating: 330it [03:18,  1.54it/s]Extractor Estimating: 331it [03:19,  1.61it/s]Extractor Estimating: 332it [03:19,  1.59it/s]Extractor Estimating: 333it [03:20,  1.57it/s]Extractor Estimating: 334it [03:21,  1.63it/s]Extractor Estimating: 335it [03:21,  1.59it/s]Extractor Estimating: 336it [03:22,  1.64it/s]Extractor Estimating: 337it [03:22,  1.65it/s]Extractor Estimating: 338it [03:23,  1.72it/s]Extractor Estimating: 339it [03:24,  1.70it/s]Extractor Estimating: 340it [03:24,  1.73it/s]Extractor Estimating: 341it [03:25,  1.64it/s]Extractor Estimating: 342it [03:25,  1.66it/s]Extractor Estimating: 343it [03:26,  1.63it/s]Extractor Estimating: 344it [03:27,  1.65it/s]Extractor Estimating: 345it [03:27,  1.66it/s]Extractor Estimating: 346it [03:28,  1.54it/s]Extractor Estimating: 347it [03:29,  1.60it/s]Extractor Estimating: 348it [03:29,  1.66it/s]Extractor Estimating: 349it [03:30,  1.65it/s]Extractor Estimating: 350it [03:30,  1.70it/s]Extractor Estimating: 351it [03:31,  1.48it/s]Extractor Estimating: 352it [03:32,  1.53it/s]Extractor Estimating: 353it [03:32,  1.60it/s]Extractor Estimating: 354it [03:33,  1.53it/s]Extractor Estimating: 355it [03:34,  1.63it/s]Extractor Estimating: 356it [03:34,  1.67it/s]Extractor Estimating: 357it [03:35,  1.75it/s]Extractor Estimating: 358it [03:35,  1.80it/s]Extractor Estimating: 359it [03:36,  1.76it/s]Extractor Estimating: 360it [03:36,  1.70it/s]Extractor Estimating: 361it [03:37,  1.72it/s]Extractor Estimating: 362it [03:38,  1.73it/s]Extractor Estimating: 363it [03:38,  1.77it/s]Extractor Estimating: 364it [03:39,  1.80it/s]Extractor Estimating: 365it [03:39,  1.75it/s]Extractor Estimating: 366it [03:40,  1.75it/s]Extractor Estimating: 367it [03:40,  1.76it/s]Extractor Estimating: 368it [03:41,  1.80it/s]Extractor Estimating: 369it [03:41,  1.75it/s]Extractor Estimating: 370it [03:42,  1.75it/s]Extractor Estimating: 371it [03:43,  1.65it/s]Extractor Estimating: 372it [03:43,  1.69it/s]Extractor Estimating: 373it [03:44,  1.74it/s]Extractor Estimating: 374it [03:44,  1.81it/s]Extractor Estimating: 375it [03:45,  2.07it/s]Extractor Estimating: 375it [03:45,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:34,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:34,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:34,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:34,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:34,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:39:36,120 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:39:36,121 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:37,420 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:38,894 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:38,894 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:43,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:43,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:43,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:43,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:39:43,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:39:45,495 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:39:45,496 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:39:47,382 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:39:47,984 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:39:47,984 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:21:30,100 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:21:30,466 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5993 mean pseudo reward: 0.9656232116400236
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16065
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16165, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16165, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.975, loss:442.7832
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.953, loss:401.7139
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.954, loss:376.7478
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 0.959, loss:362.9542
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 0.951, loss:363.4675
>> valid entity prec:0.4533, rec:0.5097, f1:0.4798
>> valid relation prec:0.0820, rec:0.0414, f1:0.0550
>> valid relation with NER prec:0.0820, rec:0.0414, f1:0.0550
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.960, loss:334.9205
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 0.970, loss:349.0210
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.955, loss:330.1643
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 0.967, loss:346.0961
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 0.964, loss:360.5601
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4603, rec:0.4795, f1:0.4697
>> valid relation prec:0.0798, rec:0.0480, f1:0.0599
>> valid relation with NER prec:0.0798, rec:0.0480, f1:0.0599
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 100, avg_time 0.969, loss:328.6200
g_step 1200, step 200, avg_time 0.948, loss:334.4442
g_step 1300, step 50, avg_time 0.961, loss:332.9718
g_step 1400, step 150, avg_time 0.975, loss:315.2177
g_step 1500, step 250, avg_time 0.957, loss:324.8715
>> valid entity prec:0.4637, rec:0.4519, f1:0.4577
>> valid relation prec:0.0775, rec:0.0354, f1:0.0486
>> valid relation with NER prec:0.0775, rec:0.0354, f1:0.0486
g_step 1600, step 100, avg_time 0.965, loss:294.3276
g_step 1700, step 200, avg_time 0.971, loss:314.5046
g_step 1800, step 50, avg_time 0.955, loss:293.9657
g_step 1900, step 150, avg_time 0.958, loss:297.4224
g_step 2000, step 250, avg_time 0.970, loss:297.6458
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4589, rec:0.4608, f1:0.4599
>> valid relation prec:0.0867, rec:0.0460, f1:0.0601
>> valid relation with NER prec:0.0867, rec:0.0460, f1:0.0601
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 100, avg_time 0.970, loss:254.0993
g_step 2200, step 200, avg_time 0.957, loss:282.5725
g_step 2300, step 50, avg_time 0.973, loss:273.7226
g_step 2400, step 150, avg_time 0.955, loss:267.7177
g_step 2500, step 250, avg_time 0.954, loss:274.2122
>> valid entity prec:0.4484, rec:0.4675, f1:0.4578
>> valid relation prec:0.0808, rec:0.0454, f1:0.0582
>> valid relation with NER prec:0.0808, rec:0.0454, f1:0.0582
g_step 2600, step 100, avg_time 0.958, loss:247.7422
g_step 2700, step 200, avg_time 0.976, loss:245.4996
g_step 2800, step 50, avg_time 0.961, loss:259.0295
g_step 2900, step 150, avg_time 0.952, loss:237.8507
g_step 3000, step 250, avg_time 0.969, loss:249.2378
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4626, rec:0.4312, f1:0.4463
>> valid relation prec:0.0855, rec:0.0469, f1:0.0605
>> valid relation with NER prec:0.0855, rec:0.0469, f1:0.0605
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 100, avg_time 0.948, loss:217.9220
g_step 3200, step 200, avg_time 0.976, loss:233.7241
g_step 3300, step 50, avg_time 0.968, loss:225.5631
g_step 3400, step 150, avg_time 0.959, loss:220.1648
g_step 3500, step 250, avg_time 0.954, loss:236.5790
>> valid entity prec:0.4470, rec:0.4793, f1:0.4626
>> valid relation prec:0.0857, rec:0.0515, f1:0.0643
>> valid relation with NER prec:0.0857, rec:0.0515, f1:0.0643
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 100, avg_time 0.954, loss:214.5216
g_step 3700, step 200, avg_time 0.963, loss:225.8036
g_step 3800, step 50, avg_time 0.956, loss:197.0227
g_step 3900, step 150, avg_time 0.973, loss:214.7701
g_step 4000, step 250, avg_time 0.964, loss:216.9464
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4517, rec:0.4491, f1:0.4504
>> valid relation prec:0.0975, rec:0.0517, f1:0.0676
>> valid relation with NER prec:0.0975, rec:0.0517, f1:0.0676
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 100, avg_time 0.978, loss:196.6902
g_step 4200, step 200, avg_time 0.958, loss:191.8700
g_step 4300, step 50, avg_time 0.954, loss:188.5116
g_step 4400, step 150, avg_time 0.970, loss:197.5517
g_step 4500, step 250, avg_time 0.963, loss:197.2077
>> valid entity prec:0.4855, rec:0.4392, f1:0.4612
>> valid relation prec:0.0899, rec:0.0517, f1:0.0657
>> valid relation with NER prec:0.0899, rec:0.0517, f1:0.0657
g_step 4600, step 100, avg_time 0.951, loss:190.0164
g_step 4700, step 200, avg_time 0.981, loss:193.5144
g_step 4800, step 50, avg_time 0.963, loss:173.7410
g_step 4900, step 150, avg_time 0.967, loss:172.4419
g_step 5000, step 250, avg_time 0.962, loss:197.2927
learning rate was adjusted to 0.0008
>> valid entity prec:0.4571, rec:0.4598, f1:0.4585
>> valid relation prec:0.0757, rec:0.0469, f1:0.0579
>> valid relation with NER prec:0.0757, rec:0.0469, f1:0.0579
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:21:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:21:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-21-30_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:21:32 - WARNING - datasets.builder -   Using custom data configuration default-5110204e4a502bec
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5110204e4a502bec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:21:36,926 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:21:36,927 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:21:36,928 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:21:36,929 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:21:37,304 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,524 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,524 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,524 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,524 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:37,524 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:21:38,523 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:21:41,914 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:21:42,004 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5110204e4a502bec/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:03,  1.59ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.67ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.42ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.95ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.32ba/s]100%|██████████| 6/6 [00:01<00:00,  4.57ba/s]100%|██████████| 6/6 [00:01<00:00,  3.75ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.82ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.14ba/s]100%|██████████| 4/4 [00:00<00:00,  5.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.62ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.45ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.01ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.57ba/s]100%|██████████| 6/6 [00:00<00:00,  8.02ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  5.12ba/s]100%|██████████| 4/4 [00:00<00:00,  5.54ba/s]
[INFO|trainer.py:414] 2023-08-28 23:21:48,225 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:21:48,385 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:21:48,385 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-28 23:21:48,385 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:21:48,386 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:21:48,386 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:21:48,386 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:21:48,386 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:01<09:14,  1.18s/it]  0%|          | 2/470 [00:01<06:41,  1.17it/s]  1%|          | 3/470 [00:02<05:28,  1.42it/s]  1%|          | 4/470 [00:02<04:59,  1.56it/s]  1%|          | 5/470 [00:03<04:40,  1.66it/s]  1%|▏         | 6/470 [00:03<03:57,  1.96it/s]  1%|▏         | 7/470 [00:04<03:29,  2.21it/s]  2%|▏         | 8/470 [00:04<03:17,  2.34it/s]  2%|▏         | 9/470 [00:04<02:57,  2.60it/s]  2%|▏         | 10/470 [00:05<02:43,  2.82it/s]  2%|▏         | 11/470 [00:05<02:33,  2.99it/s]  3%|▎         | 12/470 [00:05<02:26,  3.12it/s]  3%|▎         | 13/470 [00:05<02:30,  3.04it/s]  3%|▎         | 14/470 [00:06<02:48,  2.71it/s]  3%|▎         | 15/470 [00:06<02:36,  2.90it/s]  3%|▎         | 16/470 [00:06<02:28,  3.06it/s]  4%|▎         | 17/470 [00:07<02:22,  3.17it/s]  4%|▍         | 18/470 [00:07<02:18,  3.26it/s]  4%|▍         | 19/470 [00:07<02:15,  3.32it/s]  4%|▍         | 20/470 [00:08<02:13,  3.36it/s]  4%|▍         | 21/470 [00:08<02:12,  3.40it/s]  5%|▍         | 22/470 [00:08<02:10,  3.42it/s]  5%|▍         | 23/470 [00:09<02:09,  3.44it/s]  5%|▌         | 24/470 [00:09<02:16,  3.28it/s]  5%|▌         | 25/470 [00:09<02:13,  3.33it/s]  6%|▌         | 26/470 [00:09<02:11,  3.37it/s]  6%|▌         | 27/470 [00:10<02:10,  3.41it/s]  6%|▌         | 28/470 [00:10<02:09,  3.43it/s]  6%|▌         | 29/470 [00:10<02:08,  3.44it/s]  6%|▋         | 30/470 [00:11<02:07,  3.46it/s]  7%|▋         | 31/470 [00:11<02:06,  3.46it/s]  7%|▋         | 32/470 [00:11<02:19,  3.14it/s]  7%|▋         | 33/470 [00:12<02:15,  3.23it/s]  7%|▋         | 34/470 [00:12<02:20,  3.10it/s]  7%|▋         | 35/470 [00:12<02:15,  3.21it/s]  8%|▊         | 36/470 [00:12<02:12,  3.28it/s]  8%|▊         | 37/470 [00:13<02:09,  3.34it/s]  8%|▊         | 38/470 [00:13<02:07,  3.38it/s]  8%|▊         | 39/470 [00:13<02:06,  3.41it/s]  9%|▊         | 40/470 [00:14<02:05,  3.43it/s]  9%|▊         | 41/470 [00:14<02:04,  3.44it/s]  9%|▉         | 42/470 [00:14<02:03,  3.46it/s]  9%|▉         | 43/470 [00:15<02:08,  3.32it/s]  9%|▉         | 44/470 [00:15<02:06,  3.37it/s] 10%|▉         | 45/470 [00:15<02:04,  3.40it/s] 10%|▉         | 46/470 [00:15<02:04,  3.41it/s] 10%|█         | 47/470 [00:16<02:03,  3.43it/s] 10%|█         | 48/470 [00:16<02:02,  3.44it/s] 10%|█         | 49/470 [00:16<02:01,  3.45it/s] 11%|█         | 50/470 [00:17<02:01,  3.46it/s] 11%|█         | 51/470 [00:17<02:00,  3.47it/s] 11%|█         | 52/470 [00:17<02:00,  3.47it/s] 11%|█▏        | 53/470 [00:17<02:00,  3.47it/s] 11%|█▏        | 54/470 [00:18<02:07,  3.26it/s] 12%|█▏        | 55/470 [00:18<02:05,  3.32it/s] 12%|█▏        | 56/470 [00:18<02:03,  3.37it/s] 12%|█▏        | 57/470 [00:19<02:01,  3.39it/s] 12%|█▏        | 58/470 [00:19<02:00,  3.42it/s] 13%|█▎        | 59/470 [00:19<01:59,  3.44it/s] 13%|█▎        | 60/470 [00:19<01:58,  3.45it/s] 13%|█▎        | 61/470 [00:20<01:58,  3.45it/s] 13%|█▎        | 62/470 [00:20<01:57,  3.46it/s] 13%|█▎        | 63/470 [00:20<01:57,  3.47it/s] 14%|█▎        | 64/470 [00:21<01:57,  3.47it/s] 14%|█▍        | 65/470 [00:21<02:00,  3.35it/s] 14%|█▍        | 66/470 [00:21<01:59,  3.38it/s] 14%|█▍        | 67/470 [00:22<01:58,  3.41it/s] 14%|█▍        | 68/470 [00:22<01:57,  3.43it/s] 15%|█▍        | 69/470 [00:22<01:56,  3.44it/s] 15%|█▍        | 70/470 [00:22<01:55,  3.45it/s] 15%|█▌        | 71/470 [00:23<01:55,  3.45it/s] 15%|█▌        | 72/470 [00:23<01:55,  3.46it/s] 16%|█▌        | 73/470 [00:23<01:54,  3.46it/s] 16%|█▌        | 74/470 [00:24<01:54,  3.46it/s] 16%|█▌        | 75/470 [00:24<01:53,  3.47it/s] 16%|█▌        | 76/470 [00:24<02:12,  2.98it/s] 16%|█▋        | 77/470 [00:25<02:06,  3.12it/s] 17%|█▋        | 78/470 [00:25<02:01,  3.22it/s] 17%|█▋        | 79/470 [00:25<01:58,  3.29it/s] 17%|█▋        | 80/470 [00:25<01:56,  3.34it/s] 17%|█▋        | 81/470 [00:26<01:55,  3.38it/s] 17%|█▋        | 82/470 [00:26<01:53,  3.41it/s] 18%|█▊        | 83/470 [00:26<01:52,  3.43it/s] 18%|█▊        | 84/470 [00:27<01:52,  3.44it/s] 18%|█▊        | 85/470 [00:27<01:51,  3.45it/s] 18%|█▊        | 86/470 [00:27<01:57,  3.28it/s] 19%|█▊        | 87/470 [00:27<01:54,  3.34it/s] 19%|█▊        | 88/470 [00:28<01:53,  3.38it/s] 19%|█▉        | 89/470 [00:28<01:51,  3.41it/s] 19%|█▉        | 90/470 [00:28<01:51,  3.42it/s] 19%|█▉        | 91/470 [00:29<01:50,  3.44it/s] 20%|█▉        | 92/470 [00:29<01:49,  3.45it/s] 20%|█▉        | 93/470 [00:29<01:49,  3.46it/s] 20%|██        | 94/470 [00:29<01:41,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 23:22:18,330 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:22:18,330 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:22:18,330 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.29it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.20it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.45it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.66it/s][A
  6%|▌         | 27/435 [00:00<00:14, 28.92it/s][A
  7%|▋         | 32/435 [00:00<00:12, 32.98it/s][A
  9%|▊         | 37/435 [00:00<00:10, 36.25it/s][A
 10%|▉         | 42/435 [00:01<00:10, 38.88it/s][A
 11%|█         | 47/435 [00:01<00:09, 40.79it/s][A
 12%|█▏        | 52/435 [00:01<00:09, 42.30it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 43.34it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 44.07it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 44.05it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 44.13it/s][A
 18%|█▊        | 77/435 [00:01<00:08, 44.35it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.70it/s][A
 20%|██        | 87/435 [00:02<00:07, 45.01it/s][A
 21%|██        | 92/435 [00:02<00:07, 45.23it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 45.47it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 45.64it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 45.75it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 45.37it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 45.20it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 45.08it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 45.25it/s][A
 30%|███       | 132/435 [00:03<00:06, 45.24it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.39it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 45.58it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.60it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.71it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 42.58it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 43.41it/s][A
 38%|███▊      | 167/435 [00:03<00:06, 43.99it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 44.31it/s][A
 41%|████      | 177/435 [00:04<00:05, 44.69it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.91it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 45.10it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.39it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.11it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.11it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 45.26it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 45.29it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 45.34it/s][A
 51%|█████     | 222/435 [00:05<00:04, 45.39it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 45.45it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 45.54it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.47it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.31it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 45.34it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 45.25it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 45.35it/s][A
 60%|██████    | 262/435 [00:05<00:03, 45.43it/s][A
 61%|██████▏   | 267/435 [00:06<00:03, 45.37it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 45.43it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 45.47it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 45.40it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 45.40it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 43.33it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.03it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 44.46it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.72it/s][A
 72%|███████▏  | 312/435 [00:07<00:02, 44.99it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 45.25it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 45.31it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 45.35it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 45.02it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 45.05it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 45.22it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 45.39it/s][A
 81%|████████  | 352/435 [00:07<00:01, 45.42it/s][A
 82%|████████▏ | 357/435 [00:08<00:01, 45.40it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 45.51it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 45.57it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 45.36it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.20it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.08it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.26it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.41it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 39.51it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 42.73it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 43.63it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 44.17it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 44.55it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 44.80it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 43.47it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 44.16it/s][A
                                                 [A                                                
100%|██████████| 435/435 [00:09<00:00, 44.16it/s][A 20%|██        | 94/470 [00:39<01:41,  3.70it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:22:28,625 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 23:22:28,993 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:22:34,016 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:22:34,196 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:22:34,404 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [01:01<1:00:27,  9.67s/it] 20%|██        | 96/470 [01:01<42:51,  6.88s/it]   21%|██        | 97/470 [01:02<30:28,  4.90s/it] 21%|██        | 98/470 [01:02<21:48,  3.52s/it] 21%|██        | 99/470 [01:02<15:46,  2.55s/it] 21%|██▏       | 100/470 [01:03<11:32,  1.87s/it] 21%|██▏       | 101/470 [01:03<08:36,  1.40s/it] 22%|██▏       | 102/470 [01:03<06:32,  1.07s/it] 22%|██▏       | 103/470 [01:03<05:06,  1.20it/s] 22%|██▏       | 104/470 [01:04<04:05,  1.49it/s] 22%|██▏       | 105/470 [01:04<03:23,  1.79it/s] 23%|██▎       | 106/470 [01:04<02:53,  2.09it/s] 23%|██▎       | 107/470 [01:05<02:44,  2.21it/s] 23%|██▎       | 108/470 [01:05<02:26,  2.47it/s] 23%|██▎       | 109/470 [01:05<02:13,  2.70it/s] 23%|██▎       | 110/470 [01:06<02:04,  2.88it/s] 24%|██▎       | 111/470 [01:06<01:58,  3.02it/s] 24%|██▍       | 112/470 [01:06<01:54,  3.13it/s] 24%|██▍       | 113/470 [01:06<01:50,  3.22it/s] 24%|██▍       | 114/470 [01:07<01:48,  3.27it/s] 24%|██▍       | 115/470 [01:07<01:46,  3.32it/s] 25%|██▍       | 116/470 [01:07<01:45,  3.35it/s] 25%|██▍       | 117/470 [01:08<01:48,  3.27it/s] 25%|██▌       | 118/470 [01:08<01:46,  3.31it/s] 25%|██▌       | 119/470 [01:08<01:44,  3.35it/s] 26%|██▌       | 120/470 [01:09<01:43,  3.37it/s] 26%|██▌       | 121/470 [01:09<01:43,  3.38it/s] 26%|██▌       | 122/470 [01:09<01:42,  3.40it/s] 26%|██▌       | 123/470 [01:09<01:42,  3.40it/s] 26%|██▋       | 124/470 [01:10<01:41,  3.41it/s] 27%|██▋       | 125/470 [01:10<01:41,  3.41it/s] 27%|██▋       | 126/470 [01:10<01:40,  3.41it/s] 27%|██▋       | 127/470 [01:11<01:40,  3.42it/s] 27%|██▋       | 128/470 [01:11<01:45,  3.24it/s] 27%|██▋       | 129/470 [01:11<01:43,  3.29it/s] 28%|██▊       | 130/470 [01:12<01:42,  3.33it/s] 28%|██▊       | 131/470 [01:12<01:40,  3.36it/s] 28%|██▊       | 132/470 [01:12<01:40,  3.38it/s] 28%|██▊       | 133/470 [01:12<01:39,  3.39it/s] 29%|██▊       | 134/470 [01:13<01:38,  3.40it/s] 29%|██▊       | 135/470 [01:13<01:38,  3.40it/s] 29%|██▉       | 136/470 [01:13<01:38,  3.40it/s] 29%|██▉       | 137/470 [01:14<01:37,  3.41it/s] 29%|██▉       | 138/470 [01:14<01:37,  3.41it/s] 30%|██▉       | 139/470 [01:14<01:41,  3.27it/s] 30%|██▉       | 140/470 [01:14<01:39,  3.32it/s] 30%|███       | 141/470 [01:15<01:38,  3.34it/s] 30%|███       | 142/470 [01:15<01:37,  3.37it/s] 30%|███       | 143/470 [01:15<01:36,  3.38it/s] 31%|███       | 144/470 [01:16<01:36,  3.39it/s] 31%|███       | 145/470 [01:16<01:36,  3.36it/s] 31%|███       | 146/470 [01:16<01:35,  3.38it/s] 31%|███▏      | 147/470 [01:17<01:35,  3.39it/s] 31%|███▏      | 148/470 [01:17<01:34,  3.40it/s] 32%|███▏      | 149/470 [01:17<01:34,  3.40it/s] 32%|███▏      | 150/470 [01:17<01:33,  3.41it/s] 32%|███▏      | 151/470 [01:18<01:33,  3.41it/s] 32%|███▏      | 152/470 [01:18<01:33,  3.41it/s] 33%|███▎      | 153/470 [01:18<01:32,  3.41it/s] 33%|███▎      | 154/470 [01:19<01:32,  3.41it/s] 33%|███▎      | 155/470 [01:19<01:41,  3.11it/s] 33%|███▎      | 156/470 [01:19<01:38,  3.20it/s] 33%|███▎      | 157/470 [01:20<01:36,  3.26it/s] 34%|███▎      | 158/470 [01:20<01:34,  3.30it/s] 34%|███▍      | 159/470 [01:20<01:33,  3.34it/s] 34%|███▍      | 160/470 [01:20<01:32,  3.36it/s] 34%|███▍      | 161/470 [01:21<01:31,  3.38it/s] 34%|███▍      | 162/470 [01:21<01:30,  3.39it/s] 35%|███▍      | 163/470 [01:21<01:30,  3.40it/s] 35%|███▍      | 164/470 [01:22<01:29,  3.40it/s] 35%|███▌      | 165/470 [01:22<01:32,  3.29it/s] 35%|███▌      | 166/470 [01:22<01:31,  3.33it/s] 36%|███▌      | 167/470 [01:23<01:29,  3.37it/s] 36%|███▌      | 168/470 [01:23<01:28,  3.40it/s] 36%|███▌      | 169/470 [01:23<01:27,  3.42it/s] 36%|███▌      | 170/470 [01:23<01:27,  3.44it/s] 36%|███▋      | 171/470 [01:24<01:26,  3.44it/s] 37%|███▋      | 172/470 [01:24<01:26,  3.45it/s] 37%|███▋      | 173/470 [01:24<01:25,  3.46it/s] 37%|███▋      | 174/470 [01:25<01:25,  3.46it/s] 37%|███▋      | 175/470 [01:25<01:25,  3.46it/s] 37%|███▋      | 176/470 [01:25<01:34,  3.10it/s] 38%|███▊      | 177/470 [01:26<01:31,  3.20it/s] 38%|███▊      | 178/470 [01:26<01:29,  3.28it/s] 38%|███▊      | 179/470 [01:26<01:27,  3.33it/s] 38%|███▊      | 180/470 [01:26<01:26,  3.37it/s] 39%|███▊      | 181/470 [01:27<01:25,  3.40it/s] 39%|███▊      | 182/470 [01:27<01:24,  3.42it/s] 39%|███▉      | 183/470 [01:27<01:23,  3.43it/s] 39%|███▉      | 184/470 [01:28<01:23,  3.44it/s] 39%|███▉      | 185/470 [01:28<01:22,  3.45it/s] 40%|███▉      | 186/470 [01:28<01:22,  3.45it/s] 40%|███▉      | 187/470 [01:28<01:25,  3.33it/s] 40%|████      | 188/470 [01:29<01:18,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 23:23:17,562 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:23:17,562 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:23:17,562 >>   Batch size = 8
{'eval_loss': 1.1086870431900024, 'eval_runtime': 9.8543, 'eval_samples_per_second': 353.044, 'eval_steps_per_second': 44.143, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.11it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.88it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.63it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.68it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.08it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.77it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.53it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.34it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.30it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.48it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.54it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.55it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.44it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.40it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.32it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.15it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.13it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.99it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.22it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.34it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.47it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.50it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 43.49it/s][A
 28%|██▊       | 123/435 [00:02<00:07, 44.05it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 44.36it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.60it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 44.78it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 44.96it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.07it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.28it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.04it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 45.13it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.20it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.32it/s][A
 41%|████      | 178/435 [00:03<00:05, 45.24it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.27it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.17it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.23it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.29it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.24it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 45.27it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.23it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.25it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.34it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.36it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.22it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.33it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.24it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.24it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 43.96it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 44.44it/s][A
 60%|██████    | 263/435 [00:05<00:03, 44.81it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 44.94it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 44.98it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 44.95it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 44.99it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.17it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 45.00it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 45.05it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 45.14it/s][A
 71%|███████   | 308/435 [00:06<00:02, 45.33it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 45.48it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.41it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.26it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.21it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.18it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.20it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.20it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.24it/s][A
 81%|████████  | 353/435 [00:07<00:01, 45.30it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 45.40it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.45it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.35it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.24it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.22it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.20it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.28it/s][A
 90%|█████████ | 393/435 [00:08<00:01, 33.77it/s][A
 91%|█████████▏| 398/435 [00:08<00:01, 36.62it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 38.95it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 40.81it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 42.11it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 43.11it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 43.84it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 44.33it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 44.20it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.20it/s][A 40%|████      | 188/470 [01:38<01:18,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:23:27,509 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 23:23:27,817 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:23:31,852 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:23:32,261 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:23:32,479 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:59<43:50,  9.36s/it] 40%|████      | 190/470 [02:00<31:08,  6.67s/it] 41%|████      | 191/470 [02:00<22:08,  4.76s/it] 41%|████      | 192/470 [02:00<15:50,  3.42s/it] 41%|████      | 193/470 [02:01<11:27,  2.48s/it] 41%|████▏     | 194/470 [02:01<08:23,  1.82s/it] 41%|████▏     | 195/470 [02:01<06:15,  1.37s/it] 42%|████▏     | 196/470 [02:01<04:45,  1.04s/it] 42%|████▏     | 197/470 [02:02<03:43,  1.22it/s] 42%|████▏     | 198/470 [02:02<02:59,  1.51it/s] 42%|████▏     | 199/470 [02:02<02:29,  1.82it/s] 43%|████▎     | 200/470 [02:03<02:16,  1.98it/s] 43%|████▎     | 201/470 [02:03<01:58,  2.27it/s] 43%|████▎     | 202/470 [02:03<01:46,  2.52it/s] 43%|████▎     | 203/470 [02:04<01:37,  2.74it/s] 43%|████▎     | 204/470 [02:04<01:31,  2.91it/s] 44%|████▎     | 205/470 [02:04<01:27,  3.04it/s] 44%|████▍     | 206/470 [02:04<01:23,  3.15it/s] 44%|████▍     | 207/470 [02:05<01:21,  3.22it/s] 44%|████▍     | 208/470 [02:05<01:19,  3.28it/s] 44%|████▍     | 209/470 [02:05<01:18,  3.32it/s] 45%|████▍     | 210/470 [02:06<01:24,  3.09it/s] 45%|████▍     | 211/470 [02:06<01:21,  3.18it/s] 45%|████▌     | 212/470 [02:06<01:19,  3.25it/s] 45%|████▌     | 213/470 [02:07<01:17,  3.30it/s] 46%|████▌     | 214/470 [02:07<01:16,  3.35it/s] 46%|████▌     | 215/470 [02:07<01:15,  3.38it/s] 46%|████▌     | 216/470 [02:07<01:14,  3.40it/s] 46%|████▌     | 217/470 [02:08<01:13,  3.42it/s] 46%|████▋     | 218/470 [02:08<01:13,  3.43it/s] 47%|████▋     | 219/470 [02:08<01:12,  3.44it/s] 47%|████▋     | 220/470 [02:09<01:12,  3.45it/s] 47%|████▋     | 221/470 [02:09<01:14,  3.35it/s] 47%|████▋     | 222/470 [02:09<01:13,  3.38it/s] 47%|████▋     | 223/470 [02:09<01:12,  3.41it/s] 48%|████▊     | 224/470 [02:10<01:11,  3.42it/s] 48%|████▊     | 225/470 [02:10<01:11,  3.44it/s] 48%|████▊     | 226/470 [02:10<01:10,  3.44it/s] 48%|████▊     | 227/470 [02:11<01:10,  3.45it/s] 49%|████▊     | 228/470 [02:11<01:10,  3.46it/s] 49%|████▊     | 229/470 [02:11<01:09,  3.46it/s] 49%|████▉     | 230/470 [02:11<01:09,  3.46it/s] 49%|████▉     | 231/470 [02:12<01:09,  3.46it/s] 49%|████▉     | 232/470 [02:12<01:24,  2.82it/s] 50%|████▉     | 233/470 [02:13<01:19,  2.99it/s] 50%|████▉     | 234/470 [02:13<01:15,  3.12it/s] 50%|█████     | 235/470 [02:13<01:13,  3.22it/s] 50%|█████     | 236/470 [02:13<01:11,  3.29it/s] 50%|█████     | 237/470 [02:14<01:09,  3.34it/s] 51%|█████     | 238/470 [02:14<01:08,  3.37it/s] 51%|█████     | 239/470 [02:14<01:07,  3.40it/s] 51%|█████     | 240/470 [02:15<01:07,  3.42it/s] 51%|█████▏    | 241/470 [02:15<01:06,  3.43it/s] 51%|█████▏    | 242/470 [02:15<01:11,  3.18it/s] 52%|█████▏    | 243/470 [02:16<01:09,  3.26it/s] 52%|█████▏    | 244/470 [02:16<01:08,  3.32it/s] 52%|█████▏    | 245/470 [02:16<01:06,  3.36it/s] 52%|█████▏    | 246/470 [02:16<01:06,  3.39it/s] 53%|█████▎    | 247/470 [02:17<01:05,  3.41it/s] 53%|█████▎    | 248/470 [02:17<01:04,  3.42it/s] 53%|█████▎    | 249/470 [02:17<01:04,  3.43it/s] 53%|█████▎    | 250/470 [02:18<01:03,  3.44it/s] 53%|█████▎    | 251/470 [02:18<01:03,  3.45it/s] 54%|█████▎    | 252/470 [02:18<01:03,  3.45it/s] 54%|█████▍    | 253/470 [02:18<01:02,  3.46it/s] 54%|█████▍    | 254/470 [02:19<01:02,  3.45it/s] 54%|█████▍    | 255/470 [02:19<01:02,  3.46it/s] 54%|█████▍    | 256/470 [02:19<01:01,  3.46it/s] 55%|█████▍    | 257/470 [02:20<01:01,  3.46it/s] 55%|█████▍    | 258/470 [02:20<01:01,  3.46it/s] 55%|█████▌    | 259/470 [02:20<01:00,  3.46it/s] 55%|█████▌    | 260/470 [02:21<01:05,  3.20it/s] 56%|█████▌    | 261/470 [02:21<01:03,  3.27it/s] 56%|█████▌    | 262/470 [02:21<01:02,  3.33it/s] 56%|█████▌    | 263/470 [02:21<01:01,  3.36it/s] 56%|█████▌    | 264/470 [02:22<01:00,  3.40it/s] 56%|█████▋    | 265/470 [02:22<01:00,  3.41it/s] 57%|█████▋    | 266/470 [02:22<00:59,  3.43it/s] 57%|█████▋    | 267/470 [02:23<00:59,  3.42it/s] 57%|█████▋    | 268/470 [02:23<00:59,  3.42it/s] 57%|█████▋    | 269/470 [02:23<00:58,  3.42it/s] 57%|█████▋    | 270/470 [02:23<00:58,  3.41it/s] 58%|█████▊    | 271/470 [02:24<01:02,  3.19it/s] 58%|█████▊    | 272/470 [02:24<01:00,  3.26it/s] 58%|█████▊    | 273/470 [02:24<00:59,  3.30it/s] 58%|█████▊    | 274/470 [02:25<00:58,  3.33it/s] 59%|█████▊    | 275/470 [02:25<00:58,  3.36it/s] 59%|█████▊    | 276/470 [02:25<00:57,  3.37it/s] 59%|█████▉    | 277/470 [02:26<00:57,  3.38it/s] 59%|█████▉    | 278/470 [02:26<00:56,  3.39it/s] 59%|█████▉    | 279/470 [02:26<00:56,  3.40it/s] 60%|█████▉    | 280/470 [02:26<00:55,  3.40it/s] 60%|█████▉    | 281/470 [02:27<00:55,  3.40it/s] 60%|██████    | 282/470 [02:27<01:00,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 23:24:15,999 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:24:15,999 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:24:15,999 >>   Batch size = 8
{'eval_loss': 1.1257106065750122, 'eval_runtime': 9.8052, 'eval_samples_per_second': 354.811, 'eval_steps_per_second': 44.364, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.63it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.72it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.97it/s][A
  5%|▌         | 23/435 [00:00<00:08, 47.13it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.52it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.95it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.25it/s][A
 10%|▉         | 43/435 [00:00<00:08, 44.96it/s][A
 11%|█         | 48/435 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.17it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.44it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.53it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.63it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.60it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.36it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.09it/s][A
 20%|██        | 88/435 [00:01<00:07, 44.81it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 44.81it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.06it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.23it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.40it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 45.48it/s][A
 27%|██▋       | 118/435 [00:02<00:06, 45.60it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.45it/s][A
 29%|██▉       | 128/435 [00:02<00:07, 39.58it/s][A
 31%|███       | 133/435 [00:02<00:07, 41.23it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 42.47it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 43.45it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 44.08it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 44.59it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 44.87it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 45.02it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 44.69it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.54it/s][A
 41%|████      | 178/435 [00:03<00:05, 44.70it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.16it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.30it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.49it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.57it/s][A
 48%|████▊     | 208/435 [00:04<00:04, 45.41it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.01it/s][A
 50%|█████     | 218/435 [00:04<00:04, 44.90it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 44.86it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 44.97it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.16it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.34it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.48it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.57it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 45.37it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 45.19it/s][A
 60%|██████    | 263/435 [00:05<00:04, 39.38it/s][A
 62%|██████▏   | 268/435 [00:05<00:04, 41.16it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 42.39it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 43.39it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 44.07it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 44.56it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 44.87it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 45.06it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 44.72it/s][A
 71%|███████   | 308/435 [00:06<00:02, 44.53it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 44.67it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 44.74it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.05it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.15it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.29it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.44it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.42it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.19it/s][A
 81%|████████  | 353/435 [00:07<00:01, 45.05it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 44.90it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 44.98it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.02it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.14it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.36it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.44it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.42it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 45.34it/s][A
 91%|█████████▏| 398/435 [00:08<00:01, 36.17it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 38.68it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 40.62it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 42.00it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 43.05it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 43.81it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 44.37it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 44.64it/s][A                                                 
                                                 [A 60%|██████    | 282/470 [02:37<01:00,  3.13it/s]
100%|██████████| 435/435 [00:09<00:00, 44.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:24:25,936 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-28 23:24:26,251 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:24:31,622 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:24:32,264 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:24:32,756 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [03:00<31:18, 10.05s/it] 60%|██████    | 284/470 [03:00<22:06,  7.13s/it] 61%|██████    | 285/470 [03:00<15:39,  5.08s/it] 61%|██████    | 286/470 [03:01<11:10,  3.64s/it] 61%|██████    | 287/470 [03:01<08:02,  2.64s/it] 61%|██████▏   | 288/470 [03:01<05:51,  1.93s/it] 61%|██████▏   | 289/470 [03:02<04:20,  1.44s/it] 62%|██████▏   | 290/470 [03:02<03:17,  1.10s/it] 62%|██████▏   | 291/470 [03:02<02:33,  1.17it/s] 62%|██████▏   | 292/470 [03:03<02:02,  1.46it/s] 62%|██████▏   | 293/470 [03:03<01:40,  1.76it/s] 63%|██████▎   | 294/470 [03:03<01:25,  2.06it/s] 63%|██████▎   | 295/470 [03:03<01:16,  2.29it/s] 63%|██████▎   | 296/470 [03:04<01:08,  2.54it/s] 63%|██████▎   | 297/470 [03:04<01:02,  2.75it/s] 63%|██████▎   | 298/470 [03:04<00:58,  2.93it/s] 64%|██████▎   | 299/470 [03:05<00:55,  3.06it/s] 64%|██████▍   | 300/470 [03:05<00:53,  3.16it/s] 64%|██████▍   | 301/470 [03:05<00:52,  3.24it/s] 64%|██████▍   | 302/470 [03:05<00:51,  3.29it/s] 64%|██████▍   | 303/470 [03:06<00:50,  3.33it/s] 65%|██████▍   | 304/470 [03:06<00:49,  3.36it/s] 65%|██████▍   | 305/470 [03:06<00:48,  3.37it/s] 65%|██████▌   | 306/470 [03:07<00:51,  3.20it/s] 65%|██████▌   | 307/470 [03:07<00:49,  3.26it/s] 66%|██████▌   | 308/470 [03:07<00:48,  3.31it/s] 66%|██████▌   | 309/470 [03:08<00:48,  3.34it/s] 66%|██████▌   | 310/470 [03:08<00:47,  3.36it/s] 66%|██████▌   | 311/470 [03:08<00:47,  3.38it/s] 66%|██████▋   | 312/470 [03:08<00:46,  3.39it/s] 67%|██████▋   | 313/470 [03:09<00:46,  3.40it/s] 67%|██████▋   | 314/470 [03:09<00:45,  3.41it/s] 67%|██████▋   | 315/470 [03:09<00:45,  3.41it/s] 67%|██████▋   | 316/470 [03:10<00:45,  3.42it/s] 67%|██████▋   | 317/470 [03:10<00:50,  3.05it/s] 68%|██████▊   | 318/470 [03:10<00:48,  3.16it/s] 68%|██████▊   | 319/470 [03:11<00:46,  3.23it/s] 68%|██████▊   | 320/470 [03:11<00:45,  3.28it/s] 68%|██████▊   | 321/470 [03:11<00:44,  3.32it/s] 69%|██████▊   | 322/470 [03:11<00:44,  3.35it/s] 69%|██████▊   | 323/470 [03:12<00:43,  3.37it/s] 69%|██████▉   | 324/470 [03:12<00:43,  3.39it/s] 69%|██████▉   | 325/470 [03:12<00:42,  3.39it/s] 69%|██████▉   | 326/470 [03:13<00:42,  3.40it/s] 70%|██████▉   | 327/470 [03:13<00:45,  3.18it/s] 70%|██████▉   | 328/470 [03:13<00:43,  3.25it/s] 70%|███████   | 329/470 [03:14<00:42,  3.30it/s] 70%|███████   | 330/470 [03:14<00:41,  3.33it/s] 70%|███████   | 331/470 [03:14<00:41,  3.36it/s] 71%|███████   | 332/470 [03:14<00:40,  3.37it/s] 71%|███████   | 333/470 [03:15<00:40,  3.39it/s] 71%|███████   | 334/470 [03:15<00:40,  3.40it/s] 71%|███████▏  | 335/470 [03:15<00:39,  3.40it/s] 71%|███████▏  | 336/470 [03:16<00:39,  3.41it/s] 72%|███████▏  | 337/470 [03:16<00:38,  3.41it/s] 72%|███████▏  | 338/470 [03:16<00:41,  3.19it/s] 72%|███████▏  | 339/470 [03:17<00:40,  3.26it/s] 72%|███████▏  | 340/470 [03:17<00:39,  3.30it/s] 73%|███████▎  | 341/470 [03:17<00:38,  3.34it/s] 73%|███████▎  | 342/470 [03:17<00:38,  3.36it/s] 73%|███████▎  | 343/470 [03:18<00:37,  3.38it/s] 73%|███████▎  | 344/470 [03:18<00:37,  3.39it/s] 73%|███████▎  | 345/470 [03:18<00:36,  3.40it/s] 74%|███████▎  | 346/470 [03:19<00:36,  3.40it/s] 74%|███████▍  | 347/470 [03:19<00:36,  3.41it/s] 74%|███████▍  | 348/470 [03:19<00:35,  3.41it/s] 74%|███████▍  | 349/470 [03:20<00:35,  3.41it/s] 74%|███████▍  | 350/470 [03:20<00:35,  3.41it/s] 75%|███████▍  | 351/470 [03:20<00:34,  3.41it/s] 75%|███████▍  | 352/470 [03:20<00:34,  3.41it/s] 75%|███████▌  | 353/470 [03:21<00:34,  3.41it/s] 75%|███████▌  | 354/470 [03:21<00:33,  3.42it/s] 76%|███████▌  | 355/470 [03:21<00:33,  3.42it/s] 76%|███████▌  | 356/470 [03:22<00:33,  3.42it/s] 76%|███████▌  | 357/470 [03:22<00:34,  3.27it/s] 76%|███████▌  | 358/470 [03:22<00:33,  3.31it/s] 76%|███████▋  | 359/470 [03:22<00:33,  3.34it/s] 77%|███████▋  | 360/470 [03:23<00:32,  3.36it/s] 77%|███████▋  | 361/470 [03:23<00:32,  3.38it/s] 77%|███████▋  | 362/470 [03:23<00:31,  3.39it/s] 77%|███████▋  | 363/470 [03:24<00:31,  3.40it/s] 77%|███████▋  | 364/470 [03:24<00:31,  3.40it/s] 78%|███████▊  | 365/470 [03:24<00:30,  3.41it/s] 78%|███████▊  | 366/470 [03:25<00:30,  3.41it/s] 78%|███████▊  | 367/470 [03:25<00:30,  3.41it/s] 78%|███████▊  | 368/470 [03:25<00:31,  3.24it/s] 79%|███████▊  | 369/470 [03:25<00:30,  3.29it/s] 79%|███████▊  | 370/470 [03:26<00:30,  3.33it/s] 79%|███████▉  | 371/470 [03:26<00:29,  3.35it/s] 79%|███████▉  | 372/470 [03:26<00:29,  3.37it/s] 79%|███████▉  | 373/470 [03:27<00:28,  3.38it/s] 80%|███████▉  | 374/470 [03:27<00:28,  3.39it/s] 80%|███████▉  | 375/470 [03:27<00:27,  3.40it/s] 80%|████████  | 376/470 [03:27<00:25,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 23:25:16,351 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:25:16,351 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:25:16,351 >>   Batch size = 8
{'eval_loss': 1.1455739736557007, 'eval_runtime': 9.7999, 'eval_samples_per_second': 355.002, 'eval_steps_per_second': 44.388, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.76it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.41it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.46it/s][A
  5%|▌         | 23/435 [00:00<00:11, 37.28it/s][A
  6%|▋         | 28/435 [00:00<00:10, 39.70it/s][A
  8%|▊         | 33/435 [00:00<00:09, 41.43it/s][A
  9%|▊         | 38/435 [00:00<00:09, 42.74it/s][A
 10%|▉         | 43/435 [00:00<00:08, 43.61it/s][A
 11%|█         | 48/435 [00:01<00:08, 44.23it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 44.60it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 44.82it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 44.65it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.72it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 45.02it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.09it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.30it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.45it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.43it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.49it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.30it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 44.87it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 44.89it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 44.96it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 45.02it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 45.25it/s][A
 31%|███       | 133/435 [00:02<00:06, 45.43it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 45.55it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.48it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.25it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.15it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 43.35it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 44.08it/s][A
 39%|███▊      | 168/435 [00:03<00:06, 44.43it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 44.81it/s][A
 41%|████      | 178/435 [00:03<00:05, 45.01it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.23it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.21it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.16it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 44.88it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 44.96it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 44.99it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.13it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.29it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 45.46it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.58it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.41it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.17it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 44.99it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.08it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 45.05it/s][A
 59%|█████▉    | 258/435 [00:05<00:03, 45.23it/s][A
 60%|██████    | 263/435 [00:05<00:03, 45.26it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 45.39it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 45.52it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 45.44it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.15it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.07it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 45.13it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 34.67it/s][A
 70%|██████▉   | 303/435 [00:06<00:03, 37.49it/s][A
 71%|███████   | 308/435 [00:06<00:03, 39.62it/s][A
 72%|███████▏  | 313/435 [00:07<00:02, 41.29it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 42.51it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 43.34it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 43.94it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 44.43it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 44.31it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 44.30it/s][A
 80%|████████  | 348/435 [00:07<00:01, 44.38it/s][A
 81%|████████  | 353/435 [00:07<00:01, 44.76it/s][A
 82%|████████▏ | 358/435 [00:08<00:01, 44.93it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.31it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.45it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.51it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.41it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.14it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 45.00it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 44.89it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 45.09it/s][A
 93%|█████████▎| 403/435 [00:09<00:00, 45.08it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 45.28it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.45it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.55it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 45.43it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 45.18it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 39.28it/s][A                                                 
                                                 [A 80%|████████  | 376/470 [03:37<00:25,  3.64it/s]
100%|██████████| 435/435 [00:09<00:00, 39.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:25:26,420 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-28 23:25:26,850 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:25:31,101 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:25:31,180 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:25:31,233 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:54<12:27,  8.03s/it] 80%|████████  | 378/470 [03:54<08:50,  5.77s/it] 81%|████████  | 379/470 [03:54<06:15,  4.13s/it] 81%|████████  | 380/470 [03:55<04:27,  2.98s/it] 81%|████████  | 381/470 [03:55<03:13,  2.17s/it] 81%|████████▏ | 382/470 [03:55<02:21,  1.61s/it] 81%|████████▏ | 383/470 [03:56<01:45,  1.21s/it] 82%|████████▏ | 384/470 [03:56<01:20,  1.07it/s] 82%|████████▏ | 385/470 [03:56<01:03,  1.34it/s] 82%|████████▏ | 386/470 [03:56<00:51,  1.64it/s] 82%|████████▏ | 387/470 [03:57<00:42,  1.95it/s] 83%|████████▎ | 388/470 [03:57<00:38,  2.10it/s] 83%|████████▎ | 389/470 [03:57<00:34,  2.38it/s] 83%|████████▎ | 390/470 [03:58<00:30,  2.62it/s] 83%|████████▎ | 391/470 [03:58<00:28,  2.81it/s] 83%|████████▎ | 392/470 [04:00<00:55,  1.41it/s] 84%|████████▎ | 393/470 [04:00<00:44,  1.71it/s] 84%|████████▍ | 394/470 [04:00<00:38,  1.96it/s] 84%|████████▍ | 395/470 [04:00<00:33,  2.25it/s] 84%|████████▍ | 396/470 [04:01<00:29,  2.50it/s] 84%|████████▍ | 397/470 [04:01<00:26,  2.72it/s] 85%|████████▍ | 398/470 [04:01<00:24,  2.90it/s] 85%|████████▍ | 399/470 [04:02<00:23,  3.04it/s] 85%|████████▌ | 400/470 [04:02<00:22,  3.14it/s] 85%|████████▌ | 401/470 [04:02<00:21,  3.22it/s] 86%|████████▌ | 402/470 [04:02<00:20,  3.28it/s] 86%|████████▌ | 403/470 [04:03<00:20,  3.32it/s] 86%|████████▌ | 404/470 [04:03<00:19,  3.35it/s] 86%|████████▌ | 405/470 [04:03<00:21,  3.01it/s] 86%|████████▋ | 406/470 [04:04<00:20,  3.12it/s] 87%|████████▋ | 407/470 [04:04<00:19,  3.20it/s] 87%|████████▋ | 408/470 [04:04<00:18,  3.27it/s] 87%|████████▋ | 409/470 [04:05<00:18,  3.31it/s] 87%|████████▋ | 410/470 [04:05<00:17,  3.35it/s] 87%|████████▋ | 411/470 [04:05<00:17,  3.38it/s] 88%|████████▊ | 412/470 [04:06<00:17,  3.41it/s] 88%|████████▊ | 413/470 [04:06<00:16,  3.42it/s] 88%|████████▊ | 414/470 [04:06<00:16,  3.44it/s] 88%|████████▊ | 415/470 [04:06<00:16,  3.27it/s] 89%|████████▊ | 416/470 [04:07<00:16,  3.33it/s] 89%|████████▊ | 417/470 [04:07<00:15,  3.37it/s] 89%|████████▉ | 418/470 [04:07<00:15,  3.40it/s] 89%|████████▉ | 419/470 [04:08<00:14,  3.42it/s] 89%|████████▉ | 420/470 [04:08<00:14,  3.43it/s] 90%|████████▉ | 421/470 [04:08<00:14,  3.44it/s] 90%|████████▉ | 422/470 [04:08<00:13,  3.45it/s] 90%|█████████ | 423/470 [04:09<00:13,  3.46it/s] 90%|█████████ | 424/470 [04:09<00:13,  3.46it/s] 90%|█████████ | 425/470 [04:09<00:12,  3.46it/s] 91%|█████████ | 426/470 [04:10<00:13,  3.35it/s] 91%|█████████ | 427/470 [04:10<00:12,  3.38it/s] 91%|█████████ | 428/470 [04:10<00:12,  3.41it/s] 91%|█████████▏| 429/470 [04:11<00:11,  3.42it/s] 91%|█████████▏| 430/470 [04:11<00:11,  3.44it/s] 92%|█████████▏| 431/470 [04:11<00:11,  3.45it/s] 92%|█████████▏| 432/470 [04:11<00:11,  3.45it/s] 92%|█████████▏| 433/470 [04:12<00:10,  3.46it/s] 92%|█████████▏| 434/470 [04:12<00:10,  3.46it/s] 93%|█████████▎| 435/470 [04:12<00:10,  3.46it/s] 93%|█████████▎| 436/470 [04:13<00:09,  3.46it/s] 93%|█████████▎| 437/470 [04:13<00:10,  3.15it/s] 93%|█████████▎| 438/470 [04:13<00:09,  3.24it/s] 93%|█████████▎| 439/470 [04:13<00:09,  3.31it/s] 94%|█████████▎| 440/470 [04:14<00:08,  3.35it/s] 94%|█████████▍| 441/470 [04:14<00:08,  3.38it/s] 94%|█████████▍| 442/470 [04:14<00:08,  3.41it/s] 94%|█████████▍| 443/470 [04:15<00:07,  3.43it/s] 94%|█████████▍| 444/470 [04:15<00:07,  3.44it/s] 95%|█████████▍| 445/470 [04:15<00:07,  3.45it/s] 95%|█████████▍| 446/470 [04:16<00:06,  3.45it/s] 95%|█████████▌| 447/470 [04:16<00:06,  3.45it/s] 95%|█████████▌| 448/470 [04:16<00:06,  3.15it/s] 96%|█████████▌| 449/470 [04:16<00:06,  3.23it/s] 96%|█████████▌| 450/470 [04:17<00:06,  3.30it/s] 96%|█████████▌| 451/470 [04:17<00:05,  3.35it/s] 96%|█████████▌| 452/470 [04:17<00:05,  3.38it/s] 96%|█████████▋| 453/470 [04:18<00:04,  3.41it/s] 97%|█████████▋| 454/470 [04:18<00:04,  3.42it/s] 97%|█████████▋| 455/470 [04:18<00:04,  3.44it/s] 97%|█████████▋| 456/470 [04:19<00:04,  3.44it/s] 97%|█████████▋| 457/470 [04:19<00:03,  3.45it/s] 97%|█████████▋| 458/470 [04:19<00:03,  3.45it/s] 98%|█████████▊| 459/470 [04:19<00:03,  3.39it/s] 98%|█████████▊| 460/470 [04:20<00:02,  3.41it/s] 98%|█████████▊| 461/470 [04:20<00:02,  3.43it/s] 98%|█████████▊| 462/470 [04:20<00:02,  3.32it/s] 99%|█████████▊| 463/470 [04:21<00:02,  3.36it/s] 99%|█████████▊| 464/470 [04:21<00:01,  3.39it/s] 99%|█████████▉| 465/470 [04:21<00:01,  3.41it/s] 99%|█████████▉| 466/470 [04:21<00:01,  3.43it/s] 99%|█████████▉| 467/470 [04:22<00:00,  3.44it/s]100%|█████████▉| 468/470 [04:22<00:00,  3.44it/s]100%|█████████▉| 469/470 [04:22<00:00,  3.45it/s]100%|██████████| 470/470 [04:23<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 23:26:11,427 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:26:11,427 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:26:11,427 >>   Batch size = 8
{'eval_loss': 1.1546610593795776, 'eval_runtime': 9.86, 'eval_samples_per_second': 352.839, 'eval_steps_per_second': 44.118, 'epoch': 4.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 57.07it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.49it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.47it/s][A
  5%|▌         | 23/435 [00:00<00:11, 35.22it/s][A
  6%|▋         | 28/435 [00:00<00:10, 38.19it/s][A
  8%|▊         | 33/435 [00:00<00:09, 40.37it/s][A
  9%|▊         | 38/435 [00:00<00:09, 41.84it/s][A
 10%|▉         | 43/435 [00:01<00:09, 43.13it/s][A
 11%|█         | 48/435 [00:01<00:08, 43.88it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 44.46it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 44.77it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 44.48it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 44.35it/s][A
 17%|█▋        | 73/435 [00:01<00:08, 44.56it/s][A
 18%|█▊        | 78/435 [00:01<00:09, 37.38it/s][A
 19%|█▉        | 84/435 [00:01<00:08, 40.97it/s][A
 20%|██        | 89/435 [00:02<00:08, 42.26it/s][A
 22%|██▏       | 94/435 [00:02<00:07, 43.24it/s][A
 23%|██▎       | 99/435 [00:02<00:07, 43.97it/s][A
 24%|██▍       | 104/435 [00:02<00:07, 44.46it/s][A
 25%|██▌       | 109/435 [00:02<00:07, 44.85it/s][A
 26%|██▌       | 114/435 [00:02<00:07, 44.99it/s][A
 27%|██▋       | 119/435 [00:02<00:06, 45.16it/s][A
 29%|██▊       | 124/435 [00:02<00:06, 44.70it/s][A
 30%|██▉       | 129/435 [00:02<00:06, 44.58it/s][A
 31%|███       | 134/435 [00:03<00:06, 44.81it/s][A
 32%|███▏      | 139/435 [00:03<00:06, 45.04it/s][A
 33%|███▎      | 144/435 [00:03<00:06, 45.30it/s][A
 34%|███▍      | 149/435 [00:03<00:06, 45.38it/s][A
 35%|███▌      | 154/435 [00:03<00:09, 30.54it/s][A
 37%|███▋      | 159/435 [00:03<00:08, 33.98it/s][A
 38%|███▊      | 164/435 [00:03<00:07, 36.82it/s][A
 39%|███▉      | 169/435 [00:04<00:06, 39.13it/s][A
 40%|████      | 174/435 [00:04<00:06, 40.91it/s][A
 41%|████      | 179/435 [00:04<00:06, 42.33it/s][A
 42%|████▏     | 184/435 [00:04<00:05, 43.24it/s][A
 43%|████▎     | 189/435 [00:04<00:05, 43.86it/s][A
 45%|████▍     | 194/435 [00:04<00:05, 43.98it/s][A
 46%|████▌     | 199/435 [00:04<00:05, 43.90it/s][A
 47%|████▋     | 204/435 [00:04<00:05, 44.22it/s][A
 48%|████▊     | 209/435 [00:04<00:05, 44.56it/s][A
 49%|████▉     | 214/435 [00:05<00:04, 44.84it/s][A
 50%|█████     | 219/435 [00:05<00:04, 45.12it/s][A
 51%|█████▏    | 224/435 [00:05<00:04, 45.37it/s][A
 53%|█████▎    | 229/435 [00:05<00:04, 45.32it/s][A
 54%|█████▍    | 234/435 [00:05<00:04, 45.52it/s][A
 55%|█████▍    | 239/435 [00:05<00:04, 45.17it/s][A
 56%|█████▌    | 244/435 [00:05<00:04, 44.94it/s][A
 57%|█████▋    | 249/435 [00:05<00:04, 44.96it/s][A
 58%|█████▊    | 254/435 [00:05<00:04, 44.93it/s][A
 60%|█████▉    | 259/435 [00:06<00:03, 45.16it/s][A
 61%|██████    | 264/435 [00:06<00:03, 45.27it/s][A
 62%|██████▏   | 269/435 [00:06<00:03, 45.46it/s][A
 63%|██████▎   | 274/435 [00:06<00:03, 45.48it/s][A
 64%|██████▍   | 279/435 [00:06<00:03, 45.59it/s][A
 65%|██████▌   | 284/435 [00:06<00:04, 35.53it/s][A
 66%|██████▋   | 289/435 [00:06<00:03, 38.11it/s][A
 68%|██████▊   | 294/435 [00:06<00:03, 40.10it/s][A
 69%|██████▊   | 299/435 [00:07<00:03, 41.62it/s][A
 70%|██████▉   | 304/435 [00:07<00:03, 42.84it/s][A
 71%|███████   | 309/435 [00:07<00:02, 43.63it/s][A
 72%|███████▏  | 314/435 [00:07<00:02, 44.26it/s][A
 73%|███████▎  | 319/435 [00:07<00:02, 44.54it/s][A
 74%|███████▍  | 324/435 [00:07<00:02, 44.51it/s][A
 76%|███████▌  | 329/435 [00:07<00:02, 44.38it/s][A
 77%|███████▋  | 334/435 [00:07<00:02, 44.46it/s][A
 78%|███████▊  | 339/435 [00:07<00:02, 44.71it/s][A
 79%|███████▉  | 344/435 [00:07<00:02, 45.00it/s][A
 80%|████████  | 349/435 [00:08<00:01, 45.29it/s][A
 81%|████████▏ | 354/435 [00:08<00:01, 45.45it/s][A
 83%|████████▎ | 359/435 [00:08<00:01, 45.43it/s][A
 84%|████████▎ | 364/435 [00:08<00:01, 45.40it/s][A
 85%|████████▍ | 369/435 [00:08<00:01, 45.15it/s][A
 86%|████████▌ | 374/435 [00:08<00:01, 44.98it/s][A
 87%|████████▋ | 379/435 [00:08<00:01, 44.87it/s][A
 88%|████████▊ | 384/435 [00:08<00:01, 45.01it/s][A
 89%|████████▉ | 389/435 [00:08<00:01, 45.21it/s][A
 91%|█████████ | 394/435 [00:09<00:00, 45.33it/s][A
 92%|█████████▏| 399/435 [00:09<00:00, 45.33it/s][A
 93%|█████████▎| 404/435 [00:09<00:00, 45.49it/s][A
 94%|█████████▍| 409/435 [00:09<00:00, 45.53it/s][A
 95%|█████████▌| 414/435 [00:09<00:00, 45.25it/s][A
 96%|█████████▋| 419/435 [00:09<00:00, 33.50it/s][A
 97%|█████████▋| 424/435 [00:09<00:00, 36.48it/s][A
 99%|█████████▊| 429/435 [00:10<00:00, 38.82it/s][A
100%|█████████▉| 434/435 [00:10<00:00, 40.58it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:10<00:00, 40.58it/s][A100%|██████████| 470/470 [04:33<00:00,  3.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:26:21,688 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-28 23:26:22,139 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:26:27,253 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:26:27,501 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:26:27,570 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:26:37,199 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:26:37,240 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94 (score: 1.1086870431900024).
                                                 100%|██████████| 470/470 [05:08<00:00,  3.69it/s]100%|██████████| 470/470 [05:08<00:00,  1.53it/s]
[INFO|trainer.py:1894] 2023-08-28 23:26:57,202 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 23:26:57,753 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:27:02,213 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:27:02,354 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:27:02,434 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:27:02,925 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   train_loss               =     0.3652
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   train_runtime            = 0:05:08.12
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   train_samples_per_second =     97.365
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:02,926 >>   train_steps_per_second   =      1.525
{'eval_loss': 1.1618092060089111, 'eval_runtime': 10.1578, 'eval_samples_per_second': 342.497, 'eval_steps_per_second': 42.824, 'epoch': 5.0}
{'train_runtime': 308.1203, 'train_samples_per_second': 97.365, 'train_steps_per_second': 1.525, 'train_loss': 0.3652140515915891, 'epoch': 5.0}
08/28/2023 23:27:03 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:27:03,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:27:03,224 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:27:03,225 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 56.18it/s]  3%|▎         | 12/435 [00:00<00:08, 49.96it/s]  4%|▍         | 18/435 [00:00<00:08, 48.18it/s]  5%|▌         | 23/435 [00:00<00:08, 47.49it/s]  6%|▋         | 28/435 [00:00<00:08, 47.09it/s]  8%|▊         | 33/435 [00:00<00:08, 46.76it/s]  9%|▊         | 38/435 [00:00<00:08, 46.42it/s] 10%|▉         | 43/435 [00:00<00:08, 46.04it/s] 11%|█         | 48/435 [00:01<00:08, 45.55it/s] 12%|█▏        | 53/435 [00:01<00:08, 45.22it/s] 13%|█▎        | 58/435 [00:01<00:08, 45.33it/s] 14%|█▍        | 63/435 [00:01<00:08, 45.47it/s] 16%|█▌        | 68/435 [00:01<00:08, 45.55it/s] 17%|█▋        | 73/435 [00:01<00:07, 45.76it/s] 18%|█▊        | 78/435 [00:01<00:07, 45.71it/s] 19%|█▉        | 83/435 [00:01<00:07, 45.88it/s] 20%|██        | 88/435 [00:01<00:07, 45.75it/s] 21%|██▏       | 93/435 [00:02<00:07, 45.47it/s] 23%|██▎       | 98/435 [00:02<00:07, 42.90it/s] 24%|██▎       | 103/435 [00:02<00:07, 43.78it/s] 25%|██▍       | 108/435 [00:02<00:07, 44.35it/s] 26%|██▌       | 113/435 [00:02<00:07, 44.81it/s] 27%|██▋       | 118/435 [00:02<00:07, 45.15it/s] 28%|██▊       | 123/435 [00:02<00:06, 45.29it/s] 29%|██▉       | 128/435 [00:02<00:06, 45.47it/s] 31%|███       | 133/435 [00:02<00:06, 45.51it/s] 32%|███▏      | 138/435 [00:03<00:06, 45.13it/s] 33%|███▎      | 143/435 [00:03<00:06, 45.16it/s] 34%|███▍      | 148/435 [00:03<00:06, 45.31it/s] 35%|███▌      | 153/435 [00:03<00:06, 45.47it/s] 36%|███▋      | 158/435 [00:03<00:06, 45.45it/s] 37%|███▋      | 163/435 [00:03<00:05, 45.60it/s] 39%|███▊      | 168/435 [00:03<00:05, 45.77it/s] 40%|███▉      | 173/435 [00:03<00:05, 45.66it/s] 41%|████      | 178/435 [00:03<00:05, 45.65it/s] 42%|████▏     | 183/435 [00:04<00:05, 45.30it/s] 43%|████▎     | 188/435 [00:04<00:05, 45.23it/s] 44%|████▍     | 193/435 [00:04<00:05, 45.26it/s] 46%|████▌     | 198/435 [00:04<00:05, 45.35it/s] 47%|████▋     | 203/435 [00:04<00:05, 45.37it/s] 48%|████▊     | 208/435 [00:04<00:04, 45.55it/s] 49%|████▉     | 213/435 [00:04<00:04, 45.70it/s] 50%|█████     | 218/435 [00:04<00:04, 45.69it/s] 51%|█████▏    | 223/435 [00:04<00:04, 45.62it/s] 52%|█████▏    | 228/435 [00:04<00:04, 45.38it/s] 54%|█████▎    | 233/435 [00:05<00:04, 45.21it/s] 55%|█████▍    | 238/435 [00:05<00:04, 41.48it/s] 56%|█████▌    | 243/435 [00:05<00:04, 42.74it/s] 57%|█████▋    | 248/435 [00:05<00:04, 43.46it/s] 58%|█████▊    | 253/435 [00:05<00:04, 44.18it/s] 59%|█████▉    | 258/435 [00:05<00:03, 44.79it/s] 60%|██████    | 263/435 [00:05<00:03, 45.22it/s] 62%|██████▏   | 268/435 [00:05<00:03, 45.31it/s] 63%|██████▎   | 273/435 [00:06<00:03, 45.04it/s] 64%|██████▍   | 278/435 [00:06<00:03, 44.94it/s] 65%|██████▌   | 283/435 [00:06<00:03, 44.79it/s] 66%|██████▌   | 288/435 [00:06<00:03, 45.05it/s] 67%|██████▋   | 293/435 [00:06<00:03, 45.15it/s] 69%|██████▊   | 298/435 [00:06<00:03, 45.40it/s] 70%|██████▉   | 303/435 [00:06<00:02, 45.57it/s] 71%|███████   | 308/435 [00:06<00:02, 45.68it/s] 72%|███████▏  | 313/435 [00:06<00:02, 45.66it/s] 73%|███████▎  | 318/435 [00:07<00:02, 45.61it/s] 74%|███████▍  | 323/435 [00:07<00:02, 42.58it/s] 75%|███████▌  | 328/435 [00:07<00:02, 43.50it/s] 77%|███████▋  | 333/435 [00:07<00:02, 44.02it/s] 78%|███████▊  | 338/435 [00:07<00:02, 44.50it/s] 79%|███████▉  | 343/435 [00:07<00:02, 44.80it/s] 80%|████████  | 348/435 [00:07<00:01, 45.18it/s] 81%|████████  | 353/435 [00:07<00:01, 45.40it/s] 82%|████████▏ | 358/435 [00:07<00:01, 45.51it/s] 83%|████████▎ | 363/435 [00:08<00:01, 45.06it/s] 85%|████████▍ | 368/435 [00:08<00:01, 45.23it/s] 86%|████████▌ | 373/435 [00:08<00:01, 45.25it/s] 87%|████████▋ | 378/435 [00:08<00:01, 45.24it/s] 88%|████████▊ | 383/435 [00:08<00:01, 45.28it/s] 89%|████████▉ | 388/435 [00:08<00:01, 45.46it/s] 90%|█████████ | 393/435 [00:08<00:00, 45.52it/s] 91%|█████████▏| 398/435 [00:08<00:00, 45.63it/s] 93%|█████████▎| 403/435 [00:08<00:00, 45.48it/s] 94%|█████████▍| 408/435 [00:09<00:00, 45.30it/s] 95%|█████████▍| 413/435 [00:09<00:00, 45.26it/s] 96%|█████████▌| 418/435 [00:09<00:00, 45.29it/s] 97%|█████████▋| 423/435 [00:09<00:00, 45.40it/s] 98%|█████████▊| 428/435 [00:09<00:00, 45.36it/s]100%|█████████▉| 433/435 [00:09<00:00, 45.49it/s]100%|██████████| 435/435 [00:09<00:00, 45.22it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:27:12,862 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   eval_loss               =     1.1087
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   eval_runtime            = 0:00:09.63
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   eval_samples_per_second =    360.973
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   eval_steps_per_second   =     45.135
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:27:12,863 >>   perplexity              =     3.0304
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:27,153 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:27,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:27,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:27,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:27,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:27:28,298 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:27:28,300 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:28,622 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:29,797 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:29,797 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:32,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:32,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:32,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:32,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:32,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:27:33,328 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:27:33,329 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:33,703 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:33,950 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:33,950 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:09,  1.40it/s]Extractor Predicting: 14it [00:09,  1.41it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:11,  1.44it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:15,  1.50it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:17,  1.44it/s]Extractor Predicting: 26it [00:18,  1.42it/s]Extractor Predicting: 27it [00:18,  1.43it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:22,  1.44it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:24,  1.49it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:25,  1.50it/s]Extractor Predicting: 38it [00:26,  1.49it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.50it/s]Extractor Predicting: 41it [00:28,  1.51it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.46it/s]Extractor Predicting: 44it [00:30,  1.49it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:31,  1.49it/s]Extractor Predicting: 47it [00:32,  1.49it/s]Extractor Predicting: 48it [00:32,  1.45it/s]Extractor Predicting: 49it [00:33,  1.47it/s]Extractor Predicting: 50it [00:34,  1.49it/s]Extractor Predicting: 51it [00:34,  1.52it/s]Extractor Predicting: 52it [00:35,  1.52it/s]Extractor Predicting: 53it [00:36,  1.47it/s]Extractor Predicting: 54it [00:36,  1.47it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:38,  1.49it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.44it/s]Extractor Predicting: 59it [00:40,  1.50it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:41,  1.57it/s]Extractor Predicting: 62it [00:42,  1.54it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:44,  1.53it/s]Extractor Predicting: 66it [00:45,  1.41it/s]Extractor Predicting: 67it [00:45,  1.45it/s]Extractor Predicting: 68it [00:46,  1.46it/s]Extractor Predicting: 69it [00:46,  1.52it/s]Extractor Predicting: 70it [00:47,  1.53it/s]Extractor Predicting: 71it [00:48,  1.56it/s]Extractor Predicting: 72it [00:48,  1.51it/s]Extractor Predicting: 73it [00:49,  1.48it/s]Extractor Predicting: 74it [00:50,  1.49it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:51,  1.47it/s]Extractor Predicting: 77it [00:52,  1.50it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:53,  1.55it/s]Extractor Predicting: 80it [00:54,  1.54it/s]Extractor Predicting: 81it [00:54,  1.53it/s]Extractor Predicting: 82it [00:55,  1.53it/s]Extractor Predicting: 83it [00:56,  1.52it/s]Extractor Predicting: 84it [00:56,  1.53it/s]Extractor Predicting: 85it [00:57,  1.54it/s]Extractor Predicting: 86it [00:58,  1.56it/s]Extractor Predicting: 87it [00:58,  1.59it/s]Extractor Predicting: 88it [00:59,  1.54it/s]Extractor Predicting: 89it [01:00,  1.55it/s]Extractor Predicting: 90it [01:00,  1.57it/s]Extractor Predicting: 91it [01:01,  1.60it/s]Extractor Predicting: 92it [01:01,  1.64it/s]Extractor Predicting: 93it [01:02,  1.59it/s]Extractor Predicting: 94it [01:03,  1.58it/s]Extractor Predicting: 95it [01:03,  1.53it/s]Extractor Predicting: 96it [01:04,  1.55it/s]Extractor Predicting: 97it [01:05,  1.56it/s]Extractor Predicting: 98it [01:05,  1.57it/s]Extractor Predicting: 99it [01:06,  1.56it/s]Extractor Predicting: 100it [01:07,  1.49it/s]Extractor Predicting: 101it [01:07,  1.50it/s]Extractor Predicting: 102it [01:08,  1.59it/s]Extractor Predicting: 103it [01:08,  1.61it/s]Extractor Predicting: 104it [01:09,  1.60it/s]Extractor Predicting: 105it [01:10,  1.52it/s]Extractor Predicting: 106it [01:10,  1.54it/s]Extractor Predicting: 107it [01:11,  1.56it/s]Extractor Predicting: 108it [01:12,  1.57it/s]Extractor Predicting: 109it [01:12,  1.59it/s]Extractor Predicting: 110it [01:13,  1.54it/s]Extractor Predicting: 111it [01:14,  1.56it/s]Extractor Predicting: 112it [01:14,  1.59it/s]Extractor Predicting: 113it [01:15,  1.64it/s]Extractor Predicting: 114it [01:15,  1.65it/s]Extractor Predicting: 115it [01:16,  1.65it/s]Extractor Predicting: 116it [01:17,  1.61it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:18,  1.59it/s]Extractor Predicting: 119it [01:19,  1.56it/s]Extractor Predicting: 120it [01:19,  1.56it/s]Extractor Predicting: 121it [01:20,  1.55it/s]Extractor Predicting: 122it [01:21,  1.53it/s]Extractor Predicting: 123it [01:21,  1.55it/s]Extractor Predicting: 124it [01:22,  1.55it/s]Extractor Predicting: 125it [01:22,  1.56it/s]Extractor Predicting: 126it [01:23,  1.54it/s]Extractor Predicting: 127it [01:24,  1.57it/s]Extractor Predicting: 128it [01:24,  1.57it/s]Extractor Predicting: 129it [01:25,  1.55it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.55it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:28,  1.52it/s]Extractor Predicting: 134it [01:28,  1.51it/s]Extractor Predicting: 135it [01:29,  1.51it/s]Extractor Predicting: 136it [01:30,  1.36it/s]Extractor Predicting: 137it [01:31,  1.41it/s]Extractor Predicting: 138it [01:31,  1.41it/s]Extractor Predicting: 139it [01:32,  1.44it/s]Extractor Predicting: 140it [01:33,  1.42it/s]Extractor Predicting: 141it [01:33,  1.47it/s]Extractor Predicting: 142it [01:34,  1.46it/s]Extractor Predicting: 143it [01:35,  1.48it/s]Extractor Predicting: 144it [01:35,  1.84it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:26,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:26,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:26,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:26,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:26,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:29:26,717 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:29:26,718 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:29:27,023 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:29:28,073 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:29:28,074 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:30,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:30,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:30,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:30,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:29:30,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:29:31,162 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:29:31,163 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:29:31,436 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:29:31,594 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:29:31,594 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.18146718146718147,
  "recall": 0.054038516815176775,
  "score": 0.08327796234772979,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.48it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.63it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.65it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:25,  1.61it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:28,  1.46it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.55it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.52it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.52it/s]Extractor Predicting: 73it [00:46,  1.52it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.52it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.60it/s]Extractor Predicting: 80it [00:51,  1.62it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.60it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:55,  1.51it/s]Extractor Predicting: 88it [00:56,  1.51it/s]Extractor Predicting: 89it [00:57,  1.46it/s]Extractor Predicting: 90it [00:57,  1.47it/s]Extractor Predicting: 91it [00:58,  1.46it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [00:59,  1.52it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:01,  1.51it/s]Extractor Predicting: 97it [01:02,  1.43it/s]Extractor Predicting: 98it [01:03,  1.44it/s]Extractor Predicting: 99it [01:03,  1.48it/s]Extractor Predicting: 100it [01:04,  1.50it/s]Extractor Predicting: 101it [01:05,  1.54it/s]Extractor Predicting: 102it [01:05,  1.52it/s]Extractor Predicting: 103it [01:06,  1.50it/s]Extractor Predicting: 104it [01:07,  1.52it/s]Extractor Predicting: 105it [01:07,  1.51it/s]Extractor Predicting: 106it [01:08,  1.52it/s]Extractor Predicting: 107it [01:09,  1.55it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:10,  1.53it/s]Extractor Predicting: 110it [01:11,  1.52it/s]Extractor Predicting: 111it [01:11,  1.54it/s]Extractor Predicting: 112it [01:12,  1.52it/s]Extractor Predicting: 113it [01:13,  1.51it/s]Extractor Predicting: 114it [01:13,  1.53it/s]Extractor Predicting: 115it [01:14,  1.49it/s]Extractor Predicting: 116it [01:15,  1.52it/s]Extractor Predicting: 117it [01:15,  1.52it/s]Extractor Predicting: 118it [01:16,  1.52it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:17,  1.52it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:19,  1.55it/s]Extractor Predicting: 124it [01:20,  1.57it/s]Extractor Predicting: 125it [01:20,  1.60it/s]Extractor Predicting: 126it [01:21,  1.60it/s]Extractor Predicting: 127it [01:22,  1.58it/s]Extractor Predicting: 128it [01:22,  1.61it/s]Extractor Predicting: 129it [01:23,  1.59it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:24,  1.62it/s]Extractor Predicting: 132it [01:25,  1.63it/s]Extractor Predicting: 133it [01:25,  1.65it/s]Extractor Predicting: 134it [01:26,  1.60it/s]Extractor Predicting: 135it [01:26,  1.64it/s]Extractor Predicting: 136it [01:27,  1.65it/s]Extractor Predicting: 137it [01:28,  1.65it/s]Extractor Predicting: 138it [01:28,  1.63it/s]Extractor Predicting: 139it [01:29,  1.63it/s]Extractor Predicting: 140it [01:30,  1.64it/s]Extractor Predicting: 141it [01:30,  1.60it/s]Extractor Predicting: 142it [01:31,  1.60it/s]Extractor Predicting: 143it [01:31,  1.62it/s]Extractor Predicting: 144it [01:32,  1.60it/s]Extractor Predicting: 145it [01:33,  1.63it/s]Extractor Predicting: 146it [01:33,  1.66it/s]Extractor Predicting: 147it [01:34,  1.66it/s]Extractor Predicting: 148it [01:34,  1.69it/s]Extractor Predicting: 149it [01:35,  1.51it/s]Extractor Predicting: 150it [01:36,  1.57it/s]Extractor Predicting: 151it [01:36,  1.60it/s]Extractor Predicting: 152it [01:37,  1.65it/s]Extractor Predicting: 153it [01:38,  1.64it/s]Extractor Predicting: 154it [01:38,  1.61it/s]Extractor Predicting: 155it [01:39,  1.68it/s]Extractor Predicting: 156it [01:39,  1.68it/s]Extractor Predicting: 157it [01:40,  1.76it/s]Extractor Predicting: 158it [01:40,  1.79it/s]Extractor Predicting: 159it [01:41,  1.75it/s]Extractor Predicting: 160it [01:42,  1.70it/s]Extractor Predicting: 161it [01:42,  1.68it/s]Extractor Predicting: 162it [01:43,  1.71it/s]Extractor Predicting: 163it [01:43,  1.70it/s]Extractor Predicting: 164it [01:44,  1.71it/s]Extractor Predicting: 165it [01:45,  1.72it/s]Extractor Predicting: 166it [01:45,  1.68it/s]Extractor Predicting: 167it [01:46,  1.71it/s]Extractor Predicting: 168it [01:46,  1.69it/s]Extractor Predicting: 169it [01:47,  1.76it/s]Extractor Predicting: 170it [01:47,  1.74it/s]Extractor Predicting: 171it [01:48,  1.71it/s]Extractor Predicting: 172it [01:49,  1.65it/s]Extractor Predicting: 173it [01:49,  1.63it/s]Extractor Predicting: 174it [01:50,  1.60it/s]Extractor Predicting: 175it [01:51,  1.55it/s]Extractor Predicting: 176it [01:51,  1.56it/s]Extractor Predicting: 177it [01:52,  1.56it/s]Extractor Predicting: 178it [01:53,  1.53it/s]Extractor Predicting: 179it [01:53,  1.52it/s]Extractor Predicting: 180it [01:54,  1.53it/s]Extractor Predicting: 181it [01:55,  1.54it/s]Extractor Predicting: 182it [01:55,  1.53it/s]Extractor Predicting: 183it [01:56,  1.54it/s]Extractor Predicting: 184it [01:57,  1.53it/s]Extractor Predicting: 185it [01:57,  1.50it/s]Extractor Predicting: 186it [01:58,  1.51it/s]Extractor Predicting: 187it [01:59,  1.49it/s]Extractor Predicting: 188it [01:59,  1.50it/s]Extractor Predicting: 189it [02:00,  1.50it/s]Extractor Predicting: 190it [02:01,  1.53it/s]Extractor Predicting: 191it [02:01,  1.49it/s]Extractor Predicting: 192it [02:02,  1.48it/s]Extractor Predicting: 193it [02:03,  1.49it/s]Extractor Predicting: 194it [02:03,  1.49it/s]Extractor Predicting: 195it [02:04,  1.49it/s]Extractor Predicting: 196it [02:05,  1.51it/s]Extractor Predicting: 197it [02:05,  1.51it/s]Extractor Predicting: 198it [02:06,  1.53it/s]Extractor Predicting: 199it [02:06,  1.55it/s]Extractor Predicting: 200it [02:07,  1.53it/s]Extractor Predicting: 201it [02:08,  1.50it/s]Extractor Predicting: 202it [02:08,  1.58it/s]Extractor Predicting: 203it [02:09,  1.55it/s]Extractor Predicting: 204it [02:10,  1.57it/s]Extractor Predicting: 205it [02:10,  1.57it/s]Extractor Predicting: 206it [02:11,  1.56it/s]Extractor Predicting: 207it [02:12,  1.54it/s]Extractor Predicting: 208it [02:12,  1.54it/s]Extractor Predicting: 209it [02:13,  1.48it/s]Extractor Predicting: 210it [02:14,  1.41it/s]Extractor Predicting: 211it [02:14,  1.43it/s]Extractor Predicting: 212it [02:15,  1.45it/s]Extractor Predicting: 213it [02:16,  1.45it/s]Extractor Predicting: 214it [02:17,  1.45it/s]Extractor Predicting: 215it [02:17,  1.49it/s]Extractor Predicting: 216it [02:18,  1.47it/s]Extractor Predicting: 217it [02:19,  1.50it/s]Extractor Predicting: 218it [02:19,  1.46it/s]Extractor Predicting: 219it [02:20,  1.46it/s]Extractor Predicting: 220it [02:21,  1.49it/s]Extractor Predicting: 221it [02:21,  1.47it/s]Extractor Predicting: 222it [02:22,  1.47it/s]Extractor Predicting: 223it [02:23,  1.51it/s]Extractor Predicting: 224it [02:23,  1.50it/s]Extractor Predicting: 225it [02:24,  1.50it/s]Extractor Predicting: 226it [02:25,  1.50it/s]Extractor Predicting: 227it [02:25,  1.55it/s]Extractor Predicting: 228it [02:26,  1.54it/s]Extractor Predicting: 229it [02:26,  1.54it/s]Extractor Predicting: 230it [02:27,  1.60it/s]Extractor Predicting: 231it [02:28,  1.60it/s]Extractor Predicting: 232it [02:28,  1.63it/s]Extractor Predicting: 233it [02:29,  1.60it/s]Extractor Predicting: 234it [02:30,  1.52it/s]Extractor Predicting: 235it [02:30,  1.53it/s]Extractor Predicting: 236it [02:31,  1.53it/s]Extractor Predicting: 237it [02:32,  1.52it/s]Extractor Predicting: 238it [02:32,  1.54it/s]Extractor Predicting: 239it [02:33,  1.53it/s]Extractor Predicting: 240it [02:34,  1.55it/s]Extractor Predicting: 241it [02:34,  1.47it/s]Extractor Predicting: 242it [02:35,  1.51it/s]Extractor Predicting: 243it [02:36,  1.51it/s]Extractor Predicting: 244it [02:36,  1.53it/s]Extractor Predicting: 245it [02:37,  1.56it/s]Extractor Predicting: 246it [02:37,  1.55it/s]Extractor Predicting: 247it [02:38,  1.56it/s]Extractor Predicting: 248it [02:39,  1.55it/s]Extractor Predicting: 249it [02:40,  1.45it/s]Extractor Predicting: 250it [02:40,  1.50it/s]Extractor Predicting: 251it [02:41,  1.36it/s]Extractor Predicting: 252it [02:42,  1.42it/s]Extractor Predicting: 253it [02:42,  1.43it/s]Extractor Predicting: 254it [02:43,  1.48it/s]Extractor Predicting: 255it [02:44,  1.45it/s]Extractor Predicting: 256it [02:44,  1.39it/s]Extractor Predicting: 257it [02:45,  1.42it/s]Extractor Predicting: 258it [02:46,  1.43it/s]Extractor Predicting: 259it [02:47,  1.44it/s]Extractor Predicting: 260it [02:47,  1.44it/s]Extractor Predicting: 261it [02:48,  1.49it/s]Extractor Predicting: 262it [02:49,  1.47it/s]Extractor Predicting: 263it [02:49,  1.46it/s]Extractor Predicting: 264it [02:50,  1.45it/s]Extractor Predicting: 265it [02:51,  1.42it/s]Extractor Predicting: 266it [02:51,  1.43it/s]Extractor Predicting: 267it [02:52,  1.44it/s]Extractor Predicting: 268it [02:53,  1.42it/s]Extractor Predicting: 269it [02:53,  1.46it/s]Extractor Predicting: 270it [02:54,  1.47it/s]Extractor Predicting: 271it [02:55,  1.42it/s]Extractor Predicting: 272it [02:56,  1.44it/s]Extractor Predicting: 273it [02:56,  1.46it/s]Extractor Predicting: 274it [02:57,  1.46it/s]Extractor Predicting: 275it [02:58,  1.49it/s]Extractor Predicting: 276it [02:58,  1.53it/s]Extractor Predicting: 277it [02:59,  1.53it/s]Extractor Predicting: 278it [02:59,  1.53it/s]Extractor Predicting: 279it [03:00,  1.50it/s]Extractor Predicting: 280it [03:01,  1.46it/s]Extractor Predicting: 281it [03:02,  1.47it/s]Extractor Predicting: 282it [03:02,  1.56it/s]Extractor Predicting: 282it [03:02,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:49,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:49,823 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:49,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:49,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:49,824 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:32:51,009 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:32:51,010 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:51,794 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:53,024 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:53,081 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:56,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:56,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:56,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:56,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:56,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:32:57,983 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:32:57,984 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:58,698 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:59,076 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:59,076 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22423918846769889,
  "recall": 0.1242603550295858,
  "score": 0.1599086236436322,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 5it [00:03,  1.48it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:33:06,380 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:06,381 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:33:06,492 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:06,493 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:33:06,596 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:33:23,233 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:33:23,293 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:33:23,723 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:33:23,724 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:33:23,953 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:33:24,137 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3442622950819672,
  "recall": 0.0875,
  "score": 0.13953488372093023,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:33:24,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:25,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:25,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:26,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:27,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:27,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:28,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:29,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:29,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:30,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:31,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:32,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:32,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:33,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:34,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:34,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:35,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:36,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:36,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:55, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:37,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:37,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:38,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:39,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:40,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:40,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:41,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:41,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:42,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:43,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:44,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:45,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:47,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:47,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:48,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:24<02:38, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-28 23:33:49,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:49,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:50,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:50,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:51,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:51,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:52,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:53,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:53,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:54,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:54,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:55,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:56,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:57,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:57,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:58,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:58,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:33:59,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:00,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:36<02:23, 11.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:00,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:01,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:01,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:02,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:03,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:03,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:04,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:04,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:05,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:05,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:06,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:07,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:07,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:08,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:09,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:10,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:11,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:11,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:12,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:48<02:12, 12.07s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:13,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:13,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:14,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:14,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:16,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:16,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:17,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:17,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:18,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:19,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:19,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:20,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:21,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:22,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:23,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:24,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:25,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:26,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:02<02:07, 12.73s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:26,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:27,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:28,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:29,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:29,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:30,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:31,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:32,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:32,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:33,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:33,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:34,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:35,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:35,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:36,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:37,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:37,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:38,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:38,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:14<01:54, 12.69s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:39,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:40,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:40,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:41,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:41,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:42,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:43,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:43,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:44,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:44,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:45,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:45,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:46,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:46,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:47,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:47,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:48,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:48,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:49,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:50,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:50,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:26<01:39, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 23:34:51,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:51,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:52,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:53,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:53,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:54,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:55,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:56,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:56,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:57,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:58,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:58,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:59,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:34:59,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:00,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:01,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:01,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:02,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:03,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:38<01:26, 12.34s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:03,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:04,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:05,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:05,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:06,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:06,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:07,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:07,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:08,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:09,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:10,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:10,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:11,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:11,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:12,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:13,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:13,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:14,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:14,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:50<01:12, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:15,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:15,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:16,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:17,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:17,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:18,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:18,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:19,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:20,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:20,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:21,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:22,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:22,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:23,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:23,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:24,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:24,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:25,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:26,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:26,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:02<01:00, 12.12s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:27,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:27,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:28,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:29,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:30,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:30,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:31,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:31,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:32,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:33,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:33,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:34,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:34,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:35,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:35,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:36,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:37,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:37,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:38,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:38,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:39,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:15<00:49, 12.29s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:40,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:40,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:41,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:41,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:42,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:42,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:43,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:43,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:44,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:45,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:45,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:46,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:46,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:47,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:48,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:48,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:49,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:49,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:50,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:50,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:26<00:35, 12.00s/it][WARNING|generation_utils.py:914] 2023-08-28 23:35:51,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:51,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:52,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:52,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:53,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:53,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:54,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:54,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:55,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:55,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:56,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:57,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:57,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:57,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:58,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:59,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:35:59,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:00,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:00,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:01,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:01,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:37<00:23, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:02,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:02,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:03,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:04,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:05,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:05,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:06,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:07,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:07,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:08,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:08,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:09,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:10,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:10,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:11,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:11,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:12,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:12,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:13,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:49<00:11, 11.76s/it][WARNING|generation_utils.py:914] 2023-08-28 23:36:14,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:14,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:15,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:16,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:16,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:17,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:18,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:18,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:19,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:19,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:20,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:21,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:21,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:22,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:23,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:23,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:24,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:24,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:25,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:26,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:36:27,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:03<00:00, 12.28s/it]Generating: 100%|██████████| 15/15 [03:03<00:00, 12.20s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:35,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:35,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:35,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:35,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:35,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:36:36,454 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:36:36,455 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:36:36,823 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:36:37,995 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:36:37,995 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:39,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:39,796 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:39,796 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:39,796 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:36:39,796 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:36:40,279 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:36:40,280 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:36:40,583 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:36:40,898 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:36:40,898 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : country .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : genre .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9796875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 6697
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6797, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.58it/s]Extractor Estimating: 3it [00:01,  1.69it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.46it/s]Extractor Estimating: 6it [00:03,  1.48it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.68it/s]Extractor Estimating: 12it [00:07,  1.67it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.68it/s]Extractor Estimating: 15it [00:09,  1.64it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.68it/s]Extractor Estimating: 19it [00:11,  1.72it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.60it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.63it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:16,  1.62it/s]Extractor Estimating: 28it [00:17,  1.62it/s]Extractor Estimating: 29it [00:17,  1.64it/s]Extractor Estimating: 30it [00:18,  1.54it/s]Extractor Estimating: 31it [00:19,  1.53it/s]Extractor Estimating: 32it [00:19,  1.54it/s]Extractor Estimating: 33it [00:20,  1.57it/s]Extractor Estimating: 34it [00:21,  1.63it/s]Extractor Estimating: 35it [00:21,  1.60it/s]Extractor Estimating: 36it [00:22,  1.60it/s]Extractor Estimating: 37it [00:23,  1.56it/s]Extractor Estimating: 38it [00:23,  1.54it/s]Extractor Estimating: 39it [00:24,  1.57it/s]Extractor Estimating: 40it [00:24,  1.60it/s]Extractor Estimating: 41it [00:25,  1.48it/s]Extractor Estimating: 42it [00:26,  1.54it/s]Extractor Estimating: 43it [00:26,  1.60it/s]Extractor Estimating: 44it [00:27,  1.61it/s]Extractor Estimating: 45it [00:28,  1.58it/s]Extractor Estimating: 46it [00:28,  1.59it/s]Extractor Estimating: 47it [00:29,  1.48it/s]Extractor Estimating: 48it [00:30,  1.50it/s]Extractor Estimating: 49it [00:30,  1.54it/s]Extractor Estimating: 50it [00:31,  1.53it/s]Extractor Estimating: 51it [00:32,  1.61it/s]Extractor Estimating: 52it [00:32,  1.65it/s]Extractor Estimating: 53it [00:33,  1.67it/s]Extractor Estimating: 54it [00:33,  1.61it/s]Extractor Estimating: 55it [00:34,  1.67it/s]Extractor Estimating: 56it [00:34,  1.67it/s]Extractor Estimating: 57it [00:35,  1.68it/s]Extractor Estimating: 58it [00:36,  1.60it/s]Extractor Estimating: 59it [00:36,  1.62it/s]Extractor Estimating: 60it [00:37,  1.70it/s]Extractor Estimating: 61it [00:37,  1.76it/s]Extractor Estimating: 62it [00:38,  1.83it/s]Extractor Estimating: 63it [00:39,  1.75it/s]Extractor Estimating: 64it [00:39,  1.73it/s]Extractor Estimating: 65it [00:40,  1.71it/s]Extractor Estimating: 66it [00:40,  1.67it/s]Extractor Estimating: 67it [00:41,  1.68it/s]Extractor Estimating: 68it [00:42,  1.70it/s]Extractor Estimating: 69it [00:42,  1.68it/s]Extractor Estimating: 70it [00:43,  1.72it/s]Extractor Estimating: 71it [00:43,  1.72it/s]Extractor Estimating: 72it [00:44,  1.67it/s]Extractor Estimating: 73it [00:45,  1.66it/s]Extractor Estimating: 74it [00:45,  1.73it/s]Extractor Estimating: 75it [00:46,  1.75it/s]Extractor Estimating: 76it [00:46,  1.75it/s]Extractor Estimating: 77it [00:47,  1.77it/s]Extractor Estimating: 78it [00:47,  1.83it/s]Extractor Estimating: 79it [00:48,  1.81it/s]Extractor Estimating: 80it [00:48,  1.79it/s]Extractor Estimating: 81it [00:49,  1.74it/s]Extractor Estimating: 82it [00:50,  1.74it/s]Extractor Estimating: 83it [00:50,  1.71it/s]Extractor Estimating: 84it [00:51,  1.77it/s]Extractor Estimating: 85it [00:51,  1.73it/s]Extractor Estimating: 86it [00:52,  1.74it/s]Extractor Estimating: 87it [00:52,  1.74it/s]Extractor Estimating: 88it [00:53,  1.70it/s]Extractor Estimating: 89it [00:54,  1.72it/s]Extractor Estimating: 90it [00:54,  1.76it/s]Extractor Estimating: 91it [00:55,  1.66it/s]Extractor Estimating: 92it [00:55,  1.68it/s]Extractor Estimating: 93it [00:56,  1.74it/s]Extractor Estimating: 94it [00:56,  1.75it/s]Extractor Estimating: 95it [00:57,  1.79it/s]Extractor Estimating: 96it [00:58,  1.71it/s]Extractor Estimating: 97it [00:58,  1.75it/s]Extractor Estimating: 98it [00:59,  1.67it/s]Extractor Estimating: 99it [00:59,  1.71it/s]Extractor Estimating: 100it [01:00,  1.74it/s]Extractor Estimating: 101it [01:00,  1.82it/s]Extractor Estimating: 102it [01:01,  1.77it/s]Extractor Estimating: 103it [01:02,  1.67it/s]Extractor Estimating: 104it [01:02,  1.73it/s]Extractor Estimating: 105it [01:03,  1.71it/s]Extractor Estimating: 106it [01:03,  1.74it/s]Extractor Estimating: 107it [01:04,  1.74it/s]Extractor Estimating: 108it [01:05,  1.70it/s]Extractor Estimating: 109it [01:05,  1.79it/s]Extractor Estimating: 110it [01:06,  1.82it/s]Extractor Estimating: 111it [01:06,  1.81it/s]Extractor Estimating: 112it [01:07,  1.91it/s]Extractor Estimating: 113it [01:07,  1.88it/s]Extractor Estimating: 114it [01:08,  1.85it/s]Extractor Estimating: 115it [01:08,  1.88it/s]Extractor Estimating: 116it [01:09,  1.84it/s]Extractor Estimating: 117it [01:09,  1.77it/s]Extractor Estimating: 118it [01:10,  1.69it/s]Extractor Estimating: 119it [01:11,  1.72it/s]Extractor Estimating: 120it [01:11,  1.76it/s]Extractor Estimating: 121it [01:12,  1.75it/s]Extractor Estimating: 122it [01:12,  1.78it/s]Extractor Estimating: 123it [01:13,  1.74it/s]Extractor Estimating: 124it [01:14,  1.61it/s]Extractor Estimating: 125it [01:14,  1.58it/s]Extractor Estimating: 126it [01:15,  1.72it/s]Extractor Estimating: 127it [01:15,  1.82it/s]Extractor Estimating: 128it [01:16,  1.81it/s]Extractor Estimating: 129it [01:16,  1.86it/s]Extractor Estimating: 130it [01:17,  1.86it/s]Extractor Estimating: 131it [01:17,  1.89it/s]Extractor Estimating: 132it [01:18,  1.76it/s]Extractor Estimating: 133it [01:18,  1.86it/s]Extractor Estimating: 134it [01:19,  1.84it/s]Extractor Estimating: 135it [01:20,  1.85it/s]Extractor Estimating: 136it [01:20,  1.99it/s]Extractor Estimating: 137it [01:21,  1.96it/s]Extractor Estimating: 138it [01:21,  1.97it/s]Extractor Estimating: 139it [01:22,  1.82it/s]Extractor Estimating: 140it [01:22,  1.78it/s]Extractor Estimating: 141it [01:23,  1.85it/s]Extractor Estimating: 142it [01:23,  1.91it/s]Extractor Estimating: 143it [01:24,  1.97it/s]Extractor Estimating: 144it [01:24,  1.95it/s]Extractor Estimating: 145it [01:25,  1.78it/s]Extractor Estimating: 146it [01:25,  1.86it/s]Extractor Estimating: 147it [01:26,  1.85it/s]Extractor Estimating: 148it [01:26,  1.88it/s]Extractor Estimating: 149it [01:27,  1.85it/s]Extractor Estimating: 150it [01:28,  1.85it/s]Extractor Estimating: 151it [01:28,  1.78it/s]Extractor Estimating: 152it [01:29,  1.83it/s]Extractor Estimating: 153it [01:29,  1.79it/s]Extractor Estimating: 154it [01:30,  1.70it/s]Extractor Estimating: 155it [01:30,  1.76it/s]Extractor Estimating: 156it [01:31,  1.75it/s]Extractor Estimating: 157it [01:32,  1.77it/s]Extractor Estimating: 158it [01:32,  1.76it/s]Extractor Estimating: 159it [01:33,  1.79it/s]Extractor Estimating: 160it [01:33,  1.82it/s]Extractor Estimating: 161it [01:34,  1.83it/s]Extractor Estimating: 162it [01:34,  1.87it/s]Extractor Estimating: 163it [01:35,  1.79it/s]Extractor Estimating: 164it [01:35,  1.82it/s]Extractor Estimating: 165it [01:36,  1.86it/s]Extractor Estimating: 166it [01:36,  1.87it/s]Extractor Estimating: 167it [01:37,  1.84it/s]Extractor Estimating: 168it [01:38,  1.82it/s]Extractor Estimating: 169it [01:38,  1.93it/s]Extractor Estimating: 170it [01:39,  1.91it/s]Extractor Estimating: 171it [01:39,  1.91it/s]Extractor Estimating: 172it [01:40,  1.87it/s]Extractor Estimating: 173it [01:40,  1.72it/s]Extractor Estimating: 174it [01:41,  1.76it/s]Extractor Estimating: 175it [01:41,  1.77it/s]Extractor Estimating: 176it [01:42,  1.81it/s]Extractor Estimating: 177it [01:42,  1.80it/s]Extractor Estimating: 178it [01:43,  1.80it/s]Extractor Estimating: 179it [01:44,  1.82it/s]Extractor Estimating: 180it [01:44,  1.79it/s]Extractor Estimating: 181it [01:45,  1.81it/s]Extractor Estimating: 182it [01:45,  1.73it/s]Extractor Estimating: 183it [01:46,  1.78it/s]Extractor Estimating: 184it [01:46,  1.84it/s]Extractor Estimating: 185it [01:47,  1.84it/s]Extractor Estimating: 186it [01:47,  1.82it/s]Extractor Estimating: 187it [01:48,  1.82it/s]Extractor Estimating: 188it [01:49,  1.81it/s]Extractor Estimating: 189it [01:49,  1.78it/s]Extractor Estimating: 190it [01:50,  1.82it/s]Extractor Estimating: 191it [01:50,  1.77it/s]Extractor Estimating: 192it [01:51,  1.79it/s]Extractor Estimating: 193it [01:51,  1.78it/s]Extractor Estimating: 194it [01:52,  1.80it/s]Extractor Estimating: 195it [01:52,  1.82it/s]Extractor Estimating: 196it [01:53,  1.70it/s]Extractor Estimating: 197it [01:54,  1.69it/s]Extractor Estimating: 198it [01:54,  1.73it/s]Extractor Estimating: 199it [01:55,  1.73it/s]Extractor Estimating: 200it [01:56,  1.67it/s]Extractor Estimating: 201it [01:56,  1.63it/s]Extractor Estimating: 202it [01:57,  1.55it/s]Extractor Estimating: 203it [01:57,  1.60it/s]Extractor Estimating: 204it [01:58,  1.63it/s]Extractor Estimating: 205it [01:59,  1.64it/s]Extractor Estimating: 206it [01:59,  1.65it/s]Extractor Estimating: 207it [02:00,  1.65it/s]Extractor Estimating: 208it [02:00,  1.63it/s]Extractor Estimating: 209it [02:01,  1.66it/s]Extractor Estimating: 210it [02:02,  1.72it/s]Extractor Estimating: 211it [02:02,  1.69it/s]Extractor Estimating: 212it [02:03,  1.72it/s]Extractor Estimating: 213it [02:03,  1.71it/s]Extractor Estimating: 214it [02:04,  1.70it/s]Extractor Estimating: 215it [02:05,  1.64it/s]Extractor Estimating: 216it [02:05,  1.63it/s]Extractor Estimating: 217it [02:06,  1.65it/s]Extractor Estimating: 218it [02:07,  1.53it/s]Extractor Estimating: 219it [02:07,  1.52it/s]Extractor Estimating: 220it [02:08,  1.59it/s]Extractor Estimating: 221it [02:08,  1.63it/s]Extractor Estimating: 222it [02:09,  1.61it/s]Extractor Estimating: 223it [02:10,  1.60it/s]Extractor Estimating: 224it [02:10,  1.64it/s]Extractor Estimating: 225it [02:11,  1.70it/s]Extractor Estimating: 226it [02:11,  1.73it/s]Extractor Estimating: 227it [02:12,  1.79it/s]Extractor Estimating: 228it [02:12,  1.80it/s]Extractor Estimating: 229it [02:13,  1.86it/s]Extractor Estimating: 230it [02:13,  1.86it/s]Extractor Estimating: 231it [02:14,  1.84it/s]Extractor Estimating: 232it [02:15,  1.88it/s]Extractor Estimating: 233it [02:15,  1.86it/s]Extractor Estimating: 234it [02:16,  1.83it/s]Extractor Estimating: 235it [02:16,  1.81it/s]Extractor Estimating: 236it [02:17,  1.85it/s]Extractor Estimating: 237it [02:17,  1.87it/s]Extractor Estimating: 238it [02:18,  1.85it/s]Extractor Estimating: 239it [02:18,  1.85it/s]Extractor Estimating: 240it [02:19,  1.87it/s]Extractor Estimating: 241it [02:19,  1.93it/s]Extractor Estimating: 242it [02:20,  1.91it/s]Extractor Estimating: 243it [02:20,  1.95it/s]Extractor Estimating: 244it [02:21,  1.82it/s]Extractor Estimating: 245it [02:21,  1.87it/s]Extractor Estimating: 246it [02:22,  1.83it/s]Extractor Estimating: 247it [02:23,  1.83it/s]Extractor Estimating: 248it [02:23,  1.91it/s]Extractor Estimating: 249it [02:24,  1.91it/s]Extractor Estimating: 250it [02:24,  1.96it/s]Extractor Estimating: 251it [02:25,  1.94it/s]Extractor Estimating: 252it [02:25,  1.87it/s]Extractor Estimating: 253it [02:26,  1.84it/s]Extractor Estimating: 254it [02:26,  1.78it/s]Extractor Estimating: 255it [02:27,  1.81it/s]Extractor Estimating: 256it [02:27,  1.82it/s]Extractor Estimating: 257it [02:28,  1.83it/s]Extractor Estimating: 258it [02:28,  1.84it/s]Extractor Estimating: 259it [02:29,  1.84it/s]Extractor Estimating: 260it [02:30,  1.84it/s]Extractor Estimating: 261it [02:30,  1.79it/s]Extractor Estimating: 262it [02:31,  1.80it/s]Extractor Estimating: 263it [02:31,  1.85it/s]Extractor Estimating: 264it [02:32,  1.80it/s]Extractor Estimating: 265it [02:32,  1.85it/s]Extractor Estimating: 266it [02:33,  1.92it/s]Extractor Estimating: 267it [02:33,  1.91it/s]Extractor Estimating: 268it [02:34,  1.88it/s]Extractor Estimating: 269it [02:34,  1.91it/s]Extractor Estimating: 270it [02:35,  1.89it/s]Extractor Estimating: 271it [02:35,  1.86it/s]Extractor Estimating: 272it [02:36,  1.85it/s]Extractor Estimating: 273it [02:37,  1.76it/s]Extractor Estimating: 274it [02:37,  1.83it/s]Extractor Estimating: 275it [02:38,  1.84it/s]Extractor Estimating: 276it [02:38,  1.80it/s]Extractor Estimating: 277it [02:39,  1.73it/s]Extractor Estimating: 278it [02:39,  1.75it/s]Extractor Estimating: 279it [02:40,  1.79it/s]Extractor Estimating: 280it [02:41,  1.78it/s]Extractor Estimating: 281it [02:41,  1.81it/s]Extractor Estimating: 282it [02:42,  1.77it/s]Extractor Estimating: 283it [02:42,  1.75it/s]Extractor Estimating: 284it [02:43,  1.78it/s]Extractor Estimating: 285it [02:43,  1.71it/s]Extractor Estimating: 286it [02:44,  1.73it/s]Extractor Estimating: 287it [02:45,  1.66it/s]Extractor Estimating: 288it [02:45,  1.68it/s]Extractor Estimating: 289it [02:46,  1.72it/s]Extractor Estimating: 290it [02:46,  1.72it/s]Extractor Estimating: 291it [02:47,  1.73it/s]Extractor Estimating: 292it [02:48,  1.75it/s]Extractor Estimating: 293it [02:48,  1.77it/s]Extractor Estimating: 294it [02:49,  1.75it/s]Extractor Estimating: 295it [02:49,  1.75it/s]Extractor Estimating: 296it [02:50,  1.76it/s]Extractor Estimating: 297it [02:50,  1.81it/s]Extractor Estimating: 298it [02:51,  1.79it/s]Extractor Estimating: 299it [02:51,  1.78it/s]Extractor Estimating: 300it [02:52,  1.73it/s]Extractor Estimating: 301it [02:53,  1.80it/s]Extractor Estimating: 302it [02:53,  1.68it/s]Extractor Estimating: 303it [02:54,  1.80it/s]Extractor Estimating: 304it [02:54,  1.80it/s]Extractor Estimating: 305it [02:55,  1.83it/s]Extractor Estimating: 306it [02:55,  1.88it/s]Extractor Estimating: 307it [02:56,  1.89it/s]Extractor Estimating: 308it [02:56,  1.92it/s]Extractor Estimating: 309it [02:57,  1.91it/s]Extractor Estimating: 310it [02:57,  1.95it/s]Extractor Estimating: 311it [02:58,  1.98it/s]Extractor Estimating: 312it [02:58,  1.86it/s]Extractor Estimating: 313it [02:59,  1.87it/s]Extractor Estimating: 314it [02:59,  1.90it/s]Extractor Estimating: 315it [03:00,  1.99it/s]Extractor Estimating: 316it [03:00,  1.98it/s]Extractor Estimating: 317it [03:01,  1.93it/s]Extractor Estimating: 318it [03:01,  1.93it/s]Extractor Estimating: 319it [03:02,  1.76it/s]Extractor Estimating: 320it [03:03,  1.76it/s]Extractor Estimating: 321it [03:03,  1.82it/s]Extractor Estimating: 322it [03:04,  1.91it/s]Extractor Estimating: 323it [03:04,  1.87it/s]Extractor Estimating: 324it [03:05,  1.90it/s]Extractor Estimating: 325it [03:05,  1.82it/s]Extractor Estimating: 326it [03:06,  1.83it/s]Extractor Estimating: 327it [03:07,  1.80it/s]Extractor Estimating: 328it [03:07,  1.80it/s]Extractor Estimating: 329it [03:08,  1.69it/s]Extractor Estimating: 330it [03:08,  1.69it/s]Extractor Estimating: 331it [03:09,  1.69it/s]Extractor Estimating: 332it [03:10,  1.63it/s]Extractor Estimating: 333it [03:10,  1.68it/s]Extractor Estimating: 334it [03:11,  1.64it/s]Extractor Estimating: 335it [03:11,  1.66it/s]Extractor Estimating: 336it [03:12,  1.67it/s]Extractor Estimating: 337it [03:13,  1.67it/s]Extractor Estimating: 338it [03:13,  1.75it/s]Extractor Estimating: 339it [03:14,  1.74it/s]Extractor Estimating: 340it [03:14,  1.70it/s]Extractor Estimating: 341it [03:15,  1.71it/s]Extractor Estimating: 342it [03:15,  1.71it/s]Extractor Estimating: 343it [03:16,  1.73it/s]Extractor Estimating: 344it [03:17,  1.77it/s]Extractor Estimating: 345it [03:17,  1.77it/s]Extractor Estimating: 346it [03:18,  1.65it/s]Extractor Estimating: 347it [03:18,  1.72it/s]Extractor Estimating: 348it [03:19,  1.74it/s]Extractor Estimating: 349it [03:19,  1.73it/s]Extractor Estimating: 350it [03:20,  1.77it/s]Extractor Estimating: 351it [03:21,  1.75it/s]Extractor Estimating: 352it [03:21,  1.78it/s]Extractor Estimating: 353it [03:22,  1.76it/s]Extractor Estimating: 354it [03:22,  1.84it/s]Extractor Estimating: 355it [03:23,  1.84it/s]Extractor Estimating: 356it [03:23,  1.89it/s]Extractor Estimating: 357it [03:24,  1.86it/s]Extractor Estimating: 358it [03:24,  1.88it/s]Extractor Estimating: 359it [03:25,  1.85it/s]Extractor Estimating: 360it [03:25,  1.87it/s]Extractor Estimating: 361it [03:26,  1.93it/s]Extractor Estimating: 362it [03:26,  1.91it/s]Extractor Estimating: 363it [03:27,  1.90it/s]Extractor Estimating: 364it [03:28,  1.85it/s]Extractor Estimating: 365it [03:28,  1.81it/s]Extractor Estimating: 366it [03:29,  1.80it/s]Extractor Estimating: 367it [03:29,  1.83it/s]Extractor Estimating: 368it [03:30,  1.82it/s]Extractor Estimating: 369it [03:30,  1.82it/s]Extractor Estimating: 370it [03:31,  1.83it/s]Extractor Estimating: 371it [03:31,  1.83it/s]Extractor Estimating: 372it [03:32,  1.74it/s]Extractor Estimating: 373it [03:33,  1.77it/s]Extractor Estimating: 374it [03:33,  1.79it/s]Extractor Estimating: 375it [03:33,  2.19it/s]Extractor Estimating: 375it [03:33,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:33,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:33,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:33,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:33,613 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:33,613 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:40:34,677 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:40:34,678 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:40:35,429 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:40:36,588 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:40:36,649 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:39,855 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:39,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:39,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:39,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:40:39,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:40:40,721 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:40:40,722 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:40:41,357 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:40:41,664 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:40:41,664 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:45:59,944 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:46:00,337 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7489 mean pseudo reward: 0.9492998908377437
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16143
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16243, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16243, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.947, loss:467.9327
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.960, loss:454.3572
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.957, loss:449.3865
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.951, loss:430.1736
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.939, loss:429.3869
>> valid entity prec:0.4590, rec:0.4882, f1:0.4731
>> valid relation prec:0.0887, rec:0.0443, f1:0.0590
>> valid relation with NER prec:0.0887, rec:0.0443, f1:0.0590
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.266, loss:440.0292
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.955, loss:427.4050
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.956, loss:436.0898
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.960, loss:404.2120
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.946, loss:409.1463
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4782, rec:0.5046, f1:0.4911
>> valid relation prec:0.1138, rec:0.0627, f1:0.0808
>> valid relation with NER prec:0.1138, rec:0.0627, f1:0.0808
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.255, loss:422.3851
g_step 1200, step 261, avg_time 0.960, loss:429.5567
g_step 1300, step 48, avg_time 0.954, loss:400.6275
g_step 1400, step 148, avg_time 0.953, loss:395.3126
g_step 1500, step 248, avg_time 0.946, loss:407.3200
>> valid entity prec:0.4549, rec:0.4238, f1:0.4388
>> valid relation prec:0.0844, rec:0.0325, f1:0.0469
>> valid relation with NER prec:0.0844, rec:0.0325, f1:0.0469
g_step 1600, step 35, avg_time 2.246, loss:391.9843
g_step 1700, step 135, avg_time 0.952, loss:381.2230
g_step 1800, step 235, avg_time 0.954, loss:367.4353
g_step 1900, step 22, avg_time 0.955, loss:407.6760
g_step 2000, step 122, avg_time 0.952, loss:365.6299
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4602, rec:0.4617, f1:0.4609
>> valid relation prec:0.0851, rec:0.0348, f1:0.0494
>> valid relation with NER prec:0.0851, rec:0.0348, f1:0.0494
g_step 2100, step 222, avg_time 2.233, loss:364.1082
g_step 2200, step 9, avg_time 0.952, loss:367.3651
g_step 2300, step 109, avg_time 0.945, loss:324.7917
g_step 2400, step 209, avg_time 0.955, loss:338.0883
g_step 2500, step 309, avg_time 0.972, loss:388.4261
>> valid entity prec:0.4429, rec:0.4694, f1:0.4558
>> valid relation prec:0.0934, rec:0.0460, f1:0.0616
>> valid relation with NER prec:0.0934, rec:0.0460, f1:0.0616
g_step 2600, step 96, avg_time 2.229, loss:341.6739
g_step 2700, step 196, avg_time 0.963, loss:342.4733
g_step 2800, step 296, avg_time 0.949, loss:338.9555
g_step 2900, step 83, avg_time 0.942, loss:318.0560
g_step 3000, step 183, avg_time 0.969, loss:339.3761
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4619, rec:0.4273, f1:0.4439
>> valid relation prec:0.0871, rec:0.0414, f1:0.0561
>> valid relation with NER prec:0.0871, rec:0.0414, f1:0.0561
g_step 3100, step 283, avg_time 2.238, loss:340.9734
g_step 3200, step 70, avg_time 0.954, loss:291.8481
g_step 3300, step 170, avg_time 0.966, loss:313.7434
g_step 3400, step 270, avg_time 0.943, loss:314.1246
g_step 3500, step 57, avg_time 0.958, loss:308.0057
>> valid entity prec:0.4552, rec:0.4658, f1:0.4604
>> valid relation prec:0.0850, rec:0.0420, f1:0.0562
>> valid relation with NER prec:0.0850, rec:0.0420, f1:0.0562
g_step 3600, step 157, avg_time 2.225, loss:294.8627
g_step 3700, step 257, avg_time 0.954, loss:310.2241
g_step 3800, step 44, avg_time 0.935, loss:298.4044
g_step 3900, step 144, avg_time 0.953, loss:275.6799
g_step 4000, step 244, avg_time 0.957, loss:291.9626
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4420, rec:0.4616, f1:0.4515
>> valid relation prec:0.0749, rec:0.0414, f1:0.0533
>> valid relation with NER prec:0.0749, rec:0.0414, f1:0.0533
g_step 4100, step 31, avg_time 2.219, loss:298.1830
g_step 4200, step 131, avg_time 0.968, loss:286.0858
g_step 4300, step 231, avg_time 0.944, loss:288.8485
g_step 4400, step 18, avg_time 0.937, loss:270.0308
g_step 4500, step 118, avg_time 0.961, loss:268.3350
>> valid entity prec:0.4705, rec:0.4165, f1:0.4419
>> valid relation prec:0.0756, rec:0.0345, f1:0.0474
>> valid relation with NER prec:0.0756, rec:0.0345, f1:0.0474
g_step 4600, step 218, avg_time 2.235, loss:268.8336
g_step 4700, step 5, avg_time 0.952, loss:296.0643
g_step 4800, step 105, avg_time 0.963, loss:264.6045
g_step 4900, step 205, avg_time 0.949, loss:265.7880
g_step 5000, step 305, avg_time 0.952, loss:268.8578
learning rate was adjusted to 0.0008
>> valid entity prec:0.4399, rec:0.4419, f1:0.4409
>> valid relation prec:0.0794, rec:0.0425, f1:0.0554
>> valid relation with NER prec:0.0794, rec:0.0425, f1:0.0554
g_step 5100, step 92, avg_time 2.228, loss:242.7007
g_step 5200, step 192, avg_time 0.947, loss:251.4019
g_step 5300, step 292, avg_time 0.964, loss:271.1804
g_step 5400, step 79, avg_time 0.943, loss:235.1721
g_step 5500, step 179, avg_time 0.945, loss:245.1546
>> valid entity prec:0.4501, rec:0.4062, f1:0.4270
>> valid relation prec:0.0816, rec:0.0385, f1:0.0523
>> valid relation with NER prec:0.0816, rec:0.0385, f1:0.0523
g_step 5600, step 279, avg_time 2.265, loss:252.3199
g_step 5700, step 66, avg_time 0.938, loss:230.1949
g_step 5800, step 166, avg_time 0.975, loss:244.6640
g_step 5900, step 266, avg_time 0.956, loss:254.5223
g_step 6000, step 53, avg_time 0.941, loss:226.5361
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4571, rec:0.4578, f1:0.4574
>> valid relation prec:0.0839, rec:0.0460, f1:0.0594
>> valid relation with NER prec:0.0839, rec:0.0460, f1:0.0594
g_step 6100, step 153, avg_time 2.242, loss:233.4672
g_step 6200, step 253, avg_time 0.959, loss:237.3910
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:46:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:46:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-45-59_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:46:01 - WARNING - datasets.builder -   Using custom data configuration default-cd6abbf2f2f86a01
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-cd6abbf2f2f86a01/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:46:05,147 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:46:05,148 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:46:05,149 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:46:05,150 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:46:05,302 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:46:05,381 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:46:05,944 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:46:09,142 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:46:09,163 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-cd6abbf2f2f86a01/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.05ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.99ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.45ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.47ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.70ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.87ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.97ba/s]100%|██████████| 8/8 [00:01<00:00,  4.95ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.82ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.20ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.37ba/s]100%|██████████| 4/4 [00:00<00:00,  5.52ba/s]100%|██████████| 4/4 [00:00<00:00,  4.94ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.33ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.13ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.84ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.20ba/s]100%|██████████| 8/8 [00:00<00:00, 10.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.67ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.20ba/s]100%|██████████| 4/4 [00:00<00:00, 10.40ba/s]100%|██████████| 4/4 [00:00<00:00,  9.29ba/s]
[INFO|trainer.py:414] 2023-08-29 01:46:13,674 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:46:13,753 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:46:13,754 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:46:13,754 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:46:13,754 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:46:13,754 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:46:13,754 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:46:13,754 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.32it/s]  0%|          | 2/585 [00:00<02:50,  3.41it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:48,  3.44it/s]  1%|          | 7/585 [00:02<02:48,  3.43it/s]  1%|▏         | 8/585 [00:02<02:48,  3.43it/s]  2%|▏         | 9/585 [00:02<02:48,  3.43it/s]  2%|▏         | 10/585 [00:02<02:47,  3.42it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:47,  3.42it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:46,  3.42it/s]  3%|▎         | 15/585 [00:04<02:46,  3.42it/s]  3%|▎         | 16/585 [00:04<02:52,  3.30it/s]  3%|▎         | 17/585 [00:04<02:50,  3.34it/s]  3%|▎         | 18/585 [00:05<02:48,  3.36it/s]  3%|▎         | 19/585 [00:05<02:47,  3.37it/s]  3%|▎         | 20/585 [00:05<02:47,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.39it/s]  4%|▍         | 22/585 [00:06<02:45,  3.40it/s]  4%|▍         | 23/585 [00:06<02:45,  3.40it/s]  4%|▍         | 24/585 [00:07<02:44,  3.40it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:44,  3.40it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:43,  3.41it/s]  5%|▍         | 29/585 [00:08<02:43,  3.41it/s]  5%|▌         | 30/585 [00:08<02:42,  3.41it/s]  5%|▌         | 31/585 [00:09<02:42,  3.41it/s]  5%|▌         | 32/585 [00:09<02:42,  3.41it/s]  6%|▌         | 33/585 [00:09<02:42,  3.41it/s]  6%|▌         | 34/585 [00:10<02:47,  3.29it/s]  6%|▌         | 35/585 [00:10<02:45,  3.32it/s]  6%|▌         | 36/585 [00:10<02:43,  3.35it/s]  6%|▋         | 37/585 [00:10<02:42,  3.37it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:40,  3.41it/s]  7%|▋         | 40/585 [00:11<02:39,  3.42it/s]  7%|▋         | 41/585 [00:12<02:38,  3.43it/s]  7%|▋         | 42/585 [00:12<02:37,  3.44it/s]  7%|▋         | 43/585 [00:12<02:37,  3.44it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:36,  3.45it/s]  8%|▊         | 46/585 [00:13<02:36,  3.45it/s]  8%|▊         | 47/585 [00:13<02:35,  3.45it/s]  8%|▊         | 48/585 [00:14<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:35,  3.45it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:38,  3.37it/s]  9%|▉         | 52/585 [00:15<02:36,  3.40it/s]  9%|▉         | 53/585 [00:15<02:35,  3.41it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.45it/s] 10%|▉         | 58/585 [00:17<02:32,  3.45it/s] 10%|█         | 59/585 [00:17<02:32,  3.45it/s] 10%|█         | 60/585 [00:17<02:32,  3.45it/s] 10%|█         | 61/585 [00:17<02:31,  3.45it/s] 11%|█         | 62/585 [00:18<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:31,  3.45it/s] 11%|█         | 64/585 [00:18<02:30,  3.45it/s] 11%|█         | 65/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.38it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.42it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:23<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.45it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.45it/s] 15%|█▍        | 86/585 [00:25<02:29,  3.35it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.38it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 89/585 [00:26<02:25,  3.42it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 93/585 [00:27<02:30,  3.28it/s] 16%|█▌        | 94/585 [00:27<02:27,  3.33it/s] 16%|█▌        | 95/585 [00:27<02:25,  3.36it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.41it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.42it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.44it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.44it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.45it/s] 18%|█▊        | 103/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 104/585 [00:30<02:22,  3.37it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.41it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.42it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.44it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:33<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.45it/s] 20%|██        | 117/585 [00:34<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 01:46:48,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:46:48,003 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:46:48,003 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.08it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.55it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.77it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.25it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.73it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.50it/s][A
 10%|▉         | 42/435 [00:00<00:08, 44.06it/s][A
 11%|█         | 47/435 [00:01<00:08, 44.59it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 44.96it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 44.95it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 45.11it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 45.15it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 45.17it/s][A
 18%|█▊        | 77/435 [00:01<00:07, 45.17it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 44.83it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.92it/s][A
 21%|██        | 92/435 [00:02<00:07, 45.13it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 45.21it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 45.28it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 45.43it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 45.35it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 45.24it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 45.12it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 44.94it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.98it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.08it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 45.05it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.32it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.38it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 45.36it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 45.29it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 45.08it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 45.01it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.87it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 45.04it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 45.08it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.30it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.41it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.34it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 45.26it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 45.10it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 44.95it/s][A
 51%|█████     | 222/435 [00:04<00:04, 45.01it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 45.01it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 45.18it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.30it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.43it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 45.41it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 45.32it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 45.08it/s][A
 60%|██████    | 262/435 [00:05<00:03, 44.94it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.86it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.28it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.66it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.87it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 45.09it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 45.26it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 45.20it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 45.09it/s][A
 71%|███████   | 307/435 [00:06<00:02, 44.91it/s][A
 72%|███████▏  | 312/435 [00:06<00:02, 44.82it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.94it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 45.08it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 45.18it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 45.28it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 45.23it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 45.18it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.86it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.83it/s][A
 82%|████████▏ | 357/435 [00:07<00:01, 44.78it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.97it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.68it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.90it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.15it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.38it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.22it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.11it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 44.99it/s][A
 92%|█████████▏| 402/435 [00:08<00:00, 44.88it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.97it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 45.12it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 45.26it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 45.38it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 45.43it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 43.65it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 43.65it/s][A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:46:58,021 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:46:58,325 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:47:01,950 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:47:02,310 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:47:02,414 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<56:08,  7.21s/it] 20%|██        | 119/585 [00:57<39:59,  5.15s/it] 21%|██        | 120/585 [00:58<28:37,  3.69s/it] 21%|██        | 121/585 [00:58<20:40,  2.67s/it] 21%|██        | 122/585 [00:58<15:07,  1.96s/it] 21%|██        | 123/585 [00:59<11:14,  1.46s/it] 21%|██        | 124/585 [00:59<08:31,  1.11s/it] 21%|██▏       | 125/585 [00:59<06:37,  1.16it/s] 22%|██▏       | 126/585 [00:59<05:18,  1.44it/s] 22%|██▏       | 127/585 [01:00<04:22,  1.74it/s] 22%|██▏       | 128/585 [01:00<03:43,  2.04it/s] 22%|██▏       | 129/585 [01:00<03:16,  2.32it/s] 22%|██▏       | 130/585 [01:01<03:01,  2.50it/s] 22%|██▏       | 131/585 [01:01<02:46,  2.72it/s] 23%|██▎       | 132/585 [01:01<02:35,  2.90it/s] 23%|██▎       | 133/585 [01:02<02:28,  3.05it/s] 23%|██▎       | 134/585 [01:02<02:22,  3.16it/s] 23%|██▎       | 135/585 [01:02<02:18,  3.24it/s] 23%|██▎       | 136/585 [01:02<02:15,  3.30it/s] 23%|██▎       | 137/585 [01:03<02:13,  3.35it/s] 24%|██▎       | 138/585 [01:03<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:03<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:04<02:10,  3.42it/s] 24%|██▍       | 141/585 [01:04<02:12,  3.36it/s] 24%|██▍       | 142/585 [01:04<02:10,  3.39it/s] 24%|██▍       | 143/585 [01:04<02:09,  3.41it/s] 25%|██▍       | 144/585 [01:05<02:08,  3.42it/s] 25%|██▍       | 145/585 [01:05<02:08,  3.43it/s] 25%|██▍       | 146/585 [01:05<02:07,  3.44it/s] 25%|██▌       | 147/585 [01:06<02:07,  3.44it/s] 25%|██▌       | 148/585 [01:06<02:06,  3.44it/s] 25%|██▌       | 149/585 [01:06<02:06,  3.45it/s] 26%|██▌       | 150/585 [01:06<02:06,  3.45it/s] 26%|██▌       | 151/585 [01:07<02:05,  3.45it/s] 26%|██▌       | 152/585 [01:07<02:08,  3.37it/s] 26%|██▌       | 153/585 [01:07<02:07,  3.39it/s] 26%|██▋       | 154/585 [01:08<02:06,  3.41it/s] 26%|██▋       | 155/585 [01:08<02:05,  3.42it/s] 27%|██▋       | 156/585 [01:08<02:05,  3.43it/s] 27%|██▋       | 157/585 [01:09<02:04,  3.44it/s] 27%|██▋       | 158/585 [01:09<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:09<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:09<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:10<02:02,  3.45it/s] 28%|██▊       | 162/585 [01:10<02:02,  3.45it/s] 28%|██▊       | 163/585 [01:10<02:06,  3.35it/s] 28%|██▊       | 164/585 [01:11<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:11<02:03,  3.40it/s] 28%|██▊       | 166/585 [01:11<02:02,  3.41it/s] 29%|██▊       | 167/585 [01:11<02:02,  3.42it/s] 29%|██▊       | 168/585 [01:12<02:01,  3.43it/s] 29%|██▉       | 169/585 [01:12<02:01,  3.44it/s] 29%|██▉       | 170/585 [01:12<02:00,  3.44it/s] 29%|██▉       | 171/585 [01:13<02:00,  3.44it/s] 29%|██▉       | 172/585 [01:13<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:13<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:14<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:14<01:58,  3.45it/s] 30%|███       | 176/585 [01:14<01:58,  3.45it/s] 30%|███       | 177/585 [01:14<01:58,  3.45it/s] 30%|███       | 178/585 [01:15<01:57,  3.45it/s] 31%|███       | 179/585 [01:15<01:57,  3.45it/s] 31%|███       | 180/585 [01:15<01:57,  3.45it/s] 31%|███       | 181/585 [01:16<01:57,  3.45it/s] 31%|███       | 182/585 [01:16<02:01,  3.33it/s] 31%|███▏      | 183/585 [01:16<01:59,  3.37it/s] 31%|███▏      | 184/585 [01:16<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:17<01:57,  3.41it/s] 32%|███▏      | 186/585 [01:17<01:56,  3.42it/s] 32%|███▏      | 187/585 [01:17<01:55,  3.43it/s] 32%|███▏      | 188/585 [01:18<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:18<01:54,  3.44it/s] 32%|███▏      | 190/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:19<01:54,  3.45it/s] 33%|███▎      | 193/585 [01:19<01:55,  3.39it/s] 33%|███▎      | 194/585 [01:19<01:54,  3.41it/s] 33%|███▎      | 195/585 [01:20<01:54,  3.42it/s] 34%|███▎      | 196/585 [01:20<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:20<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:21<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:21<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:21<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:21<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:22<01:51,  3.45it/s] 35%|███▍      | 203/585 [01:22<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:22<01:53,  3.37it/s] 35%|███▌      | 205/585 [01:23<01:51,  3.39it/s] 35%|███▌      | 206/585 [01:23<01:51,  3.41it/s] 35%|███▌      | 207/585 [01:23<01:50,  3.42it/s] 36%|███▌      | 208/585 [01:23<01:50,  3.42it/s] 36%|███▌      | 209/585 [01:24<01:49,  3.43it/s] 36%|███▌      | 210/585 [01:24<01:49,  3.44it/s] 36%|███▌      | 211/585 [01:24<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:25<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:25<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:25<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:25<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:26<01:47,  3.45it/s] 37%|███▋      | 217/585 [01:26<01:48,  3.41it/s] 37%|███▋      | 218/585 [01:26<01:47,  3.42it/s] 37%|███▋      | 219/585 [01:27<01:46,  3.42it/s] 38%|███▊      | 220/585 [01:27<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:27<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:28<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:28<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:28<01:44,  3.44it/s] 38%|███▊      | 225/585 [01:28<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:29<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:29<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:29<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 231/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:31<01:42,  3.45it/s] 40%|████      | 234/585 [01:31<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 01:47:45,303 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:47:45,303 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:47:45,303 >>   Batch size = 8
{'eval_loss': 1.1382333040237427, 'eval_runtime': 9.6765, 'eval_samples_per_second': 359.532, 'eval_steps_per_second': 44.954, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.22it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.37it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.67it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.74it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.28it/s][A
  7%|▋         | 32/435 [00:00<00:08, 45.97it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.62it/s][A
 10%|▉         | 42/435 [00:00<00:08, 45.11it/s][A
 11%|█         | 47/435 [00:01<00:08, 45.09it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 45.05it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 45.31it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 45.26it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 45.46it/s][A
 17%|█▋        | 72/435 [00:01<00:07, 45.38it/s][A
 18%|█▊        | 77/435 [00:01<00:07, 45.42it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 45.29it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.98it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.95it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 44.95it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 45.12it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 45.28it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 45.34it/s][A
 27%|██▋       | 117/435 [00:02<00:06, 45.45it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 45.44it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 45.16it/s][A
 30%|███       | 132/435 [00:02<00:06, 45.05it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.88it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.93it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.10it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.19it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 45.33it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 45.41it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 45.31it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 45.23it/s][A
 41%|████      | 177/435 [00:03<00:05, 45.03it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.84it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 45.02it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.14it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.24it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.26it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 45.37it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 45.31it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 45.25it/s][A
 51%|█████     | 222/435 [00:04<00:04, 45.04it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 43.61it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.51it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 44.80it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 44.91it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 45.13it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 45.11it/s][A
 60%|██████    | 262/435 [00:05<00:03, 45.01it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 44.86it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 44.88it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 45.03it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 45.14it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 45.22it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 45.36it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 45.30it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 45.21it/s][A
 71%|███████   | 307/435 [00:06<00:02, 45.06it/s][A
 72%|███████▏  | 312/435 [00:06<00:02, 44.93it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 44.99it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 43.24it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 43.94it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 44.72it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 44.85it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 44.94it/s][A
 81%|████████  | 352/435 [00:07<00:01, 44.84it/s][A
 82%|████████▏ | 357/435 [00:07<00:01, 44.84it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 44.73it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.89it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 45.03it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.28it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.34it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.30it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.18it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 45.08it/s][A
 92%|█████████▏| 402/435 [00:08<00:00, 44.84it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 44.88it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.89it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 45.16it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 45.36it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 45.43it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 45.37it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 45.37it/s][A 40%|████      | 234/585 [01:41<01:41,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:47:55,129 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:47:55,373 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:47:58,278 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:47:58,514 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:47:58,620 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:54<41:23,  7.10s/it] 40%|████      | 236/585 [01:54<29:29,  5.07s/it] 41%|████      | 237/585 [01:55<21:05,  3.64s/it] 41%|████      | 238/585 [01:55<15:13,  2.63s/it] 41%|████      | 239/585 [01:55<11:08,  1.93s/it] 41%|████      | 240/585 [01:55<08:16,  1.44s/it] 41%|████      | 241/585 [01:56<06:16,  1.10s/it] 41%|████▏     | 242/585 [01:56<04:53,  1.17it/s] 42%|████▏     | 243/585 [01:56<03:54,  1.46it/s] 42%|████▏     | 244/585 [01:57<03:13,  1.76it/s] 42%|████▏     | 245/585 [01:57<02:45,  2.06it/s] 42%|████▏     | 246/585 [01:57<02:25,  2.33it/s] 42%|████▏     | 247/585 [01:58<02:14,  2.51it/s] 42%|████▏     | 248/585 [01:58<02:03,  2.72it/s] 43%|████▎     | 249/585 [01:58<01:55,  2.90it/s] 43%|████▎     | 250/585 [01:58<01:50,  3.03it/s] 43%|████▎     | 251/585 [01:59<01:46,  3.14it/s] 43%|████▎     | 252/585 [01:59<01:43,  3.21it/s] 43%|████▎     | 253/585 [01:59<01:41,  3.27it/s] 43%|████▎     | 254/585 [02:00<01:40,  3.31it/s] 44%|████▎     | 255/585 [02:00<01:38,  3.34it/s] 44%|████▍     | 256/585 [02:00<01:37,  3.36it/s] 44%|████▍     | 257/585 [02:01<01:37,  3.37it/s] 44%|████▍     | 258/585 [02:01<01:40,  3.25it/s] 44%|████▍     | 259/585 [02:01<01:38,  3.29it/s] 44%|████▍     | 260/585 [02:01<01:37,  3.33it/s] 45%|████▍     | 261/585 [02:02<01:36,  3.35it/s] 45%|████▍     | 262/585 [02:02<01:35,  3.37it/s] 45%|████▍     | 263/585 [02:02<01:35,  3.38it/s] 45%|████▌     | 264/585 [02:03<01:34,  3.39it/s] 45%|████▌     | 265/585 [02:03<01:34,  3.40it/s] 45%|████▌     | 266/585 [02:03<01:33,  3.40it/s] 46%|████▌     | 267/585 [02:03<01:33,  3.40it/s] 46%|████▌     | 268/585 [02:04<01:33,  3.40it/s] 46%|████▌     | 269/585 [02:04<01:34,  3.33it/s] 46%|████▌     | 270/585 [02:04<01:33,  3.35it/s] 46%|████▋     | 271/585 [02:05<01:33,  3.37it/s] 46%|████▋     | 272/585 [02:05<01:32,  3.38it/s] 47%|████▋     | 273/585 [02:05<01:32,  3.39it/s] 47%|████▋     | 274/585 [02:06<01:31,  3.39it/s] 47%|████▋     | 275/585 [02:06<01:31,  3.40it/s] 47%|████▋     | 276/585 [02:06<01:30,  3.40it/s] 47%|████▋     | 277/585 [02:06<01:30,  3.40it/s] 48%|████▊     | 278/585 [02:07<01:30,  3.40it/s] 48%|████▊     | 279/585 [02:07<01:29,  3.40it/s] 48%|████▊     | 280/585 [02:07<01:32,  3.29it/s] 48%|████▊     | 281/585 [02:08<01:31,  3.32it/s] 48%|████▊     | 282/585 [02:08<01:30,  3.35it/s] 48%|████▊     | 283/585 [02:08<01:29,  3.37it/s] 49%|████▊     | 284/585 [02:09<01:29,  3.38it/s] 49%|████▊     | 285/585 [02:09<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:09<01:30,  3.32it/s] 49%|████▉     | 287/585 [02:09<01:29,  3.34it/s] 49%|████▉     | 288/585 [02:10<01:28,  3.36it/s] 49%|████▉     | 289/585 [02:10<01:27,  3.37it/s] 50%|████▉     | 290/585 [02:10<01:27,  3.38it/s] 50%|████▉     | 291/585 [02:11<01:26,  3.39it/s] 50%|████▉     | 292/585 [02:11<01:26,  3.40it/s] 50%|█████     | 293/585 [02:11<01:25,  3.41it/s] 50%|█████     | 294/585 [02:11<01:24,  3.43it/s] 50%|█████     | 295/585 [02:12<01:24,  3.44it/s] 51%|█████     | 296/585 [02:12<01:23,  3.44it/s] 51%|█████     | 297/585 [02:12<01:25,  3.35it/s] 51%|█████     | 298/585 [02:13<01:24,  3.38it/s] 51%|█████     | 299/585 [02:13<01:24,  3.40it/s] 51%|█████▏    | 300/585 [02:13<01:23,  3.42it/s] 51%|█████▏    | 301/585 [02:14<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:14<01:22,  3.44it/s] 52%|█████▏    | 303/585 [02:14<01:21,  3.44it/s] 52%|█████▏    | 304/585 [02:14<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:15<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:15<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:15<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:16<01:22,  3.34it/s] 53%|█████▎    | 309/585 [02:16<01:21,  3.37it/s] 53%|█████▎    | 310/585 [02:16<01:20,  3.40it/s] 53%|█████▎    | 311/585 [02:16<01:20,  3.41it/s] 53%|█████▎    | 312/585 [02:17<01:19,  3.43it/s] 54%|█████▎    | 313/585 [02:17<01:19,  3.43it/s] 54%|█████▎    | 314/585 [02:17<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:18<01:18,  3.44it/s] 54%|█████▍    | 316/585 [02:18<01:18,  3.45it/s] 54%|█████▍    | 317/585 [02:18<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:18<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:19<01:19,  3.36it/s] 55%|█████▍    | 320/585 [02:19<01:18,  3.39it/s] 55%|█████▍    | 321/585 [02:19<01:17,  3.41it/s] 55%|█████▌    | 322/585 [02:20<01:16,  3.42it/s] 55%|█████▌    | 323/585 [02:20<01:16,  3.43it/s] 55%|█████▌    | 324/585 [02:20<01:16,  3.43it/s] 56%|█████▌    | 325/585 [02:21<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:21<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:21<01:14,  3.44it/s] 56%|█████▌    | 328/585 [02:21<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:22<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:22<01:15,  3.36it/s] 57%|█████▋    | 331/585 [02:22<01:14,  3.39it/s] 57%|█████▋    | 332/585 [02:23<01:14,  3.41it/s] 57%|█████▋    | 333/585 [02:23<01:13,  3.42it/s] 57%|█████▋    | 334/585 [02:23<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:23<01:12,  3.44it/s] 57%|█████▋    | 336/585 [02:24<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:24<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:24<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 341/585 [02:25<01:12,  3.39it/s] 58%|█████▊    | 342/585 [02:26<01:11,  3.41it/s] 59%|█████▊    | 343/585 [02:26<01:10,  3.42it/s] 59%|█████▉    | 344/585 [02:26<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:26<01:09,  3.43it/s] 59%|█████▉    | 346/585 [02:27<01:09,  3.44it/s] 59%|█████▉    | 347/585 [02:27<01:09,  3.44it/s] 59%|█████▉    | 348/585 [02:27<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:28<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:28<01:08,  3.45it/s] 60%|██████    | 351/585 [02:28<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 01:48:42,434 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:48:42,434 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:48:42,434 >>   Batch size = 8
{'eval_loss': 1.1505941152572632, 'eval_runtime': 9.6695, 'eval_samples_per_second': 359.79, 'eval_steps_per_second': 44.987, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.22it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.30it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.63it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.67it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.29it/s][A
  7%|▋         | 32/435 [00:00<00:08, 46.07it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.78it/s][A
 10%|▉         | 42/435 [00:00<00:08, 45.22it/s][A
 11%|█         | 47/435 [00:01<00:08, 45.07it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 45.23it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 45.26it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 45.20it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 45.20it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 45.32it/s][A
 18%|█▊        | 77/435 [00:01<00:07, 45.38it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 45.21it/s][A
 20%|██        | 87/435 [00:01<00:07, 45.00it/s][A
 21%|██        | 92/435 [00:02<00:07, 44.73it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 45.07it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 45.11it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 45.13it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 45.12it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 45.18it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 45.24it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 45.12it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.86it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 44.81it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 43.57it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 44.19it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 44.61it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 44.88it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 45.10it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 45.22it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 45.20it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.96it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.68it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 44.75it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.00it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.23it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.29it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 45.35it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 45.39it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 45.17it/s][A
 51%|█████     | 222/435 [00:04<00:04, 45.04it/s][A
 52%|█████▏    | 227/435 [00:05<00:04, 44.77it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.96it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 44.95it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.08it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 45.23it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 45.42it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 45.40it/s][A
 60%|██████    | 262/435 [00:05<00:03, 45.27it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 45.09it/s][A
 63%|██████▎   | 272/435 [00:06<00:03, 45.00it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.86it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 44.97it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 45.12it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 45.28it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 45.30it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 45.30it/s][A
 71%|███████   | 307/435 [00:06<00:02, 45.21it/s][A
 72%|███████▏  | 312/435 [00:06<00:02, 45.08it/s][A
 73%|███████▎  | 317/435 [00:07<00:02, 45.00it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.96it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.97it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 45.13it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 45.26it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 45.36it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 45.24it/s][A
 81%|████████  | 352/435 [00:07<00:01, 45.24it/s][A
 82%|████████▏ | 357/435 [00:07<00:01, 45.08it/s][A
 83%|████████▎ | 362/435 [00:08<00:01, 45.00it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 44.95it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.98it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.17it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.36it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.32it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.34it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 45.24it/s][A
 92%|█████████▏| 402/435 [00:08<00:00, 45.16it/s][A
 94%|█████████▎| 407/435 [00:09<00:00, 45.05it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 44.98it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 43.10it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 43.86it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.36it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.67it/s][A
                                                 [A                                                 
100%|██████████| 435/435 [00:09<00:00, 44.67it/s][A 60%|██████    | 351/585 [02:38<01:07,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:48:52,182 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:48:52,341 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:48:55,916 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:48:56,058 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:48:56,097 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:49<24:37,  6.34s/it] 60%|██████    | 353/585 [02:49<17:33,  4.54s/it] 61%|██████    | 354/585 [02:49<12:34,  3.27s/it] 61%|██████    | 355/585 [02:50<09:06,  2.37s/it] 61%|██████    | 356/585 [02:50<06:40,  1.75s/it] 61%|██████    | 357/585 [02:50<04:59,  1.31s/it] 61%|██████    | 358/585 [02:50<03:48,  1.01s/it] 61%|██████▏   | 359/585 [02:51<02:59,  1.26it/s] 62%|██████▏   | 360/585 [02:51<02:24,  1.55it/s] 62%|██████▏   | 361/585 [02:51<02:00,  1.86it/s] 62%|██████▏   | 362/585 [02:52<01:43,  2.15it/s] 62%|██████▏   | 363/585 [02:52<01:31,  2.42it/s] 62%|██████▏   | 364/585 [02:52<01:25,  2.57it/s] 62%|██████▏   | 365/585 [02:52<01:19,  2.78it/s] 63%|██████▎   | 366/585 [02:53<01:20,  2.73it/s] 63%|██████▎   | 367/585 [02:53<01:30,  2.41it/s] 63%|██████▎   | 368/585 [02:54<01:22,  2.64it/s] 63%|██████▎   | 369/585 [02:54<01:16,  2.84it/s] 63%|██████▎   | 370/585 [02:54<01:11,  2.99it/s] 63%|██████▎   | 371/585 [02:55<01:08,  3.10it/s] 64%|██████▎   | 372/585 [02:55<01:06,  3.19it/s] 64%|██████▍   | 373/585 [02:55<01:07,  3.12it/s] 64%|██████▍   | 374/585 [02:55<01:05,  3.21it/s] 64%|██████▍   | 375/585 [02:56<01:04,  3.26it/s] 64%|██████▍   | 376/585 [02:56<01:03,  3.31it/s] 64%|██████▍   | 377/585 [02:56<01:02,  3.34it/s] 65%|██████▍   | 378/585 [02:57<01:01,  3.36it/s] 65%|██████▍   | 379/585 [02:57<01:01,  3.37it/s] 65%|██████▍   | 380/585 [02:57<01:00,  3.39it/s] 65%|██████▌   | 381/585 [02:58<01:00,  3.39it/s] 65%|██████▌   | 382/585 [02:58<00:59,  3.40it/s] 65%|██████▌   | 383/585 [02:58<00:59,  3.40it/s] 66%|██████▌   | 384/585 [02:58<01:00,  3.34it/s] 66%|██████▌   | 385/585 [02:59<00:59,  3.36it/s] 66%|██████▌   | 386/585 [02:59<00:58,  3.38it/s] 66%|██████▌   | 387/585 [02:59<00:58,  3.39it/s] 66%|██████▋   | 388/585 [03:00<00:58,  3.39it/s] 66%|██████▋   | 389/585 [03:00<00:57,  3.40it/s] 67%|██████▋   | 390/585 [03:00<00:57,  3.40it/s] 67%|██████▋   | 391/585 [03:00<00:56,  3.40it/s] 67%|██████▋   | 392/585 [03:01<00:56,  3.41it/s] 67%|██████▋   | 393/585 [03:01<00:56,  3.41it/s] 67%|██████▋   | 394/585 [03:01<00:56,  3.41it/s] 68%|██████▊   | 395/585 [03:02<00:56,  3.35it/s] 68%|██████▊   | 396/585 [03:02<00:56,  3.37it/s] 68%|██████▊   | 397/585 [03:02<00:55,  3.38it/s] 68%|██████▊   | 398/585 [03:03<00:55,  3.39it/s] 68%|██████▊   | 399/585 [03:03<00:54,  3.39it/s] 68%|██████▊   | 400/585 [03:03<00:54,  3.40it/s] 69%|██████▊   | 401/585 [03:03<00:54,  3.40it/s] 69%|██████▊   | 402/585 [03:04<00:53,  3.40it/s] 69%|██████▉   | 403/585 [03:04<00:53,  3.41it/s] 69%|██████▉   | 404/585 [03:04<00:53,  3.41it/s] 69%|██████▉   | 405/585 [03:05<00:52,  3.41it/s] 69%|██████▉   | 406/585 [03:05<00:54,  3.30it/s] 70%|██████▉   | 407/585 [03:05<00:53,  3.33it/s] 70%|██████▉   | 408/585 [03:06<00:52,  3.35it/s] 70%|██████▉   | 409/585 [03:06<00:52,  3.37it/s] 70%|███████   | 410/585 [03:06<00:51,  3.38it/s] 70%|███████   | 411/585 [03:06<00:51,  3.39it/s] 70%|███████   | 412/585 [03:07<00:50,  3.39it/s] 71%|███████   | 413/585 [03:07<00:50,  3.40it/s] 71%|███████   | 414/585 [03:07<00:50,  3.40it/s] 71%|███████   | 415/585 [03:08<00:49,  3.40it/s] 71%|███████   | 416/585 [03:08<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:08<00:50,  3.30it/s] 71%|███████▏  | 418/585 [03:08<00:50,  3.33it/s] 72%|███████▏  | 419/585 [03:09<00:49,  3.36it/s] 72%|███████▏  | 420/585 [03:09<00:48,  3.37it/s] 72%|███████▏  | 421/585 [03:09<00:48,  3.38it/s] 72%|███████▏  | 422/585 [03:10<00:48,  3.39it/s] 72%|███████▏  | 423/585 [03:10<00:47,  3.40it/s] 72%|███████▏  | 424/585 [03:10<00:47,  3.40it/s] 73%|███████▎  | 425/585 [03:11<00:48,  3.29it/s] 73%|███████▎  | 426/585 [03:11<00:47,  3.32it/s] 73%|███████▎  | 427/585 [03:11<00:47,  3.35it/s] 73%|███████▎  | 428/585 [03:11<00:46,  3.37it/s] 73%|███████▎  | 429/585 [03:12<00:46,  3.38it/s] 74%|███████▎  | 430/585 [03:12<00:45,  3.38it/s] 74%|███████▎  | 431/585 [03:12<00:45,  3.39it/s] 74%|███████▍  | 432/585 [03:13<00:45,  3.40it/s] 74%|███████▍  | 433/585 [03:13<00:44,  3.40it/s] 74%|███████▍  | 434/585 [03:13<00:44,  3.40it/s] 74%|███████▍  | 435/585 [03:14<00:44,  3.40it/s] 75%|███████▍  | 436/585 [03:14<00:45,  3.25it/s] 75%|███████▍  | 437/585 [03:14<00:44,  3.29it/s] 75%|███████▍  | 438/585 [03:14<00:44,  3.33it/s] 75%|███████▌  | 439/585 [03:15<00:43,  3.35it/s] 75%|███████▌  | 440/585 [03:15<00:43,  3.37it/s] 75%|███████▌  | 441/585 [03:15<00:42,  3.38it/s] 76%|███████▌  | 442/585 [03:16<00:42,  3.39it/s] 76%|███████▌  | 443/585 [03:16<00:41,  3.39it/s] 76%|███████▌  | 444/585 [03:16<00:41,  3.39it/s] 76%|███████▌  | 445/585 [03:16<00:41,  3.40it/s] 76%|███████▌  | 446/585 [03:17<00:40,  3.40it/s] 76%|███████▋  | 447/585 [03:17<00:41,  3.34it/s] 77%|███████▋  | 448/585 [03:17<00:40,  3.36it/s] 77%|███████▋  | 449/585 [03:18<00:40,  3.37it/s] 77%|███████▋  | 450/585 [03:18<00:39,  3.38it/s] 77%|███████▋  | 451/585 [03:18<00:39,  3.39it/s] 77%|███████▋  | 452/585 [03:19<00:39,  3.39it/s] 77%|███████▋  | 453/585 [03:19<00:38,  3.40it/s] 78%|███████▊  | 454/585 [03:19<00:38,  3.40it/s] 78%|███████▊  | 455/585 [03:19<00:38,  3.40it/s] 78%|███████▊  | 456/585 [03:20<00:37,  3.40it/s] 78%|███████▊  | 457/585 [03:20<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:20<00:38,  3.29it/s] 78%|███████▊  | 459/585 [03:21<00:37,  3.32it/s] 79%|███████▊  | 460/585 [03:21<00:37,  3.35it/s] 79%|███████▉  | 461/585 [03:21<00:36,  3.36it/s] 79%|███████▉  | 462/585 [03:22<00:36,  3.38it/s] 79%|███████▉  | 463/585 [03:22<00:35,  3.39it/s] 79%|███████▉  | 464/585 [03:22<00:35,  3.39it/s] 79%|███████▉  | 465/585 [03:22<00:35,  3.39it/s] 80%|███████▉  | 466/585 [03:23<00:35,  3.40it/s] 80%|███████▉  | 467/585 [03:23<00:34,  3.40it/s] 80%|████████  | 468/585 [03:23<00:34,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 01:49:37,621 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:49:37,622 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:49:37,622 >>   Batch size = 8
{'eval_loss': 1.1651899814605713, 'eval_runtime': 9.6744, 'eval_samples_per_second': 359.61, 'eval_steps_per_second': 44.964, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/435 [00:00<00:08, 47.80it/s][A
  5%|▌         | 22/435 [00:00<00:08, 46.91it/s][A
  6%|▌         | 27/435 [00:00<00:08, 46.35it/s][A
  7%|▋         | 32/435 [00:00<00:08, 46.00it/s][A
  9%|▊         | 37/435 [00:00<00:08, 45.81it/s][A
 10%|▉         | 42/435 [00:00<00:08, 45.21it/s][A
 11%|█         | 47/435 [00:01<00:08, 45.14it/s][A
 12%|█▏        | 52/435 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 57/435 [00:01<00:08, 45.25it/s][A
 14%|█▍        | 62/435 [00:01<00:08, 45.36it/s][A
 15%|█▌        | 67/435 [00:01<00:08, 45.42it/s][A
 17%|█▋        | 72/435 [00:01<00:08, 45.26it/s][A
 18%|█▊        | 77/435 [00:01<00:07, 45.26it/s][A
 19%|█▉        | 82/435 [00:01<00:07, 45.15it/s][A
 20%|██        | 87/435 [00:01<00:07, 44.97it/s][A
 21%|██        | 92/435 [00:02<00:07, 45.04it/s][A
 22%|██▏       | 97/435 [00:02<00:07, 45.18it/s][A
 23%|██▎       | 102/435 [00:02<00:07, 45.16it/s][A
 25%|██▍       | 107/435 [00:02<00:07, 45.28it/s][A
 26%|██▌       | 112/435 [00:02<00:07, 45.26it/s][A
 27%|██▋       | 117/435 [00:02<00:07, 45.25it/s][A
 28%|██▊       | 122/435 [00:02<00:06, 45.32it/s][A
 29%|██▉       | 127/435 [00:02<00:06, 45.09it/s][A
 30%|███       | 132/435 [00:02<00:06, 44.97it/s][A
 31%|███▏      | 137/435 [00:03<00:06, 45.04it/s][A
 33%|███▎      | 142/435 [00:03<00:06, 44.86it/s][A
 34%|███▍      | 147/435 [00:03<00:06, 45.15it/s][A
 35%|███▍      | 152/435 [00:03<00:06, 45.14it/s][A
 36%|███▌      | 157/435 [00:03<00:06, 45.26it/s][A
 37%|███▋      | 162/435 [00:03<00:06, 45.35it/s][A
 38%|███▊      | 167/435 [00:03<00:05, 45.22it/s][A
 40%|███▉      | 172/435 [00:03<00:05, 45.20it/s][A
 41%|████      | 177/435 [00:03<00:05, 44.96it/s][A
 42%|████▏     | 182/435 [00:04<00:05, 44.97it/s][A
 43%|████▎     | 187/435 [00:04<00:05, 45.00it/s][A
 44%|████▍     | 192/435 [00:04<00:05, 45.07it/s][A
 45%|████▌     | 197/435 [00:04<00:05, 45.18it/s][A
 46%|████▋     | 202/435 [00:04<00:05, 45.32it/s][A
 48%|████▊     | 207/435 [00:04<00:05, 45.30it/s][A
 49%|████▊     | 212/435 [00:04<00:04, 45.31it/s][A
 50%|████▉     | 217/435 [00:04<00:04, 45.22it/s][A
 51%|█████     | 222/435 [00:04<00:04, 45.07it/s][A
 52%|█████▏    | 227/435 [00:04<00:04, 45.02it/s][A
 53%|█████▎    | 232/435 [00:05<00:04, 44.94it/s][A
 54%|█████▍    | 237/435 [00:05<00:04, 45.12it/s][A
 56%|█████▌    | 242/435 [00:05<00:04, 45.25it/s][A
 57%|█████▋    | 247/435 [00:05<00:04, 45.28it/s][A
 58%|█████▊    | 252/435 [00:05<00:04, 45.32it/s][A
 59%|█████▉    | 257/435 [00:05<00:03, 45.29it/s][A
 60%|██████    | 262/435 [00:05<00:03, 45.20it/s][A
 61%|██████▏   | 267/435 [00:05<00:03, 45.02it/s][A
 63%|██████▎   | 272/435 [00:05<00:03, 45.00it/s][A
 64%|██████▎   | 277/435 [00:06<00:03, 44.93it/s][A
 65%|██████▍   | 282/435 [00:06<00:03, 43.95it/s][A
 66%|██████▌   | 287/435 [00:06<00:03, 44.43it/s][A
 67%|██████▋   | 292/435 [00:06<00:03, 44.72it/s][A
 68%|██████▊   | 297/435 [00:06<00:03, 44.92it/s][A
 69%|██████▉   | 302/435 [00:06<00:02, 45.14it/s][A
 71%|███████   | 307/435 [00:06<00:02, 45.08it/s][A
 72%|███████▏  | 312/435 [00:06<00:02, 45.12it/s][A
 73%|███████▎  | 317/435 [00:06<00:02, 45.16it/s][A
 74%|███████▍  | 322/435 [00:07<00:02, 44.86it/s][A
 75%|███████▌  | 327/435 [00:07<00:02, 44.84it/s][A
 76%|███████▋  | 332/435 [00:07<00:02, 44.91it/s][A
 77%|███████▋  | 337/435 [00:07<00:02, 45.18it/s][A
 79%|███████▊  | 342/435 [00:07<00:02, 45.39it/s][A
 80%|███████▉  | 347/435 [00:07<00:01, 45.32it/s][A
 81%|████████  | 352/435 [00:07<00:01, 45.27it/s][A
 82%|████████▏ | 357/435 [00:07<00:01, 45.25it/s][A
 83%|████████▎ | 362/435 [00:07<00:01, 45.10it/s][A
 84%|████████▍ | 367/435 [00:08<00:01, 45.11it/s][A
 86%|████████▌ | 372/435 [00:08<00:01, 44.94it/s][A
 87%|████████▋ | 377/435 [00:08<00:01, 45.07it/s][A
 88%|████████▊ | 382/435 [00:08<00:01, 45.20it/s][A
 89%|████████▉ | 387/435 [00:08<00:01, 45.29it/s][A
 90%|█████████ | 392/435 [00:08<00:00, 45.39it/s][A
 91%|█████████▏| 397/435 [00:08<00:00, 45.31it/s][A
 92%|█████████▏| 402/435 [00:08<00:00, 45.24it/s][A
 94%|█████████▎| 407/435 [00:08<00:00, 45.11it/s][A
 95%|█████████▍| 412/435 [00:09<00:00, 45.06it/s][A
 96%|█████████▌| 417/435 [00:09<00:00, 45.02it/s][A
 97%|█████████▋| 422/435 [00:09<00:00, 43.54it/s][A
 98%|█████████▊| 427/435 [00:09<00:00, 44.23it/s][A
 99%|█████████▉| 432/435 [00:09<00:00, 44.64it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:33<00:34,  3.40it/s]
100%|██████████| 435/435 [00:09<00:00, 44.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:49:47,373 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:49:47,591 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:49:50,530 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:49:50,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:49:50,761 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:44<12:11,  6.30s/it] 80%|████████  | 470/585 [03:44<08:37,  4.50s/it] 81%|████████  | 471/585 [03:44<06:09,  3.24s/it] 81%|████████  | 472/585 [03:45<04:26,  2.36s/it] 81%|████████  | 473/585 [03:45<03:14,  1.74s/it] 81%|████████  | 474/585 [03:45<02:24,  1.31s/it] 81%|████████  | 475/585 [03:45<01:50,  1.00s/it] 81%|████████▏ | 476/585 [03:46<01:25,  1.27it/s] 82%|████████▏ | 477/585 [03:46<01:09,  1.56it/s] 82%|████████▏ | 478/585 [03:46<00:57,  1.87it/s] 82%|████████▏ | 479/585 [03:47<00:49,  2.16it/s] 82%|████████▏ | 480/585 [03:47<00:43,  2.43it/s] 82%|████████▏ | 481/585 [03:47<00:39,  2.66it/s] 82%|████████▏ | 482/585 [03:47<00:36,  2.85it/s] 83%|████████▎ | 483/585 [03:48<00:34,  2.93it/s] 83%|████████▎ | 484/585 [03:48<00:32,  3.06it/s] 83%|████████▎ | 485/585 [03:48<00:31,  3.16it/s] 83%|████████▎ | 486/585 [03:49<00:30,  3.23it/s] 83%|████████▎ | 487/585 [03:49<00:29,  3.28it/s] 83%|████████▎ | 488/585 [03:49<00:29,  3.32it/s] 84%|████████▎ | 489/585 [03:50<00:28,  3.35it/s] 84%|████████▍ | 490/585 [03:50<00:28,  3.37it/s] 84%|████████▍ | 491/585 [03:50<00:27,  3.38it/s] 84%|████████▍ | 492/585 [03:50<00:27,  3.39it/s] 84%|████████▍ | 493/585 [03:51<00:27,  3.40it/s] 84%|████████▍ | 494/585 [03:51<00:28,  3.24it/s] 85%|████████▍ | 495/585 [03:51<00:27,  3.29it/s] 85%|████████▍ | 496/585 [03:52<00:26,  3.33it/s] 85%|████████▍ | 497/585 [03:52<00:26,  3.35it/s] 85%|████████▌ | 498/585 [03:52<00:25,  3.37it/s] 85%|████████▌ | 499/585 [03:52<00:25,  3.38it/s] 85%|████████▌ | 500/585 [03:53<00:29,  2.90it/s]                                                  85%|████████▌ | 500/585 [03:53<00:29,  2.90it/s] 86%|████████▌ | 501/585 [03:54<00:36,  2.32it/s] 86%|████████▌ | 502/585 [03:54<00:32,  2.57it/s] 86%|████████▌ | 503/585 [03:54<00:31,  2.64it/s] 86%|████████▌ | 504/585 [03:55<00:28,  2.83it/s] 86%|████████▋ | 505/585 [03:55<00:26,  2.98it/s] 86%|████████▋ | 506/585 [03:55<00:25,  3.10it/s] 87%|████████▋ | 507/585 [03:55<00:24,  3.18it/s] 87%|████████▋ | 508/585 [03:56<00:23,  3.25it/s] 87%|████████▋ | 509/585 [03:56<00:23,  3.30it/s] 87%|████████▋ | 510/585 [03:56<00:22,  3.33it/s] 87%|████████▋ | 511/585 [03:57<00:22,  3.35it/s] 88%|████████▊ | 512/585 [03:57<00:21,  3.37it/s] 88%|████████▊ | 513/585 [03:57<00:21,  3.38it/s] 88%|████████▊ | 514/585 [03:58<00:22,  3.21it/s] 88%|████████▊ | 515/585 [03:58<00:21,  3.27it/s] 88%|████████▊ | 516/585 [03:58<00:20,  3.31it/s] 88%|████████▊ | 517/585 [03:58<00:20,  3.34it/s] 89%|████████▊ | 518/585 [03:59<00:19,  3.36it/s] 89%|████████▊ | 519/585 [03:59<00:19,  3.37it/s] 89%|████████▉ | 520/585 [03:59<00:19,  3.38it/s] 89%|████████▉ | 521/585 [04:00<00:18,  3.39it/s] 89%|████████▉ | 522/585 [04:00<00:18,  3.40it/s] 89%|████████▉ | 523/585 [04:00<00:18,  3.40it/s] 90%|████████▉ | 524/585 [04:00<00:17,  3.40it/s] 90%|████████▉ | 525/585 [04:01<00:18,  3.33it/s] 90%|████████▉ | 526/585 [04:01<00:17,  3.36it/s] 90%|█████████ | 527/585 [04:01<00:17,  3.37it/s] 90%|█████████ | 528/585 [04:02<00:16,  3.38it/s] 90%|█████████ | 529/585 [04:02<00:16,  3.39it/s] 91%|█████████ | 530/585 [04:02<00:16,  3.40it/s] 91%|█████████ | 531/585 [04:03<00:15,  3.40it/s] 91%|█████████ | 532/585 [04:03<00:15,  3.40it/s] 91%|█████████ | 533/585 [04:03<00:15,  3.41it/s] 91%|█████████▏| 534/585 [04:03<00:14,  3.41it/s] 91%|█████████▏| 535/585 [04:04<00:14,  3.41it/s] 92%|█████████▏| 536/585 [04:04<00:14,  3.33it/s] 92%|█████████▏| 537/585 [04:04<00:14,  3.36it/s] 92%|█████████▏| 538/585 [04:05<00:13,  3.37it/s] 92%|█████████▏| 539/585 [04:05<00:13,  3.38it/s] 92%|█████████▏| 540/585 [04:05<00:13,  3.39it/s] 92%|█████████▏| 541/585 [04:05<00:12,  3.40it/s] 93%|█████████▎| 542/585 [04:06<00:12,  3.40it/s] 93%|█████████▎| 543/585 [04:06<00:12,  3.40it/s] 93%|█████████▎| 544/585 [04:06<00:12,  3.40it/s] 93%|█████████▎| 545/585 [04:07<00:11,  3.40it/s] 93%|█████████▎| 546/585 [04:07<00:11,  3.40it/s] 94%|█████████▎| 547/585 [04:07<00:11,  3.30it/s] 94%|█████████▎| 548/585 [04:08<00:11,  3.33it/s] 94%|█████████▍| 549/585 [04:08<00:10,  3.36it/s] 94%|█████████▍| 550/585 [04:08<00:10,  3.37it/s] 94%|█████████▍| 551/585 [04:08<00:10,  3.38it/s] 94%|█████████▍| 552/585 [04:09<00:09,  3.39it/s] 95%|█████████▍| 553/585 [04:09<00:09,  3.39it/s] 95%|█████████▍| 554/585 [04:09<00:09,  3.40it/s] 95%|█████████▍| 555/585 [04:10<00:08,  3.40it/s] 95%|█████████▌| 556/585 [04:10<00:08,  3.40it/s] 95%|█████████▌| 557/585 [04:10<00:08,  3.40it/s] 95%|█████████▌| 558/585 [04:11<00:08,  3.27it/s] 96%|█████████▌| 559/585 [04:11<00:07,  3.31it/s] 96%|█████████▌| 560/585 [04:11<00:07,  3.34it/s] 96%|█████████▌| 561/585 [04:11<00:07,  3.36it/s] 96%|█████████▌| 562/585 [04:12<00:06,  3.37it/s] 96%|█████████▌| 563/585 [04:12<00:06,  3.38it/s] 96%|█████████▋| 564/585 [04:12<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:13<00:05,  3.40it/s] 97%|█████████▋| 566/585 [04:13<00:05,  3.40it/s] 97%|█████████▋| 567/585 [04:13<00:05,  3.40it/s] 97%|█████████▋| 568/585 [04:13<00:04,  3.41it/s] 97%|█████████▋| 569/585 [04:14<00:04,  3.41it/s] 97%|█████████▋| 570/585 [04:14<00:04,  3.41it/s] 98%|█████████▊| 571/585 [04:14<00:04,  3.41it/s] 98%|█████████▊| 572/585 [04:15<00:03,  3.41it/s] 98%|█████████▊| 573/585 [04:15<00:03,  3.41it/s] 98%|█████████▊| 574/585 [04:15<00:03,  3.17it/s] 98%|█████████▊| 575/585 [04:16<00:03,  3.24it/s] 98%|█████████▊| 576/585 [04:16<00:02,  3.29it/s] 99%|█████████▊| 577/585 [04:16<00:02,  3.32it/s] 99%|█████████▉| 578/585 [04:16<00:02,  3.35it/s] 99%|█████████▉| 579/585 [04:17<00:01,  3.36it/s] 99%|█████████▉| 580/585 [04:17<00:01,  3.38it/s] 99%|█████████▉| 581/585 [04:17<00:01,  3.39it/s] 99%|█████████▉| 582/585 [04:18<00:00,  3.39it/s]100%|█████████▉| 583/585 [04:18<00:00,  3.40it/s]100%|█████████▉| 584/585 [04:18<00:00,  3.18it/s]100%|██████████| 585/585 [04:19<00:00,  3.25it/s][INFO|trainer.py:2140] 2023-08-29 01:50:32,857 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:50:32,857 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:50:32,857 >>   Batch size = 8
{'eval_loss': 1.1828964948654175, 'eval_runtime': 9.6612, 'eval_samples_per_second': 360.102, 'eval_steps_per_second': 45.026, 'epoch': 4.0}
{'loss': 0.3443, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|▏         | 6/435 [00:00<00:07, 56.56it/s][A
  3%|▎         | 12/435 [00:00<00:08, 49.49it/s][A
  4%|▍         | 18/435 [00:00<00:08, 47.46it/s][A
  5%|▌         | 23/435 [00:00<00:08, 46.64it/s][A
  6%|▋         | 28/435 [00:00<00:08, 46.18it/s][A
  8%|▊         | 33/435 [00:00<00:08, 45.63it/s][A
  9%|▊         | 38/435 [00:00<00:08, 45.35it/s][A
 10%|▉         | 43/435 [00:00<00:08, 45.21it/s][A
 11%|█         | 48/435 [00:01<00:08, 45.22it/s][A
 12%|█▏        | 53/435 [00:01<00:08, 45.29it/s][A
 13%|█▎        | 58/435 [00:01<00:08, 45.34it/s][A
 14%|█▍        | 63/435 [00:01<00:08, 45.46it/s][A
 16%|█▌        | 68/435 [00:01<00:08, 45.46it/s][A
 17%|█▋        | 73/435 [00:01<00:07, 45.32it/s][A
 18%|█▊        | 78/435 [00:01<00:07, 45.12it/s][A
 19%|█▉        | 83/435 [00:01<00:07, 45.06it/s][A
 20%|██        | 88/435 [00:01<00:07, 45.06it/s][A
 21%|██▏       | 93/435 [00:02<00:07, 45.12it/s][A
 23%|██▎       | 98/435 [00:02<00:07, 45.11it/s][A
 24%|██▎       | 103/435 [00:02<00:07, 45.27it/s][A
 25%|██▍       | 108/435 [00:02<00:07, 45.39it/s][A
 26%|██▌       | 113/435 [00:02<00:07, 44.51it/s][A
 27%|██▋       | 118/435 [00:02<00:07, 44.68it/s][A
 28%|██▊       | 123/435 [00:02<00:06, 44.73it/s][A
 29%|██▉       | 128/435 [00:02<00:06, 44.79it/s][A
 31%|███       | 133/435 [00:02<00:06, 44.90it/s][A
 32%|███▏      | 138/435 [00:03<00:06, 44.98it/s][A
 33%|███▎      | 143/435 [00:03<00:06, 45.06it/s][A
 34%|███▍      | 148/435 [00:03<00:06, 45.26it/s][A
 35%|███▌      | 153/435 [00:03<00:06, 45.22it/s][A
 36%|███▋      | 158/435 [00:03<00:06, 45.22it/s][A
 37%|███▋      | 163/435 [00:03<00:06, 45.16it/s][A
 39%|███▊      | 168/435 [00:03<00:05, 45.13it/s][A
 40%|███▉      | 173/435 [00:03<00:05, 45.03it/s][A
 41%|████      | 178/435 [00:03<00:05, 45.11it/s][A
 42%|████▏     | 183/435 [00:04<00:05, 45.11it/s][A
 43%|████▎     | 188/435 [00:04<00:05, 45.24it/s][A
 44%|████▍     | 193/435 [00:04<00:05, 45.26it/s][A
 46%|████▌     | 198/435 [00:04<00:05, 45.37it/s][A
 47%|████▋     | 203/435 [00:04<00:05, 45.28it/s][A
 48%|████▊     | 208/435 [00:04<00:05, 45.15it/s][A
 49%|████▉     | 213/435 [00:04<00:04, 45.15it/s][A
 50%|█████     | 218/435 [00:04<00:04, 45.04it/s][A
 51%|█████▏    | 223/435 [00:04<00:04, 44.95it/s][A
 52%|█████▏    | 228/435 [00:05<00:04, 45.00it/s][A
 54%|█████▎    | 233/435 [00:05<00:04, 45.08it/s][A
 55%|█████▍    | 238/435 [00:05<00:04, 45.28it/s][A
 56%|█████▌    | 243/435 [00:05<00:04, 45.26it/s][A
 57%|█████▋    | 248/435 [00:05<00:04, 45.20it/s][A
 58%|█████▊    | 253/435 [00:05<00:04, 43.28it/s][A
 59%|█████▉    | 258/435 [00:05<00:04, 44.02it/s][A
 60%|██████    | 263/435 [00:05<00:03, 44.33it/s][A
 62%|██████▏   | 268/435 [00:05<00:03, 44.33it/s][A
 63%|██████▎   | 273/435 [00:06<00:03, 44.63it/s][A
 64%|██████▍   | 278/435 [00:06<00:03, 44.87it/s][A
 65%|██████▌   | 283/435 [00:06<00:03, 45.05it/s][A
 66%|██████▌   | 288/435 [00:06<00:03, 45.09it/s][A
 67%|██████▋   | 293/435 [00:06<00:03, 44.92it/s][A
 69%|██████▊   | 298/435 [00:06<00:03, 45.05it/s][A
 70%|██████▉   | 303/435 [00:06<00:02, 45.27it/s][A
 71%|███████   | 308/435 [00:06<00:02, 45.23it/s][A
 72%|███████▏  | 313/435 [00:06<00:02, 45.24it/s][A
 73%|███████▎  | 318/435 [00:07<00:02, 45.12it/s][A
 74%|███████▍  | 323/435 [00:07<00:02, 45.20it/s][A
 75%|███████▌  | 328/435 [00:07<00:02, 45.20it/s][A
 77%|███████▋  | 333/435 [00:07<00:02, 45.23it/s][A
 78%|███████▊  | 338/435 [00:07<00:02, 45.03it/s][A
 79%|███████▉  | 343/435 [00:07<00:02, 45.16it/s][A
 80%|████████  | 348/435 [00:07<00:01, 45.26it/s][A
 81%|████████  | 353/435 [00:07<00:01, 45.22it/s][A
 82%|████████▏ | 358/435 [00:07<00:01, 45.21it/s][A
 83%|████████▎ | 363/435 [00:08<00:01, 45.22it/s][A
 85%|████████▍ | 368/435 [00:08<00:01, 45.18it/s][A
 86%|████████▌ | 373/435 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 378/435 [00:08<00:01, 45.14it/s][A
 88%|████████▊ | 383/435 [00:08<00:01, 45.16it/s][A
 89%|████████▉ | 388/435 [00:08<00:01, 44.27it/s][A
 90%|█████████ | 393/435 [00:08<00:00, 44.72it/s][A
 91%|█████████▏| 398/435 [00:08<00:00, 44.81it/s][A
 93%|█████████▎| 403/435 [00:08<00:00, 44.97it/s][A
 94%|█████████▍| 408/435 [00:09<00:00, 44.98it/s][A
 95%|█████████▍| 413/435 [00:09<00:00, 45.01it/s][A
 96%|█████████▌| 418/435 [00:09<00:00, 45.06it/s][A
 97%|█████████▋| 423/435 [00:09<00:00, 45.08it/s][A
 98%|█████████▊| 428/435 [00:09<00:00, 44.96it/s][A
100%|█████████▉| 433/435 [00:09<00:00, 45.04it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:28<00:00,  3.25it/s]
100%|██████████| 435/435 [00:09<00:00, 45.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:50:42,732 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:50:42,920 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:50:46,194 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:50:46,342 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:50:46,419 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:50:55,144 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:50:55,174 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 1.1382333040237427).
                                                 100%|██████████| 585/585 [04:52<00:00,  3.25it/s]100%|██████████| 585/585 [04:52<00:00,  2.00it/s]
[INFO|trainer.py:1894] 2023-08-29 01:51:06,332 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:51:06,706 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:51:10,650 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:51:10,983 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:51:11,123 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:51:11,964 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,964 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,965 >>   train_loss               =     0.3418
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,965 >>   train_runtime            = 0:04:52.31
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,965 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,965 >>   train_samples_per_second =    128.286
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:11,965 >>   train_steps_per_second   =      2.001
{'eval_loss': 1.1893824338912964, 'eval_runtime': 9.6567, 'eval_samples_per_second': 360.269, 'eval_steps_per_second': 45.047, 'epoch': 5.0}
{'train_runtime': 292.3151, 'train_samples_per_second': 128.286, 'train_steps_per_second': 2.001, 'train_loss': 0.34182258671165533, 'epoch': 5.0}
08/29/2023 01:51:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:51:12,467 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:51:12,467 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 01:51:12,467 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|▏         | 6/435 [00:00<00:07, 56.25it/s]  3%|▎         | 12/435 [00:00<00:08, 49.95it/s]  4%|▍         | 18/435 [00:00<00:08, 48.00it/s]  5%|▌         | 23/435 [00:00<00:08, 47.30it/s]  6%|▋         | 28/435 [00:00<00:08, 46.97it/s]  8%|▊         | 33/435 [00:00<00:08, 46.64it/s]  9%|▊         | 38/435 [00:00<00:08, 46.35it/s] 10%|▉         | 43/435 [00:00<00:08, 45.97it/s] 11%|█         | 48/435 [00:01<00:08, 45.48it/s] 12%|█▏        | 53/435 [00:01<00:08, 45.18it/s] 13%|█▎        | 58/435 [00:01<00:08, 45.21it/s] 14%|█▍        | 63/435 [00:01<00:09, 38.71it/s] 16%|█▌        | 68/435 [00:01<00:09, 40.64it/s] 17%|█▋        | 73/435 [00:01<00:08, 42.15it/s] 18%|█▊        | 78/435 [00:01<00:08, 43.31it/s] 19%|█▉        | 83/435 [00:01<00:08, 43.94it/s] 20%|██        | 88/435 [00:01<00:07, 44.59it/s] 21%|██▏       | 93/435 [00:02<00:07, 45.03it/s] 23%|██▎       | 98/435 [00:02<00:07, 45.33it/s] 24%|██▎       | 103/435 [00:02<00:07, 44.88it/s] 25%|██▍       | 108/435 [00:02<00:07, 44.74it/s] 26%|██▌       | 113/435 [00:02<00:07, 44.77it/s] 27%|██▋       | 118/435 [00:02<00:07, 44.99it/s] 28%|██▊       | 123/435 [00:02<00:06, 45.12it/s] 29%|██▉       | 128/435 [00:02<00:06, 45.28it/s] 31%|███       | 133/435 [00:02<00:06, 45.42it/s] 32%|███▏      | 138/435 [00:03<00:06, 45.54it/s] 33%|███▎      | 143/435 [00:03<00:06, 45.65it/s] 34%|███▍      | 148/435 [00:03<00:06, 45.52it/s] 35%|███▌      | 153/435 [00:03<00:06, 45.45it/s] 36%|███▋      | 158/435 [00:03<00:06, 45.40it/s] 37%|███▋      | 163/435 [00:03<00:05, 45.46it/s] 39%|███▊      | 168/435 [00:03<00:05, 45.40it/s] 40%|███▉      | 173/435 [00:03<00:05, 45.43it/s] 41%|████      | 178/435 [00:03<00:05, 45.43it/s] 42%|████▏     | 183/435 [00:04<00:05, 45.62it/s] 43%|████▎     | 188/435 [00:04<00:05, 45.55it/s] 44%|████▍     | 193/435 [00:04<00:05, 45.44it/s] 46%|████▌     | 198/435 [00:04<00:05, 40.64it/s] 47%|████▋     | 203/435 [00:04<00:05, 42.13it/s] 48%|████▊     | 208/435 [00:04<00:05, 43.26it/s] 49%|████▉     | 213/435 [00:04<00:05, 44.01it/s] 50%|█████     | 218/435 [00:04<00:04, 44.52it/s] 51%|█████▏    | 223/435 [00:04<00:04, 44.82it/s] 52%|█████▏    | 228/435 [00:05<00:04, 45.09it/s] 54%|█████▎    | 233/435 [00:05<00:04, 45.17it/s] 55%|█████▍    | 238/435 [00:05<00:04, 44.81it/s] 56%|█████▌    | 243/435 [00:05<00:04, 44.78it/s] 57%|█████▋    | 248/435 [00:05<00:04, 44.97it/s] 58%|█████▊    | 253/435 [00:05<00:04, 45.22it/s] 59%|█████▉    | 258/435 [00:05<00:03, 45.35it/s] 60%|██████    | 263/435 [00:05<00:03, 45.58it/s] 62%|██████▏   | 268/435 [00:05<00:03, 45.55it/s] 63%|██████▎   | 273/435 [00:06<00:03, 45.56it/s] 64%|██████▍   | 278/435 [00:06<00:03, 45.39it/s] 65%|██████▌   | 283/435 [00:06<00:03, 45.12it/s] 66%|██████▌   | 288/435 [00:06<00:03, 44.99it/s] 67%|██████▋   | 293/435 [00:06<00:03, 45.07it/s] 69%|██████▊   | 298/435 [00:06<00:03, 45.02it/s] 70%|██████▉   | 303/435 [00:06<00:02, 45.34it/s] 71%|███████   | 308/435 [00:06<00:02, 45.38it/s] 72%|███████▏  | 313/435 [00:06<00:02, 45.51it/s] 73%|███████▎  | 318/435 [00:07<00:02, 45.52it/s] 74%|███████▍  | 323/435 [00:07<00:02, 45.60it/s] 75%|███████▌  | 328/435 [00:07<00:02, 45.36it/s] 77%|███████▋  | 333/435 [00:07<00:02, 43.10it/s] 78%|███████▊  | 338/435 [00:07<00:02, 43.77it/s] 79%|███████▉  | 343/435 [00:07<00:02, 44.27it/s] 80%|████████  | 348/435 [00:07<00:01, 44.65it/s] 81%|████████  | 353/435 [00:07<00:01, 44.93it/s] 82%|████████▏ | 358/435 [00:07<00:01, 45.06it/s] 83%|████████▎ | 363/435 [00:08<00:01, 45.15it/s] 85%|████████▍ | 368/435 [00:08<00:01, 45.25it/s] 86%|████████▌ | 373/435 [00:08<00:01, 44.97it/s] 87%|████████▋ | 378/435 [00:08<00:01, 44.89it/s] 88%|████████▊ | 383/435 [00:08<00:01, 45.08it/s] 89%|████████▉ | 388/435 [00:08<00:01, 45.27it/s] 90%|█████████ | 393/435 [00:08<00:00, 45.44it/s] 91%|█████████▏| 398/435 [00:08<00:00, 45.49it/s] 93%|█████████▎| 403/435 [00:08<00:00, 45.47it/s] 94%|█████████▍| 408/435 [00:09<00:00, 45.44it/s] 95%|█████████▍| 413/435 [00:09<00:00, 45.28it/s] 96%|█████████▌| 418/435 [00:09<00:00, 45.15it/s] 97%|█████████▋| 423/435 [00:09<00:00, 45.12it/s] 98%|█████████▊| 428/435 [00:09<00:00, 45.23it/s]100%|█████████▉| 433/435 [00:09<00:00, 45.24it/s]100%|██████████| 435/435 [00:09<00:00, 44.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:51:22,169 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   eval_loss               =     1.1382
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   eval_runtime            = 0:00:09.70
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   eval_samples_per_second =    358.604
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   eval_steps_per_second   =     44.838
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:51:22,169 >>   perplexity              =     3.1212
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:32,165 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:32,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:32,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:32,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:32,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:51:33,122 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:51:33,124 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:51:33,431 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:51:34,568 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:51:34,568 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:36,471 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:36,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:36,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:36,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:36,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:51:36,923 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:51:36,924 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:51:37,215 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:51:37,431 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:51:37,431 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:18,  1.43it/s]Extractor Predicting: 28it [00:19,  1.48it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:22,  1.38it/s]Extractor Predicting: 34it [00:23,  1.43it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.47it/s]Extractor Predicting: 38it [00:25,  1.49it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.50it/s]Extractor Predicting: 41it [00:28,  1.47it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:29,  1.51it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:30,  1.51it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:32,  1.47it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:34,  1.50it/s]Extractor Predicting: 52it [00:35,  1.51it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.48it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.56it/s]Extractor Predicting: 61it [00:41,  1.56it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.52it/s]Extractor Predicting: 64it [00:43,  1.53it/s]Extractor Predicting: 65it [00:43,  1.54it/s]Extractor Predicting: 66it [00:44,  1.52it/s]Extractor Predicting: 67it [00:45,  1.54it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:46,  1.59it/s]Extractor Predicting: 70it [00:47,  1.58it/s]Extractor Predicting: 71it [00:47,  1.58it/s]Extractor Predicting: 72it [00:48,  1.55it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:50,  1.53it/s]Extractor Predicting: 76it [00:50,  1.51it/s]Extractor Predicting: 77it [00:51,  1.55it/s]Extractor Predicting: 78it [00:52,  1.55it/s]Extractor Predicting: 79it [00:52,  1.58it/s]Extractor Predicting: 80it [00:53,  1.57it/s]Extractor Predicting: 81it [00:54,  1.56it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:55,  1.56it/s]Extractor Predicting: 84it [00:56,  1.57it/s]Extractor Predicting: 85it [00:56,  1.58it/s]Extractor Predicting: 86it [00:57,  1.59it/s]Extractor Predicting: 87it [00:57,  1.62it/s]Extractor Predicting: 88it [00:58,  1.59it/s]Extractor Predicting: 89it [00:59,  1.59it/s]Extractor Predicting: 90it [00:59,  1.61it/s]Extractor Predicting: 91it [01:00,  1.63it/s]Extractor Predicting: 92it [01:00,  1.67it/s]Extractor Predicting: 93it [01:01,  1.64it/s]Extractor Predicting: 94it [01:02,  1.62it/s]Extractor Predicting: 95it [01:02,  1.63it/s]Extractor Predicting: 96it [01:03,  1.63it/s]Extractor Predicting: 97it [01:04,  1.62it/s]Extractor Predicting: 98it [01:04,  1.59it/s]Extractor Predicting: 99it [01:05,  1.57it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:07,  1.61it/s]Extractor Predicting: 103it [01:07,  1.62it/s]Extractor Predicting: 104it [01:08,  1.61it/s]Extractor Predicting: 105it [01:09,  1.61it/s]Extractor Predicting: 106it [01:09,  1.61it/s]Extractor Predicting: 107it [01:10,  1.60it/s]Extractor Predicting: 108it [01:10,  1.59it/s]Extractor Predicting: 109it [01:11,  1.60it/s]Extractor Predicting: 110it [01:12,  1.61it/s]Extractor Predicting: 111it [01:12,  1.61it/s]Extractor Predicting: 112it [01:13,  1.63it/s]Extractor Predicting: 113it [01:14,  1.50it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.58it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:18,  1.53it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:20,  1.53it/s]Extractor Predicting: 124it [01:21,  1.54it/s]Extractor Predicting: 125it [01:21,  1.56it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.55it/s]Extractor Predicting: 128it [01:23,  1.56it/s]Extractor Predicting: 129it [01:24,  1.54it/s]Extractor Predicting: 130it [01:25,  1.52it/s]Extractor Predicting: 131it [01:25,  1.54it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.51it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:28,  1.52it/s]Extractor Predicting: 136it [01:29,  1.46it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.47it/s]Extractor Predicting: 139it [01:31,  1.49it/s]Extractor Predicting: 140it [01:31,  1.48it/s]Extractor Predicting: 141it [01:32,  1.50it/s]Extractor Predicting: 142it [01:33,  1.49it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:34,  1.88it/s]Extractor Predicting: 144it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:22,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:23,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:23,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:23,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:23,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:53:23,952 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:53:23,953 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:53:24,278 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:53:25,391 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:53:25,391 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:26,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:26,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:26,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:26,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:53:26,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:53:27,313 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:53:27,314 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:53:27,618 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:53:27,822 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:53:27,822 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19487577639751552,
  "recall": 0.07214716872664559,
  "score": 0.10530732116635198,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.65it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:28,  1.58it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:35,  1.58it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.56it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:41,  1.56it/s]Extractor Predicting: 68it [00:42,  1.56it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:45,  1.56it/s]Extractor Predicting: 74it [00:46,  1.54it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:47,  1.55it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.48it/s]Extractor Predicting: 81it [00:51,  1.52it/s]Extractor Predicting: 82it [00:51,  1.54it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:52,  1.52it/s]Extractor Predicting: 85it [00:53,  1.50it/s]Extractor Predicting: 86it [00:54,  1.48it/s]Extractor Predicting: 87it [00:54,  1.50it/s]Extractor Predicting: 88it [00:55,  1.51it/s]Extractor Predicting: 89it [00:56,  1.48it/s]Extractor Predicting: 90it [00:57,  1.49it/s]Extractor Predicting: 91it [00:57,  1.47it/s]Extractor Predicting: 92it [00:58,  1.51it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.52it/s]Extractor Predicting: 95it [01:00,  1.54it/s]Extractor Predicting: 96it [01:00,  1.50it/s]Extractor Predicting: 97it [01:01,  1.49it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:02,  1.51it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.53it/s]Extractor Predicting: 102it [01:04,  1.51it/s]Extractor Predicting: 103it [01:05,  1.49it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:06,  1.52it/s]Extractor Predicting: 106it [01:07,  1.51it/s]Extractor Predicting: 107it [01:08,  1.53it/s]Extractor Predicting: 108it [01:08,  1.52it/s]Extractor Predicting: 109it [01:09,  1.51it/s]Extractor Predicting: 110it [01:10,  1.51it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:11,  1.51it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:12,  1.54it/s]Extractor Predicting: 115it [01:13,  1.50it/s]Extractor Predicting: 116it [01:14,  1.52it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:15,  1.51it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:16,  1.55it/s]Extractor Predicting: 121it [01:17,  1.54it/s]Extractor Predicting: 122it [01:18,  1.55it/s]Extractor Predicting: 123it [01:18,  1.54it/s]Extractor Predicting: 124it [01:19,  1.57it/s]Extractor Predicting: 125it [01:19,  1.59it/s]Extractor Predicting: 126it [01:20,  1.57it/s]Extractor Predicting: 127it [01:21,  1.57it/s]Extractor Predicting: 128it [01:21,  1.61it/s]Extractor Predicting: 129it [01:22,  1.62it/s]Extractor Predicting: 130it [01:23,  1.58it/s]Extractor Predicting: 131it [01:23,  1.61it/s]Extractor Predicting: 132it [01:24,  1.62it/s]Extractor Predicting: 133it [01:24,  1.65it/s]Extractor Predicting: 134it [01:25,  1.60it/s]Extractor Predicting: 135it [01:26,  1.63it/s]Extractor Predicting: 136it [01:26,  1.63it/s]Extractor Predicting: 137it [01:27,  1.65it/s]Extractor Predicting: 138it [01:27,  1.63it/s]Extractor Predicting: 139it [01:28,  1.63it/s]Extractor Predicting: 140it [01:29,  1.64it/s]Extractor Predicting: 141it [01:29,  1.58it/s]Extractor Predicting: 142it [01:30,  1.58it/s]Extractor Predicting: 143it [01:31,  1.60it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 145it [01:32,  1.65it/s]Extractor Predicting: 146it [01:32,  1.67it/s]Extractor Predicting: 147it [01:33,  1.68it/s]Extractor Predicting: 148it [01:34,  1.71it/s]Extractor Predicting: 149it [01:34,  1.69it/s]Extractor Predicting: 150it [01:35,  1.70it/s]Extractor Predicting: 151it [01:35,  1.71it/s]Extractor Predicting: 152it [01:36,  1.73it/s]Extractor Predicting: 153it [01:36,  1.70it/s]Extractor Predicting: 154it [01:37,  1.70it/s]Extractor Predicting: 155it [01:38,  1.75it/s]Extractor Predicting: 156it [01:38,  1.69it/s]Extractor Predicting: 157it [01:39,  1.77it/s]Extractor Predicting: 158it [01:39,  1.80it/s]Extractor Predicting: 159it [01:40,  1.76it/s]Extractor Predicting: 160it [01:40,  1.72it/s]Extractor Predicting: 161it [01:41,  1.71it/s]Extractor Predicting: 162it [01:42,  1.71it/s]Extractor Predicting: 163it [01:42,  1.75it/s]Extractor Predicting: 164it [01:43,  1.75it/s]Extractor Predicting: 165it [01:43,  1.76it/s]Extractor Predicting: 166it [01:44,  1.73it/s]Extractor Predicting: 167it [01:44,  1.76it/s]Extractor Predicting: 168it [01:45,  1.70it/s]Extractor Predicting: 169it [01:46,  1.77it/s]Extractor Predicting: 170it [01:46,  1.76it/s]Extractor Predicting: 171it [01:47,  1.76it/s]Extractor Predicting: 172it [01:47,  1.69it/s]Extractor Predicting: 173it [01:48,  1.67it/s]Extractor Predicting: 174it [01:49,  1.62it/s]Extractor Predicting: 175it [01:49,  1.57it/s]Extractor Predicting: 176it [01:50,  1.58it/s]Extractor Predicting: 177it [01:51,  1.58it/s]Extractor Predicting: 178it [01:51,  1.56it/s]Extractor Predicting: 179it [01:52,  1.40it/s]Extractor Predicting: 180it [01:53,  1.44it/s]Extractor Predicting: 181it [01:53,  1.47it/s]Extractor Predicting: 182it [01:54,  1.49it/s]Extractor Predicting: 183it [01:55,  1.50it/s]Extractor Predicting: 184it [01:55,  1.49it/s]Extractor Predicting: 185it [01:56,  1.48it/s]Extractor Predicting: 186it [01:57,  1.49it/s]Extractor Predicting: 187it [01:57,  1.50it/s]Extractor Predicting: 188it [01:58,  1.50it/s]Extractor Predicting: 189it [01:59,  1.50it/s]Extractor Predicting: 190it [01:59,  1.52it/s]Extractor Predicting: 191it [02:00,  1.49it/s]Extractor Predicting: 192it [02:01,  1.47it/s]Extractor Predicting: 193it [02:01,  1.48it/s]Extractor Predicting: 194it [02:02,  1.48it/s]Extractor Predicting: 195it [02:03,  1.50it/s]Extractor Predicting: 196it [02:03,  1.51it/s]Extractor Predicting: 197it [02:04,  1.50it/s]Extractor Predicting: 198it [02:05,  1.53it/s]Extractor Predicting: 199it [02:05,  1.52it/s]Extractor Predicting: 200it [02:06,  1.50it/s]Extractor Predicting: 201it [02:07,  1.48it/s]Extractor Predicting: 202it [02:07,  1.57it/s]Extractor Predicting: 203it [02:08,  1.54it/s]Extractor Predicting: 204it [02:09,  1.55it/s]Extractor Predicting: 205it [02:09,  1.53it/s]Extractor Predicting: 206it [02:10,  1.53it/s]Extractor Predicting: 207it [02:11,  1.52it/s]Extractor Predicting: 208it [02:11,  1.52it/s]Extractor Predicting: 209it [02:12,  1.46it/s]Extractor Predicting: 210it [02:13,  1.49it/s]Extractor Predicting: 211it [02:13,  1.49it/s]Extractor Predicting: 212it [02:14,  1.49it/s]Extractor Predicting: 213it [02:15,  1.50it/s]Extractor Predicting: 214it [02:15,  1.47it/s]Extractor Predicting: 215it [02:16,  1.51it/s]Extractor Predicting: 216it [02:17,  1.49it/s]Extractor Predicting: 217it [02:17,  1.50it/s]Extractor Predicting: 218it [02:18,  1.51it/s]Extractor Predicting: 219it [02:19,  1.49it/s]Extractor Predicting: 220it [02:19,  1.51it/s]Extractor Predicting: 221it [02:20,  1.48it/s]Extractor Predicting: 222it [02:21,  1.47it/s]Extractor Predicting: 223it [02:21,  1.51it/s]Extractor Predicting: 224it [02:22,  1.50it/s]Extractor Predicting: 225it [02:23,  1.51it/s]Extractor Predicting: 226it [02:23,  1.51it/s]Extractor Predicting: 227it [02:24,  1.56it/s]Extractor Predicting: 228it [02:25,  1.55it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:26,  1.58it/s]Extractor Predicting: 231it [02:26,  1.60it/s]Extractor Predicting: 232it [02:27,  1.63it/s]Extractor Predicting: 233it [02:28,  1.60it/s]Extractor Predicting: 234it [02:28,  1.59it/s]Extractor Predicting: 235it [02:29,  1.58it/s]Extractor Predicting: 236it [02:30,  1.56it/s]Extractor Predicting: 237it [02:30,  1.54it/s]Extractor Predicting: 238it [02:31,  1.56it/s]Extractor Predicting: 239it [02:32,  1.54it/s]Extractor Predicting: 240it [02:32,  1.55it/s]Extractor Predicting: 241it [02:33,  1.49it/s]Extractor Predicting: 242it [02:34,  1.53it/s]Extractor Predicting: 243it [02:34,  1.52it/s]Extractor Predicting: 244it [02:35,  1.54it/s]Extractor Predicting: 245it [02:36,  1.57it/s]Extractor Predicting: 246it [02:36,  1.55it/s]Extractor Predicting: 247it [02:37,  1.56it/s]Extractor Predicting: 248it [02:37,  1.55it/s]Extractor Predicting: 249it [02:38,  1.54it/s]Extractor Predicting: 250it [02:39,  1.56it/s]Extractor Predicting: 251it [02:39,  1.57it/s]Extractor Predicting: 252it [02:40,  1.57it/s]Extractor Predicting: 253it [02:41,  1.55it/s]Extractor Predicting: 254it [02:41,  1.57it/s]Extractor Predicting: 255it [02:42,  1.52it/s]Extractor Predicting: 256it [02:43,  1.45it/s]Extractor Predicting: 257it [02:43,  1.47it/s]Extractor Predicting: 258it [02:44,  1.47it/s]Extractor Predicting: 259it [02:45,  1.50it/s]Extractor Predicting: 260it [02:45,  1.48it/s]Extractor Predicting: 261it [02:46,  1.51it/s]Extractor Predicting: 262it [02:47,  1.49it/s]Extractor Predicting: 263it [02:47,  1.48it/s]Extractor Predicting: 264it [02:48,  1.49it/s]Extractor Predicting: 265it [02:49,  1.44it/s]Extractor Predicting: 266it [02:50,  1.44it/s]Extractor Predicting: 267it [02:50,  1.45it/s]Extractor Predicting: 268it [02:51,  1.43it/s]Extractor Predicting: 269it [02:52,  1.47it/s]Extractor Predicting: 270it [02:52,  1.49it/s]Extractor Predicting: 271it [02:53,  1.45it/s]Extractor Predicting: 272it [02:54,  1.47it/s]Extractor Predicting: 273it [02:54,  1.48it/s]Extractor Predicting: 274it [02:55,  1.51it/s]Extractor Predicting: 275it [02:56,  1.53it/s]Extractor Predicting: 276it [02:56,  1.55it/s]Extractor Predicting: 277it [02:57,  1.55it/s]Extractor Predicting: 278it [02:58,  1.40it/s]Extractor Predicting: 279it [02:58,  1.43it/s]Extractor Predicting: 280it [02:59,  1.42it/s]Extractor Predicting: 281it [03:00,  1.42it/s]Extractor Predicting: 282it [03:00,  1.53it/s]Extractor Predicting: 282it [03:00,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,211 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:56:41,164 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:56:41,165 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:41,787 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:42,851 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:42,851 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,944 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,969 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,969 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:56:46,633 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:56:46,634 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:47,236 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:47,405 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:47,405 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22021749876421157,
  "recall": 0.13180473372781065,
  "score": 0.16490838423098278,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 5it [00:03,  1.49it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3181818181818182,
  "recall": 0.0875,
  "score": 0.13725490196078433,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
