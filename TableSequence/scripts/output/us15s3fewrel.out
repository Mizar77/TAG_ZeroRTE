/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:35, 35.33s/it]Extractor Estimating: 2it [00:37, 15.66s/it]Extractor Estimating: 3it [00:38,  9.05s/it]Extractor Estimating: 4it [00:39,  5.74s/it]Extractor Estimating: 5it [00:40,  4.09s/it]Extractor Estimating: 6it [00:40,  2.91s/it]Extractor Estimating: 7it [00:42,  2.35s/it]Extractor Estimating: 8it [00:42,  1.81s/it]Extractor Estimating: 9it [00:43,  1.44s/it]Extractor Estimating: 10it [00:44,  1.21s/it]Extractor Estimating: 11it [00:44,  1.01s/it]Extractor Estimating: 12it [00:45,  1.10it/s]Extractor Estimating: 13it [00:45,  1.19it/s]Extractor Estimating: 14it [00:46,  1.26it/s]Extractor Estimating: 15it [00:47,  1.31it/s]Extractor Estimating: 16it [00:47,  1.37it/s]Extractor Estimating: 17it [00:48,  1.40it/s]Extractor Estimating: 18it [00:49,  1.43it/s]Extractor Estimating: 19it [00:49,  1.51it/s]Extractor Estimating: 20it [00:50,  1.51it/s]Extractor Estimating: 21it [00:52,  1.01it/s]Extractor Estimating: 22it [00:52,  1.13it/s]Extractor Estimating: 23it [00:53,  1.21it/s]Extractor Estimating: 24it [00:54,  1.22it/s]Extractor Estimating: 25it [00:55,  1.33it/s]Extractor Estimating: 26it [00:55,  1.39it/s]Extractor Estimating: 27it [00:56,  1.48it/s]Extractor Estimating: 28it [00:56,  1.47it/s]Extractor Estimating: 29it [00:57,  1.54it/s]Extractor Estimating: 30it [00:58,  1.61it/s]Extractor Estimating: 31it [00:58,  1.59it/s]Extractor Estimating: 32it [00:59,  1.62it/s]Extractor Estimating: 33it [01:00,  1.53it/s]Extractor Estimating: 34it [01:00,  1.57it/s]Extractor Estimating: 35it [01:01,  1.59it/s]Extractor Estimating: 36it [01:01,  1.56it/s]Extractor Estimating: 37it [01:02,  1.56it/s]Extractor Estimating: 38it [01:03,  1.51it/s]Extractor Estimating: 39it [01:03,  1.56it/s]Extractor Estimating: 40it [01:04,  1.56it/s]Extractor Estimating: 41it [01:05,  1.54it/s]Extractor Estimating: 42it [01:05,  1.60it/s]Extractor Estimating: 43it [01:06,  1.56it/s]Extractor Estimating: 44it [01:07,  1.55it/s]Extractor Estimating: 45it [01:07,  1.59it/s]Extractor Estimating: 46it [01:08,  1.21it/s]Extractor Estimating: 47it [01:09,  1.28it/s]Extractor Estimating: 48it [01:10,  1.33it/s]Extractor Estimating: 49it [01:10,  1.38it/s]Extractor Estimating: 50it [01:11,  1.38it/s]Extractor Estimating: 51it [01:12,  1.43it/s]Extractor Estimating: 52it [01:12,  1.51it/s]Extractor Estimating: 53it [01:13,  1.53it/s]Extractor Estimating: 54it [01:14,  1.59it/s]Extractor Estimating: 55it [01:14,  1.61it/s]Extractor Estimating: 56it [01:15,  1.63it/s]Extractor Estimating: 57it [01:16,  1.56it/s]Extractor Estimating: 58it [01:16,  1.63it/s]Extractor Estimating: 59it [01:17,  1.62it/s]Extractor Estimating: 60it [01:17,  1.57it/s]Extractor Estimating: 61it [01:18,  1.53it/s]Extractor Estimating: 62it [01:19,  1.59it/s]Extractor Estimating: 63it [01:19,  1.59it/s]Extractor Estimating: 64it [01:20,  1.57it/s]Extractor Estimating: 65it [01:21,  1.51it/s]Extractor Estimating: 66it [01:21,  1.58it/s]Extractor Estimating: 67it [01:22,  1.61it/s]Extractor Estimating: 68it [01:22,  1.63it/s]Extractor Estimating: 69it [01:23,  1.62it/s]Extractor Estimating: 70it [01:24,  1.53it/s]Extractor Estimating: 71it [01:24,  1.53it/s]Extractor Estimating: 72it [01:25,  1.55it/s]Extractor Estimating: 73it [01:26,  1.52it/s]Extractor Estimating: 74it [01:26,  1.57it/s]Extractor Estimating: 75it [01:27,  1.58it/s]Extractor Estimating: 76it [01:28,  1.61it/s]Extractor Estimating: 77it [01:28,  1.63it/s]Extractor Estimating: 78it [01:29,  1.62it/s]Extractor Estimating: 79it [01:29,  1.62it/s]Extractor Estimating: 80it [01:30,  1.56it/s]Extractor Estimating: 81it [01:31,  1.57it/s]Extractor Estimating: 82it [01:31,  1.58it/s]Extractor Estimating: 83it [01:32,  1.56it/s]Extractor Estimating: 84it [01:33,  1.54it/s]Extractor Estimating: 85it [01:33,  1.50it/s]Extractor Estimating: 86it [01:35,  1.03s/it]Extractor Estimating: 87it [01:36,  1.10it/s]Extractor Estimating: 88it [01:37,  1.20it/s]Extractor Estimating: 89it [01:37,  1.31it/s]Extractor Estimating: 90it [01:38,  1.32it/s]Extractor Estimating: 91it [01:39,  1.35it/s]Extractor Estimating: 92it [01:39,  1.44it/s]Extractor Estimating: 93it [01:40,  1.48it/s]Extractor Estimating: 94it [01:40,  1.54it/s]Extractor Estimating: 95it [01:41,  1.58it/s]Extractor Estimating: 96it [01:42,  1.51it/s]Extractor Estimating: 97it [01:42,  1.51it/s]Extractor Estimating: 98it [01:43,  1.58it/s]Extractor Estimating: 99it [01:44,  1.55it/s]Extractor Estimating: 100it [01:44,  1.56it/s]Extractor Estimating: 101it [01:45,  1.44it/s]Extractor Estimating: 102it [01:46,  1.50it/s]Extractor Estimating: 103it [01:46,  1.51it/s]Extractor Estimating: 104it [01:47,  1.39it/s]Extractor Estimating: 105it [01:48,  1.47it/s]Extractor Estimating: 106it [01:48,  1.49it/s]Extractor Estimating: 107it [01:49,  1.52it/s]Extractor Estimating: 108it [01:50,  1.59it/s]Extractor Estimating: 109it [01:50,  1.61it/s]Extractor Estimating: 110it [01:51,  1.58it/s]Extractor Estimating: 111it [01:51,  1.59it/s]Extractor Estimating: 112it [01:52,  1.58it/s]Extractor Estimating: 113it [01:53,  1.61it/s]Extractor Estimating: 114it [01:53,  1.61it/s]Extractor Estimating: 115it [01:54,  1.61it/s]Extractor Estimating: 116it [01:55,  1.61it/s]Extractor Estimating: 117it [01:55,  1.64it/s]Extractor Estimating: 118it [01:56,  1.66it/s]Extractor Estimating: 119it [01:56,  1.66it/s]Extractor Estimating: 120it [01:57,  1.61it/s]Extractor Estimating: 121it [01:58,  1.58it/s]Extractor Estimating: 122it [01:58,  1.60it/s]Extractor Estimating: 123it [01:59,  1.62it/s]Extractor Estimating: 124it [02:00,  1.61it/s]Extractor Estimating: 125it [02:00,  1.64it/s]Extractor Estimating: 126it [02:01,  1.55it/s]Extractor Estimating: 127it [02:01,  1.56it/s]Extractor Estimating: 128it [02:02,  1.53it/s]Extractor Estimating: 129it [02:03,  1.55it/s]Extractor Estimating: 130it [02:03,  1.51it/s]Extractor Estimating: 131it [02:04,  1.51it/s]Extractor Estimating: 132it [02:05,  1.48it/s]Extractor Estimating: 133it [02:05,  1.50it/s]Extractor Estimating: 134it [02:06,  1.48it/s]Extractor Estimating: 135it [02:07,  1.48it/s]Extractor Estimating: 136it [02:08,  1.45it/s]Extractor Estimating: 137it [02:08,  1.47it/s]Extractor Estimating: 138it [02:09,  1.48it/s]Extractor Estimating: 139it [02:10,  1.46it/s]Extractor Estimating: 140it [02:10,  1.49it/s]Extractor Estimating: 141it [02:11,  1.51it/s]Extractor Estimating: 142it [02:12,  1.54it/s]Extractor Estimating: 143it [02:12,  1.59it/s]Extractor Estimating: 144it [02:13,  1.52it/s]Extractor Estimating: 145it [02:14,  1.47it/s]Extractor Estimating: 146it [02:14,  1.43it/s]Extractor Estimating: 147it [02:15,  1.46it/s]Extractor Estimating: 148it [02:16,  1.43it/s]Extractor Estimating: 149it [02:16,  1.47it/s]Extractor Estimating: 150it [02:17,  1.49it/s]Extractor Estimating: 151it [02:18,  1.54it/s]Extractor Estimating: 152it [02:18,  1.59it/s]Extractor Estimating: 153it [02:19,  1.65it/s]Extractor Estimating: 154it [02:19,  1.62it/s]Extractor Estimating: 155it [02:20,  1.63it/s]Extractor Estimating: 156it [02:21,  1.65it/s]Extractor Estimating: 157it [02:21,  1.67it/s]Extractor Estimating: 158it [02:22,  1.69it/s]Extractor Estimating: 159it [02:22,  1.75it/s]Extractor Estimating: 160it [02:23,  1.67it/s]Extractor Estimating: 161it [02:24,  1.64it/s]Extractor Estimating: 162it [02:24,  1.57it/s]Extractor Estimating: 163it [02:25,  1.54it/s]Extractor Estimating: 164it [02:25,  1.59it/s]Extractor Estimating: 165it [02:26,  1.59it/s]Extractor Estimating: 166it [02:27,  1.55it/s]Extractor Estimating: 167it [02:27,  1.57it/s]Extractor Estimating: 168it [02:28,  1.58it/s]Extractor Estimating: 169it [02:29,  1.60it/s]Extractor Estimating: 170it [02:29,  1.56it/s]Extractor Estimating: 171it [02:30,  1.57it/s]Extractor Estimating: 172it [02:31,  1.58it/s]Extractor Estimating: 173it [02:31,  1.64it/s]Extractor Estimating: 174it [02:32,  1.65it/s]Extractor Estimating: 175it [02:32,  1.63it/s]Extractor Estimating: 176it [02:33,  1.64it/s]Extractor Estimating: 177it [02:34,  1.62it/s]Extractor Estimating: 178it [02:34,  1.60it/s]Extractor Estimating: 179it [02:35,  1.52it/s]Extractor Estimating: 180it [02:36,  1.55it/s]Extractor Estimating: 181it [02:36,  1.60it/s]Extractor Estimating: 182it [02:37,  1.48it/s]Extractor Estimating: 183it [02:38,  1.49it/s]Extractor Estimating: 184it [02:38,  1.54it/s]Extractor Estimating: 185it [02:39,  1.58it/s]Extractor Estimating: 186it [02:39,  1.58it/s]Extractor Estimating: 187it [02:40,  1.62it/s]Extractor Estimating: 188it [02:41,  1.57it/s]Extractor Estimating: 189it [02:41,  1.59it/s]Extractor Estimating: 190it [02:42,  1.60it/s]Extractor Estimating: 191it [02:43,  1.64it/s]Extractor Estimating: 192it [02:43,  1.67it/s]Extractor Estimating: 193it [02:44,  1.73it/s]Extractor Estimating: 194it [02:44,  1.69it/s]Extractor Estimating: 195it [02:45,  1.66it/s]Extractor Estimating: 196it [02:46,  1.56it/s]Extractor Estimating: 197it [02:46,  1.63it/s]Extractor Estimating: 198it [02:47,  1.66it/s]Extractor Estimating: 199it [02:47,  1.68it/s]Extractor Estimating: 200it [02:48,  1.70it/s]Extractor Estimating: 201it [02:48,  1.70it/s]Extractor Estimating: 202it [02:49,  1.65it/s]Extractor Estimating: 203it [02:50,  1.62it/s]Extractor Estimating: 204it [02:50,  1.60it/s]Extractor Estimating: 205it [02:51,  1.60it/s]Extractor Estimating: 206it [02:52,  1.57it/s]Extractor Estimating: 207it [02:52,  1.48it/s]Extractor Estimating: 208it [02:53,  1.46it/s]Extractor Estimating: 209it [02:54,  1.51it/s]Extractor Estimating: 210it [02:54,  1.53it/s]Extractor Estimating: 211it [02:55,  1.45it/s]Extractor Estimating: 212it [02:56,  1.52it/s]Extractor Estimating: 213it [02:56,  1.50it/s]Extractor Estimating: 214it [02:57,  1.51it/s]Extractor Estimating: 215it [02:58,  1.53it/s]Extractor Estimating: 216it [02:58,  1.59it/s]Extractor Estimating: 217it [02:59,  1.63it/s]Extractor Estimating: 218it [03:00,  1.57it/s]Extractor Estimating: 219it [03:00,  1.54it/s]Extractor Estimating: 220it [03:01,  1.49it/s]Extractor Estimating: 221it [03:02,  1.53it/s]Extractor Estimating: 222it [03:02,  1.54it/s]Extractor Estimating: 223it [03:03,  1.58it/s]Extractor Estimating: 224it [03:03,  1.57it/s]Extractor Estimating: 225it [03:04,  1.51it/s]Extractor Estimating: 226it [03:05,  1.51it/s]Extractor Estimating: 227it [03:06,  1.50it/s]Extractor Estimating: 228it [03:06,  1.53it/s]Extractor Estimating: 229it [03:07,  1.53it/s]Extractor Estimating: 230it [03:07,  1.58it/s]Extractor Estimating: 231it [03:08,  1.61it/s]Extractor Estimating: 232it [03:09,  1.59it/s]Extractor Estimating: 233it [03:09,  1.59it/s]Extractor Estimating: 234it [03:10,  1.54it/s]Extractor Estimating: 235it [03:11,  1.57it/s]Extractor Estimating: 236it [03:11,  1.59it/s]Extractor Estimating: 237it [03:12,  1.60it/s]Extractor Estimating: 238it [03:12,  1.62it/s]Extractor Estimating: 239it [03:13,  1.59it/s]Extractor Estimating: 240it [03:14,  1.53it/s]Extractor Estimating: 241it [03:14,  1.52it/s]Extractor Estimating: 242it [03:15,  1.53it/s]Extractor Estimating: 243it [03:16,  1.52it/s]Extractor Estimating: 244it [03:16,  1.47it/s]Extractor Estimating: 245it [03:17,  1.47it/s]Extractor Estimating: 246it [03:18,  1.52it/s]Extractor Estimating: 247it [03:18,  1.48it/s]Extractor Estimating: 248it [03:19,  1.48it/s]Extractor Estimating: 249it [03:20,  1.50it/s]Extractor Estimating: 250it [03:20,  1.50it/s]Extractor Estimating: 251it [03:21,  1.54it/s]Extractor Estimating: 252it [03:22,  1.59it/s]Extractor Estimating: 253it [03:22,  1.53it/s]Extractor Estimating: 254it [03:23,  1.52it/s]Extractor Estimating: 255it [03:24,  1.53it/s]Extractor Estimating: 256it [03:24,  1.54it/s]Extractor Estimating: 257it [03:25,  1.54it/s]Extractor Estimating: 258it [03:26,  1.56it/s]Extractor Estimating: 259it [03:26,  1.60it/s]Extractor Estimating: 260it [03:27,  1.63it/s]Extractor Estimating: 261it [03:27,  1.53it/s]Extractor Estimating: 262it [03:28,  1.58it/s]Extractor Estimating: 263it [03:29,  1.53it/s]Extractor Estimating: 264it [03:29,  1.54it/s]Extractor Estimating: 265it [03:30,  1.55it/s]Extractor Estimating: 266it [03:31,  1.52it/s]Extractor Estimating: 267it [03:31,  1.54it/s]Extractor Estimating: 268it [03:32,  1.40it/s]Extractor Estimating: 269it [03:33,  1.40it/s]Extractor Estimating: 270it [03:34,  1.40it/s]Extractor Estimating: 271it [03:34,  1.41it/s]Extractor Estimating: 272it [03:35,  1.47it/s]Extractor Estimating: 273it [03:36,  1.47it/s]Extractor Estimating: 274it [03:36,  1.45it/s]Extractor Estimating: 275it [03:37,  1.49it/s]Extractor Estimating: 276it [03:38,  1.51it/s]Extractor Estimating: 277it [03:38,  1.48it/s]Extractor Estimating: 278it [03:39,  1.52it/s]Extractor Estimating: 279it [03:40,  1.49it/s]Extractor Estimating: 280it [03:40,  1.45it/s]Extractor Estimating: 281it [03:41,  1.38it/s]Extractor Estimating: 282it [03:42,  1.44it/s]Extractor Estimating: 283it [03:42,  1.49it/s]Extractor Estimating: 284it [03:43,  1.52it/s]Extractor Estimating: 285it [03:44,  1.50it/s]Extractor Estimating: 286it [03:44,  1.53it/s]Extractor Estimating: 287it [03:45,  1.53it/s]Extractor Estimating: 288it [03:46,  1.56it/s]Extractor Estimating: 289it [03:46,  1.59it/s]Extractor Estimating: 290it [03:47,  1.51it/s]Extractor Estimating: 291it [03:48,  1.53it/s]Extractor Estimating: 292it [03:48,  1.50it/s]Extractor Estimating: 293it [03:49,  1.54it/s]Extractor Estimating: 294it [03:50,  1.51it/s]Extractor Estimating: 295it [03:50,  1.54it/s]Extractor Estimating: 296it [03:51,  1.51it/s]Extractor Estimating: 297it [03:52,  1.53it/s]Extractor Estimating: 298it [03:52,  1.53it/s]Extractor Estimating: 299it [03:53,  1.53it/s]Extractor Estimating: 300it [03:54,  1.54it/s]Extractor Estimating: 301it [03:54,  1.51it/s]Extractor Estimating: 302it [03:55,  1.51it/s]Extractor Estimating: 303it [03:56,  1.48it/s]Extractor Estimating: 304it [03:56,  1.49it/s]Extractor Estimating: 305it [03:57,  1.47it/s]Extractor Estimating: 306it [03:58,  1.52it/s]Extractor Estimating: 307it [03:58,  1.47it/s]Extractor Estimating: 308it [03:59,  1.45it/s]Extractor Estimating: 309it [04:00,  1.51it/s]Extractor Estimating: 310it [04:00,  1.49it/s]Extractor Estimating: 311it [04:01,  1.53it/s]Extractor Estimating: 312it [04:02,  1.54it/s]Extractor Estimating: 313it [04:02,  1.52it/s]Extractor Estimating: 314it [04:03,  1.56it/s]Extractor Estimating: 315it [04:03,  1.61it/s]Extractor Estimating: 316it [04:04,  1.57it/s]Extractor Estimating: 317it [04:05,  1.58it/s]Extractor Estimating: 318it [04:05,  1.55it/s]Extractor Estimating: 319it [04:06,  1.59it/s]Extractor Estimating: 320it [04:06,  1.64it/s]Extractor Estimating: 321it [04:07,  1.54it/s]Extractor Estimating: 322it [04:08,  1.54it/s]Extractor Estimating: 323it [04:08,  1.59it/s]Extractor Estimating: 324it [04:09,  1.59it/s]Extractor Estimating: 325it [04:10,  1.59it/s]Extractor Estimating: 326it [04:10,  1.64it/s]Extractor Estimating: 327it [04:11,  1.63it/s]Extractor Estimating: 328it [04:11,  1.66it/s]Extractor Estimating: 329it [04:12,  1.61it/s]Extractor Estimating: 330it [04:13,  1.60it/s]Extractor Estimating: 331it [04:13,  1.58it/s]Extractor Estimating: 332it [04:14,  1.55it/s]Extractor Estimating: 333it [04:15,  1.59it/s]Extractor Estimating: 334it [04:15,  1.57it/s]Extractor Estimating: 335it [04:16,  1.60it/s]Extractor Estimating: 336it [04:16,  1.66it/s]Extractor Estimating: 337it [04:17,  1.71it/s]Extractor Estimating: 338it [04:18,  1.70it/s]Extractor Estimating: 339it [04:19,  1.24it/s]Extractor Estimating: 340it [04:20,  1.33it/s]Extractor Estimating: 341it [04:20,  1.47it/s]Extractor Estimating: 342it [04:21,  1.51it/s]Extractor Estimating: 343it [04:21,  1.58it/s]Extractor Estimating: 344it [04:22,  1.60it/s]Extractor Estimating: 345it [04:22,  1.66it/s]Extractor Estimating: 346it [04:23,  1.57it/s]Extractor Estimating: 347it [04:24,  1.42it/s]Extractor Estimating: 348it [04:25,  1.47it/s]Extractor Estimating: 349it [04:25,  1.44it/s]Extractor Estimating: 350it [04:26,  1.48it/s]Extractor Estimating: 351it [04:27,  1.45it/s]Extractor Estimating: 352it [04:27,  1.52it/s]Extractor Estimating: 353it [04:28,  1.57it/s]Extractor Estimating: 354it [04:29,  1.52it/s]Extractor Estimating: 355it [04:29,  1.55it/s]Extractor Estimating: 356it [04:30,  1.58it/s]Extractor Estimating: 357it [04:31,  1.55it/s]Extractor Estimating: 358it [04:31,  1.55it/s]Extractor Estimating: 359it [04:32,  1.57it/s]Extractor Estimating: 360it [04:32,  1.55it/s]Extractor Estimating: 361it [04:33,  1.43it/s]Extractor Estimating: 362it [04:34,  1.49it/s]Extractor Estimating: 363it [04:35,  1.47it/s]Extractor Estimating: 364it [04:35,  1.48it/s]Extractor Estimating: 365it [04:36,  1.49it/s]Extractor Estimating: 366it [04:37,  1.48it/s]Extractor Estimating: 367it [04:37,  1.48it/s]Extractor Estimating: 368it [04:38,  1.48it/s]Extractor Estimating: 369it [04:39,  1.53it/s]Extractor Estimating: 370it [04:39,  1.50it/s]Extractor Estimating: 371it [04:40,  1.52it/s]Extractor Estimating: 372it [04:40,  1.56it/s]Extractor Estimating: 373it [04:41,  1.55it/s]Extractor Estimating: 374it [04:42,  1.61it/s]Extractor Estimating: 375it [04:42,  1.61it/s]Extractor Estimating: 376it [04:43,  1.54it/s]Extractor Estimating: 377it [04:44,  1.53it/s]Extractor Estimating: 378it [04:44,  1.51it/s]Extractor Estimating: 379it [04:45,  1.51it/s]Extractor Estimating: 380it [04:46,  1.57it/s]Extractor Estimating: 381it [04:46,  1.58it/s]Extractor Estimating: 382it [04:47,  1.53it/s]Extractor Estimating: 383it [04:48,  1.54it/s]Extractor Estimating: 384it [04:48,  1.60it/s]Extractor Estimating: 385it [04:49,  1.55it/s]Extractor Estimating: 386it [04:49,  1.54it/s]Extractor Estimating: 387it [04:50,  1.51it/s]Extractor Estimating: 388it [04:51,  1.55it/s]Extractor Estimating: 389it [04:51,  1.51it/s]Extractor Estimating: 390it [04:52,  1.58it/s]Extractor Estimating: 391it [04:53,  1.54it/s]Extractor Estimating: 392it [04:53,  1.54it/s]Extractor Estimating: 393it [04:54,  1.58it/s]Extractor Estimating: 394it [04:55,  1.57it/s]Extractor Estimating: 395it [04:55,  1.56it/s]Extractor Estimating: 396it [04:56,  1.52it/s]Extractor Estimating: 397it [04:57,  1.56it/s]Extractor Estimating: 398it [04:57,  1.56it/s]Extractor Estimating: 399it [04:58,  1.56it/s]Extractor Estimating: 400it [04:58,  1.57it/s]Extractor Estimating: 401it [04:59,  1.53it/s]Extractor Estimating: 402it [05:00,  1.58it/s]Extractor Estimating: 403it [05:00,  1.55it/s]Extractor Estimating: 404it [05:01,  1.56it/s]Extractor Estimating: 405it [05:02,  1.60it/s]Extractor Estimating: 406it [05:02,  1.58it/s]Extractor Estimating: 407it [05:03,  1.57it/s]Extractor Estimating: 408it [05:04,  1.62it/s]Extractor Estimating: 409it [05:04,  1.62it/s]Extractor Estimating: 410it [05:05,  1.61it/s]Extractor Estimating: 411it [05:05,  1.55it/s]Extractor Estimating: 412it [05:06,  1.47it/s]Extractor Estimating: 413it [05:07,  1.51it/s]Extractor Estimating: 414it [05:07,  1.58it/s]Extractor Estimating: 415it [05:08,  1.61it/s]Extractor Estimating: 416it [05:09,  1.56it/s]Extractor Estimating: 417it [05:09,  1.59it/s]Extractor Estimating: 418it [05:10,  1.57it/s]Extractor Estimating: 419it [05:11,  1.55it/s]Extractor Estimating: 420it [05:11,  1.60it/s]Extractor Estimating: 421it [05:12,  1.48it/s]Extractor Estimating: 422it [05:13,  1.47it/s]Extractor Estimating: 423it [05:13,  1.51it/s]Extractor Estimating: 424it [05:14,  1.54it/s]Extractor Estimating: 425it [05:14,  1.61it/s]Extractor Estimating: 426it [05:15,  1.56it/s]Extractor Estimating: 427it [05:16,  1.53it/s]Extractor Estimating: 428it [05:17,  1.49it/s]Extractor Estimating: 429it [05:17,  1.47it/s]Extractor Estimating: 430it [05:18,  1.45it/s]Extractor Estimating: 431it [05:19,  1.51it/s]Extractor Estimating: 432it [05:20,  1.32it/s]Extractor Estimating: 433it [05:20,  1.39it/s]Extractor Estimating: 434it [05:21,  1.43it/s]Extractor Estimating: 435it [05:22,  1.41it/s]Extractor Estimating: 436it [05:22,  1.39it/s]Extractor Estimating: 437it [05:23,  1.38it/s]Extractor Estimating: 438it [05:24,  1.42it/s]Extractor Estimating: 439it [05:24,  1.43it/s]Extractor Estimating: 440it [05:25,  1.50it/s]Extractor Estimating: 441it [05:26,  1.46it/s]Extractor Estimating: 442it [05:26,  1.49it/s]Extractor Estimating: 443it [05:27,  1.50it/s]Extractor Estimating: 444it [05:28,  1.51it/s]Extractor Estimating: 445it [05:28,  1.54it/s]Extractor Estimating: 446it [05:29,  1.55it/s]Extractor Estimating: 447it [05:30,  1.53it/s]Extractor Estimating: 448it [05:30,  1.51it/s]Extractor Estimating: 449it [05:31,  1.49it/s]Extractor Estimating: 450it [05:32,  1.51it/s]Extractor Estimating: 451it [05:32,  1.52it/s]Extractor Estimating: 452it [05:33,  1.54it/s]Extractor Estimating: 453it [05:34,  1.49it/s]Extractor Estimating: 454it [05:34,  1.52it/s]Extractor Estimating: 455it [05:35,  1.63it/s]Extractor Estimating: 456it [05:35,  1.57it/s]Extractor Estimating: 457it [05:36,  1.61it/s]Extractor Estimating: 458it [05:37,  1.63it/s]Extractor Estimating: 459it [05:37,  1.67it/s]Extractor Estimating: 460it [05:38,  1.64it/s]Extractor Estimating: 461it [05:38,  1.67it/s]Extractor Estimating: 462it [05:39,  1.68it/s]Extractor Estimating: 463it [05:39,  1.73it/s]Extractor Estimating: 464it [05:40,  1.72it/s]Extractor Estimating: 465it [05:41,  1.71it/s]Extractor Estimating: 466it [05:41,  1.71it/s]Extractor Estimating: 467it [05:42,  1.74it/s]Extractor Estimating: 468it [05:42,  1.73it/s]Extractor Estimating: 469it [05:43,  1.74it/s]Extractor Estimating: 470it [05:44,  1.70it/s]Extractor Estimating: 471it [05:44,  1.71it/s]Extractor Estimating: 472it [05:45,  1.65it/s]Extractor Estimating: 473it [05:45,  1.62it/s]Extractor Estimating: 474it [05:46,  1.65it/s]Extractor Estimating: 475it [05:47,  1.59it/s]Extractor Estimating: 476it [05:47,  1.62it/s]Extractor Estimating: 477it [05:48,  1.53it/s]Extractor Estimating: 478it [05:49,  1.56it/s]Extractor Estimating: 479it [05:49,  1.50it/s]Extractor Estimating: 480it [05:50,  1.53it/s]Extractor Estimating: 481it [05:51,  1.54it/s]Extractor Estimating: 482it [05:51,  1.55it/s]Extractor Estimating: 483it [05:52,  1.53it/s]Extractor Estimating: 484it [05:53,  1.54it/s]Extractor Estimating: 485it [05:53,  1.55it/s]Extractor Estimating: 486it [05:54,  1.56it/s]Extractor Estimating: 487it [05:54,  1.59it/s]Extractor Estimating: 488it [05:55,  1.61it/s]Extractor Estimating: 489it [05:56,  1.56it/s]Extractor Estimating: 490it [05:57,  1.47it/s]Extractor Estimating: 491it [05:57,  1.47it/s]Extractor Estimating: 492it [05:58,  1.48it/s]Extractor Estimating: 493it [05:59,  1.50it/s]Extractor Estimating: 494it [05:59,  1.51it/s]Extractor Estimating: 495it [06:00,  1.54it/s]Extractor Estimating: 496it [06:01,  1.48it/s]Extractor Estimating: 497it [06:01,  1.49it/s]Extractor Estimating: 498it [06:02,  1.47it/s]Extractor Estimating: 499it [06:02,  1.56it/s]Extractor Estimating: 500it [06:03,  1.61it/s]Extractor Estimating: 500it [06:03,  1.38it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9995 mean pseudo reward: 0.9461718216316317
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 30854
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30954, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=30954, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.241, loss:3483.6736
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.980, loss:2376.9188
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.984, loss:1878.3306
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.959, loss:1790.6132
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.978, loss:1608.9604
>> valid entity prec:0.4825, rec:0.6403, f1:0.5503
>> valid relation prec:0.5183, rec:0.1256, f1:0.2022
>> valid relation with NER prec:0.5183, rec:0.1256, f1:0.2022
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.269, loss:1550.7513
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.962, loss:1420.9508
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.966, loss:1394.0492
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.959, loss:1249.0481
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.967, loss:1229.0712
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5276, rec:0.5940, f1:0.5588
>> valid relation prec:0.3825, rec:0.1325, f1:0.1968
>> valid relation with NER prec:0.3825, rec:0.1325, f1:0.1968
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.242, loss:1185.1439
g_step 1200, step 366, avg_time 0.966, loss:1154.3152
g_step 1300, step 49, avg_time 0.975, loss:1063.9529
g_step 1400, step 149, avg_time 0.972, loss:1026.3445
g_step 1500, step 249, avg_time 0.967, loss:1034.7533
>> valid entity prec:0.5218, rec:0.5970, f1:0.5569
>> valid relation prec:0.4355, rec:0.1007, f1:0.1635
>> valid relation with NER prec:0.4355, rec:0.1007, f1:0.1635
g_step 1600, step 349, avg_time 2.239, loss:1057.9034
g_step 1700, step 32, avg_time 0.965, loss:999.7781
g_step 1800, step 132, avg_time 0.958, loss:962.0775
g_step 1900, step 232, avg_time 0.969, loss:964.9601
g_step 2000, step 332, avg_time 0.965, loss:955.0966
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5672, rec:0.5870, f1:0.5769
>> valid relation prec:0.4376, rec:0.1328, f1:0.2037
>> valid relation with NER prec:0.4376, rec:0.1328, f1:0.2037
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.256, loss:937.8294
g_step 2200, step 115, avg_time 0.966, loss:884.2171
g_step 2300, step 215, avg_time 0.981, loss:890.3557
g_step 2400, step 315, avg_time 0.966, loss:886.5996
g_step 2500, step 415, avg_time 0.966, loss:920.6151
>> valid entity prec:0.5863, rec:0.5351, f1:0.5595
>> valid relation prec:0.3547, rec:0.1093, f1:0.1671
>> valid relation with NER prec:0.3547, rec:0.1093, f1:0.1671
g_step 2600, step 98, avg_time 2.219, loss:814.5345
g_step 2700, step 198, avg_time 0.966, loss:852.6760
g_step 2800, step 298, avg_time 0.973, loss:853.4657
g_step 2900, step 398, avg_time 0.974, loss:880.3943
g_step 3000, step 81, avg_time 0.963, loss:785.7406
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5585, rec:0.5885, f1:0.5731
>> valid relation prec:0.3892, rec:0.1405, f1:0.2065
>> valid relation with NER prec:0.3892, rec:0.1405, f1:0.2065
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 181, avg_time 2.240, loss:811.8749
g_step 3200, step 281, avg_time 0.966, loss:823.5526
g_step 3300, step 381, avg_time 0.965, loss:818.0481
g_step 3400, step 64, avg_time 0.959, loss:759.5751
g_step 3500, step 164, avg_time 0.961, loss:750.7853
>> valid entity prec:0.4995, rec:0.5240, f1:0.5115
>> valid relation prec:0.2815, rec:0.0935, f1:0.1404
>> valid relation with NER prec:0.2815, rec:0.0935, f1:0.1404
g_step 3600, step 264, avg_time 2.247, loss:775.4689
g_step 3700, step 364, avg_time 0.967, loss:785.7395
g_step 3800, step 47, avg_time 0.962, loss:776.3855
g_step 3900, step 147, avg_time 0.973, loss:730.8586
g_step 4000, step 247, avg_time 0.970, loss:737.8452
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5271, rec:0.5302, f1:0.5286
>> valid relation prec:0.3124, rec:0.1339, f1:0.1875
>> valid relation with NER prec:0.3124, rec:0.1339, f1:0.1875
g_step 4100, step 347, avg_time 2.236, loss:730.0649
g_step 4200, step 30, avg_time 0.967, loss:749.9180
g_step 4300, step 130, avg_time 0.971, loss:679.3154
g_step 4400, step 230, avg_time 0.965, loss:719.8031
g_step 4500, step 330, avg_time 0.968, loss:720.5492
>> valid entity prec:0.5269, rec:0.5449, f1:0.5357
>> valid relation prec:0.3221, rec:0.1230, f1:0.1780
>> valid relation with NER prec:0.3221, rec:0.1230, f1:0.1780
g_step 4600, step 13, avg_time 2.235, loss:698.9467
g_step 4700, step 113, avg_time 0.970, loss:693.1516
g_step 4800, step 213, avg_time 0.983, loss:655.0790
g_step 4900, step 313, avg_time 0.962, loss:690.6375
g_step 5000, step 413, avg_time 0.958, loss:699.2741
learning rate was adjusted to 0.0008
>> valid entity prec:0.5667, rec:0.5182, f1:0.5414
>> valid relation prec:0.3398, rec:0.1004, f1:0.1550
>> valid relation with NER prec:0.3398, rec:0.1004, f1:0.1550
g_step 5100, step 96, avg_time 2.227, loss:633.6933
g_step 5200, step 196, avg_time 0.962, loss:637.1127
g_step 5300, step 296, avg_time 0.962, loss:655.4714
g_step 5400, step 396, avg_time 0.962, loss:660.0377
g_step 5500, step 79, avg_time 0.959, loss:635.1237
>> valid entity prec:0.5814, rec:0.4593, f1:0.5132
>> valid relation prec:0.3613, rec:0.1210, f1:0.1813
>> valid relation with NER prec:0.3613, rec:0.1210, f1:0.1813
g_step 5600, step 179, avg_time 2.225, loss:619.9921
g_step 5700, step 279, avg_time 0.959, loss:632.5407
g_step 5800, step 379, avg_time 0.976, loss:652.0653
g_step 5900, step 62, avg_time 0.971, loss:603.0200
g_step 6000, step 162, avg_time 0.960, loss:603.3980
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5522, rec:0.5205, f1:0.5359
>> valid relation prec:0.2963, rec:0.1087, f1:0.1590
>> valid relation with NER prec:0.2963, rec:0.1087, f1:0.1590
g_step 6100, step 262, avg_time 2.239, loss:611.9264
g_step 6200, step 362, avg_time 0.969, loss:605.7970
g_step 6300, step 45, avg_time 0.962, loss:595.0795
g_step 6400, step 145, avg_time 0.965, loss:562.4088
g_step 6500, step 245, avg_time 0.962, loss:585.1587
>> valid entity prec:0.5466, rec:0.5110, f1:0.5282
>> valid relation prec:0.2840, rec:0.1139, f1:0.1625
>> valid relation with NER prec:0.2840, rec:0.1139, f1:0.1625
g_step 6600, step 345, avg_time 2.233, loss:614.3800
g_step 6700, step 28, avg_time 0.972, loss:567.3567
g_step 6800, step 128, avg_time 0.957, loss:548.0414
g_step 6900, step 228, avg_time 0.979, loss:560.5908
g_step 7000, step 328, avg_time 0.972, loss:566.1598
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5914, rec:0.4572, f1:0.5157
>> valid relation prec:0.3028, rec:0.0995, f1:0.1498
>> valid relation with NER prec:0.3028, rec:0.0995, f1:0.1498
g_step 7100, step 11, avg_time 2.228, loss:561.6171
g_step 7200, step 111, avg_time 0.956, loss:514.9349
g_step 7300, step 211, avg_time 0.983, loss:539.0315
g_step 7400, step 311, avg_time 0.965, loss:546.8709
g_step 7500, step 411, avg_time 0.963, loss:561.9428
>> valid entity prec:0.5500, rec:0.5326, f1:0.5412
>> valid relation prec:0.2966, rec:0.1187, f1:0.1696
>> valid relation with NER prec:0.2966, rec:0.1187, f1:0.1696
g_step 7600, step 94, avg_time 2.222, loss:497.3926
g_step 7700, step 194, avg_time 0.975, loss:495.9342
g_step 7800, step 294, avg_time 0.979, loss:540.7164
g_step 7900, step 394, avg_time 0.957, loss:542.9585
g_step 8000, step 77, avg_time 0.969, loss:517.8792
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5767, rec:0.5246, f1:0.5494
>> valid relation prec:0.2831, rec:0.1153, f1:0.1638
>> valid relation with NER prec:0.2831, rec:0.1153, f1:0.1638
g_step 8100, step 177, avg_time 2.231, loss:481.2512
g_step 8200, step 277, avg_time 0.967, loss:506.1528
g_step 8300, step 377, avg_time 0.970, loss:525.9759
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:56:01 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:56:01 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-56-01_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:56:03 - WARNING - datasets.builder -   Using custom data configuration default-5b34b2fb2fa5bfaf
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5b34b2fb2fa5bfaf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:56:07,548 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:56:07,550 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:56:07,599 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:56:07,601 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:56:07,759 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:56:07,825 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:56:08,116 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:56:19,816 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:56:19,855 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5b34b2fb2fa5bfaf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:56:19 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x151f9fcd1b00> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:06,  1.44ba/s] 18%|        | 2/11 [00:00<00:03,  2.44ba/s] 27%|       | 3/11 [00:01<00:02,  3.11ba/s] 36%|      | 4/11 [00:01<00:02,  2.97ba/s] 45%|     | 5/11 [00:01<00:01,  3.40ba/s] 55%|    | 6/11 [00:01<00:01,  3.70ba/s] 64%|   | 7/11 [00:02<00:01,  3.94ba/s] 73%|  | 8/11 [00:02<00:00,  4.12ba/s] 82%| | 9/11 [00:02<00:00,  4.25ba/s] 91%| | 10/11 [00:02<00:00,  4.34ba/s]100%|| 11/11 [00:02<00:00,  3.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.38ba/s] 50%|     | 2/4 [00:00<00:00,  3.91ba/s] 75%|  | 3/4 [00:00<00:00,  4.14ba/s]100%|| 4/4 [00:00<00:00,  5.25ba/s]100%|| 4/4 [00:00<00:00,  4.65ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  3.66ba/s] 27%|       | 3/11 [00:00<00:01,  7.25ba/s] 45%|     | 5/11 [00:00<00:00,  8.67ba/s] 64%|   | 7/11 [00:00<00:00,  9.50ba/s] 82%| | 9/11 [00:01<00:00,  9.88ba/s]100%|| 11/11 [00:01<00:00, 11.97ba/s]100%|| 11/11 [00:01<00:00,  9.86ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  7.09ba/s] 75%|  | 3/4 [00:00<00:00,  9.61ba/s]100%|| 4/4 [00:00<00:00, 10.79ba/s]
[INFO|trainer.py:414] 2023-08-27 23:56:26,409 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:56:26,482 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:56:26,483 >>   Num examples = 10052
[INFO|trainer.py:1149] 2023-08-27 23:56:26,483 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:56:26,483 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:56:26,483 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:56:26,483 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:56:26,483 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:57,  3.30it/s]  0%|          | 2/785 [00:00<03:50,  3.40it/s]  0%|          | 3/785 [00:00<03:47,  3.43it/s]  1%|          | 4/785 [00:01<03:46,  3.45it/s]  1%|          | 5/785 [00:01<03:45,  3.46it/s]  1%|          | 6/785 [00:01<03:44,  3.47it/s]  1%|          | 7/785 [00:02<03:44,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.47it/s]  1%|         | 10/785 [00:02<03:43,  3.47it/s]  1%|         | 11/785 [00:03<03:42,  3.48it/s]  2%|         | 12/785 [00:03<03:42,  3.48it/s]  2%|         | 13/785 [00:03<03:42,  3.48it/s]  2%|         | 14/785 [00:04<03:41,  3.48it/s]  2%|         | 15/785 [00:04<03:41,  3.48it/s]  2%|         | 16/785 [00:04<03:41,  3.48it/s]  2%|         | 17/785 [00:04<03:40,  3.48it/s]  2%|         | 18/785 [00:05<03:40,  3.48it/s]  2%|         | 19/785 [00:05<03:40,  3.48it/s]  3%|         | 20/785 [00:05<03:47,  3.37it/s]  3%|         | 21/785 [00:06<03:44,  3.40it/s]  3%|         | 22/785 [00:06<03:43,  3.42it/s]  3%|         | 23/785 [00:06<03:41,  3.44it/s]  3%|         | 24/785 [00:06<03:40,  3.45it/s]  3%|         | 25/785 [00:07<03:39,  3.46it/s]  3%|         | 26/785 [00:07<03:39,  3.46it/s]  3%|         | 27/785 [00:07<03:38,  3.46it/s]  4%|         | 28/785 [00:08<03:38,  3.47it/s]  4%|         | 29/785 [00:08<03:38,  3.47it/s]  4%|         | 30/785 [00:08<03:37,  3.47it/s]  4%|         | 31/785 [00:08<03:37,  3.47it/s]  4%|         | 32/785 [00:09<03:36,  3.47it/s]  4%|         | 33/785 [00:09<03:36,  3.47it/s]  4%|         | 34/785 [00:09<03:36,  3.47it/s]  4%|         | 35/785 [00:10<03:36,  3.47it/s]  5%|         | 36/785 [00:10<03:35,  3.47it/s]  5%|         | 37/785 [00:10<03:36,  3.46it/s]  5%|         | 38/785 [00:10<03:35,  3.46it/s]  5%|         | 39/785 [00:11<03:35,  3.47it/s]  5%|         | 40/785 [00:11<03:34,  3.47it/s]  5%|         | 41/785 [00:11<03:34,  3.47it/s]  5%|         | 42/785 [00:12<03:34,  3.47it/s]  5%|         | 43/785 [00:12<03:33,  3.47it/s]  6%|         | 44/785 [00:12<03:33,  3.47it/s]  6%|         | 45/785 [00:13<03:33,  3.47it/s]  6%|         | 46/785 [00:13<03:33,  3.47it/s]  6%|         | 47/785 [00:13<03:32,  3.47it/s]  6%|         | 48/785 [00:13<03:32,  3.47it/s]  6%|         | 49/785 [00:14<03:32,  3.47it/s]  6%|         | 50/785 [00:14<03:31,  3.47it/s]  6%|         | 51/785 [00:14<03:31,  3.47it/s]  7%|         | 52/785 [00:15<03:31,  3.47it/s]  7%|         | 53/785 [00:15<03:30,  3.47it/s]  7%|         | 54/785 [00:15<03:30,  3.47it/s]  7%|         | 55/785 [00:15<03:30,  3.47it/s]  7%|         | 56/785 [00:16<03:30,  3.47it/s]  7%|         | 57/785 [00:16<03:29,  3.47it/s]  7%|         | 58/785 [00:16<03:29,  3.47it/s]  8%|         | 59/785 [00:17<03:29,  3.47it/s]  8%|         | 60/785 [00:17<03:28,  3.47it/s]  8%|         | 61/785 [00:17<03:28,  3.47it/s]  8%|         | 62/785 [00:17<03:28,  3.47it/s]  8%|         | 63/785 [00:18<03:28,  3.47it/s]  8%|         | 64/785 [00:18<03:27,  3.47it/s]  8%|         | 65/785 [00:18<03:27,  3.47it/s]  8%|         | 66/785 [00:19<03:27,  3.47it/s]  9%|         | 67/785 [00:19<03:27,  3.47it/s]  9%|         | 68/785 [00:19<03:26,  3.47it/s]  9%|         | 69/785 [00:19<03:26,  3.47it/s]  9%|         | 70/785 [00:20<03:26,  3.47it/s]  9%|         | 71/785 [00:20<03:25,  3.47it/s]  9%|         | 72/785 [00:20<03:25,  3.47it/s]  9%|         | 73/785 [00:21<03:25,  3.47it/s]  9%|         | 74/785 [00:21<03:25,  3.47it/s] 10%|         | 75/785 [00:21<03:24,  3.47it/s] 10%|         | 76/785 [00:21<03:24,  3.47it/s] 10%|         | 77/785 [00:22<03:24,  3.47it/s] 10%|         | 78/785 [00:22<03:23,  3.47it/s] 10%|         | 79/785 [00:22<03:23,  3.47it/s] 10%|         | 80/785 [00:23<03:23,  3.47it/s] 10%|         | 81/785 [00:23<03:22,  3.47it/s] 10%|         | 82/785 [00:23<03:22,  3.47it/s] 11%|         | 83/785 [00:23<03:22,  3.47it/s] 11%|         | 84/785 [00:24<03:22,  3.47it/s] 11%|         | 85/785 [00:24<03:21,  3.47it/s] 11%|         | 86/785 [00:24<03:21,  3.47it/s] 11%|         | 87/785 [00:25<03:21,  3.47it/s] 11%|         | 88/785 [00:25<03:21,  3.47it/s] 11%|        | 89/785 [00:25<03:20,  3.46it/s] 11%|        | 90/785 [00:25<03:20,  3.47it/s] 12%|        | 91/785 [00:26<03:20,  3.46it/s] 12%|        | 92/785 [00:26<03:19,  3.47it/s] 12%|        | 93/785 [00:26<03:19,  3.46it/s] 12%|        | 94/785 [00:27<03:19,  3.46it/s] 12%|        | 95/785 [00:27<03:19,  3.46it/s] 12%|        | 96/785 [00:27<03:18,  3.46it/s] 12%|        | 97/785 [00:27<03:18,  3.46it/s] 12%|        | 98/785 [00:28<03:18,  3.46it/s] 13%|        | 99/785 [00:28<03:18,  3.46it/s] 13%|        | 100/785 [00:28<03:17,  3.46it/s] 13%|        | 101/785 [00:29<03:17,  3.46it/s] 13%|        | 102/785 [00:29<03:17,  3.46it/s] 13%|        | 103/785 [00:29<03:16,  3.46it/s] 13%|        | 104/785 [00:30<03:16,  3.46it/s] 13%|        | 105/785 [00:30<03:16,  3.46it/s] 14%|        | 106/785 [00:30<03:16,  3.46it/s] 14%|        | 107/785 [00:30<03:15,  3.46it/s] 14%|        | 108/785 [00:31<03:15,  3.46it/s] 14%|        | 109/785 [00:31<03:15,  3.46it/s] 14%|        | 110/785 [00:31<03:14,  3.46it/s] 14%|        | 111/785 [00:32<03:14,  3.46it/s] 14%|        | 112/785 [00:32<03:24,  3.30it/s] 14%|        | 113/785 [00:32<03:20,  3.35it/s] 15%|        | 114/785 [00:32<03:18,  3.38it/s] 15%|        | 115/785 [00:33<03:16,  3.40it/s] 15%|        | 116/785 [00:33<03:15,  3.42it/s] 15%|        | 117/785 [00:33<03:14,  3.43it/s] 15%|        | 118/785 [00:34<03:13,  3.44it/s] 15%|        | 119/785 [00:34<03:13,  3.45it/s] 15%|        | 120/785 [00:34<03:20,  3.31it/s] 15%|        | 121/785 [00:35<03:17,  3.35it/s] 16%|        | 122/785 [00:35<04:39,  2.37it/s] 16%|        | 123/785 [00:36<04:12,  2.62it/s] 16%|        | 124/785 [00:36<03:54,  2.82it/s] 16%|        | 125/785 [00:36<03:40,  2.99it/s] 16%|        | 126/785 [00:36<03:31,  3.12it/s] 16%|        | 127/785 [00:37<03:24,  3.21it/s] 16%|        | 128/785 [00:37<03:20,  3.28it/s] 16%|        | 129/785 [00:37<03:30,  3.11it/s] 17%|        | 130/785 [00:38<03:24,  3.21it/s] 17%|        | 131/785 [00:38<03:19,  3.28it/s] 17%|        | 132/785 [00:38<03:15,  3.34it/s] 17%|        | 133/785 [00:38<03:13,  3.37it/s] 17%|        | 134/785 [00:39<03:11,  3.40it/s] 17%|        | 135/785 [00:39<03:10,  3.42it/s] 17%|        | 136/785 [00:39<03:09,  3.43it/s] 17%|        | 137/785 [00:40<03:08,  3.44it/s] 18%|        | 138/785 [00:40<03:07,  3.45it/s] 18%|        | 139/785 [00:40<03:07,  3.45it/s] 18%|        | 140/785 [00:41<03:13,  3.34it/s] 18%|        | 141/785 [00:41<03:10,  3.38it/s] 18%|        | 142/785 [00:41<03:09,  3.40it/s] 18%|        | 143/785 [00:41<03:07,  3.42it/s] 18%|        | 144/785 [00:42<03:06,  3.43it/s] 18%|        | 145/785 [00:42<03:06,  3.44it/s] 19%|        | 146/785 [00:42<03:05,  3.45it/s] 19%|        | 147/785 [00:43<03:05,  3.45it/s] 19%|        | 148/785 [00:43<03:04,  3.45it/s] 19%|        | 149/785 [00:43<03:03,  3.46it/s] 19%|        | 150/785 [00:43<03:03,  3.46it/s] 19%|        | 151/785 [00:44<03:03,  3.46it/s] 19%|        | 152/785 [00:44<03:07,  3.37it/s] 19%|        | 153/785 [00:44<03:05,  3.40it/s] 20%|        | 154/785 [00:45<03:04,  3.42it/s] 20%|        | 155/785 [00:45<03:03,  3.43it/s] 20%|        | 156/785 [00:45<03:02,  3.44it/s] 20%|        | 157/785 [00:45<03:02,  3.45it/s][INFO|trainer.py:2140] 2023-08-27 23:57:12,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:57:12,519 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-27 23:57:12,519 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.60it/s][A
  3%|         | 12/437 [00:00<00:08, 49.68it/s][A
  4%|         | 18/437 [00:00<00:08, 47.52it/s][A
  5%|         | 23/437 [00:00<00:08, 46.82it/s][A
  6%|         | 28/437 [00:00<00:08, 46.25it/s][A
  8%|         | 33/437 [00:00<00:08, 46.10it/s][A
  9%|         | 38/437 [00:00<00:08, 45.82it/s][A
 10%|         | 43/437 [00:00<00:08, 45.14it/s][A
 11%|         | 48/437 [00:01<00:08, 45.02it/s][A
 12%|        | 53/437 [00:01<00:08, 44.63it/s][A
 13%|        | 58/437 [00:01<00:08, 44.89it/s][A
 14%|        | 63/437 [00:01<00:08, 44.18it/s][A
 16%|        | 68/437 [00:01<00:08, 44.53it/s][A
 17%|        | 73/437 [00:01<00:08, 44.89it/s][A
 18%|        | 78/437 [00:01<00:07, 45.09it/s][A
 19%|        | 83/437 [00:01<00:07, 45.11it/s][A
 20%|        | 88/437 [00:01<00:07, 44.97it/s][A
 21%|       | 93/437 [00:02<00:07, 44.88it/s][A
 22%|       | 98/437 [00:02<00:07, 44.93it/s][A
 24%|       | 103/437 [00:02<00:07, 45.02it/s][A
 25%|       | 108/437 [00:02<00:07, 45.07it/s][A
 26%|       | 113/437 [00:02<00:07, 45.08it/s][A
 27%|       | 118/437 [00:02<00:07, 45.27it/s][A
 28%|       | 123/437 [00:02<00:06, 45.32it/s][A
 29%|       | 128/437 [00:02<00:06, 45.25it/s][A
 30%|       | 133/437 [00:02<00:06, 45.10it/s][A
 32%|      | 138/437 [00:03<00:06, 45.20it/s][A
 33%|      | 143/437 [00:03<00:06, 45.08it/s][A
 34%|      | 148/437 [00:03<00:06, 45.12it/s][A
 35%|      | 153/437 [00:03<00:06, 45.07it/s][A
 36%|      | 158/437 [00:03<00:06, 45.17it/s][A
 37%|      | 163/437 [00:03<00:06, 45.22it/s][A
 38%|      | 168/437 [00:03<00:05, 45.35it/s][A
 40%|      | 173/437 [00:03<00:05, 45.23it/s][A
 41%|      | 178/437 [00:03<00:05, 43.95it/s][A
 42%|     | 183/437 [00:04<00:05, 44.31it/s][A
 43%|     | 188/437 [00:04<00:05, 44.62it/s][A
 44%|     | 193/437 [00:04<00:05, 44.63it/s][A
 45%|     | 198/437 [00:04<00:05, 43.52it/s][A
 46%|     | 203/437 [00:04<00:05, 44.18it/s][A
 48%|     | 208/437 [00:04<00:05, 44.49it/s][A
 49%|     | 213/437 [00:04<00:05, 44.75it/s][A
 50%|     | 218/437 [00:04<00:04, 44.70it/s][A
 51%|     | 223/437 [00:04<00:04, 44.84it/s][A
 52%|    | 228/437 [00:05<00:04, 45.00it/s][A
 53%|    | 233/437 [00:05<00:04, 44.98it/s][A
 54%|    | 238/437 [00:05<00:04, 44.87it/s][A
 56%|    | 243/437 [00:05<00:04, 44.91it/s][A
 57%|    | 248/437 [00:05<00:04, 45.09it/s][A
 58%|    | 253/437 [00:05<00:04, 45.17it/s][A
 59%|    | 258/437 [00:05<00:03, 45.24it/s][A
 60%|    | 263/437 [00:05<00:03, 45.11it/s][A
 61%|   | 268/437 [00:05<00:03, 45.10it/s][A
 62%|   | 273/437 [00:06<00:03, 45.17it/s][A
 64%|   | 278/437 [00:06<00:03, 45.04it/s][A
 65%|   | 283/437 [00:06<00:03, 45.01it/s][A
 66%|   | 288/437 [00:06<00:03, 45.06it/s][A
 67%|   | 293/437 [00:06<00:03, 45.18it/s][A
 68%|   | 298/437 [00:06<00:03, 43.94it/s][A
 69%|   | 303/437 [00:06<00:02, 44.79it/s][A
 70%|   | 308/437 [00:06<00:02, 44.85it/s][A
 72%|  | 313/437 [00:06<00:02, 45.01it/s][A
 73%|  | 318/437 [00:07<00:02, 45.04it/s][A
 74%|  | 323/437 [00:07<00:02, 44.99it/s][A
 75%|  | 328/437 [00:07<00:02, 44.84it/s][A
 76%|  | 333/437 [00:07<00:02, 43.98it/s][A
 77%|  | 338/437 [00:07<00:02, 44.32it/s][A
 78%|  | 343/437 [00:07<00:02, 44.75it/s][A
 80%|  | 348/437 [00:07<00:01, 44.83it/s][A
 81%|  | 353/437 [00:07<00:01, 45.00it/s][A
 82%| | 358/437 [00:07<00:01, 45.01it/s][A
 83%| | 363/437 [00:08<00:01, 45.10it/s][A
 84%| | 368/437 [00:08<00:01, 44.97it/s][A
 85%| | 373/437 [00:08<00:01, 44.80it/s][A
 86%| | 378/437 [00:08<00:01, 44.93it/s][A
 88%| | 383/437 [00:08<00:01, 44.98it/s][A
 89%| | 388/437 [00:08<00:01, 45.19it/s][A
 90%| | 393/437 [00:08<00:00, 45.12it/s][A
 91%| | 398/437 [00:08<00:00, 45.27it/s][A
 92%|| 403/437 [00:08<00:00, 45.14it/s][A
 93%|| 408/437 [00:09<00:00, 45.28it/s][A
 95%|| 413/437 [00:09<00:00, 45.15it/s][A
 96%|| 418/437 [00:09<00:00, 45.02it/s][A
 97%|| 423/437 [00:09<00:00, 45.04it/s][A
 98%|| 428/437 [00:09<00:00, 45.15it/s][A
 99%|| 433/437 [00:09<00:00, 45.19it/s][A                                                 
                                                 [A 20%|        | 157/785 [00:55<03:02,  3.45it/s]
100%|| 437/437 [00:09<00:00, 45.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:57:22,470 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-27 23:57:22,644 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:57:25,968 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:57:26,185 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:57:26,263 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|        | 158/785 [01:06<1:05:46,  6.30s/it] 20%|        | 159/785 [01:06<46:56,  4.50s/it]   20%|        | 160/785 [01:06<33:43,  3.24s/it] 21%|        | 161/785 [01:07<24:28,  2.35s/it] 21%|        | 162/785 [01:07<18:01,  1.74s/it] 21%|        | 163/785 [01:07<13:30,  1.30s/it] 21%|        | 164/785 [01:08<10:20,  1.00it/s] 21%|        | 165/785 [01:08<08:08,  1.27it/s] 21%|        | 166/785 [01:08<06:35,  1.56it/s] 21%|       | 167/785 [01:08<05:30,  1.87it/s] 21%|       | 168/785 [01:09<04:45,  2.16it/s] 22%|       | 169/785 [01:09<04:13,  2.43it/s] 22%|       | 170/785 [01:09<03:57,  2.59it/s] 22%|       | 171/785 [01:10<03:39,  2.80it/s] 22%|       | 172/785 [01:10<03:26,  2.97it/s] 22%|       | 173/785 [01:10<03:17,  3.10it/s] 22%|       | 174/785 [01:10<03:10,  3.20it/s] 22%|       | 175/785 [01:11<03:06,  3.28it/s] 22%|       | 176/785 [01:11<03:02,  3.33it/s] 23%|       | 177/785 [01:11<03:00,  3.37it/s] 23%|       | 178/785 [01:12<02:58,  3.40it/s] 23%|       | 179/785 [01:12<02:57,  3.42it/s] 23%|       | 180/785 [01:12<02:56,  3.43it/s] 23%|       | 181/785 [01:13<03:01,  3.33it/s] 23%|       | 182/785 [01:13<02:58,  3.37it/s] 23%|       | 183/785 [01:13<02:57,  3.40it/s] 23%|       | 184/785 [01:13<02:55,  3.41it/s] 24%|       | 185/785 [01:14<02:55,  3.43it/s] 24%|       | 186/785 [01:14<02:54,  3.44it/s] 24%|       | 187/785 [01:14<02:53,  3.45it/s] 24%|       | 188/785 [01:15<02:52,  3.45it/s] 24%|       | 189/785 [01:15<02:52,  3.46it/s] 24%|       | 190/785 [01:15<02:51,  3.46it/s] 24%|       | 191/785 [01:15<02:51,  3.46it/s] 24%|       | 192/785 [01:16<02:58,  3.32it/s] 25%|       | 193/785 [01:16<02:56,  3.36it/s] 25%|       | 194/785 [01:16<02:54,  3.39it/s] 25%|       | 195/785 [01:17<02:53,  3.41it/s] 25%|       | 196/785 [01:17<02:51,  3.43it/s] 25%|       | 197/785 [01:17<02:51,  3.44it/s] 25%|       | 198/785 [01:18<02:50,  3.45it/s] 25%|       | 199/785 [01:18<02:49,  3.45it/s] 25%|       | 200/785 [01:18<02:49,  3.46it/s] 26%|       | 201/785 [01:18<02:48,  3.46it/s] 26%|       | 202/785 [01:19<02:48,  3.46it/s] 26%|       | 203/785 [01:19<02:54,  3.34it/s] 26%|       | 204/785 [01:19<02:52,  3.37it/s] 26%|       | 205/785 [01:20<02:50,  3.40it/s] 26%|       | 206/785 [01:20<02:49,  3.42it/s] 26%|       | 207/785 [01:20<02:48,  3.43it/s] 26%|       | 208/785 [01:20<02:47,  3.44it/s] 27%|       | 209/785 [01:21<02:47,  3.45it/s] 27%|       | 210/785 [01:21<02:46,  3.45it/s] 27%|       | 211/785 [01:21<02:46,  3.45it/s] 27%|       | 212/785 [01:22<02:45,  3.46it/s] 27%|       | 213/785 [01:22<02:45,  3.46it/s] 27%|       | 214/785 [01:22<02:48,  3.39it/s] 27%|       | 215/785 [01:22<02:47,  3.41it/s] 28%|       | 216/785 [01:23<02:46,  3.43it/s] 28%|       | 217/785 [01:23<02:45,  3.44it/s] 28%|       | 218/785 [01:23<02:44,  3.45it/s] 28%|       | 219/785 [01:24<02:43,  3.45it/s] 28%|       | 220/785 [01:24<02:43,  3.46it/s] 28%|       | 221/785 [01:24<02:42,  3.46it/s] 28%|       | 222/785 [01:24<02:42,  3.46it/s] 28%|       | 223/785 [01:25<02:42,  3.46it/s] 29%|       | 224/785 [01:25<02:41,  3.46it/s] 29%|       | 225/785 [01:25<02:44,  3.40it/s] 29%|       | 226/785 [01:26<02:43,  3.42it/s] 29%|       | 227/785 [01:26<02:42,  3.43it/s] 29%|       | 228/785 [01:26<02:41,  3.44it/s] 29%|       | 229/785 [01:27<02:41,  3.45it/s] 29%|       | 230/785 [01:27<02:40,  3.45it/s] 29%|       | 231/785 [01:27<02:40,  3.46it/s] 30%|       | 232/785 [01:27<02:39,  3.46it/s] 30%|       | 233/785 [01:28<02:39,  3.46it/s] 30%|       | 234/785 [01:28<02:39,  3.46it/s] 30%|       | 235/785 [01:28<02:38,  3.46it/s] 30%|       | 236/785 [01:29<02:38,  3.46it/s] 30%|       | 237/785 [01:29<02:38,  3.46it/s] 30%|       | 238/785 [01:29<02:37,  3.46it/s] 30%|       | 239/785 [01:29<02:37,  3.46it/s] 31%|       | 240/785 [01:30<02:37,  3.46it/s] 31%|       | 241/785 [01:30<02:41,  3.37it/s] 31%|       | 242/785 [01:30<02:39,  3.39it/s] 31%|       | 243/785 [01:31<02:38,  3.42it/s] 31%|       | 244/785 [01:31<02:37,  3.43it/s] 31%|       | 245/785 [01:31<02:36,  3.44it/s] 31%|      | 246/785 [01:31<02:36,  3.45it/s] 31%|      | 247/785 [01:32<02:35,  3.45it/s] 32%|      | 248/785 [01:32<02:35,  3.46it/s] 32%|      | 249/785 [01:32<02:35,  3.46it/s] 32%|      | 250/785 [01:33<02:34,  3.46it/s] 32%|      | 251/785 [01:33<02:34,  3.46it/s] 32%|      | 252/785 [01:33<02:47,  3.19it/s] 32%|      | 253/785 [01:34<02:42,  3.27it/s] 32%|      | 254/785 [01:34<02:39,  3.32it/s] 32%|      | 255/785 [01:34<02:37,  3.36it/s] 33%|      | 256/785 [01:34<02:35,  3.39it/s] 33%|      | 257/785 [01:35<02:34,  3.42it/s] 33%|      | 258/785 [01:35<03:46,  2.33it/s] 33%|      | 259/785 [01:36<03:23,  2.58it/s] 33%|      | 260/785 [01:36<03:07,  2.80it/s] 33%|      | 261/785 [01:36<03:01,  2.88it/s] 33%|      | 262/785 [01:37<02:52,  3.03it/s] 34%|      | 263/785 [01:37<02:45,  3.15it/s] 34%|      | 264/785 [01:37<02:40,  3.24it/s] 34%|      | 265/785 [01:38<02:37,  3.30it/s] 34%|      | 266/785 [01:38<02:34,  3.35it/s] 34%|      | 267/785 [01:38<02:33,  3.38it/s] 34%|      | 268/785 [01:38<02:31,  3.41it/s] 34%|      | 269/785 [01:39<02:30,  3.42it/s] 34%|      | 270/785 [01:39<02:29,  3.43it/s] 35%|      | 271/785 [01:39<02:29,  3.44it/s] 35%|      | 272/785 [01:40<02:34,  3.32it/s] 35%|      | 273/785 [01:40<02:32,  3.36it/s] 35%|      | 274/785 [01:40<02:30,  3.39it/s] 35%|      | 275/785 [01:40<02:29,  3.41it/s] 35%|      | 276/785 [01:41<02:28,  3.42it/s] 35%|      | 277/785 [01:41<02:27,  3.44it/s] 35%|      | 278/785 [01:41<02:27,  3.44it/s] 36%|      | 279/785 [01:42<02:26,  3.45it/s] 36%|      | 280/785 [01:42<02:26,  3.45it/s] 36%|      | 281/785 [01:42<02:25,  3.45it/s] 36%|      | 282/785 [01:42<02:25,  3.46it/s] 36%|      | 283/785 [01:43<02:31,  3.31it/s] 36%|      | 284/785 [01:43<02:29,  3.36it/s] 36%|      | 285/785 [01:43<02:27,  3.38it/s] 36%|      | 286/785 [01:44<02:26,  3.41it/s] 37%|      | 287/785 [01:44<02:25,  3.42it/s] 37%|      | 288/785 [01:44<02:24,  3.43it/s] 37%|      | 289/785 [01:45<02:24,  3.44it/s] 37%|      | 290/785 [01:45<02:23,  3.45it/s] 37%|      | 291/785 [01:45<02:23,  3.45it/s] 37%|      | 292/785 [01:45<02:22,  3.45it/s] 37%|      | 293/785 [01:46<02:22,  3.46it/s] 37%|      | 294/785 [01:46<02:24,  3.39it/s] 38%|      | 295/785 [01:46<02:23,  3.41it/s] 38%|      | 296/785 [01:47<02:22,  3.43it/s] 38%|      | 297/785 [01:47<02:21,  3.44it/s] 38%|      | 298/785 [01:47<02:21,  3.45it/s] 38%|      | 299/785 [01:47<02:20,  3.45it/s] 38%|      | 300/785 [01:48<02:20,  3.46it/s] 38%|      | 301/785 [01:48<02:20,  3.46it/s] 38%|      | 302/785 [01:48<02:19,  3.46it/s] 39%|      | 303/785 [01:49<02:19,  3.46it/s] 39%|      | 304/785 [01:49<02:18,  3.46it/s] 39%|      | 305/785 [01:49<02:23,  3.34it/s] 39%|      | 306/785 [01:50<02:21,  3.38it/s] 39%|      | 307/785 [01:50<02:20,  3.40it/s] 39%|      | 308/785 [01:50<02:19,  3.42it/s] 39%|      | 309/785 [01:50<02:18,  3.43it/s] 39%|      | 310/785 [01:51<02:18,  3.44it/s] 40%|      | 311/785 [01:51<02:17,  3.45it/s] 40%|      | 312/785 [01:51<02:17,  3.45it/s] 40%|      | 313/785 [01:52<02:16,  3.45it/s] 40%|      | 314/785 [01:52<02:16,  3.46it/s][INFO|trainer.py:2140] 2023-08-27 23:58:18,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:58:18,835 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-27 23:58:18,835 >>   Batch size = 8
{'eval_loss': 0.9358857870101929, 'eval_runtime': 9.7103, 'eval_samples_per_second': 359.308, 'eval_steps_per_second': 45.004, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.81it/s][A
  3%|         | 12/437 [00:00<00:08, 47.33it/s][A
  4%|         | 17/437 [00:00<00:09, 46.24it/s][A
  5%|         | 22/437 [00:00<00:09, 45.92it/s][A
  6%|         | 27/437 [00:00<00:08, 45.68it/s][A
  7%|         | 32/437 [00:00<00:08, 45.36it/s][A
  8%|         | 37/437 [00:00<00:08, 45.22it/s][A
 10%|         | 42/437 [00:00<00:08, 45.03it/s][A
 11%|         | 47/437 [00:01<00:08, 45.08it/s][A
 12%|        | 52/437 [00:01<00:08, 45.14it/s][A
 13%|        | 57/437 [00:01<00:08, 45.20it/s][A
 14%|        | 62/437 [00:01<00:08, 45.15it/s][A
 15%|        | 67/437 [00:01<00:08, 45.18it/s][A
 16%|        | 72/437 [00:01<00:08, 45.23it/s][A
 18%|        | 77/437 [00:01<00:07, 45.02it/s][A
 19%|        | 82/437 [00:01<00:07, 45.04it/s][A
 20%|        | 87/437 [00:01<00:07, 44.96it/s][A
 21%|        | 92/437 [00:02<00:07, 45.09it/s][A
 22%|       | 97/437 [00:02<00:07, 45.14it/s][A
 23%|       | 102/437 [00:02<00:07, 45.11it/s][A
 24%|       | 107/437 [00:02<00:07, 45.18it/s][A
 26%|       | 112/437 [00:02<00:07, 45.19it/s][A
 27%|       | 117/437 [00:02<00:07, 45.20it/s][A
 28%|       | 122/437 [00:02<00:07, 44.99it/s][A
 29%|       | 127/437 [00:02<00:06, 45.05it/s][A
 30%|       | 132/437 [00:02<00:06, 44.96it/s][A
 31%|      | 137/437 [00:03<00:06, 45.04it/s][A
 32%|      | 142/437 [00:03<00:06, 45.08it/s][A
 34%|      | 147/437 [00:03<00:06, 44.94it/s][A
 35%|      | 152/437 [00:03<00:06, 45.07it/s][A
 36%|      | 157/437 [00:03<00:06, 45.16it/s][A
 37%|      | 162/437 [00:03<00:06, 45.15it/s][A
 38%|      | 167/437 [00:03<00:06, 44.94it/s][A
 39%|      | 172/437 [00:03<00:05, 44.92it/s][A
 41%|      | 177/437 [00:03<00:05, 44.89it/s][A
 42%|     | 182/437 [00:04<00:05, 45.00it/s][A
 43%|     | 187/437 [00:04<00:05, 45.10it/s][A
 44%|     | 192/437 [00:04<00:05, 45.13it/s][A
 45%|     | 197/437 [00:04<00:05, 45.15it/s][A
 46%|     | 202/437 [00:04<00:05, 45.18it/s][A
 47%|     | 207/437 [00:04<00:05, 45.15it/s][A
 49%|     | 212/437 [00:04<00:04, 45.14it/s][A
 50%|     | 217/437 [00:04<00:04, 45.00it/s][A
 51%|     | 222/437 [00:04<00:04, 44.81it/s][A
 52%|    | 227/437 [00:05<00:04, 44.99it/s][A
 53%|    | 232/437 [00:05<00:04, 45.06it/s][A
 54%|    | 237/437 [00:05<00:04, 45.10it/s][A
 55%|    | 242/437 [00:05<00:04, 45.03it/s][A
 57%|    | 247/437 [00:05<00:04, 45.13it/s][A
 58%|    | 252/437 [00:05<00:04, 45.16it/s][A
 59%|    | 257/437 [00:05<00:04, 43.74it/s][A
 60%|    | 262/437 [00:05<00:03, 44.01it/s][A
 61%|    | 267/437 [00:05<00:03, 44.28it/s][A
 62%|   | 272/437 [00:06<00:03, 44.44it/s][A
 63%|   | 277/437 [00:06<00:03, 44.76it/s][A
 65%|   | 282/437 [00:06<00:03, 44.93it/s][A
 66%|   | 287/437 [00:06<00:03, 44.99it/s][A
 67%|   | 292/437 [00:06<00:03, 45.13it/s][A
 68%|   | 297/437 [00:06<00:03, 44.98it/s][A
 69%|   | 302/437 [00:06<00:02, 45.01it/s][A
 70%|   | 307/437 [00:06<00:02, 44.63it/s][A
 71%|  | 312/437 [00:06<00:02, 45.03it/s][A
 73%|  | 317/437 [00:07<00:02, 45.14it/s][A
 74%|  | 322/437 [00:07<00:02, 45.04it/s][A
 75%|  | 327/437 [00:07<00:02, 45.09it/s][A
 76%|  | 332/437 [00:07<00:02, 45.11it/s][A
 77%|  | 337/437 [00:07<00:02, 45.20it/s][A
 78%|  | 342/437 [00:07<00:02, 45.13it/s][A
 79%|  | 347/437 [00:07<00:01, 45.06it/s][A
 81%|  | 352/437 [00:07<00:01, 45.02it/s][A
 82%| | 357/437 [00:07<00:01, 45.06it/s][A
 83%| | 362/437 [00:08<00:01, 45.12it/s][A
 84%| | 367/437 [00:08<00:01, 45.13it/s][A
 85%| | 372/437 [00:08<00:01, 45.14it/s][A
 86%| | 377/437 [00:08<00:01, 45.21it/s][A
 87%| | 382/437 [00:08<00:01, 45.23it/s][A
 89%| | 387/437 [00:08<00:01, 45.15it/s][A
 90%| | 392/437 [00:08<00:01, 43.48it/s][A
 91%| | 397/437 [00:08<00:00, 43.96it/s][A
 92%|| 402/437 [00:08<00:00, 44.38it/s][A
 93%|| 407/437 [00:09<00:00, 44.58it/s][A
 94%|| 412/437 [00:09<00:00, 44.74it/s][A
 95%|| 417/437 [00:09<00:00, 44.89it/s][A
 97%|| 422/437 [00:09<00:00, 45.05it/s][A
 98%|| 427/437 [00:09<00:00, 45.12it/s][A
 99%|| 432/437 [00:09<00:00, 44.86it/s][A
100%|| 437/437 [00:09<00:00, 44.93it/s][A                                                 
                                                 [A 40%|      | 314/785 [02:02<02:16,  3.46it/s]
100%|| 437/437 [00:09<00:00, 44.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:58:28,769 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-27 23:58:28,956 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:58:31,575 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:58:31,659 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:58:31,707 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|      | 315/785 [02:11<47:49,  6.11s/it] 40%|      | 316/785 [02:12<34:08,  4.37s/it] 40%|      | 317/785 [02:12<24:32,  3.15s/it] 41%|      | 318/785 [02:12<17:49,  2.29s/it] 41%|      | 319/785 [02:13<13:07,  1.69s/it] 41%|      | 320/785 [02:13<09:51,  1.27s/it] 41%|      | 321/785 [02:13<07:33,  1.02it/s] 41%|      | 322/785 [02:14<05:57,  1.29it/s] 41%|      | 323/785 [02:14<04:50,  1.59it/s] 41%|     | 324/785 [02:14<04:03,  1.90it/s] 41%|     | 325/785 [02:14<03:30,  2.19it/s] 42%|     | 326/785 [02:15<03:07,  2.45it/s] 42%|     | 327/785 [02:15<02:54,  2.62it/s] 42%|     | 328/785 [02:15<02:42,  2.82it/s] 42%|     | 329/785 [02:16<02:33,  2.97it/s] 42%|     | 330/785 [02:16<02:27,  3.09it/s] 42%|     | 331/785 [02:16<02:22,  3.18it/s] 42%|     | 332/785 [02:17<02:19,  3.25it/s] 42%|     | 333/785 [02:17<02:17,  3.30it/s] 43%|     | 334/785 [02:17<02:15,  3.33it/s] 43%|     | 335/785 [02:17<02:14,  3.35it/s] 43%|     | 336/785 [02:18<02:13,  3.37it/s] 43%|     | 337/785 [02:18<02:12,  3.38it/s] 43%|     | 338/785 [02:18<02:13,  3.35it/s] 43%|     | 339/785 [02:19<02:12,  3.37it/s] 43%|     | 340/785 [02:19<02:11,  3.38it/s] 43%|     | 341/785 [02:19<02:11,  3.39it/s] 44%|     | 342/785 [02:19<02:10,  3.40it/s] 44%|     | 343/785 [02:20<02:09,  3.40it/s] 44%|     | 344/785 [02:20<02:09,  3.41it/s] 44%|     | 345/785 [02:20<02:09,  3.41it/s] 44%|     | 346/785 [02:21<02:08,  3.41it/s] 44%|     | 347/785 [02:21<02:08,  3.41it/s] 44%|     | 348/785 [02:21<02:08,  3.41it/s] 44%|     | 349/785 [02:22<02:12,  3.30it/s] 45%|     | 350/785 [02:22<02:10,  3.33it/s] 45%|     | 351/785 [02:22<02:09,  3.35it/s] 45%|     | 352/785 [02:22<02:08,  3.37it/s] 45%|     | 353/785 [02:23<02:07,  3.38it/s] 45%|     | 354/785 [02:23<02:07,  3.39it/s] 45%|     | 355/785 [02:23<02:06,  3.40it/s] 45%|     | 356/785 [02:24<02:06,  3.40it/s] 45%|     | 357/785 [02:24<02:05,  3.41it/s] 46%|     | 358/785 [02:24<02:05,  3.41it/s] 46%|     | 359/785 [02:24<02:04,  3.41it/s] 46%|     | 360/785 [02:25<02:07,  3.34it/s] 46%|     | 361/785 [02:25<02:06,  3.36it/s] 46%|     | 362/785 [02:25<02:05,  3.38it/s] 46%|     | 363/785 [02:26<02:04,  3.39it/s] 46%|     | 364/785 [02:26<02:03,  3.40it/s] 46%|     | 365/785 [02:26<02:03,  3.40it/s] 47%|     | 366/785 [02:27<02:03,  3.40it/s] 47%|     | 367/785 [02:27<02:02,  3.41it/s] 47%|     | 368/785 [02:27<02:02,  3.41it/s] 47%|     | 369/785 [02:27<02:01,  3.41it/s] 47%|     | 370/785 [02:28<02:01,  3.41it/s] 47%|     | 371/785 [02:28<02:03,  3.36it/s] 47%|     | 372/785 [02:28<02:02,  3.38it/s] 48%|     | 373/785 [02:29<02:01,  3.39it/s] 48%|     | 374/785 [02:29<02:01,  3.39it/s] 48%|     | 375/785 [02:29<02:00,  3.40it/s] 48%|     | 376/785 [02:29<02:00,  3.40it/s] 48%|     | 377/785 [02:30<01:59,  3.41it/s] 48%|     | 378/785 [02:30<01:59,  3.41it/s] 48%|     | 379/785 [02:30<01:59,  3.41it/s] 48%|     | 380/785 [02:31<01:58,  3.41it/s] 49%|     | 381/785 [02:31<01:58,  3.41it/s] 49%|     | 382/785 [02:31<02:01,  3.32it/s] 49%|     | 383/785 [02:32<02:00,  3.35it/s] 49%|     | 384/785 [02:32<01:59,  3.37it/s] 49%|     | 385/785 [02:32<01:58,  3.38it/s] 49%|     | 386/785 [02:32<01:57,  3.39it/s] 49%|     | 387/785 [02:33<01:57,  3.40it/s] 49%|     | 388/785 [02:33<01:56,  3.40it/s] 50%|     | 389/785 [02:33<01:56,  3.40it/s] 50%|     | 390/785 [02:34<01:55,  3.41it/s] 50%|     | 391/785 [02:34<01:55,  3.41it/s] 50%|     | 392/785 [02:34<01:55,  3.41it/s] 50%|     | 393/785 [02:34<01:54,  3.41it/s] 50%|     | 394/785 [02:35<01:54,  3.41it/s] 50%|     | 395/785 [02:35<01:54,  3.41it/s] 50%|     | 396/785 [02:35<01:56,  3.33it/s] 51%|     | 397/785 [02:36<01:55,  3.35it/s] 51%|     | 398/785 [02:36<01:54,  3.37it/s] 51%|     | 399/785 [02:36<01:54,  3.38it/s] 51%|     | 400/785 [02:37<01:58,  3.24it/s] 51%|     | 401/785 [02:37<01:56,  3.29it/s] 51%|     | 402/785 [02:37<01:55,  3.32it/s] 51%|    | 403/785 [02:38<01:54,  3.35it/s] 51%|    | 404/785 [02:38<01:53,  3.37it/s] 52%|    | 405/785 [02:38<01:52,  3.38it/s] 52%|    | 406/785 [02:38<01:51,  3.39it/s] 52%|    | 407/785 [02:39<01:51,  3.40it/s] 52%|    | 408/785 [02:39<01:50,  3.40it/s] 52%|    | 409/785 [02:39<01:50,  3.40it/s] 52%|    | 410/785 [02:40<01:50,  3.40it/s] 52%|    | 411/785 [02:40<01:57,  3.20it/s] 52%|    | 412/785 [02:40<01:54,  3.26it/s] 53%|    | 413/785 [02:40<01:52,  3.30it/s] 53%|    | 414/785 [02:41<01:51,  3.33it/s] 53%|    | 415/785 [02:41<01:50,  3.36it/s] 53%|    | 416/785 [02:41<01:49,  3.37it/s] 53%|    | 417/785 [02:42<01:48,  3.38it/s] 53%|    | 418/785 [02:42<01:48,  3.39it/s] 53%|    | 419/785 [02:42<01:47,  3.39it/s] 54%|    | 420/785 [02:43<01:47,  3.40it/s] 54%|    | 421/785 [02:43<01:47,  3.40it/s] 54%|    | 422/785 [02:43<01:52,  3.22it/s] 54%|    | 423/785 [02:43<01:50,  3.28it/s] 54%|    | 424/785 [02:44<01:48,  3.34it/s] 54%|    | 425/785 [02:44<01:46,  3.37it/s] 54%|    | 426/785 [02:44<01:45,  3.40it/s] 54%|    | 427/785 [02:45<01:44,  3.41it/s] 55%|    | 428/785 [02:45<01:44,  3.43it/s] 55%|    | 429/785 [02:45<01:43,  3.44it/s] 55%|    | 430/785 [02:46<01:42,  3.45it/s] 55%|    | 431/785 [02:46<01:42,  3.45it/s] 55%|    | 432/785 [02:46<01:42,  3.45it/s] 55%|    | 433/785 [02:46<01:48,  3.24it/s] 55%|    | 434/785 [02:47<01:46,  3.30it/s] 55%|    | 435/785 [02:47<01:44,  3.35it/s] 56%|    | 436/785 [02:47<01:43,  3.38it/s] 56%|    | 437/785 [02:48<01:42,  3.40it/s] 56%|    | 438/785 [02:48<01:41,  3.41it/s] 56%|    | 439/785 [02:48<01:40,  3.43it/s] 56%|    | 440/785 [02:48<01:40,  3.44it/s] 56%|    | 441/785 [02:49<01:39,  3.44it/s] 56%|    | 442/785 [02:49<01:39,  3.45it/s] 56%|    | 443/785 [02:49<01:39,  3.45it/s] 57%|    | 444/785 [02:50<01:45,  3.22it/s] 57%|    | 445/785 [02:50<01:43,  3.29it/s] 57%|    | 446/785 [02:50<01:41,  3.34it/s] 57%|    | 447/785 [02:51<01:40,  3.37it/s] 57%|    | 448/785 [02:51<01:39,  3.40it/s] 57%|    | 449/785 [02:51<01:38,  3.42it/s] 57%|    | 450/785 [02:51<01:37,  3.43it/s] 57%|    | 451/785 [02:52<01:37,  3.44it/s] 58%|    | 452/785 [02:52<01:36,  3.44it/s] 58%|    | 453/785 [02:52<01:36,  3.45it/s] 58%|    | 454/785 [02:53<01:35,  3.45it/s] 58%|    | 455/785 [02:53<01:38,  3.36it/s] 58%|    | 456/785 [02:53<01:37,  3.39it/s] 58%|    | 457/785 [02:53<01:36,  3.41it/s] 58%|    | 458/785 [02:54<01:35,  3.42it/s] 58%|    | 459/785 [02:54<01:34,  3.43it/s] 59%|    | 460/785 [02:54<01:34,  3.44it/s] 59%|    | 461/785 [02:55<01:33,  3.45it/s] 59%|    | 462/785 [02:55<01:33,  3.45it/s] 59%|    | 463/785 [02:55<01:33,  3.45it/s] 59%|    | 464/785 [02:56<01:32,  3.45it/s] 59%|    | 465/785 [02:56<01:32,  3.46it/s] 59%|    | 466/785 [02:56<01:50,  2.88it/s] 59%|    | 467/785 [02:57<01:44,  3.03it/s] 60%|    | 468/785 [02:57<01:40,  3.15it/s] 60%|    | 469/785 [02:57<01:37,  3.23it/s] 60%|    | 470/785 [02:57<01:35,  3.30it/s] 60%|    | 471/785 [02:58<01:33,  3.35it/s][INFO|trainer.py:2140] 2023-08-27 23:59:24,743 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:59:24,743 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-27 23:59:24,743 >>   Batch size = 8
{'eval_loss': 0.9300477504730225, 'eval_runtime': 9.7146, 'eval_samples_per_second': 359.15, 'eval_steps_per_second': 44.984, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.49it/s][A
  3%|         | 12/437 [00:00<00:08, 49.38it/s][A
  4%|         | 17/437 [00:00<00:08, 47.41it/s][A
  5%|         | 22/437 [00:00<00:08, 46.42it/s][A
  6%|         | 27/437 [00:00<00:08, 46.02it/s][A
  7%|         | 32/437 [00:00<00:08, 45.59it/s][A
  8%|         | 37/437 [00:00<00:08, 45.52it/s][A
 10%|         | 42/437 [00:00<00:08, 45.07it/s][A
 11%|         | 47/437 [00:01<00:08, 45.06it/s][A
 12%|        | 52/437 [00:01<00:09, 40.86it/s][A
 13%|        | 57/437 [00:01<00:09, 42.20it/s][A
 14%|        | 62/437 [00:01<00:08, 43.01it/s][A
 15%|        | 67/437 [00:01<00:08, 43.78it/s][A
 16%|        | 72/437 [00:01<00:08, 44.00it/s][A
 18%|        | 77/437 [00:01<00:08, 44.71it/s][A
 19%|        | 82/437 [00:01<00:07, 44.87it/s][A
 20%|        | 87/437 [00:01<00:07, 44.92it/s][A
 21%|        | 92/437 [00:02<00:07, 44.58it/s][A
 22%|       | 97/437 [00:02<00:07, 44.48it/s][A
 23%|       | 102/437 [00:02<00:07, 44.65it/s][A
 24%|       | 107/437 [00:02<00:07, 44.74it/s][A
 26%|       | 112/437 [00:02<00:07, 45.02it/s][A
 27%|       | 117/437 [00:02<00:07, 45.08it/s][A
 28%|       | 122/437 [00:02<00:06, 45.29it/s][A
 29%|       | 127/437 [00:02<00:06, 45.27it/s][A
 30%|       | 132/437 [00:02<00:06, 45.15it/s][A
 31%|      | 137/437 [00:03<00:06, 44.89it/s][A
 32%|      | 142/437 [00:03<00:06, 44.72it/s][A
 34%|      | 147/437 [00:03<00:06, 44.72it/s][A
 35%|      | 152/437 [00:03<00:06, 44.84it/s][A
 36%|      | 157/437 [00:03<00:06, 44.94it/s][A
 37%|      | 162/437 [00:03<00:06, 45.05it/s][A
 38%|      | 167/437 [00:03<00:05, 45.14it/s][A
 39%|      | 172/437 [00:03<00:05, 45.24it/s][A
 41%|      | 177/437 [00:03<00:05, 45.19it/s][A
 42%|     | 182/437 [00:04<00:05, 45.12it/s][A
 43%|     | 187/437 [00:04<00:05, 44.15it/s][A
 44%|     | 192/437 [00:04<00:05, 44.34it/s][A
 45%|     | 197/437 [00:04<00:05, 44.61it/s][A
 46%|     | 202/437 [00:04<00:05, 44.73it/s][A
 47%|     | 207/437 [00:04<00:05, 44.96it/s][A
 49%|     | 212/437 [00:04<00:04, 45.11it/s][A
 50%|     | 217/437 [00:04<00:04, 45.18it/s][A
 51%|     | 222/437 [00:04<00:04, 45.09it/s][A
 52%|    | 227/437 [00:05<00:04, 44.98it/s][A
 53%|    | 232/437 [00:05<00:04, 44.84it/s][A
 54%|    | 237/437 [00:05<00:04, 44.89it/s][A
 55%|    | 242/437 [00:05<00:04, 44.92it/s][A
 57%|    | 247/437 [00:05<00:04, 44.87it/s][A
 58%|    | 252/437 [00:05<00:04, 44.97it/s][A
 59%|    | 257/437 [00:05<00:03, 45.04it/s][A
 60%|    | 262/437 [00:05<00:03, 45.16it/s][A
 61%|    | 267/437 [00:05<00:03, 45.20it/s][A
 62%|   | 272/437 [00:06<00:03, 45.14it/s][A
 63%|   | 277/437 [00:06<00:03, 45.07it/s][A
 65%|   | 282/437 [00:06<00:03, 45.06it/s][A
 66%|   | 287/437 [00:06<00:03, 43.86it/s][A
 67%|   | 292/437 [00:06<00:03, 44.35it/s][A
 68%|   | 297/437 [00:06<00:03, 44.51it/s][A
 69%|   | 302/437 [00:06<00:03, 44.72it/s][A
 70%|   | 307/437 [00:06<00:02, 44.86it/s][A
 71%|  | 312/437 [00:06<00:02, 45.09it/s][A
 73%|  | 317/437 [00:07<00:02, 45.00it/s][A
 74%|  | 322/437 [00:07<00:02, 45.14it/s][A
 75%|  | 327/437 [00:07<00:02, 45.02it/s][A
 76%|  | 332/437 [00:07<00:02, 45.04it/s][A
 77%|  | 337/437 [00:07<00:02, 45.03it/s][A
 78%|  | 342/437 [00:07<00:02, 45.07it/s][A
 79%|  | 347/437 [00:07<00:01, 45.04it/s][A
 81%|  | 352/437 [00:07<00:01, 45.05it/s][A
 82%| | 357/437 [00:07<00:01, 45.16it/s][A
 83%| | 362/437 [00:08<00:01, 45.11it/s][A
 84%| | 367/437 [00:08<00:01, 45.18it/s][A
 85%| | 372/437 [00:08<00:01, 45.12it/s][A
 86%| | 377/437 [00:08<00:01, 45.13it/s][A
 87%| | 382/437 [00:08<00:01, 45.06it/s][A
 89%| | 387/437 [00:08<00:01, 45.07it/s][A
 90%| | 392/437 [00:08<00:00, 45.07it/s][A
 91%| | 397/437 [00:08<00:00, 45.11it/s][A
 92%|| 402/437 [00:08<00:00, 45.13it/s][A
 93%|| 407/437 [00:09<00:00, 45.13it/s][A
 94%|| 412/437 [00:09<00:00, 44.91it/s][A
 95%|| 417/437 [00:09<00:00, 45.24it/s][A
 97%|| 422/437 [00:09<00:00, 43.67it/s][A
 98%|| 427/437 [00:09<00:00, 44.21it/s][A
 99%|| 432/437 [00:09<00:00, 44.39it/s][A
100%|| 437/437 [00:09<00:00, 44.67it/s][A                                                 
                                                 [A 60%|    | 471/785 [03:08<01:33,  3.35it/s]
100%|| 437/437 [00:09<00:00, 44.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:59:34,737 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-27 23:59:34,939 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:59:37,978 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:59:38,139 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:59:38,210 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|    | 472/785 [03:19<34:38,  6.64s/it] 60%|    | 473/785 [03:19<24:38,  4.74s/it] 60%|    | 474/785 [03:20<17:38,  3.40s/it] 61%|    | 475/785 [03:20<12:45,  2.47s/it] 61%|    | 476/785 [03:20<09:20,  1.81s/it] 61%|    | 477/785 [03:21<06:57,  1.36s/it] 61%|    | 478/785 [03:21<05:18,  1.04s/it] 61%|    | 479/785 [03:21<04:08,  1.23it/s] 61%|    | 480/785 [03:21<03:19,  1.53it/s] 61%|   | 481/785 [03:22<02:45,  1.83it/s] 61%|   | 482/785 [03:22<02:21,  2.14it/s] 62%|   | 483/785 [03:22<02:05,  2.41it/s] 62%|   | 484/785 [03:23<01:58,  2.55it/s] 62%|   | 485/785 [03:23<01:48,  2.76it/s] 62%|   | 486/785 [03:23<01:41,  2.94it/s] 62%|   | 487/785 [03:24<01:36,  3.08it/s] 62%|   | 488/785 [03:24<01:33,  3.19it/s] 62%|   | 489/785 [03:24<01:30,  3.27it/s] 62%|   | 490/785 [03:24<01:28,  3.32it/s] 63%|   | 491/785 [03:25<01:27,  3.36it/s] 63%|   | 492/785 [03:25<01:26,  3.39it/s] 63%|   | 493/785 [03:25<01:25,  3.41it/s] 63%|   | 494/785 [03:26<01:25,  3.42it/s] 63%|   | 495/785 [03:26<01:30,  3.19it/s] 63%|   | 496/785 [03:26<01:28,  3.27it/s] 63%|   | 497/785 [03:27<01:26,  3.32it/s] 63%|   | 498/785 [03:27<01:25,  3.37it/s] 64%|   | 499/785 [03:27<01:24,  3.39it/s] 64%|   | 500/785 [03:27<01:23,  3.41it/s]                                                  64%|   | 500/785 [03:27<01:23,  3.41it/s] 64%|   | 501/785 [03:28<01:22,  3.42it/s] 64%|   | 502/785 [03:28<01:22,  3.44it/s] 64%|   | 503/785 [03:28<01:21,  3.45it/s] 64%|   | 504/785 [03:29<01:21,  3.45it/s] 64%|   | 505/785 [03:29<01:21,  3.46it/s] 64%|   | 506/785 [03:29<01:24,  3.31it/s] 65%|   | 507/785 [03:29<01:22,  3.35it/s] 65%|   | 508/785 [03:30<01:21,  3.38it/s] 65%|   | 509/785 [03:30<01:20,  3.41it/s] 65%|   | 510/785 [03:30<01:20,  3.42it/s] 65%|   | 511/785 [03:31<01:19,  3.43it/s] 65%|   | 512/785 [03:31<01:19,  3.44it/s] 65%|   | 513/785 [03:31<01:18,  3.45it/s] 65%|   | 514/785 [03:31<01:18,  3.45it/s] 66%|   | 515/785 [03:32<01:18,  3.46it/s] 66%|   | 516/785 [03:32<01:17,  3.46it/s] 66%|   | 517/785 [03:32<01:19,  3.35it/s] 66%|   | 518/785 [03:33<01:18,  3.39it/s] 66%|   | 519/785 [03:33<01:17,  3.41it/s] 66%|   | 520/785 [03:33<01:17,  3.43it/s] 66%|   | 521/785 [03:34<01:16,  3.44it/s] 66%|   | 522/785 [03:34<01:16,  3.44it/s] 67%|   | 523/785 [03:34<01:15,  3.45it/s] 67%|   | 524/785 [03:34<01:15,  3.46it/s] 67%|   | 525/785 [03:35<01:15,  3.46it/s] 67%|   | 526/785 [03:35<01:35,  2.72it/s] 67%|   | 527/785 [03:36<01:43,  2.48it/s] 67%|   | 528/785 [03:36<01:34,  2.71it/s] 67%|   | 529/785 [03:36<01:28,  2.90it/s] 68%|   | 530/785 [03:37<01:23,  3.05it/s] 68%|   | 531/785 [03:37<01:20,  3.17it/s] 68%|   | 532/785 [03:37<01:17,  3.25it/s] 68%|   | 533/785 [03:37<01:16,  3.31it/s] 68%|   | 534/785 [03:38<01:14,  3.35it/s] 68%|   | 535/785 [03:38<01:15,  3.29it/s] 68%|   | 536/785 [03:38<01:14,  3.34it/s] 68%|   | 537/785 [03:39<01:13,  3.38it/s] 69%|   | 538/785 [03:39<01:12,  3.40it/s] 69%|   | 539/785 [03:39<01:11,  3.42it/s] 69%|   | 540/785 [03:39<01:11,  3.43it/s] 69%|   | 541/785 [03:40<01:10,  3.44it/s] 69%|   | 542/785 [03:40<01:10,  3.45it/s] 69%|   | 543/785 [03:40<01:10,  3.45it/s] 69%|   | 544/785 [03:41<01:09,  3.46it/s] 69%|   | 545/785 [03:41<01:09,  3.46it/s] 70%|   | 546/785 [03:41<01:12,  3.31it/s] 70%|   | 547/785 [03:42<01:10,  3.36it/s] 70%|   | 548/785 [03:42<01:09,  3.39it/s] 70%|   | 549/785 [03:42<01:09,  3.41it/s] 70%|   | 550/785 [03:42<01:08,  3.43it/s] 70%|   | 551/785 [03:43<01:08,  3.43it/s] 70%|   | 552/785 [03:43<01:07,  3.44it/s] 70%|   | 553/785 [03:43<01:07,  3.45it/s] 71%|   | 554/785 [03:44<01:06,  3.45it/s] 71%|   | 555/785 [03:44<01:06,  3.45it/s] 71%|   | 556/785 [03:44<01:06,  3.46it/s] 71%|   | 557/785 [03:45<01:10,  3.22it/s] 71%|   | 558/785 [03:45<01:09,  3.29it/s] 71%|   | 559/785 [03:45<01:07,  3.34it/s] 71%|  | 560/785 [03:45<01:06,  3.37it/s] 71%|  | 561/785 [03:46<01:05,  3.40it/s] 72%|  | 562/785 [03:46<01:05,  3.42it/s] 72%|  | 563/785 [03:46<01:04,  3.43it/s] 72%|  | 564/785 [03:47<01:04,  3.44it/s] 72%|  | 565/785 [03:47<01:03,  3.45it/s] 72%|  | 566/785 [03:47<01:03,  3.45it/s] 72%|  | 567/785 [03:47<01:03,  3.45it/s] 72%|  | 568/785 [03:48<01:06,  3.26it/s] 72%|  | 569/785 [03:48<01:05,  3.31it/s] 73%|  | 570/785 [03:48<01:04,  3.36it/s] 73%|  | 571/785 [03:49<01:03,  3.39it/s] 73%|  | 572/785 [03:49<01:02,  3.41it/s] 73%|  | 573/785 [03:49<01:01,  3.43it/s] 73%|  | 574/785 [03:49<01:01,  3.44it/s] 73%|  | 575/785 [03:50<01:00,  3.44it/s] 73%|  | 576/785 [03:50<01:00,  3.45it/s] 74%|  | 577/785 [03:50<01:00,  3.45it/s] 74%|  | 578/785 [03:51<00:59,  3.45it/s] 74%|  | 579/785 [03:51<01:01,  3.33it/s] 74%|  | 580/785 [03:51<01:00,  3.37it/s] 74%|  | 581/785 [03:52<01:00,  3.40it/s] 74%|  | 582/785 [03:52<00:59,  3.42it/s] 74%|  | 583/785 [03:52<00:58,  3.43it/s] 74%|  | 584/785 [03:52<00:58,  3.44it/s] 75%|  | 585/785 [03:53<00:58,  3.45it/s] 75%|  | 586/785 [03:53<00:57,  3.45it/s] 75%|  | 587/785 [03:53<00:57,  3.45it/s] 75%|  | 588/785 [03:54<00:56,  3.46it/s] 75%|  | 589/785 [03:54<00:56,  3.46it/s] 75%|  | 590/785 [03:54<00:57,  3.37it/s] 75%|  | 591/785 [03:54<00:57,  3.39it/s] 75%|  | 592/785 [03:55<00:56,  3.41it/s] 76%|  | 593/785 [03:55<00:55,  3.43it/s] 76%|  | 594/785 [03:55<00:55,  3.44it/s] 76%|  | 595/785 [03:56<00:55,  3.45it/s] 76%|  | 596/785 [03:56<00:54,  3.45it/s] 76%|  | 597/785 [03:56<00:54,  3.45it/s] 76%|  | 598/785 [03:56<00:54,  3.45it/s] 76%|  | 599/785 [03:57<00:53,  3.45it/s] 76%|  | 600/785 [03:57<00:53,  3.45it/s] 77%|  | 601/785 [03:57<00:54,  3.38it/s] 77%|  | 602/785 [03:58<00:53,  3.40it/s] 77%|  | 603/785 [03:58<00:53,  3.42it/s] 77%|  | 604/785 [03:58<00:52,  3.43it/s] 77%|  | 605/785 [03:59<00:52,  3.44it/s] 77%|  | 606/785 [03:59<00:51,  3.45it/s] 77%|  | 607/785 [03:59<00:51,  3.45it/s] 77%|  | 608/785 [03:59<00:51,  3.46it/s] 78%|  | 609/785 [04:00<00:50,  3.46it/s] 78%|  | 610/785 [04:00<00:50,  3.46it/s] 78%|  | 611/785 [04:00<00:50,  3.46it/s] 78%|  | 612/785 [04:01<00:52,  3.33it/s] 78%|  | 613/785 [04:01<00:51,  3.36it/s] 78%|  | 614/785 [04:01<00:50,  3.39it/s] 78%|  | 615/785 [04:01<00:49,  3.41it/s] 78%|  | 616/785 [04:02<00:49,  3.43it/s] 79%|  | 617/785 [04:02<00:48,  3.44it/s] 79%|  | 618/785 [04:02<00:48,  3.44it/s] 79%|  | 619/785 [04:03<00:48,  3.45it/s] 79%|  | 620/785 [04:03<00:47,  3.45it/s] 79%|  | 621/785 [04:03<00:47,  3.46it/s] 79%|  | 622/785 [04:03<00:47,  3.46it/s] 79%|  | 623/785 [04:04<00:47,  3.38it/s] 79%|  | 624/785 [04:04<00:47,  3.40it/s] 80%|  | 625/785 [04:04<00:46,  3.42it/s] 80%|  | 626/785 [04:05<00:46,  3.43it/s] 80%|  | 627/785 [04:05<00:45,  3.44it/s] 80%|  | 628/785 [04:05<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 00:00:32,255 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:00:32,255 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 00:00:32,255 >>   Batch size = 8
{'eval_loss': 0.926408052444458, 'eval_runtime': 9.747, 'eval_samples_per_second': 357.957, 'eval_steps_per_second': 44.834, 'epoch': 3.0}
{'loss': 0.8483, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.59it/s][A
  3%|         | 12/437 [00:00<00:08, 49.36it/s][A
  4%|         | 17/437 [00:00<00:08, 47.32it/s][A
  5%|         | 22/437 [00:00<00:08, 46.62it/s][A
  6%|         | 27/437 [00:00<00:08, 46.00it/s][A
  7%|         | 32/437 [00:00<00:08, 45.64it/s][A
  8%|         | 37/437 [00:00<00:08, 45.40it/s][A
 10%|         | 42/437 [00:00<00:08, 45.11it/s][A
 11%|         | 47/437 [00:01<00:08, 45.18it/s][A
 12%|        | 52/437 [00:01<00:08, 45.29it/s][A
 13%|        | 57/437 [00:01<00:08, 45.34it/s][A
 14%|        | 62/437 [00:01<00:08, 45.23it/s][A
 15%|        | 67/437 [00:01<00:08, 45.24it/s][A
 16%|        | 72/437 [00:01<00:08, 45.12it/s][A
 18%|        | 77/437 [00:01<00:07, 45.17it/s][A
 19%|        | 82/437 [00:01<00:07, 45.01it/s][A
 20%|        | 87/437 [00:01<00:07, 44.99it/s][A
 21%|        | 92/437 [00:02<00:07, 45.00it/s][A
 22%|       | 97/437 [00:02<00:07, 45.15it/s][A
 23%|       | 102/437 [00:02<00:07, 45.15it/s][A
 24%|       | 107/437 [00:02<00:07, 45.21it/s][A
 26%|       | 112/437 [00:02<00:07, 45.19it/s][A
 27%|       | 117/437 [00:02<00:07, 45.16it/s][A
 28%|       | 122/437 [00:02<00:06, 45.22it/s][A
 29%|       | 127/437 [00:02<00:06, 45.08it/s][A
 30%|       | 132/437 [00:02<00:06, 45.03it/s][A
 31%|      | 137/437 [00:03<00:06, 45.04it/s][A
 32%|      | 142/437 [00:03<00:06, 45.09it/s][A
 34%|      | 147/437 [00:03<00:06, 44.64it/s][A
 35%|      | 152/437 [00:03<00:06, 44.90it/s][A
 36%|      | 157/437 [00:03<00:06, 45.03it/s][A
 37%|      | 162/437 [00:03<00:06, 45.04it/s][A
 38%|      | 167/437 [00:03<00:05, 45.16it/s][A
 39%|      | 172/437 [00:03<00:05, 45.03it/s][A
 41%|      | 177/437 [00:03<00:05, 45.08it/s][A
 42%|     | 182/437 [00:04<00:05, 45.07it/s][A
 43%|     | 187/437 [00:04<00:05, 45.14it/s][A
 44%|     | 192/437 [00:04<00:05, 45.10it/s][A
 45%|     | 197/437 [00:04<00:05, 45.17it/s][A
 46%|     | 202/437 [00:04<00:05, 45.15it/s][A
 47%|     | 207/437 [00:04<00:05, 45.21it/s][A
 49%|     | 212/437 [00:04<00:04, 45.14it/s][A
 50%|     | 217/437 [00:04<00:04, 45.20it/s][A
 51%|     | 222/437 [00:04<00:04, 45.12it/s][A
 52%|    | 227/437 [00:05<00:04, 45.13it/s][A
 53%|    | 232/437 [00:05<00:04, 45.14it/s][A
 54%|    | 237/437 [00:05<00:04, 45.10it/s][A
 55%|    | 242/437 [00:05<00:04, 45.21it/s][A
 57%|    | 247/437 [00:05<00:04, 45.09it/s][A
 58%|    | 252/437 [00:05<00:04, 45.25it/s][A
 59%|    | 257/437 [00:05<00:03, 45.08it/s][A
 60%|    | 262/437 [00:05<00:03, 45.21it/s][A
 61%|    | 267/437 [00:05<00:03, 45.02it/s][A
 62%|   | 272/437 [00:06<00:03, 45.09it/s][A
 63%|   | 277/437 [00:06<00:03, 45.13it/s][A
 65%|   | 282/437 [00:06<00:03, 45.11it/s][A
 66%|   | 287/437 [00:06<00:03, 45.10it/s][A
 67%|   | 292/437 [00:06<00:03, 44.96it/s][A
 68%|   | 297/437 [00:06<00:03, 45.18it/s][A
 69%|   | 302/437 [00:06<00:02, 45.02it/s][A
 70%|   | 307/437 [00:06<00:02, 45.11it/s][A
 71%|  | 312/437 [00:06<00:02, 44.95it/s][A
 73%|  | 317/437 [00:07<00:02, 45.00it/s][A
 74%|  | 322/437 [00:07<00:02, 44.86it/s][A
 75%|  | 327/437 [00:07<00:02, 44.92it/s][A
 76%|  | 332/437 [00:07<00:02, 44.78it/s][A
 77%|  | 337/437 [00:07<00:02, 45.06it/s][A
 78%|  | 342/437 [00:07<00:02, 45.26it/s][A
 79%|  | 347/437 [00:07<00:01, 45.18it/s][A
 81%|  | 352/437 [00:07<00:01, 45.25it/s][A
 82%| | 357/437 [00:07<00:01, 45.09it/s][A
 83%| | 362/437 [00:08<00:01, 45.16it/s][A
 84%| | 367/437 [00:08<00:01, 45.04it/s][A
 85%| | 372/437 [00:08<00:01, 45.10it/s][A
 86%| | 377/437 [00:08<00:01, 45.11it/s][A
 87%| | 382/437 [00:08<00:01, 45.13it/s][A
 89%| | 387/437 [00:08<00:01, 45.14it/s][A
 90%| | 392/437 [00:08<00:00, 45.07it/s][A
 91%| | 397/437 [00:08<00:00, 45.25it/s][A
 92%|| 402/437 [00:08<00:00, 45.15it/s][A
 93%|| 407/437 [00:08<00:00, 45.17it/s][A
 94%|| 412/437 [00:09<00:00, 44.99it/s][A
 95%|| 417/437 [00:09<00:00, 45.10it/s][A
 97%|| 422/437 [00:09<00:00, 45.14it/s][A
 98%|| 427/437 [00:09<00:00, 44.59it/s][A
 99%|| 432/437 [00:09<00:00, 44.84it/s][A
100%|| 437/437 [00:09<00:00, 44.98it/s][A                                                 
                                                 [A 80%|  | 628/785 [04:15<00:45,  3.45it/s]
100%|| 437/437 [00:09<00:00, 44.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:00:42,068 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 00:00:42,147 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:00:44,649 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:00:44,778 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:00:44,820 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|  | 629/785 [04:24<15:22,  5.92s/it] 80%|  | 630/785 [04:25<10:56,  4.23s/it] 80%|  | 631/785 [04:25<07:49,  3.05s/it] 81%|  | 632/785 [04:25<05:40,  2.22s/it] 81%|  | 633/785 [04:25<04:09,  1.64s/it] 81%|  | 634/785 [04:26<03:07,  1.24s/it] 81%|  | 635/785 [04:26<02:23,  1.05it/s] 81%|  | 636/785 [04:26<01:52,  1.32it/s] 81%|  | 637/785 [04:27<01:31,  1.62it/s] 81%| | 638/785 [04:27<01:16,  1.92it/s] 81%| | 639/785 [04:27<01:05,  2.21it/s] 82%| | 640/785 [04:28<00:58,  2.47it/s] 82%| | 641/785 [04:28<00:54,  2.62it/s] 82%| | 642/785 [04:28<00:50,  2.81it/s] 82%| | 643/785 [04:28<00:47,  2.97it/s] 82%| | 644/785 [04:29<00:45,  3.09it/s] 82%| | 645/785 [04:29<00:44,  3.18it/s] 82%| | 646/785 [04:29<00:42,  3.24it/s] 82%| | 647/785 [04:30<00:41,  3.30it/s] 83%| | 648/785 [04:30<00:41,  3.33it/s] 83%| | 649/785 [04:30<00:40,  3.35it/s] 83%| | 650/785 [04:30<00:40,  3.37it/s] 83%| | 651/785 [04:31<00:39,  3.38it/s] 83%| | 652/785 [04:31<00:40,  3.26it/s] 83%| | 653/785 [04:31<00:39,  3.31it/s] 83%| | 654/785 [04:32<00:39,  3.34it/s] 83%| | 655/785 [04:32<00:39,  3.33it/s] 84%| | 656/785 [04:32<00:38,  3.35it/s] 84%| | 657/785 [04:33<00:37,  3.37it/s] 84%| | 658/785 [04:33<00:37,  3.38it/s] 84%| | 659/785 [04:33<00:37,  3.39it/s] 84%| | 660/785 [04:33<00:36,  3.40it/s] 84%| | 661/785 [04:34<00:36,  3.40it/s] 84%| | 662/785 [04:34<00:36,  3.41it/s] 84%| | 663/785 [04:34<00:37,  3.28it/s] 85%| | 664/785 [04:35<00:36,  3.32it/s] 85%| | 665/785 [04:35<00:36,  3.26it/s] 85%| | 666/785 [04:35<00:43,  2.73it/s] 85%| | 667/785 [04:36<00:49,  2.37it/s] 85%| | 668/785 [04:36<00:44,  2.61it/s] 85%| | 669/785 [04:37<00:41,  2.80it/s] 85%| | 670/785 [04:37<00:38,  2.96it/s] 85%| | 671/785 [04:37<00:36,  3.09it/s] 86%| | 672/785 [04:38<00:35,  3.18it/s] 86%| | 673/785 [04:38<00:34,  3.25it/s] 86%| | 674/785 [04:38<00:33,  3.30it/s] 86%| | 675/785 [04:38<00:33,  3.33it/s] 86%| | 676/785 [04:39<00:32,  3.35it/s] 86%| | 677/785 [04:39<00:32,  3.37it/s] 86%| | 678/785 [04:39<00:31,  3.38it/s] 86%| | 679/785 [04:40<00:32,  3.27it/s] 87%| | 680/785 [04:40<00:31,  3.31it/s] 87%| | 681/785 [04:40<00:31,  3.34it/s] 87%| | 682/785 [04:40<00:30,  3.37it/s] 87%| | 683/785 [04:41<00:30,  3.38it/s] 87%| | 684/785 [04:41<00:29,  3.39it/s] 87%| | 685/785 [04:41<00:29,  3.40it/s] 87%| | 686/785 [04:42<00:29,  3.40it/s] 88%| | 687/785 [04:42<00:28,  3.41it/s] 88%| | 688/785 [04:42<00:28,  3.41it/s] 88%| | 689/785 [04:43<00:28,  3.41it/s] 88%| | 690/785 [04:43<00:29,  3.23it/s] 88%| | 691/785 [04:43<00:28,  3.28it/s] 88%| | 692/785 [04:43<00:28,  3.32it/s] 88%| | 693/785 [04:44<00:27,  3.35it/s] 88%| | 694/785 [04:44<00:27,  3.37it/s] 89%| | 695/785 [04:44<00:26,  3.38it/s] 89%| | 696/785 [04:45<00:26,  3.39it/s] 89%| | 697/785 [04:45<00:25,  3.40it/s] 89%| | 698/785 [04:45<00:25,  3.40it/s] 89%| | 699/785 [04:46<00:25,  3.41it/s] 89%| | 700/785 [04:46<00:24,  3.41it/s] 89%| | 701/785 [04:46<00:25,  3.26it/s] 89%| | 702/785 [04:46<00:25,  3.30it/s] 90%| | 703/785 [04:47<00:24,  3.33it/s] 90%| | 704/785 [04:47<00:24,  3.35it/s] 90%| | 705/785 [04:47<00:23,  3.37it/s] 90%| | 706/785 [04:48<00:23,  3.38it/s] 90%| | 707/785 [04:48<00:23,  3.39it/s] 90%| | 708/785 [04:48<00:22,  3.40it/s] 90%| | 709/785 [04:48<00:22,  3.40it/s] 90%| | 710/785 [04:49<00:22,  3.40it/s] 91%| | 711/785 [04:49<00:21,  3.41it/s] 91%| | 712/785 [04:49<00:21,  3.32it/s] 91%| | 713/785 [04:50<00:21,  3.35it/s] 91%| | 714/785 [04:50<00:21,  3.37it/s] 91%| | 715/785 [04:50<00:20,  3.38it/s] 91%| | 716/785 [04:51<00:20,  3.39it/s] 91%|| 717/785 [04:51<00:20,  3.39it/s] 91%|| 718/785 [04:51<00:19,  3.40it/s] 92%|| 719/785 [04:51<00:19,  3.40it/s] 92%|| 720/785 [04:52<00:19,  3.41it/s] 92%|| 721/785 [04:52<00:18,  3.41it/s] 92%|| 722/785 [04:52<00:18,  3.41it/s] 92%|| 723/785 [04:53<00:18,  3.38it/s] 92%|| 724/785 [04:53<00:17,  3.39it/s] 92%|| 725/785 [04:53<00:17,  3.40it/s] 92%|| 726/785 [04:54<00:17,  3.40it/s] 93%|| 727/785 [04:54<00:17,  3.40it/s] 93%|| 728/785 [04:54<00:16,  3.41it/s] 93%|| 729/785 [04:54<00:16,  3.41it/s] 93%|| 730/785 [04:55<00:16,  3.41it/s] 93%|| 731/785 [04:55<00:15,  3.41it/s] 93%|| 732/785 [04:55<00:15,  3.41it/s] 93%|| 733/785 [04:56<00:15,  3.41it/s] 94%|| 734/785 [04:56<00:15,  3.34it/s] 94%|| 735/785 [04:56<00:14,  3.36it/s] 94%|| 736/785 [04:56<00:14,  3.37it/s] 94%|| 737/785 [04:57<00:14,  3.38it/s] 94%|| 738/785 [04:57<00:13,  3.39it/s] 94%|| 739/785 [04:57<00:13,  3.40it/s] 94%|| 740/785 [04:58<00:13,  3.40it/s] 94%|| 741/785 [04:58<00:12,  3.41it/s] 95%|| 742/785 [04:58<00:12,  3.41it/s] 95%|| 743/785 [04:59<00:12,  3.41it/s] 95%|| 744/785 [04:59<00:12,  3.41it/s] 95%|| 745/785 [04:59<00:12,  3.31it/s] 95%|| 746/785 [04:59<00:11,  3.34it/s] 95%|| 747/785 [05:00<00:11,  3.36it/s] 95%|| 748/785 [05:00<00:10,  3.38it/s] 95%|| 749/785 [05:00<00:10,  3.38it/s] 96%|| 750/785 [05:01<00:10,  3.39it/s] 96%|| 751/785 [05:01<00:10,  3.40it/s] 96%|| 752/785 [05:01<00:09,  3.40it/s] 96%|| 753/785 [05:01<00:09,  3.40it/s] 96%|| 754/785 [05:02<00:09,  3.41it/s] 96%|| 755/785 [05:02<00:08,  3.41it/s] 96%|| 756/785 [05:02<00:08,  3.24it/s] 96%|| 757/785 [05:03<00:08,  3.29it/s] 97%|| 758/785 [05:03<00:08,  3.33it/s] 97%|| 759/785 [05:03<00:07,  3.35it/s] 97%|| 760/785 [05:04<00:07,  3.37it/s] 97%|| 761/785 [05:04<00:07,  3.38it/s] 97%|| 762/785 [05:04<00:06,  3.39it/s] 97%|| 763/785 [05:04<00:06,  3.40it/s] 97%|| 764/785 [05:05<00:06,  3.40it/s] 97%|| 765/785 [05:05<00:05,  3.40it/s] 98%|| 766/785 [05:05<00:05,  3.40it/s] 98%|| 767/785 [05:06<00:05,  3.29it/s] 98%|| 768/785 [05:06<00:05,  3.32it/s] 98%|| 769/785 [05:06<00:04,  3.34it/s] 98%|| 770/785 [05:07<00:04,  3.36it/s] 98%|| 771/785 [05:07<00:04,  3.38it/s] 98%|| 772/785 [05:07<00:04,  3.06it/s] 98%|| 773/785 [05:08<00:03,  3.16it/s] 99%|| 774/785 [05:08<00:03,  3.23it/s] 99%|| 775/785 [05:08<00:03,  3.28it/s] 99%|| 776/785 [05:08<00:02,  3.32it/s] 99%|| 777/785 [05:09<00:02,  3.35it/s] 99%|| 778/785 [05:09<00:02,  3.37it/s] 99%|| 779/785 [05:09<00:01,  3.38it/s] 99%|| 780/785 [05:10<00:01,  3.39it/s] 99%|| 781/785 [05:10<00:01,  3.39it/s]100%|| 782/785 [05:10<00:00,  3.17it/s]100%|| 783/785 [05:11<00:00,  3.24it/s]100%|| 784/785 [05:11<00:00,  3.29it/s]100%|| 785/785 [05:11<00:00,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 00:01:38,111 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:01:38,111 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 00:01:38,111 >>   Batch size = 8
{'eval_loss': 0.9330317974090576, 'eval_runtime': 9.6807, 'eval_samples_per_second': 360.407, 'eval_steps_per_second': 45.141, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.35it/s][A
  3%|         | 12/437 [00:00<00:08, 49.23it/s][A
  4%|         | 17/437 [00:00<00:08, 47.37it/s][A
  5%|         | 22/437 [00:00<00:08, 46.44it/s][A
  6%|         | 27/437 [00:00<00:08, 45.96it/s][A
  7%|         | 32/437 [00:00<00:08, 45.55it/s][A
  8%|         | 37/437 [00:00<00:08, 45.30it/s][A
 10%|         | 42/437 [00:00<00:08, 45.21it/s][A
 11%|         | 47/437 [00:01<00:08, 45.18it/s][A
 12%|        | 52/437 [00:01<00:08, 45.27it/s][A
 13%|        | 57/437 [00:01<00:08, 45.31it/s][A
 14%|        | 62/437 [00:01<00:08, 45.20it/s][A
 15%|        | 67/437 [00:01<00:08, 45.15it/s][A
 16%|        | 72/437 [00:01<00:08, 45.03it/s][A
 18%|        | 77/437 [00:01<00:08, 44.95it/s][A
 19%|        | 82/437 [00:01<00:07, 44.97it/s][A
 20%|        | 87/437 [00:01<00:07, 43.92it/s][A
 21%|        | 92/437 [00:02<00:07, 44.37it/s][A
 22%|       | 97/437 [00:02<00:07, 44.71it/s][A
 23%|       | 102/437 [00:02<00:07, 44.93it/s][A
 24%|       | 107/437 [00:02<00:07, 44.91it/s][A
 26%|       | 112/437 [00:02<00:07, 45.01it/s][A
 27%|       | 117/437 [00:02<00:07, 45.01it/s][A
 28%|       | 122/437 [00:02<00:06, 45.01it/s][A
 29%|       | 127/437 [00:02<00:06, 44.85it/s][A
 30%|       | 132/437 [00:02<00:06, 44.81it/s][A
 31%|      | 137/437 [00:03<00:06, 45.00it/s][A
 32%|      | 142/437 [00:03<00:06, 45.12it/s][A
 34%|      | 147/437 [00:03<00:06, 45.19it/s][A
 35%|      | 152/437 [00:03<00:06, 45.20it/s][A
 36%|      | 157/437 [00:03<00:06, 45.19it/s][A
 37%|      | 162/437 [00:03<00:06, 45.22it/s][A
 38%|      | 167/437 [00:03<00:05, 45.16it/s][A
 39%|      | 172/437 [00:03<00:05, 45.02it/s][A
 41%|      | 177/437 [00:03<00:05, 44.99it/s][A
 42%|     | 182/437 [00:04<00:05, 45.03it/s][A
 43%|     | 187/437 [00:04<00:05, 45.12it/s][A
 44%|     | 192/437 [00:04<00:05, 45.17it/s][A
 45%|     | 197/437 [00:04<00:05, 45.16it/s][A
 46%|     | 202/437 [00:04<00:05, 45.18it/s][A
 47%|     | 207/437 [00:04<00:05, 45.15it/s][A
 49%|     | 212/437 [00:04<00:04, 45.15it/s][A
 50%|     | 217/437 [00:04<00:04, 45.09it/s][A
 51%|     | 222/437 [00:04<00:04, 44.05it/s][A
 52%|    | 227/437 [00:05<00:04, 44.52it/s][A
 53%|    | 232/437 [00:05<00:04, 44.61it/s][A
 54%|    | 237/437 [00:05<00:04, 44.87it/s][A
 55%|    | 242/437 [00:05<00:04, 44.86it/s][A
 57%|    | 247/437 [00:05<00:04, 45.01it/s][A
 58%|    | 252/437 [00:05<00:04, 44.94it/s][A
 59%|    | 257/437 [00:05<00:04, 44.94it/s][A
 60%|    | 262/437 [00:05<00:03, 45.01it/s][A
 61%|    | 267/437 [00:05<00:03, 44.97it/s][A
 62%|   | 272/437 [00:06<00:03, 45.07it/s][A
 63%|   | 277/437 [00:06<00:03, 45.08it/s][A
 65%|   | 282/437 [00:06<00:03, 45.18it/s][A
 66%|   | 287/437 [00:06<00:03, 45.12it/s][A
 67%|   | 292/437 [00:06<00:03, 45.15it/s][A
 68%|   | 297/437 [00:06<00:03, 45.17it/s][A
 69%|   | 302/437 [00:06<00:02, 45.17it/s][A
 70%|   | 307/437 [00:06<00:02, 45.13it/s][A
 71%|  | 312/437 [00:06<00:02, 44.99it/s][A
 73%|  | 317/437 [00:07<00:02, 45.10it/s][A
 74%|  | 322/437 [00:07<00:02, 45.04it/s][A
 75%|  | 327/437 [00:07<00:02, 45.17it/s][A
 76%|  | 332/437 [00:07<00:02, 45.11it/s][A
 77%|  | 337/437 [00:07<00:02, 45.19it/s][A
 78%|  | 342/437 [00:07<00:02, 45.13it/s][A
 79%|  | 347/437 [00:07<00:01, 45.16it/s][A
 81%|  | 352/437 [00:07<00:01, 45.17it/s][A
 82%| | 357/437 [00:07<00:01, 43.15it/s][A
 83%| | 362/437 [00:08<00:01, 43.81it/s][A
 84%| | 367/437 [00:08<00:01, 44.25it/s][A
 85%| | 372/437 [00:08<00:01, 44.60it/s][A
 86%| | 377/437 [00:08<00:01, 44.73it/s][A
 87%| | 382/437 [00:08<00:01, 44.84it/s][A
 89%| | 387/437 [00:08<00:01, 44.92it/s][A
 90%| | 392/437 [00:08<00:00, 45.02it/s][A
 91%| | 397/437 [00:08<00:00, 44.76it/s][A
 92%|| 402/437 [00:08<00:00, 45.00it/s][A
 93%|| 407/437 [00:09<00:00, 44.96it/s][A
 94%|| 412/437 [00:09<00:00, 44.99it/s][A
 95%|| 417/437 [00:09<00:00, 45.03it/s][A
 97%|| 422/437 [00:09<00:00, 45.16it/s][A
 98%|| 427/437 [00:09<00:00, 45.09it/s][A
 99%|| 432/437 [00:09<00:00, 45.16it/s][A
100%|| 437/437 [00:09<00:00, 45.16it/s][A                                                 
                                                 [A100%|| 785/785 [05:21<00:00,  3.32it/s]
100%|| 437/437 [00:09<00:00, 45.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:01:48,030 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 00:01:48,206 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:01:51,320 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:01:51,561 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:01:51,660 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:01:58,737 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:01:58,757 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471 (score: 0.926408052444458).
                                                 100%|| 785/785 [05:47<00:00,  3.32it/s]100%|| 785/785 [05:47<00:00,  2.26it/s]
[INFO|trainer.py:1894] 2023-08-28 00:02:13,973 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:02:14,290 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:02:18,578 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:02:18,780 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:02:18,900 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:02:19,621 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   train_loss               =     0.8326
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   train_runtime            = 0:05:47.38
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   train_samples            =      10052
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   train_samples_per_second =    144.683
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:19,621 >>   train_steps_per_second   =       2.26
{'eval_loss': 0.9321764707565308, 'eval_runtime': 9.7094, 'eval_samples_per_second': 359.342, 'eval_steps_per_second': 45.008, 'epoch': 5.0}
{'train_runtime': 347.3802, 'train_samples_per_second': 144.683, 'train_steps_per_second': 2.26, 'train_loss': 0.8325813633621119, 'epoch': 5.0}
08/28/2023 00:02:20 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:02:20,051 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:02:20,051 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 00:02:20,051 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 55.68it/s]  3%|         | 12/437 [00:00<00:08, 49.66it/s]  4%|         | 18/437 [00:00<00:08, 47.88it/s]  5%|         | 23/437 [00:00<00:08, 47.13it/s]  6%|         | 28/437 [00:00<00:08, 46.71it/s]  8%|         | 33/437 [00:00<00:08, 46.42it/s]  9%|         | 38/437 [00:00<00:08, 46.33it/s] 10%|         | 43/437 [00:00<00:08, 45.85it/s] 11%|         | 48/437 [00:01<00:08, 45.41it/s] 12%|        | 53/437 [00:01<00:08, 45.20it/s] 13%|        | 58/437 [00:01<00:08, 45.27it/s] 14%|        | 63/437 [00:01<00:08, 45.29it/s] 16%|        | 68/437 [00:01<00:08, 45.51it/s] 17%|        | 73/437 [00:01<00:07, 45.54it/s] 18%|        | 78/437 [00:01<00:09, 39.43it/s] 19%|        | 83/437 [00:01<00:08, 41.21it/s] 20%|        | 88/437 [00:01<00:08, 42.51it/s] 21%|       | 93/437 [00:02<00:07, 43.55it/s] 22%|       | 98/437 [00:02<00:07, 44.13it/s] 24%|       | 103/437 [00:02<00:07, 44.58it/s] 25%|       | 108/437 [00:02<00:07, 44.87it/s] 26%|       | 113/437 [00:02<00:07, 45.02it/s] 27%|       | 118/437 [00:02<00:07, 44.83it/s] 28%|       | 123/437 [00:02<00:07, 44.67it/s] 29%|       | 128/437 [00:02<00:06, 44.82it/s] 30%|       | 133/437 [00:02<00:06, 44.99it/s] 32%|      | 138/437 [00:03<00:06, 45.32it/s] 33%|      | 143/437 [00:03<00:06, 45.52it/s] 34%|      | 148/437 [00:03<00:06, 45.30it/s] 35%|      | 153/437 [00:03<00:06, 45.74it/s] 36%|      | 158/437 [00:03<00:06, 45.56it/s] 37%|      | 163/437 [00:03<00:06, 45.24it/s] 38%|      | 168/437 [00:03<00:05, 45.00it/s] 40%|      | 173/437 [00:03<00:05, 45.01it/s] 41%|      | 178/437 [00:03<00:05, 45.15it/s] 42%|     | 183/437 [00:04<00:05, 45.30it/s] 43%|     | 188/437 [00:04<00:05, 45.39it/s] 44%|     | 193/437 [00:04<00:05, 45.59it/s] 45%|     | 198/437 [00:04<00:05, 45.64it/s] 46%|     | 203/437 [00:04<00:05, 45.50it/s] 48%|     | 208/437 [00:04<00:05, 45.24it/s] 49%|     | 213/437 [00:04<00:05, 43.26it/s] 50%|     | 218/437 [00:04<00:04, 43.88it/s] 51%|     | 223/437 [00:04<00:04, 44.33it/s] 52%|    | 228/437 [00:05<00:04, 44.75it/s] 53%|    | 233/437 [00:05<00:04, 45.02it/s] 54%|    | 238/437 [00:05<00:04, 45.27it/s] 56%|    | 243/437 [00:05<00:04, 45.34it/s] 57%|    | 248/437 [00:05<00:04, 45.35it/s] 58%|    | 253/437 [00:05<00:04, 45.02it/s] 59%|    | 258/437 [00:05<00:03, 44.95it/s] 60%|    | 263/437 [00:05<00:03, 44.94it/s] 61%|   | 268/437 [00:05<00:03, 45.16it/s] 62%|   | 273/437 [00:06<00:03, 45.29it/s] 64%|   | 278/437 [00:06<00:03, 45.52it/s] 65%|   | 283/437 [00:06<00:03, 45.56it/s] 66%|   | 288/437 [00:06<00:03, 45.56it/s] 67%|   | 293/437 [00:06<00:03, 45.32it/s] 68%|   | 298/437 [00:06<00:03, 45.18it/s] 69%|   | 303/437 [00:06<00:02, 45.07it/s] 70%|   | 308/437 [00:06<00:02, 44.97it/s] 72%|  | 313/437 [00:06<00:02, 45.15it/s] 73%|  | 318/437 [00:07<00:02, 45.22it/s] 74%|  | 323/437 [00:07<00:02, 45.40it/s] 75%|  | 328/437 [00:07<00:02, 45.45it/s] 76%|  | 333/437 [00:07<00:02, 45.56it/s] 77%|  | 338/437 [00:07<00:02, 45.40it/s] 78%|  | 343/437 [00:07<00:02, 45.33it/s] 80%|  | 348/437 [00:07<00:01, 45.09it/s] 81%|  | 353/437 [00:07<00:01, 42.14it/s] 82%| | 358/437 [00:07<00:01, 43.21it/s] 83%| | 363/437 [00:08<00:01, 43.91it/s] 84%| | 368/437 [00:08<00:01, 44.33it/s] 85%| | 373/437 [00:08<00:01, 44.69it/s] 86%| | 378/437 [00:08<00:01, 44.91it/s] 88%| | 383/437 [00:08<00:01, 45.23it/s] 89%| | 388/437 [00:08<00:01, 45.23it/s] 90%| | 393/437 [00:08<00:00, 44.81it/s] 91%| | 398/437 [00:08<00:00, 44.76it/s] 92%|| 403/437 [00:08<00:00, 44.81it/s] 93%|| 408/437 [00:09<00:00, 44.98it/s] 95%|| 413/437 [00:09<00:00, 45.23it/s] 96%|| 418/437 [00:09<00:00, 45.37it/s] 97%|| 423/437 [00:09<00:00, 45.44it/s] 98%|| 428/437 [00:09<00:00, 45.50it/s] 99%|| 433/437 [00:09<00:00, 45.31it/s]100%|| 437/437 [00:09<00:00, 45.03it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:02:29,773 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   eval_loss               =     0.9264
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   eval_runtime            = 0:00:09.72
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   eval_samples_per_second =    358.866
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   eval_steps_per_second   =     44.948
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:02:29,773 >>   perplexity              =     2.5254
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_3', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:22<06:59, 22.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:41<06:04, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:57<05:18, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:15<04:49, 18.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:34<04:40, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:49<04:04, 17.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:10<03:59, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:28<03:39, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:47<03:23, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [03:04<03:01, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:22<02:41, 17.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:40<02:24, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:56<02:02, 17.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:16<01:49, 18.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:33<01:29, 17.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:54<01:15, 18.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:10<00:54, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:26<00:34, 17.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:48<00:18, 18.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [06:04<00:00, 17.93s/it]Generating: 100%|| 20/20 [06:04<00:00, 18.23s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:29, 29.34s/it]Extractor Estimating: 2it [00:31, 13.16s/it]Extractor Estimating: 3it [00:32,  7.69s/it]Extractor Estimating: 4it [00:33,  4.93s/it]Extractor Estimating: 5it [00:34,  3.61s/it]Extractor Estimating: 6it [00:34,  2.59s/it]Extractor Estimating: 7it [00:36,  2.14s/it]Extractor Estimating: 8it [00:36,  1.67s/it]Extractor Estimating: 9it [00:37,  1.35s/it]Extractor Estimating: 10it [00:38,  1.13s/it]Extractor Estimating: 11it [00:38,  1.04it/s]Extractor Estimating: 12it [00:39,  1.15it/s]Extractor Estimating: 13it [00:40,  1.23it/s]Extractor Estimating: 14it [00:40,  1.29it/s]Extractor Estimating: 15it [00:41,  1.34it/s]Extractor Estimating: 16it [00:42,  1.39it/s]Extractor Estimating: 17it [00:42,  1.41it/s]Extractor Estimating: 18it [00:43,  1.42it/s]Extractor Estimating: 19it [00:43,  1.50it/s]Extractor Estimating: 20it [00:44,  1.50it/s]Extractor Estimating: 21it [00:46,  1.01it/s]Extractor Estimating: 22it [00:47,  1.13it/s]Extractor Estimating: 23it [00:47,  1.22it/s]Extractor Estimating: 24it [00:48,  1.25it/s]Extractor Estimating: 25it [00:49,  1.35it/s]Extractor Estimating: 26it [00:49,  1.41it/s]Extractor Estimating: 27it [00:50,  1.49it/s]Extractor Estimating: 28it [00:50,  1.50it/s]Extractor Estimating: 29it [00:51,  1.53it/s]Extractor Estimating: 30it [00:52,  1.60it/s]Extractor Estimating: 31it [00:52,  1.57it/s]Extractor Estimating: 32it [00:53,  1.61it/s]Extractor Estimating: 33it [00:54,  1.54it/s]Extractor Estimating: 34it [00:54,  1.53it/s]Extractor Estimating: 35it [00:55,  1.57it/s]Extractor Estimating: 36it [00:56,  1.53it/s]Extractor Estimating: 37it [00:56,  1.54it/s]Extractor Estimating: 38it [00:57,  1.51it/s]Extractor Estimating: 39it [00:57,  1.55it/s]Extractor Estimating: 40it [00:58,  1.55it/s]Extractor Estimating: 41it [00:59,  1.53it/s]Extractor Estimating: 42it [00:59,  1.59it/s]Extractor Estimating: 43it [01:00,  1.58it/s]Extractor Estimating: 44it [01:01,  1.55it/s]Extractor Estimating: 45it [01:01,  1.59it/s]Extractor Estimating: 46it [01:03,  1.15it/s]Extractor Estimating: 47it [01:03,  1.24it/s]Extractor Estimating: 48it [01:04,  1.20it/s]Extractor Estimating: 49it [01:05,  1.29it/s]Extractor Estimating: 50it [01:06,  1.32it/s]Extractor Estimating: 51it [01:06,  1.39it/s]Extractor Estimating: 52it [01:07,  1.47it/s]Extractor Estimating: 53it [01:08,  1.46it/s]Extractor Estimating: 54it [01:08,  1.53it/s]Extractor Estimating: 55it [01:09,  1.58it/s]Extractor Estimating: 56it [01:09,  1.61it/s]Extractor Estimating: 57it [01:10,  1.55it/s]Extractor Estimating: 58it [01:11,  1.60it/s]Extractor Estimating: 59it [01:11,  1.59it/s]Extractor Estimating: 60it [01:12,  1.56it/s]Extractor Estimating: 61it [01:13,  1.53it/s]Extractor Estimating: 62it [01:13,  1.58it/s]Extractor Estimating: 63it [01:14,  1.57it/s]Extractor Estimating: 64it [01:14,  1.56it/s]Extractor Estimating: 65it [01:15,  1.53it/s]Extractor Estimating: 66it [01:16,  1.59it/s]Extractor Estimating: 67it [01:16,  1.62it/s]Extractor Estimating: 68it [01:17,  1.63it/s]Extractor Estimating: 69it [01:18,  1.62it/s]Extractor Estimating: 70it [01:18,  1.57it/s]Extractor Estimating: 71it [01:19,  1.53it/s]Extractor Estimating: 72it [01:20,  1.54it/s]Extractor Estimating: 73it [01:20,  1.52it/s]Extractor Estimating: 74it [01:21,  1.57it/s]Extractor Estimating: 75it [01:21,  1.59it/s]Extractor Estimating: 76it [01:22,  1.60it/s]Extractor Estimating: 77it [01:23,  1.62it/s]Extractor Estimating: 78it [01:23,  1.61it/s]Extractor Estimating: 79it [01:24,  1.61it/s]Extractor Estimating: 80it [01:25,  1.59it/s]Extractor Estimating: 81it [01:25,  1.44it/s]Extractor Estimating: 82it [01:26,  1.48it/s]Extractor Estimating: 83it [01:27,  1.48it/s]Extractor Estimating: 84it [01:27,  1.48it/s]Extractor Estimating: 85it [01:28,  1.47it/s]Extractor Estimating: 86it [01:30,  1.06s/it]Extractor Estimating: 87it [01:31,  1.08it/s]Extractor Estimating: 88it [01:31,  1.20it/s]Extractor Estimating: 89it [01:32,  1.29it/s]Extractor Estimating: 90it [01:33,  1.30it/s]Extractor Estimating: 91it [01:33,  1.36it/s]Extractor Estimating: 92it [01:34,  1.45it/s]Extractor Estimating: 93it [01:34,  1.49it/s]Extractor Estimating: 94it [01:35,  1.52it/s]Extractor Estimating: 95it [01:36,  1.56it/s]Extractor Estimating: 96it [01:36,  1.52it/s]Extractor Estimating: 97it [01:37,  1.51it/s]Extractor Estimating: 98it [01:38,  1.58it/s]Extractor Estimating: 99it [01:38,  1.53it/s]Extractor Estimating: 100it [01:39,  1.55it/s]Extractor Estimating: 101it [01:40,  1.53it/s]Extractor Estimating: 102it [01:40,  1.56it/s]Extractor Estimating: 103it [01:41,  1.54it/s]Extractor Estimating: 104it [01:42,  1.51it/s]Extractor Estimating: 105it [01:42,  1.56it/s]Extractor Estimating: 106it [01:43,  1.57it/s]Extractor Estimating: 107it [01:43,  1.58it/s]Extractor Estimating: 108it [01:44,  1.63it/s]Extractor Estimating: 109it [01:45,  1.61it/s]Extractor Estimating: 110it [01:45,  1.57it/s]Extractor Estimating: 111it [01:46,  1.60it/s]Extractor Estimating: 112it [01:47,  1.59it/s]Extractor Estimating: 113it [01:47,  1.62it/s]Extractor Estimating: 114it [01:48,  1.61it/s]Extractor Estimating: 115it [01:48,  1.61it/s]Extractor Estimating: 116it [01:49,  1.63it/s]Extractor Estimating: 117it [01:50,  1.63it/s]Extractor Estimating: 118it [01:50,  1.65it/s]Extractor Estimating: 119it [01:51,  1.64it/s]Extractor Estimating: 120it [01:52,  1.59it/s]Extractor Estimating: 121it [01:52,  1.60it/s]Extractor Estimating: 122it [01:53,  1.57it/s]Extractor Estimating: 123it [01:53,  1.60it/s]Extractor Estimating: 124it [01:54,  1.60it/s]Extractor Estimating: 125it [01:55,  1.63it/s]Extractor Estimating: 126it [01:55,  1.57it/s]Extractor Estimating: 127it [01:56,  1.54it/s]Extractor Estimating: 128it [01:57,  1.52it/s]Extractor Estimating: 129it [01:57,  1.54it/s]Extractor Estimating: 130it [01:58,  1.50it/s]Extractor Estimating: 131it [01:59,  1.52it/s]Extractor Estimating: 132it [01:59,  1.47it/s]Extractor Estimating: 133it [02:00,  1.49it/s]Extractor Estimating: 134it [02:01,  1.47it/s]Extractor Estimating: 135it [02:01,  1.47it/s]Extractor Estimating: 136it [02:02,  1.46it/s]Extractor Estimating: 137it [02:03,  1.46it/s]Extractor Estimating: 138it [02:03,  1.47it/s]Extractor Estimating: 139it [02:04,  1.45it/s]Extractor Estimating: 140it [02:05,  1.48it/s]Extractor Estimating: 141it [02:05,  1.50it/s]Extractor Estimating: 142it [02:06,  1.48it/s]Extractor Estimating: 143it [02:07,  1.54it/s]Extractor Estimating: 144it [02:07,  1.50it/s]Extractor Estimating: 145it [02:08,  1.47it/s]Extractor Estimating: 146it [02:09,  1.43it/s]Extractor Estimating: 147it [02:10,  1.42it/s]Extractor Estimating: 148it [02:10,  1.41it/s]Extractor Estimating: 149it [02:11,  1.35it/s]Extractor Estimating: 150it [02:12,  1.41it/s]Extractor Estimating: 151it [02:12,  1.47it/s]Extractor Estimating: 152it [02:13,  1.52it/s]Extractor Estimating: 153it [02:14,  1.59it/s]Extractor Estimating: 154it [02:14,  1.60it/s]Extractor Estimating: 155it [02:15,  1.61it/s]Extractor Estimating: 156it [02:15,  1.64it/s]Extractor Estimating: 157it [02:16,  1.67it/s]Extractor Estimating: 158it [02:17,  1.67it/s]Extractor Estimating: 159it [02:17,  1.73it/s]Extractor Estimating: 160it [02:18,  1.71it/s]Extractor Estimating: 161it [02:18,  1.67it/s]Extractor Estimating: 162it [02:19,  1.59it/s]Extractor Estimating: 163it [02:20,  1.55it/s]Extractor Estimating: 164it [02:20,  1.58it/s]Extractor Estimating: 165it [02:21,  1.62it/s]Extractor Estimating: 166it [02:22,  1.57it/s]Extractor Estimating: 167it [02:22,  1.58it/s]Extractor Estimating: 168it [02:23,  1.59it/s]Extractor Estimating: 169it [02:23,  1.58it/s]Extractor Estimating: 170it [02:24,  1.60it/s]Extractor Estimating: 171it [02:25,  1.59it/s]Extractor Estimating: 172it [02:25,  1.61it/s]Extractor Estimating: 173it [02:26,  1.65it/s]Extractor Estimating: 174it [02:27,  1.63it/s]Extractor Estimating: 175it [02:27,  1.64it/s]Extractor Estimating: 176it [02:28,  1.65it/s]Extractor Estimating: 177it [02:28,  1.63it/s]Extractor Estimating: 178it [02:29,  1.60it/s]Extractor Estimating: 179it [02:30,  1.49it/s]Extractor Estimating: 180it [02:30,  1.55it/s]Extractor Estimating: 181it [02:31,  1.60it/s]Extractor Estimating: 182it [02:32,  1.62it/s]Extractor Estimating: 183it [02:32,  1.59it/s]Extractor Estimating: 184it [02:33,  1.58it/s]Extractor Estimating: 185it [02:33,  1.63it/s]Extractor Estimating: 186it [02:34,  1.61it/s]Extractor Estimating: 187it [02:35,  1.64it/s]Extractor Estimating: 188it [02:35,  1.58it/s]Extractor Estimating: 189it [02:36,  1.57it/s]Extractor Estimating: 190it [02:37,  1.59it/s]Extractor Estimating: 191it [02:37,  1.62it/s]Extractor Estimating: 192it [02:38,  1.67it/s]Extractor Estimating: 193it [02:38,  1.72it/s]Extractor Estimating: 194it [02:39,  1.68it/s]Extractor Estimating: 195it [02:40,  1.62it/s]Extractor Estimating: 196it [02:40,  1.53it/s]Extractor Estimating: 197it [02:41,  1.62it/s]Extractor Estimating: 198it [02:41,  1.65it/s]Extractor Estimating: 199it [02:42,  1.67it/s]Extractor Estimating: 200it [02:43,  1.65it/s]Extractor Estimating: 201it [02:43,  1.66it/s]Extractor Estimating: 202it [02:44,  1.62it/s]Extractor Estimating: 203it [02:44,  1.61it/s]Extractor Estimating: 204it [02:45,  1.59it/s]Extractor Estimating: 205it [02:46,  1.56it/s]Extractor Estimating: 206it [02:46,  1.54it/s]Extractor Estimating: 207it [02:47,  1.46it/s]Extractor Estimating: 208it [02:48,  1.45it/s]Extractor Estimating: 209it [02:49,  1.51it/s]Extractor Estimating: 210it [02:49,  1.53it/s]Extractor Estimating: 211it [02:50,  1.44it/s]Extractor Estimating: 212it [02:51,  1.51it/s]Extractor Estimating: 213it [02:51,  1.48it/s]Extractor Estimating: 214it [02:52,  1.50it/s]Extractor Estimating: 215it [02:53,  1.51it/s]Extractor Estimating: 216it [02:53,  1.58it/s]Extractor Estimating: 217it [02:54,  1.61it/s]Extractor Estimating: 218it [02:54,  1.54it/s]Extractor Estimating: 219it [02:55,  1.52it/s]Extractor Estimating: 220it [02:56,  1.48it/s]Extractor Estimating: 221it [02:56,  1.51it/s]Extractor Estimating: 222it [02:57,  1.53it/s]Extractor Estimating: 223it [02:58,  1.43it/s]Extractor Estimating: 224it [02:59,  1.47it/s]Extractor Estimating: 225it [02:59,  1.45it/s]Extractor Estimating: 226it [03:00,  1.46it/s]Extractor Estimating: 227it [03:01,  1.47it/s]Extractor Estimating: 228it [03:01,  1.37it/s]Extractor Estimating: 229it [03:02,  1.41it/s]Extractor Estimating: 230it [03:03,  1.49it/s]Extractor Estimating: 231it [03:03,  1.54it/s]Extractor Estimating: 232it [03:04,  1.53it/s]Extractor Estimating: 233it [03:05,  1.55it/s]Extractor Estimating: 234it [03:05,  1.51it/s]Extractor Estimating: 235it [03:06,  1.54it/s]Extractor Estimating: 236it [03:06,  1.56it/s]Extractor Estimating: 237it [03:07,  1.59it/s]Extractor Estimating: 238it [03:08,  1.58it/s]Extractor Estimating: 239it [03:08,  1.56it/s]Extractor Estimating: 240it [03:09,  1.51it/s]Extractor Estimating: 241it [03:10,  1.52it/s]Extractor Estimating: 242it [03:10,  1.52it/s]Extractor Estimating: 243it [03:11,  1.48it/s]Extractor Estimating: 244it [03:12,  1.45it/s]Extractor Estimating: 245it [03:13,  1.45it/s]Extractor Estimating: 246it [03:13,  1.51it/s]Extractor Estimating: 247it [03:14,  1.47it/s]Extractor Estimating: 248it [03:15,  1.44it/s]Extractor Estimating: 249it [03:15,  1.46it/s]Extractor Estimating: 250it [03:16,  1.47it/s]Extractor Estimating: 251it [03:16,  1.55it/s]Extractor Estimating: 252it [03:17,  1.59it/s]Extractor Estimating: 253it [03:18,  1.50it/s]Extractor Estimating: 254it [03:19,  1.47it/s]Extractor Estimating: 255it [03:19,  1.50it/s]Extractor Estimating: 256it [03:20,  1.54it/s]Extractor Estimating: 257it [03:20,  1.54it/s]Extractor Estimating: 258it [03:21,  1.56it/s]Extractor Estimating: 259it [03:22,  1.56it/s]Extractor Estimating: 260it [03:22,  1.60it/s]Extractor Estimating: 261it [03:23,  1.55it/s]Extractor Estimating: 262it [03:24,  1.59it/s]Extractor Estimating: 263it [03:24,  1.53it/s]Extractor Estimating: 264it [03:25,  1.51it/s]Extractor Estimating: 265it [03:26,  1.53it/s]Extractor Estimating: 266it [03:26,  1.53it/s]Extractor Estimating: 267it [03:27,  1.55it/s]Extractor Estimating: 268it [03:28,  1.53it/s]Extractor Estimating: 269it [03:28,  1.48it/s]Extractor Estimating: 270it [03:29,  1.45it/s]Extractor Estimating: 271it [03:30,  1.46it/s]Extractor Estimating: 272it [03:30,  1.50it/s]Extractor Estimating: 273it [03:31,  1.50it/s]Extractor Estimating: 274it [03:32,  1.45it/s]Extractor Estimating: 275it [03:32,  1.49it/s]Extractor Estimating: 276it [03:33,  1.53it/s]Extractor Estimating: 277it [03:34,  1.50it/s]Extractor Estimating: 278it [03:34,  1.52it/s]Extractor Estimating: 279it [03:35,  1.48it/s]Extractor Estimating: 280it [03:36,  1.44it/s]Extractor Estimating: 281it [03:36,  1.40it/s]Extractor Estimating: 282it [03:37,  1.45it/s]Extractor Estimating: 283it [03:38,  1.50it/s]Extractor Estimating: 284it [03:39,  1.36it/s]Extractor Estimating: 285it [03:39,  1.39it/s]Extractor Estimating: 286it [03:40,  1.44it/s]Extractor Estimating: 287it [03:41,  1.49it/s]Extractor Estimating: 288it [03:41,  1.52it/s]Extractor Estimating: 289it [03:42,  1.54it/s]Extractor Estimating: 290it [03:43,  1.47it/s]Extractor Estimating: 291it [03:43,  1.50it/s]Extractor Estimating: 292it [03:44,  1.49it/s]Extractor Estimating: 293it [03:45,  1.53it/s]Extractor Estimating: 294it [03:45,  1.48it/s]Extractor Estimating: 295it [03:46,  1.52it/s]Extractor Estimating: 296it [03:47,  1.49it/s]Extractor Estimating: 297it [03:47,  1.53it/s]Extractor Estimating: 298it [03:48,  1.52it/s]Extractor Estimating: 299it [03:49,  1.51it/s]Extractor Estimating: 300it [03:49,  1.36it/s]Extractor Estimating: 301it [03:50,  1.39it/s]Extractor Estimating: 302it [03:51,  1.43it/s]Extractor Estimating: 303it [03:51,  1.43it/s]Extractor Estimating: 304it [03:52,  1.44it/s]Extractor Estimating: 305it [03:53,  1.42it/s]Extractor Estimating: 306it [03:53,  1.47it/s]Extractor Estimating: 307it [03:54,  1.48it/s]Extractor Estimating: 308it [03:55,  1.45it/s]Extractor Estimating: 309it [03:55,  1.51it/s]Extractor Estimating: 310it [03:56,  1.47it/s]Extractor Estimating: 311it [03:57,  1.52it/s]Extractor Estimating: 312it [03:57,  1.54it/s]Extractor Estimating: 313it [03:58,  1.51it/s]Extractor Estimating: 314it [03:59,  1.55it/s]Extractor Estimating: 315it [03:59,  1.56it/s]Extractor Estimating: 316it [04:00,  1.54it/s]Extractor Estimating: 317it [04:01,  1.56it/s]Extractor Estimating: 318it [04:01,  1.54it/s]Extractor Estimating: 319it [04:02,  1.58it/s]Extractor Estimating: 320it [04:03,  1.60it/s]Extractor Estimating: 321it [04:03,  1.51it/s]Extractor Estimating: 322it [04:04,  1.55it/s]Extractor Estimating: 323it [04:04,  1.59it/s]Extractor Estimating: 324it [04:05,  1.58it/s]Extractor Estimating: 325it [04:06,  1.58it/s]Extractor Estimating: 326it [04:06,  1.62it/s]Extractor Estimating: 327it [04:07,  1.63it/s]Extractor Estimating: 328it [04:08,  1.65it/s]Extractor Estimating: 329it [04:08,  1.65it/s]Extractor Estimating: 330it [04:09,  1.59it/s]Extractor Estimating: 331it [04:09,  1.56it/s]Extractor Estimating: 332it [04:10,  1.53it/s]Extractor Estimating: 333it [04:11,  1.58it/s]Extractor Estimating: 334it [04:11,  1.56it/s]Extractor Estimating: 335it [04:12,  1.55it/s]Extractor Estimating: 336it [04:13,  1.62it/s]Extractor Estimating: 337it [04:13,  1.67it/s]Extractor Estimating: 338it [04:14,  1.67it/s]Extractor Estimating: 339it [04:15,  1.17it/s]Extractor Estimating: 340it [04:16,  1.27it/s]Extractor Estimating: 341it [04:16,  1.41it/s]Extractor Estimating: 342it [04:17,  1.47it/s]Extractor Estimating: 343it [04:18,  1.55it/s]Extractor Estimating: 344it [04:18,  1.57it/s]Extractor Estimating: 345it [04:19,  1.63it/s]Extractor Estimating: 346it [04:19,  1.56it/s]Extractor Estimating: 347it [04:20,  1.55it/s]Extractor Estimating: 348it [04:21,  1.57it/s]Extractor Estimating: 349it [04:21,  1.50it/s]Extractor Estimating: 350it [04:22,  1.52it/s]Extractor Estimating: 351it [04:23,  1.52it/s]Extractor Estimating: 352it [04:23,  1.56it/s]Extractor Estimating: 353it [04:24,  1.59it/s]Extractor Estimating: 354it [04:25,  1.54it/s]Extractor Estimating: 355it [04:25,  1.56it/s]Extractor Estimating: 356it [04:26,  1.62it/s]Extractor Estimating: 357it [04:27,  1.56it/s]Extractor Estimating: 358it [04:27,  1.55it/s]Extractor Estimating: 359it [04:28,  1.58it/s]Extractor Estimating: 360it [04:28,  1.56it/s]Extractor Estimating: 361it [04:29,  1.47it/s]Extractor Estimating: 362it [04:30,  1.50it/s]Extractor Estimating: 363it [04:31,  1.48it/s]Extractor Estimating: 364it [04:31,  1.49it/s]Extractor Estimating: 365it [04:32,  1.49it/s]Extractor Estimating: 366it [04:33,  1.49it/s]Extractor Estimating: 367it [04:33,  1.47it/s]Extractor Estimating: 368it [04:34,  1.47it/s]Extractor Estimating: 369it [04:35,  1.52it/s]Extractor Estimating: 370it [04:35,  1.49it/s]Extractor Estimating: 371it [04:36,  1.52it/s]Extractor Estimating: 372it [04:36,  1.55it/s]Extractor Estimating: 373it [04:37,  1.54it/s]Extractor Estimating: 374it [04:38,  1.60it/s]Extractor Estimating: 375it [04:38,  1.61it/s]Extractor Estimating: 376it [04:39,  1.54it/s]Extractor Estimating: 377it [04:40,  1.49it/s]Extractor Estimating: 378it [04:40,  1.48it/s]Extractor Estimating: 379it [04:41,  1.48it/s]Extractor Estimating: 380it [04:42,  1.55it/s]Extractor Estimating: 381it [04:42,  1.58it/s]Extractor Estimating: 382it [04:43,  1.50it/s]Extractor Estimating: 383it [04:44,  1.51it/s]Extractor Estimating: 384it [04:44,  1.58it/s]Extractor Estimating: 385it [04:45,  1.54it/s]Extractor Estimating: 386it [04:46,  1.55it/s]Extractor Estimating: 387it [04:47,  1.36it/s]Extractor Estimating: 388it [04:47,  1.44it/s]Extractor Estimating: 389it [04:48,  1.44it/s]Extractor Estimating: 390it [04:48,  1.52it/s]Extractor Estimating: 391it [04:49,  1.51it/s]Extractor Estimating: 392it [04:50,  1.49it/s]Extractor Estimating: 393it [04:50,  1.54it/s]Extractor Estimating: 394it [04:51,  1.54it/s]Extractor Estimating: 395it [04:52,  1.54it/s]Extractor Estimating: 396it [04:52,  1.53it/s]Extractor Estimating: 397it [04:53,  1.57it/s]Extractor Estimating: 398it [04:54,  1.56it/s]Extractor Estimating: 399it [04:54,  1.53it/s]Extractor Estimating: 400it [04:55,  1.55it/s]Extractor Estimating: 401it [04:56,  1.52it/s]Extractor Estimating: 402it [04:56,  1.57it/s]Extractor Estimating: 403it [04:57,  1.54it/s]Extractor Estimating: 404it [04:57,  1.53it/s]Extractor Estimating: 405it [04:58,  1.58it/s]Extractor Estimating: 406it [04:59,  1.57it/s]Extractor Estimating: 407it [04:59,  1.57it/s]Extractor Estimating: 408it [05:00,  1.61it/s]Extractor Estimating: 409it [05:01,  1.60it/s]Extractor Estimating: 410it [05:01,  1.58it/s]Extractor Estimating: 411it [05:02,  1.55it/s]Extractor Estimating: 412it [05:03,  1.47it/s]Extractor Estimating: 413it [05:03,  1.51it/s]Extractor Estimating: 414it [05:04,  1.55it/s]Extractor Estimating: 415it [05:04,  1.60it/s]Extractor Estimating: 416it [05:05,  1.57it/s]Extractor Estimating: 417it [05:06,  1.60it/s]Extractor Estimating: 418it [05:06,  1.57it/s]Extractor Estimating: 419it [05:07,  1.54it/s]Extractor Estimating: 420it [05:08,  1.59it/s]Extractor Estimating: 421it [05:08,  1.49it/s]Extractor Estimating: 422it [05:09,  1.48it/s]Extractor Estimating: 423it [05:10,  1.52it/s]Extractor Estimating: 424it [05:10,  1.50it/s]Extractor Estimating: 425it [05:11,  1.58it/s]Extractor Estimating: 426it [05:12,  1.54it/s]Extractor Estimating: 427it [05:12,  1.52it/s]Extractor Estimating: 428it [05:13,  1.48it/s]Extractor Estimating: 429it [05:14,  1.45it/s]Extractor Estimating: 430it [05:14,  1.43it/s]Extractor Estimating: 431it [05:15,  1.50it/s]Extractor Estimating: 432it [05:16,  1.43it/s]Extractor Estimating: 433it [05:16,  1.49it/s]Extractor Estimating: 434it [05:17,  1.47it/s]Extractor Estimating: 435it [05:18,  1.44it/s]Extractor Estimating: 436it [05:19,  1.40it/s]Extractor Estimating: 437it [05:19,  1.40it/s]Extractor Estimating: 438it [05:20,  1.46it/s]Extractor Estimating: 439it [05:21,  1.45it/s]Extractor Estimating: 440it [05:21,  1.47it/s]Extractor Estimating: 441it [05:22,  1.45it/s]Extractor Estimating: 442it [05:23,  1.48it/s]Extractor Estimating: 443it [05:23,  1.54it/s]Extractor Estimating: 444it [05:24,  1.53it/s]Extractor Estimating: 445it [05:25,  1.53it/s]Extractor Estimating: 446it [05:25,  1.54it/s]Extractor Estimating: 447it [05:26,  1.54it/s]Extractor Estimating: 448it [05:27,  1.54it/s]Extractor Estimating: 449it [05:27,  1.50it/s]Extractor Estimating: 450it [05:28,  1.49it/s]Extractor Estimating: 451it [05:29,  1.51it/s]Extractor Estimating: 452it [05:29,  1.53it/s]Extractor Estimating: 453it [05:30,  1.59it/s]Extractor Estimating: 454it [05:30,  1.59it/s]Extractor Estimating: 455it [05:31,  1.65it/s]Extractor Estimating: 456it [05:32,  1.58it/s]Extractor Estimating: 457it [05:32,  1.62it/s]Extractor Estimating: 458it [05:33,  1.65it/s]Extractor Estimating: 459it [05:33,  1.69it/s]Extractor Estimating: 460it [05:34,  1.66it/s]Extractor Estimating: 461it [05:35,  1.65it/s]Extractor Estimating: 462it [05:35,  1.67it/s]Extractor Estimating: 463it [05:36,  1.72it/s]Extractor Estimating: 464it [05:36,  1.73it/s]Extractor Estimating: 465it [05:37,  1.72it/s]Extractor Estimating: 466it [05:37,  1.72it/s]Extractor Estimating: 467it [05:38,  1.71it/s]Extractor Estimating: 468it [05:39,  1.71it/s]Extractor Estimating: 469it [05:39,  1.72it/s]Extractor Estimating: 470it [05:40,  1.71it/s]Extractor Estimating: 471it [05:40,  1.71it/s]Extractor Estimating: 472it [05:41,  1.65it/s]Extractor Estimating: 473it [05:42,  1.59it/s]Extractor Estimating: 474it [05:42,  1.63it/s]Extractor Estimating: 475it [05:43,  1.58it/s]Extractor Estimating: 476it [05:44,  1.62it/s]Extractor Estimating: 477it [05:44,  1.53it/s]Extractor Estimating: 478it [05:45,  1.53it/s]Extractor Estimating: 479it [05:46,  1.48it/s]Extractor Estimating: 480it [05:46,  1.51it/s]Extractor Estimating: 481it [05:47,  1.41it/s]Extractor Estimating: 482it [05:48,  1.45it/s]Extractor Estimating: 483it [05:48,  1.44it/s]Extractor Estimating: 484it [05:49,  1.48it/s]Extractor Estimating: 485it [05:50,  1.50it/s]Extractor Estimating: 486it [05:50,  1.54it/s]Extractor Estimating: 487it [05:51,  1.58it/s]Extractor Estimating: 488it [05:52,  1.59it/s]Extractor Estimating: 489it [05:52,  1.54it/s]Extractor Estimating: 490it [05:53,  1.46it/s]Extractor Estimating: 491it [05:54,  1.46it/s]Extractor Estimating: 492it [05:54,  1.48it/s]Extractor Estimating: 493it [05:55,  1.47it/s]Extractor Estimating: 494it [05:56,  1.48it/s]Extractor Estimating: 495it [05:56,  1.52it/s]Extractor Estimating: 496it [05:57,  1.48it/s]Extractor Estimating: 497it [05:58,  1.50it/s]Extractor Estimating: 498it [05:58,  1.48it/s]Extractor Estimating: 499it [05:59,  1.56it/s]Extractor Estimating: 500it [06:00,  1.65it/s]Extractor Estimating: 500it [06:00,  1.39it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 10261 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 31556
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31656, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31656, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.323, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.018, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.017, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 72, avg_time 1.016, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 172, avg_time 2.087, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 272, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 372, avg_time 1.033, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 44, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 144, avg_time 1.012, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 244, avg_time 2.079, loss:nan
g_step 1200, step 344, avg_time 1.013, loss:nan
g_step 1300, step 16, avg_time 1.028, loss:nan
g_step 1400, step 116, avg_time 1.047, loss:nan
g_step 1500, step 216, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.082, loss:nan
g_step 1700, step 416, avg_time 1.011, loss:nan
g_step 1800, step 88, avg_time 1.034, loss:nan
g_step 1900, step 188, avg_time 1.021, loss:nan
g_step 2000, step 288, avg_time 1.027, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 388, avg_time 2.078, loss:nan
g_step 2200, step 60, avg_time 1.019, loss:nan
g_step 2300, step 160, avg_time 1.028, loss:nan
g_step 2400, step 260, avg_time 1.033, loss:nan
g_step 2500, step 360, avg_time 1.008, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.066, loss:nan
g_step 2700, step 132, avg_time 1.027, loss:nan
g_step 2800, step 232, avg_time 1.025, loss:nan
g_step 2900, step 332, avg_time 1.031, loss:nan
g_step 3000, step 4, avg_time 1.020, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 104, avg_time 2.086, loss:nan
g_step 3200, step 204, avg_time 1.017, loss:nan
g_step 3300, step 304, avg_time 1.024, loss:nan
g_step 3400, step 404, avg_time 1.017, loss:nan
g_step 3500, step 76, avg_time 1.027, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 176, avg_time 2.074, loss:nan
g_step 3700, step 276, avg_time 1.014, loss:nan
g_step 3800, step 376, avg_time 1.026, loss:nan
g_step 3900, step 48, avg_time 1.022, loss:nan
g_step 4000, step 148, avg_time 1.038, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.082, loss:nan
g_step 4200, step 348, avg_time 1.020, loss:nan
g_step 4300, step 20, avg_time 1.010, loss:nan
g_step 4400, step 120, avg_time 1.026, loss:nan
g_step 4500, step 220, avg_time 1.009, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 320, avg_time 2.090, loss:nan
g_step 4700, step 420, avg_time 1.025, loss:nan
g_step 4800, step 92, avg_time 1.021, loss:nan
g_step 4900, step 192, avg_time 1.025, loss:nan
g_step 5000, step 292, avg_time 1.030, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 392, avg_time 2.077, loss:nan
g_step 5200, step 64, avg_time 1.012, loss:nan
g_step 5300, step 164, avg_time 1.038, loss:nan
g_step 5400, step 264, avg_time 1.021, loss:nan
g_step 5500, step 364, avg_time 1.014, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 36, avg_time 2.076, loss:nan
g_step 5700, step 136, avg_time 1.018, loss:nan
g_step 5800, step 236, avg_time 1.025, loss:nan
g_step 5900, step 336, avg_time 1.014, loss:nan
g_step 6000, step 8, avg_time 1.030, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 108, avg_time 2.086, loss:nan
g_step 6200, step 208, avg_time 1.031, loss:nan
g_step 6300, step 308, avg_time 1.010, loss:nan
g_step 6400, step 408, avg_time 1.019, loss:nan
g_step 6500, step 80, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 180, avg_time 2.090, loss:nan
g_step 6700, step 280, avg_time 1.024, loss:nan
g_step 6800, step 380, avg_time 1.034, loss:nan
g_step 6900, step 52, avg_time 1.005, loss:nan
g_step 7000, step 152, avg_time 1.038, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 252, avg_time 2.077, loss:nan
g_step 7200, step 352, avg_time 1.032, loss:nan
g_step 7300, step 24, avg_time 1.019, loss:nan
g_step 7400, step 124, avg_time 1.017, loss:nan
g_step 7500, step 224, avg_time 1.017, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 324, avg_time 2.072, loss:nan
g_step 7700, step 424, avg_time 1.034, loss:nan
g_step 7800, step 96, avg_time 1.022, loss:nan
g_step 7900, step 196, avg_time 1.018, loss:nan
g_step 8000, step 296, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 396, avg_time 2.084, loss:nan
g_step 8200, step 68, avg_time 1.004, loss:nan
g_step 8300, step 168, avg_time 1.004, loss:nan
g_step 8400, step 268, avg_time 1.024, loss:nan
g_step 8500, step 368, avg_time 1.040, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:21:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:21:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-21-12_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:21:13 - WARNING - datasets.builder -   Using custom data configuration default-532bbcd6d727170c
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-532bbcd6d727170c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:21:15,867 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:21:15,868 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:21:15,869 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:21:15,870 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:21:15,933 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:21:16,007 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:21:16,276 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:21:19,420 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:21:19,440 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-532bbcd6d727170c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 03:21:19 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14c7f8640200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.66ba/s] 18%|        | 2/11 [00:00<00:02,  3.56ba/s] 27%|       | 3/11 [00:00<00:02,  3.94ba/s] 36%|      | 4/11 [00:01<00:01,  4.15ba/s] 45%|     | 5/11 [00:01<00:01,  4.27ba/s] 55%|    | 6/11 [00:01<00:01,  4.36ba/s] 64%|   | 7/11 [00:01<00:00,  4.42ba/s] 73%|  | 8/11 [00:01<00:00,  4.46ba/s] 82%| | 9/11 [00:02<00:00,  4.49ba/s] 91%| | 10/11 [00:02<00:00,  4.49ba/s]100%|| 11/11 [00:02<00:00,  4.52ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.28ba/s] 50%|     | 2/4 [00:00<00:00,  3.88ba/s] 75%|  | 3/4 [00:00<00:00,  4.13ba/s]100%|| 4/4 [00:01<00:00,  4.09ba/s]100%|| 4/4 [00:01<00:00,  4.00ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.59ba/s] 27%|       | 3/11 [00:00<00:01,  5.90ba/s] 45%|     | 5/11 [00:00<00:00,  7.61ba/s] 64%|   | 7/11 [00:00<00:00,  8.70ba/s] 82%| | 9/11 [00:01<00:00,  9.32ba/s]100%|| 11/11 [00:01<00:00, 10.91ba/s]100%|| 11/11 [00:01<00:00,  8.70ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.85ba/s] 75%|  | 3/4 [00:00<00:00,  8.74ba/s]100%|| 4/4 [00:00<00:00,  9.80ba/s]
[INFO|trainer.py:414] 2023-08-28 03:21:26,004 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:21:26,013 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:21:26,013 >>   Num examples = 10320
[INFO|trainer.py:1149] 2023-08-28 03:21:26,013 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:21:26,013 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:21:26,013 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:21:26,013 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:21:26,013 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:00<03:44,  3.58it/s]  0%|          | 2/805 [00:00<03:41,  3.62it/s]  0%|          | 3/805 [00:00<03:40,  3.63it/s]  0%|          | 4/805 [00:01<03:40,  3.64it/s]  1%|          | 5/805 [00:01<03:41,  3.62it/s]  1%|          | 6/805 [00:01<03:41,  3.61it/s]  1%|          | 7/805 [00:01<03:41,  3.60it/s]  1%|          | 8/805 [00:02<03:41,  3.60it/s]  1%|          | 9/805 [00:02<03:41,  3.59it/s]  1%|          | 10/805 [00:02<03:41,  3.59it/s]  1%|         | 11/805 [00:03<03:47,  3.50it/s]  1%|         | 12/805 [00:03<03:45,  3.52it/s]  2%|         | 13/805 [00:03<03:43,  3.54it/s]  2%|         | 14/805 [00:03<03:42,  3.56it/s]  2%|         | 15/805 [00:04<03:41,  3.57it/s]  2%|         | 16/805 [00:04<03:40,  3.58it/s]  2%|         | 17/805 [00:04<03:40,  3.58it/s]  2%|         | 18/805 [00:05<03:39,  3.59it/s]  2%|         | 19/805 [00:05<03:39,  3.59it/s]  2%|         | 20/805 [00:05<03:38,  3.59it/s]  3%|         | 21/805 [00:05<03:38,  3.59it/s]  3%|         | 22/805 [00:06<03:44,  3.49it/s]  3%|         | 23/805 [00:06<03:42,  3.52it/s]  3%|         | 24/805 [00:06<03:40,  3.54it/s]  3%|         | 25/805 [00:07<03:39,  3.55it/s]  3%|         | 26/805 [00:07<03:38,  3.56it/s]  3%|         | 27/805 [00:07<03:38,  3.57it/s]  3%|         | 28/805 [00:07<03:37,  3.57it/s]  4%|         | 29/805 [00:08<03:37,  3.57it/s]  4%|         | 30/805 [00:08<03:36,  3.57it/s]  4%|         | 31/805 [00:08<03:36,  3.58it/s]  4%|         | 32/805 [00:08<03:36,  3.58it/s]  4%|         | 33/805 [00:09<03:38,  3.53it/s]  4%|         | 34/805 [00:09<03:37,  3.54it/s]  4%|         | 35/805 [00:09<03:36,  3.56it/s]  4%|         | 36/805 [00:10<03:35,  3.57it/s]  5%|         | 37/805 [00:10<03:34,  3.57it/s]  5%|         | 38/805 [00:10<03:34,  3.58it/s]  5%|         | 39/805 [00:10<03:34,  3.58it/s]  5%|         | 40/805 [00:11<03:33,  3.58it/s]  5%|         | 41/805 [00:11<03:33,  3.58it/s]  5%|         | 42/805 [00:11<03:33,  3.58it/s]  5%|         | 43/805 [00:12<03:32,  3.58it/s]  5%|         | 44/805 [00:12<03:37,  3.51it/s]  6%|         | 45/805 [00:12<03:35,  3.53it/s]  6%|         | 46/805 [00:12<03:34,  3.54it/s]  6%|         | 47/805 [00:13<03:33,  3.55it/s]  6%|         | 48/805 [00:13<03:32,  3.56it/s]  6%|         | 49/805 [00:13<03:32,  3.56it/s]  6%|         | 50/805 [00:14<03:31,  3.57it/s]  6%|         | 51/805 [00:14<03:31,  3.57it/s]  6%|         | 52/805 [00:14<03:30,  3.57it/s]  7%|         | 53/805 [00:14<03:30,  3.57it/s]  7%|         | 54/805 [00:15<03:30,  3.57it/s]  7%|         | 55/805 [00:15<03:29,  3.57it/s]  7%|         | 56/805 [00:15<03:29,  3.57it/s]  7%|         | 57/805 [00:15<03:29,  3.57it/s]  7%|         | 58/805 [00:16<03:29,  3.57it/s]  7%|         | 59/805 [00:16<03:29,  3.57it/s]  7%|         | 60/805 [00:16<03:28,  3.57it/s]  8%|         | 61/805 [00:17<03:32,  3.50it/s]  8%|         | 62/805 [00:17<03:34,  3.47it/s]  8%|         | 63/805 [00:17<03:32,  3.49it/s]  8%|         | 64/805 [00:17<03:35,  3.44it/s]  8%|         | 65/805 [00:18<03:32,  3.48it/s]  8%|         | 66/805 [00:18<03:30,  3.51it/s]  8%|         | 67/805 [00:18<03:29,  3.53it/s]  8%|         | 68/805 [00:19<03:27,  3.54it/s]  9%|         | 69/805 [00:19<03:27,  3.55it/s]  9%|         | 70/805 [00:19<03:26,  3.56it/s]  9%|         | 71/805 [00:19<03:26,  3.56it/s]  9%|         | 72/805 [00:20<03:24,  3.58it/s]  9%|         | 73/805 [00:20<03:23,  3.60it/s]  9%|         | 74/805 [00:20<03:22,  3.61it/s]  9%|         | 75/805 [00:21<03:21,  3.62it/s]  9%|         | 76/805 [00:21<03:21,  3.63it/s] 10%|         | 77/805 [00:21<03:20,  3.63it/s] 10%|         | 78/805 [00:21<03:20,  3.63it/s] 10%|         | 79/805 [00:22<03:19,  3.63it/s] 10%|         | 80/805 [00:22<03:19,  3.64it/s] 10%|         | 81/805 [00:22<03:18,  3.64it/s] 10%|         | 82/805 [00:23<03:34,  3.37it/s] 10%|         | 83/805 [00:23<03:29,  3.45it/s] 10%|         | 84/805 [00:23<03:25,  3.51it/s] 11%|         | 85/805 [00:23<03:23,  3.54it/s] 11%|         | 86/805 [00:24<03:21,  3.57it/s] 11%|         | 87/805 [00:24<03:19,  3.59it/s] 11%|         | 88/805 [00:24<03:18,  3.61it/s] 11%|         | 89/805 [00:24<03:18,  3.61it/s] 11%|         | 90/805 [00:25<03:17,  3.62it/s] 11%|        | 91/805 [00:25<03:16,  3.63it/s] 11%|        | 92/805 [00:25<03:16,  3.63it/s] 12%|        | 93/805 [00:26<03:16,  3.63it/s] 12%|        | 94/805 [00:26<03:15,  3.63it/s] 12%|        | 95/805 [00:26<03:15,  3.63it/s] 12%|        | 96/805 [00:26<03:15,  3.64it/s] 12%|        | 97/805 [00:27<03:14,  3.63it/s] 12%|        | 98/805 [00:27<03:14,  3.63it/s] 12%|        | 99/805 [00:27<03:14,  3.64it/s] 12%|        | 100/805 [00:28<03:22,  3.47it/s] 13%|        | 101/805 [00:28<03:20,  3.52it/s] 13%|        | 102/805 [00:28<03:18,  3.55it/s] 13%|        | 103/805 [00:28<03:16,  3.57it/s] 13%|        | 104/805 [00:29<03:15,  3.59it/s] 13%|        | 105/805 [00:29<03:14,  3.60it/s] 13%|        | 106/805 [00:29<03:13,  3.61it/s] 13%|        | 107/805 [00:29<03:13,  3.61it/s] 13%|        | 108/805 [00:30<03:12,  3.61it/s] 14%|        | 109/805 [00:30<03:12,  3.62it/s] 14%|        | 110/805 [00:30<03:11,  3.62it/s] 14%|        | 111/805 [00:31<03:21,  3.45it/s] 14%|        | 112/805 [00:31<03:17,  3.50it/s] 14%|        | 113/805 [00:31<03:15,  3.54it/s] 14%|        | 114/805 [00:31<03:13,  3.57it/s] 14%|        | 115/805 [00:32<03:12,  3.59it/s] 14%|        | 116/805 [00:32<03:11,  3.60it/s] 15%|        | 117/805 [00:32<03:10,  3.61it/s] 15%|        | 118/805 [00:33<03:10,  3.61it/s] 15%|        | 119/805 [00:33<03:09,  3.61it/s] 15%|        | 120/805 [00:33<03:09,  3.62it/s] 15%|        | 121/805 [00:33<03:08,  3.62it/s] 15%|        | 122/805 [00:34<03:11,  3.57it/s] 15%|        | 123/805 [00:34<03:09,  3.59it/s] 15%|        | 124/805 [00:34<03:09,  3.60it/s] 16%|        | 125/805 [00:34<03:08,  3.61it/s] 16%|        | 126/805 [00:35<03:07,  3.61it/s] 16%|        | 127/805 [00:35<03:07,  3.62it/s] 16%|        | 128/805 [00:35<03:06,  3.62it/s] 16%|        | 129/805 [00:36<03:06,  3.63it/s] 16%|        | 130/805 [00:36<03:06,  3.62it/s] 16%|        | 131/805 [00:36<03:06,  3.62it/s] 16%|        | 132/805 [00:36<03:05,  3.63it/s] 17%|        | 133/805 [00:37<03:08,  3.57it/s] 17%|        | 134/805 [00:37<03:07,  3.59it/s] 17%|        | 135/805 [00:37<03:06,  3.60it/s] 17%|        | 136/805 [00:38<03:05,  3.61it/s] 17%|        | 137/805 [00:38<03:04,  3.62it/s] 17%|        | 138/805 [00:38<03:04,  3.62it/s] 17%|        | 139/805 [00:38<03:04,  3.62it/s] 17%|        | 140/805 [00:39<03:03,  3.62it/s] 18%|        | 141/805 [00:39<03:03,  3.62it/s] 18%|        | 142/805 [00:39<03:02,  3.62it/s] 18%|        | 143/805 [00:39<03:02,  3.63it/s] 18%|        | 144/805 [00:40<03:06,  3.54it/s] 18%|        | 145/805 [00:40<03:05,  3.56it/s] 18%|        | 146/805 [00:40<03:03,  3.59it/s] 18%|        | 147/805 [00:41<03:02,  3.60it/s] 18%|        | 148/805 [00:41<03:02,  3.61it/s] 19%|        | 149/805 [00:41<03:01,  3.61it/s] 19%|        | 150/805 [00:41<03:01,  3.62it/s] 19%|        | 151/805 [00:42<03:00,  3.62it/s] 19%|        | 152/805 [00:42<03:00,  3.62it/s] 19%|        | 153/805 [00:42<02:59,  3.62it/s] 19%|        | 154/805 [00:43<02:59,  3.62it/s] 19%|        | 155/805 [00:43<03:06,  3.48it/s] 19%|        | 156/805 [00:43<03:04,  3.53it/s] 20%|        | 157/805 [00:43<03:02,  3.55it/s] 20%|        | 158/805 [00:44<03:00,  3.57it/s] 20%|        | 159/805 [00:44<02:59,  3.59it/s] 20%|        | 160/805 [00:44<02:58,  3.60it/s] 20%|        | 161/805 [00:44<02:58,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 03:22:11,042 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:22:11,042 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:22:11,042 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.40it/s][A
  3%|         | 12/437 [00:00<00:08, 49.41it/s][A
  4%|         | 17/437 [00:00<00:08, 47.61it/s][A
  5%|         | 22/437 [00:00<00:08, 46.68it/s][A
  6%|         | 27/437 [00:00<00:08, 46.17it/s][A
  7%|         | 32/437 [00:00<00:08, 45.78it/s][A
  8%|         | 37/437 [00:00<00:08, 45.38it/s][A
 10%|         | 42/437 [00:00<00:08, 45.11it/s][A
 11%|         | 47/437 [00:01<00:08, 45.01it/s][A
 12%|        | 52/437 [00:01<00:23, 16.46it/s][A
 13%|        | 57/437 [00:01<00:18, 20.45it/s][A
 14%|        | 62/437 [00:01<00:15, 24.53it/s][A
 15%|        | 67/437 [00:02<00:12, 28.57it/s][A
 16%|        | 72/437 [00:02<00:11, 32.22it/s][A
 18%|        | 77/437 [00:02<00:10, 35.34it/s][A
 19%|        | 82/437 [00:02<00:09, 38.00it/s][A
 20%|        | 87/437 [00:02<00:08, 39.98it/s][A
 21%|        | 92/437 [00:02<00:08, 41.11it/s][A
 22%|       | 97/437 [00:02<00:08, 41.94it/s][A
 23%|       | 102/437 [00:02<00:07, 42.73it/s][A
 24%|       | 107/437 [00:02<00:07, 43.50it/s][A
 26%|       | 112/437 [00:03<00:07, 44.08it/s][A
 27%|       | 117/437 [00:03<00:07, 44.61it/s][A
 28%|       | 122/437 [00:03<00:07, 44.92it/s][A
 29%|       | 127/437 [00:03<00:06, 45.19it/s][A
 30%|       | 132/437 [00:03<00:06, 45.23it/s][A
 31%|      | 137/437 [00:03<00:06, 45.06it/s][A
 32%|      | 142/437 [00:03<00:06, 44.91it/s][A
 34%|      | 147/437 [00:03<00:06, 44.83it/s][A
 35%|      | 152/437 [00:03<00:06, 44.85it/s][A
 36%|      | 157/437 [00:04<00:06, 44.97it/s][A
 37%|      | 162/437 [00:04<00:06, 41.51it/s][A
 38%|      | 167/437 [00:04<00:06, 42.67it/s][A
 39%|      | 172/437 [00:04<00:06, 43.50it/s][A
 41%|      | 177/437 [00:04<00:05, 44.15it/s][A
 42%|     | 182/437 [00:04<00:05, 44.65it/s][A
 43%|     | 187/437 [00:04<00:05, 44.94it/s][A
 44%|     | 192/437 [00:04<00:05, 44.99it/s][A
 45%|     | 197/437 [00:04<00:05, 44.98it/s][A
 46%|     | 202/437 [00:05<00:05, 44.80it/s][A
 47%|     | 207/437 [00:05<00:05, 44.76it/s][A
 49%|     | 212/437 [00:05<00:05, 44.86it/s][A
 50%|     | 217/437 [00:05<00:04, 45.05it/s][A
 51%|     | 222/437 [00:05<00:04, 45.17it/s][A
 52%|    | 227/437 [00:05<00:04, 45.47it/s][A
 53%|    | 232/437 [00:05<00:04, 45.47it/s][A
 54%|    | 237/437 [00:05<00:04, 45.37it/s][A
 55%|    | 242/437 [00:05<00:04, 45.13it/s][A
 57%|    | 247/437 [00:06<00:04, 44.89it/s][A
 58%|    | 252/437 [00:06<00:04, 44.86it/s][A
 59%|    | 257/437 [00:06<00:04, 44.98it/s][A
 60%|    | 262/437 [00:06<00:03, 45.13it/s][A
 61%|    | 267/437 [00:06<00:03, 45.24it/s][A
 62%|   | 272/437 [00:06<00:03, 45.40it/s][A
 63%|   | 277/437 [00:06<00:03, 45.47it/s][A
 65%|   | 282/437 [00:06<00:03, 45.28it/s][A
 66%|   | 287/437 [00:06<00:03, 45.16it/s][A
 67%|   | 292/437 [00:07<00:03, 44.94it/s][A
 68%|   | 297/437 [00:07<00:03, 43.35it/s][A
 69%|   | 302/437 [00:07<00:03, 44.01it/s][A
 70%|   | 307/437 [00:07<00:02, 44.48it/s][A
 71%|  | 312/437 [00:07<00:02, 44.78it/s][A
 73%|  | 317/437 [00:07<00:02, 45.01it/s][A
 74%|  | 322/437 [00:07<00:02, 45.16it/s][A
 75%|  | 327/437 [00:07<00:02, 45.18it/s][A
 76%|  | 332/437 [00:07<00:02, 45.04it/s][A
 77%|  | 337/437 [00:08<00:02, 44.81it/s][A
 78%|  | 342/437 [00:08<00:02, 44.73it/s][A
 79%|  | 347/437 [00:08<00:02, 44.99it/s][A
 81%|  | 352/437 [00:08<00:01, 45.13it/s][A
 82%| | 357/437 [00:08<00:01, 44.95it/s][A
 83%| | 362/437 [00:08<00:01, 45.38it/s][A
 84%| | 367/437 [00:08<00:01, 45.50it/s][A
 85%| | 372/437 [00:08<00:01, 45.36it/s][A
 86%| | 377/437 [00:08<00:01, 45.32it/s][A
 87%| | 382/437 [00:09<00:01, 45.12it/s][A
 89%| | 387/437 [00:09<00:01, 45.14it/s][A
 90%| | 392/437 [00:09<00:00, 45.12it/s][A
 91%| | 397/437 [00:09<00:00, 45.17it/s][A
 92%|| 402/437 [00:09<00:00, 45.23it/s][A
 93%|| 407/437 [00:09<00:00, 45.33it/s][A
 94%|| 412/437 [00:09<00:00, 45.45it/s][A
 95%|| 417/437 [00:09<00:00, 45.31it/s][A
 97%|| 422/437 [00:09<00:00, 45.13it/s][A
 98%|| 427/437 [00:10<00:00, 44.97it/s][A
 99%|| 432/437 [00:10<00:00, 42.91it/s][A
100%|| 437/437 [00:10<00:00, 43.78it/s][A                                                 
                                                 [A 20%|        | 161/805 [00:55<02:58,  3.61it/s]
100%|| 437/437 [00:10<00:00, 43.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:22:22,194 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 03:22:22,410 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:22:26,335 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:22:26,528 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:22:26,617 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [01:02<57:10,  5.33s/it] 20%|        | 163/805 [01:02<40:51,  3.82s/it] 20%|        | 164/805 [01:02<29:26,  2.76s/it] 20%|        | 165/805 [01:02<21:28,  2.01s/it] 21%|        | 166/805 [01:03<15:54,  1.49s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:03<12:00,  1.13s/it] 21%|        | 168/805 [01:03<09:20,  1.14it/s] 21%|        | 169/805 [01:04<07:25,  1.43it/s] 21%|        | 170/805 [01:04<06:03,  1.74it/s] 21%|        | 171/805 [01:04<05:06,  2.07it/s] 21%|       | 172/805 [01:04<04:26,  2.38it/s] 21%|       | 173/805 [01:05<03:58,  2.65it/s] 22%|       | 174/805 [01:05<03:38,  2.88it/s] 22%|       | 175/805 [01:05<03:25,  3.07it/s] 22%|       | 176/805 [01:06<03:15,  3.22it/s] 22%|       | 177/805 [01:06<03:08,  3.34it/s] 22%|       | 178/805 [01:06<03:03,  3.41it/s] 22%|       | 179/805 [01:06<03:06,  3.36it/s] 22%|       | 180/805 [01:07<03:01,  3.44it/s] 22%|       | 181/805 [01:07<02:58,  3.49it/s] 23%|       | 182/805 [01:07<02:56,  3.53it/s] 23%|       | 183/805 [01:07<02:54,  3.56it/s] 23%|       | 184/805 [01:08<02:53,  3.58it/s] 23%|       | 185/805 [01:08<02:52,  3.59it/s] 23%|       | 186/805 [01:08<02:52,  3.60it/s] 23%|       | 187/805 [01:09<02:51,  3.61it/s] 23%|       | 188/805 [01:09<02:50,  3.61it/s] 23%|       | 189/805 [01:09<02:50,  3.62it/s] 24%|       | 190/805 [01:09<02:54,  3.53it/s] 24%|       | 191/805 [01:10<02:52,  3.56it/s] 24%|       | 192/805 [01:10<02:51,  3.58it/s] 24%|       | 193/805 [01:10<02:50,  3.59it/s] 24%|       | 194/805 [01:11<02:49,  3.60it/s] 24%|       | 195/805 [01:11<02:49,  3.61it/s] 24%|       | 196/805 [01:11<02:48,  3.62it/s] 24%|       | 197/805 [01:11<02:48,  3.62it/s] 25%|       | 198/805 [01:12<02:47,  3.62it/s] 25%|       | 199/805 [01:12<02:47,  3.61it/s] 25%|       | 200/805 [01:12<02:47,  3.62it/s] 25%|       | 201/805 [01:12<02:48,  3.59it/s] 25%|       | 202/805 [01:13<02:47,  3.59it/s] 25%|       | 203/805 [01:13<02:47,  3.60it/s] 25%|       | 204/805 [01:13<02:46,  3.61it/s] 25%|       | 205/805 [01:14<02:46,  3.61it/s] 26%|       | 206/805 [01:14<02:45,  3.62it/s] 26%|       | 207/805 [01:14<02:45,  3.62it/s] 26%|       | 208/805 [01:14<02:44,  3.62it/s] 26%|       | 209/805 [01:15<02:47,  3.56it/s] 26%|       | 210/805 [01:15<02:46,  3.56it/s] 26%|       | 211/805 [01:15<02:45,  3.58it/s] 26%|       | 212/805 [01:16<02:49,  3.50it/s] 26%|       | 213/805 [01:16<02:47,  3.54it/s] 27%|       | 214/805 [01:16<02:46,  3.56it/s] 27%|       | 215/805 [01:16<02:44,  3.58it/s] 27%|       | 216/805 [01:17<02:48,  3.50it/s] 27%|       | 217/805 [01:17<02:59,  3.27it/s] 27%|       | 218/805 [01:17<03:11,  3.06it/s] 27%|       | 219/805 [01:18<03:03,  3.19it/s] 27%|       | 220/805 [01:18<02:56,  3.31it/s] 27%|       | 221/805 [01:18<02:51,  3.40it/s] 28%|       | 222/805 [01:19<02:48,  3.46it/s] 28%|       | 223/805 [01:19<02:49,  3.44it/s] 28%|       | 224/805 [01:19<02:46,  3.49it/s] 28%|       | 225/805 [01:19<02:44,  3.53it/s] 28%|       | 226/805 [01:20<02:42,  3.56it/s] 28%|       | 227/805 [01:20<02:41,  3.58it/s] 28%|       | 228/805 [01:20<02:40,  3.59it/s] 28%|       | 229/805 [01:20<02:39,  3.60it/s] 29%|       | 230/805 [01:21<02:39,  3.61it/s] 29%|       | 231/805 [01:21<02:38,  3.62it/s] 29%|       | 232/805 [01:21<02:38,  3.62it/s] 29%|       | 233/805 [01:22<02:38,  3.62it/s] 29%|       | 234/805 [01:22<02:46,  3.44it/s] 29%|       | 235/805 [01:22<02:43,  3.49it/s] 29%|       | 236/805 [01:22<02:41,  3.53it/s] 29%|       | 237/805 [01:23<02:39,  3.56it/s] 30%|       | 238/805 [01:23<02:38,  3.58it/s] 30%|       | 239/805 [01:23<02:37,  3.59it/s] 30%|       | 240/805 [01:24<02:36,  3.60it/s] 30%|       | 241/805 [01:24<02:36,  3.61it/s] 30%|       | 242/805 [01:24<02:35,  3.61it/s] 30%|       | 243/805 [01:24<02:35,  3.62it/s] 30%|       | 244/805 [01:25<02:35,  3.62it/s] 30%|       | 245/805 [01:25<02:41,  3.46it/s] 31%|       | 246/805 [01:25<02:39,  3.51it/s] 31%|       | 247/805 [01:26<02:37,  3.54it/s] 31%|       | 248/805 [01:26<02:36,  3.57it/s] 31%|       | 249/805 [01:26<02:34,  3.59it/s] 31%|       | 250/805 [01:26<02:34,  3.60it/s] 31%|       | 251/805 [01:27<02:33,  3.61it/s] 31%|      | 252/805 [01:27<02:33,  3.61it/s] 31%|      | 253/805 [01:27<02:32,  3.62it/s] 32%|      | 254/805 [01:27<02:32,  3.62it/s] 32%|      | 255/805 [01:28<02:31,  3.63it/s] 32%|      | 256/805 [01:28<02:31,  3.62it/s] 32%|      | 257/805 [01:28<02:31,  3.63it/s] 32%|      | 258/805 [01:29<02:30,  3.63it/s] 32%|      | 259/805 [01:29<02:30,  3.63it/s] 32%|      | 260/805 [01:29<02:39,  3.41it/s] 32%|      | 261/805 [01:29<02:36,  3.47it/s] 33%|      | 262/805 [01:30<02:34,  3.52it/s] 33%|      | 263/805 [01:30<02:32,  3.55it/s] 33%|      | 264/805 [01:30<02:31,  3.57it/s] 33%|      | 265/805 [01:31<02:30,  3.58it/s] 33%|      | 266/805 [01:31<02:30,  3.59it/s] 33%|      | 267/805 [01:31<02:29,  3.60it/s] 33%|      | 268/805 [01:31<02:28,  3.61it/s] 33%|      | 269/805 [01:32<02:28,  3.62it/s] 34%|      | 270/805 [01:32<02:27,  3.62it/s] 34%|      | 271/805 [01:32<02:34,  3.45it/s] 34%|      | 272/805 [01:33<02:32,  3.50it/s] 34%|      | 273/805 [01:33<02:30,  3.54it/s] 34%|      | 274/805 [01:33<02:28,  3.56it/s] 34%|      | 275/805 [01:33<02:27,  3.58it/s] 34%|      | 276/805 [01:34<02:27,  3.59it/s] 34%|      | 277/805 [01:34<02:26,  3.60it/s] 35%|      | 278/805 [01:34<02:26,  3.60it/s] 35%|      | 279/805 [01:34<02:25,  3.61it/s] 35%|      | 280/805 [01:35<02:25,  3.62it/s] 35%|      | 281/805 [01:35<02:24,  3.62it/s] 35%|      | 282/805 [01:35<02:28,  3.52it/s] 35%|      | 283/805 [01:36<02:27,  3.55it/s] 35%|      | 284/805 [01:36<02:25,  3.57it/s] 35%|      | 285/805 [01:36<02:24,  3.59it/s] 36%|      | 286/805 [01:36<02:24,  3.60it/s] 36%|      | 287/805 [01:37<02:23,  3.61it/s] 36%|      | 288/805 [01:37<02:23,  3.61it/s] 36%|      | 289/805 [01:37<02:22,  3.61it/s] 36%|      | 290/805 [01:38<02:22,  3.62it/s] 36%|      | 291/805 [01:38<02:21,  3.62it/s] 36%|      | 292/805 [01:38<02:21,  3.62it/s] 36%|      | 293/805 [01:38<02:26,  3.49it/s] 37%|      | 294/805 [01:39<02:25,  3.52it/s] 37%|      | 295/805 [01:39<02:23,  3.55it/s] 37%|      | 296/805 [01:39<02:22,  3.57it/s] 37%|      | 297/805 [01:40<02:21,  3.58it/s] 37%|      | 298/805 [01:40<02:21,  3.59it/s] 37%|      | 299/805 [01:40<02:20,  3.60it/s] 37%|      | 300/805 [01:40<02:19,  3.61it/s] 37%|      | 301/805 [01:41<02:19,  3.61it/s] 38%|      | 302/805 [01:41<02:19,  3.62it/s] 38%|      | 303/805 [01:41<02:18,  3.62it/s] 38%|      | 304/805 [01:41<02:23,  3.49it/s] 38%|      | 305/805 [01:42<02:21,  3.53it/s] 38%|      | 306/805 [01:42<02:20,  3.56it/s] 38%|      | 307/805 [01:42<02:19,  3.58it/s] 38%|      | 308/805 [01:43<02:18,  3.59it/s] 38%|      | 309/805 [01:43<02:17,  3.60it/s] 39%|      | 310/805 [01:43<02:17,  3.61it/s] 39%|      | 311/805 [01:43<02:16,  3.61it/s] 39%|      | 312/805 [01:44<02:16,  3.61it/s] 39%|      | 313/805 [01:44<02:16,  3.62it/s] 39%|      | 314/805 [01:44<02:15,  3.62it/s] 39%|      | 315/805 [01:45<02:20,  3.50it/s] 39%|      | 316/805 [01:45<02:18,  3.53it/s] 39%|      | 317/805 [01:45<02:17,  3.55it/s] 40%|      | 318/805 [01:45<02:16,  3.58it/s] 40%|      | 319/805 [01:46<02:15,  3.59it/s] 40%|      | 320/805 [01:46<02:14,  3.60it/s] 40%|      | 321/805 [01:46<02:14,  3.60it/s] 40%|      | 322/805 [01:46<02:13,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 03:23:13,038 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:23:13,038 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:23:13,038 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.3781, 'eval_samples_per_second': 336.188, 'eval_steps_per_second': 42.108, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.14it/s][A
  3%|         | 12/437 [00:00<00:08, 48.90it/s][A
  4%|         | 17/437 [00:00<00:08, 47.32it/s][A
  5%|         | 22/437 [00:00<00:08, 46.53it/s][A
  6%|         | 27/437 [00:00<00:08, 46.28it/s][A
  7%|         | 32/437 [00:00<00:08, 45.67it/s][A
  8%|         | 37/437 [00:00<00:13, 29.80it/s][A
 10%|         | 42/437 [00:01<00:11, 33.35it/s][A
 11%|         | 47/437 [00:01<00:10, 36.34it/s][A
 12%|        | 52/437 [00:01<00:09, 38.69it/s][A
 13%|        | 57/437 [00:01<00:09, 40.64it/s][A
 14%|        | 62/437 [00:01<00:08, 42.07it/s][A
 15%|        | 67/437 [00:01<00:08, 43.17it/s][A
 16%|        | 72/437 [00:01<00:08, 43.77it/s][A
 18%|        | 77/437 [00:01<00:08, 43.88it/s][A
 19%|        | 82/437 [00:01<00:08, 43.98it/s][A
 20%|        | 87/437 [00:02<00:07, 44.16it/s][A
 21%|        | 92/437 [00:02<00:07, 44.47it/s][A
 22%|       | 97/437 [00:02<00:07, 44.59it/s][A
 23%|       | 102/437 [00:02<00:07, 44.98it/s][A
 24%|       | 107/437 [00:02<00:07, 45.25it/s][A
 26%|       | 112/437 [00:02<00:07, 45.44it/s][A
 27%|       | 117/437 [00:02<00:07, 45.33it/s][A
 28%|       | 122/437 [00:02<00:06, 45.09it/s][A
 29%|       | 127/437 [00:02<00:06, 44.84it/s][A
 30%|       | 132/437 [00:03<00:06, 44.80it/s][A
 31%|      | 137/437 [00:03<00:06, 44.91it/s][A
 32%|      | 142/437 [00:03<00:06, 44.98it/s][A
 34%|      | 147/437 [00:03<00:06, 45.12it/s][A
 35%|      | 152/437 [00:03<00:06, 45.26it/s][A
 36%|      | 157/437 [00:03<00:06, 45.42it/s][A
 37%|      | 162/437 [00:03<00:06, 45.34it/s][A
 38%|      | 167/437 [00:03<00:06, 43.63it/s][A
 39%|      | 172/437 [00:03<00:06, 44.04it/s][A
 41%|      | 177/437 [00:04<00:05, 44.32it/s][A
 42%|     | 182/437 [00:04<00:05, 44.48it/s][A
 43%|     | 187/437 [00:04<00:05, 44.61it/s][A
 44%|     | 192/437 [00:04<00:05, 44.87it/s][A
 45%|     | 197/437 [00:04<00:05, 44.97it/s][A
 46%|     | 202/437 [00:04<00:05, 45.18it/s][A
 47%|     | 207/437 [00:04<00:05, 45.09it/s][A
 49%|     | 212/437 [00:04<00:05, 44.99it/s][A
 50%|     | 217/437 [00:04<00:04, 45.00it/s][A
 51%|     | 222/437 [00:05<00:04, 44.99it/s][A
 52%|    | 227/437 [00:05<00:04, 45.04it/s][A
 53%|    | 232/437 [00:05<00:04, 45.01it/s][A
 54%|    | 237/437 [00:05<00:04, 45.10it/s][A
 55%|    | 242/437 [00:05<00:04, 45.15it/s][A
 57%|    | 247/437 [00:05<00:04, 45.22it/s][A
 58%|    | 252/437 [00:05<00:04, 45.22it/s][A
 59%|    | 257/437 [00:05<00:03, 45.17it/s][A
 60%|    | 262/437 [00:05<00:03, 45.14it/s][A
 61%|    | 267/437 [00:06<00:03, 44.97it/s][A
 62%|   | 272/437 [00:06<00:03, 45.05it/s][A
 63%|   | 277/437 [00:06<00:03, 45.03it/s][A
 65%|   | 282/437 [00:06<00:03, 45.06it/s][A
 66%|   | 287/437 [00:06<00:03, 45.18it/s][A
 67%|   | 292/437 [00:06<00:03, 45.22it/s][A
 68%|   | 297/437 [00:06<00:03, 45.25it/s][A
 69%|   | 302/437 [00:06<00:02, 45.20it/s][A
 70%|   | 307/437 [00:06<00:02, 45.13it/s][A
 71%|  | 312/437 [00:07<00:02, 45.07it/s][A
 73%|  | 317/437 [00:07<00:02, 44.99it/s][A
 74%|  | 322/437 [00:07<00:02, 44.93it/s][A
 75%|  | 327/437 [00:07<00:02, 45.05it/s][A
 76%|  | 332/437 [00:07<00:02, 45.08it/s][A
 77%|  | 337/437 [00:07<00:02, 45.27it/s][A
 78%|  | 342/437 [00:07<00:02, 45.20it/s][A
 79%|  | 347/437 [00:07<00:01, 45.23it/s][A
 81%|  | 352/437 [00:07<00:01, 45.28it/s][A
 82%| | 357/437 [00:08<00:01, 45.22it/s][A
 83%| | 362/437 [00:08<00:01, 45.18it/s][A
 84%| | 367/437 [00:08<00:01, 45.04it/s][A
 85%| | 372/437 [00:08<00:01, 45.07it/s][A
 86%| | 377/437 [00:08<00:01, 45.05it/s][A
 87%| | 382/437 [00:08<00:01, 45.23it/s][A
 89%| | 387/437 [00:08<00:01, 45.16it/s][A
 90%| | 392/437 [00:08<00:00, 45.03it/s][A
 91%| | 397/437 [00:08<00:00, 45.14it/s][A
 92%|| 402/437 [00:09<00:00, 45.07it/s][A
 93%|| 407/437 [00:09<00:00, 45.04it/s][A
 94%|| 412/437 [00:09<00:00, 44.91it/s][A
 95%|| 417/437 [00:09<00:00, 45.03it/s][A
 97%|| 422/437 [00:09<00:00, 45.00it/s][A
 98%|| 427/437 [00:09<00:00, 45.13it/s][A
 99%|| 432/437 [00:09<00:00, 45.11it/s][A
100%|| 437/437 [00:09<00:00, 45.26it/s][A                                                 
                                                 [A 40%|      | 322/805 [01:56<02:13,  3.60it/s]
100%|| 437/437 [00:09<00:00, 45.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:23:23,094 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 03:23:23,261 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:23:26,542 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:23:26,722 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:23:26,788 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [02:02<38:04,  4.74s/it] 40%|      | 324/805 [02:02<27:16,  3.40s/it] 40%|      | 325/805 [02:02<19:43,  2.46s/it] 40%|      | 326/805 [02:02<14:26,  1.81s/it] 41%|      | 327/805 [02:03<10:45,  1.35s/it] 41%|      | 328/805 [02:03<08:10,  1.03s/it] 41%|      | 329/805 [02:03<06:25,  1.23it/s] 41%|      | 330/805 [02:04<05:09,  1.54it/s] 41%|      | 331/805 [02:04<04:15,  1.85it/s] 41%|      | 332/805 [02:04<03:38,  2.17it/s] 41%|     | 333/805 [02:04<03:12,  2.45it/s] 41%|     | 334/805 [02:05<02:53,  2.71it/s] 42%|     | 335/805 [02:05<02:40,  2.92it/s] 42%|     | 336/805 [02:05<02:31,  3.09it/s] 42%|     | 337/805 [02:06<02:25,  3.22it/s] 42%|     | 338/805 [02:06<02:20,  3.32it/s] 42%|     | 339/805 [02:06<02:17,  3.39it/s] 42%|     | 340/805 [02:06<02:17,  3.39it/s] 42%|     | 341/805 [02:07<02:14,  3.44it/s] 42%|     | 342/805 [02:07<02:13,  3.48it/s] 43%|     | 343/805 [02:07<02:11,  3.50it/s] 43%|     | 344/805 [02:08<02:10,  3.52it/s] 43%|     | 345/805 [02:08<02:10,  3.54it/s] 43%|     | 346/805 [02:08<02:09,  3.55it/s] 43%|     | 347/805 [02:08<02:08,  3.55it/s] 43%|     | 348/805 [02:09<02:08,  3.56it/s] 43%|     | 349/805 [02:09<02:08,  3.56it/s] 43%|     | 350/805 [02:09<02:07,  3.56it/s] 44%|     | 351/805 [02:10<02:10,  3.48it/s] 44%|     | 352/805 [02:10<02:09,  3.51it/s] 44%|     | 353/805 [02:10<02:08,  3.53it/s] 44%|     | 354/805 [02:10<02:07,  3.54it/s] 44%|     | 355/805 [02:11<02:06,  3.55it/s] 44%|     | 356/805 [02:11<02:06,  3.56it/s] 44%|     | 357/805 [02:11<02:05,  3.57it/s] 44%|     | 358/805 [02:11<02:05,  3.57it/s] 45%|     | 359/805 [02:12<02:04,  3.57it/s] 45%|     | 360/805 [02:12<02:04,  3.57it/s] 45%|     | 361/805 [02:12<02:04,  3.57it/s] 45%|     | 362/805 [02:13<02:07,  3.47it/s] 45%|     | 363/805 [02:13<02:06,  3.50it/s] 45%|     | 364/805 [02:13<02:05,  3.52it/s] 45%|     | 365/805 [02:13<02:04,  3.54it/s] 45%|     | 366/805 [02:14<02:03,  3.55it/s] 46%|     | 367/805 [02:14<02:03,  3.55it/s] 46%|     | 368/805 [02:14<02:02,  3.55it/s] 46%|     | 369/805 [02:15<02:02,  3.56it/s] 46%|     | 370/805 [02:15<02:02,  3.56it/s] 46%|     | 371/805 [02:15<02:01,  3.56it/s] 46%|     | 372/805 [02:15<02:01,  3.56it/s] 46%|     | 373/805 [02:16<02:03,  3.49it/s] 46%|     | 374/805 [02:16<02:02,  3.51it/s] 47%|     | 375/805 [02:16<02:01,  3.53it/s] 47%|     | 376/805 [02:17<02:01,  3.54it/s] 47%|     | 377/805 [02:17<02:00,  3.54it/s] 47%|     | 378/805 [02:17<02:27,  2.89it/s] 47%|     | 379/805 [02:18<02:19,  3.05it/s] 47%|     | 380/805 [02:18<02:13,  3.19it/s] 47%|     | 381/805 [02:18<02:08,  3.30it/s] 47%|     | 382/805 [02:18<02:04,  3.39it/s] 48%|     | 383/805 [02:19<02:06,  3.34it/s] 48%|     | 384/805 [02:19<02:02,  3.42it/s] 48%|     | 385/805 [02:19<02:00,  3.48it/s] 48%|     | 386/805 [02:20<01:58,  3.52it/s] 48%|     | 387/805 [02:20<01:57,  3.55it/s] 48%|     | 388/805 [02:20<01:56,  3.58it/s] 48%|     | 389/805 [02:20<01:55,  3.59it/s] 48%|     | 390/805 [02:21<01:55,  3.60it/s] 49%|     | 391/805 [02:21<01:55,  3.60it/s] 49%|     | 392/805 [02:21<01:54,  3.60it/s] 49%|     | 393/805 [02:22<01:54,  3.61it/s] 49%|     | 394/805 [02:22<01:55,  3.54it/s] 49%|     | 395/805 [02:22<01:54,  3.57it/s] 49%|     | 396/805 [02:22<01:54,  3.58it/s] 49%|     | 397/805 [02:23<01:53,  3.59it/s] 49%|     | 398/805 [02:23<01:52,  3.61it/s] 50%|     | 399/805 [02:23<01:52,  3.61it/s] 50%|     | 400/805 [02:23<01:52,  3.61it/s] 50%|     | 401/805 [02:24<01:51,  3.62it/s] 50%|     | 402/805 [02:24<01:51,  3.62it/s] 50%|     | 403/805 [02:24<01:51,  3.61it/s] 50%|     | 404/805 [02:25<01:50,  3.62it/s] 50%|     | 405/805 [02:25<01:55,  3.46it/s] 50%|     | 406/805 [02:25<01:53,  3.51it/s] 51%|     | 407/805 [02:25<01:52,  3.54it/s] 51%|     | 408/805 [02:26<01:51,  3.56it/s] 51%|     | 409/805 [02:26<01:50,  3.58it/s] 51%|     | 410/805 [02:26<01:49,  3.60it/s] 51%|     | 411/805 [02:27<01:49,  3.60it/s] 51%|     | 412/805 [02:27<01:48,  3.61it/s] 51%|    | 413/805 [02:27<01:48,  3.61it/s] 51%|    | 414/805 [02:27<01:54,  3.41it/s] 52%|    | 415/805 [02:28<01:52,  3.47it/s] 52%|    | 416/805 [02:28<01:50,  3.51it/s] 52%|    | 417/805 [02:28<01:49,  3.55it/s] 52%|    | 418/805 [02:29<01:48,  3.57it/s] 52%|    | 419/805 [02:29<01:47,  3.59it/s] 52%|    | 420/805 [02:29<01:46,  3.60it/s] 52%|    | 421/805 [02:29<01:46,  3.61it/s] 52%|    | 422/805 [02:30<01:46,  3.61it/s] 53%|    | 423/805 [02:30<01:45,  3.62it/s] 53%|    | 424/805 [02:30<01:45,  3.62it/s] 53%|    | 425/805 [02:31<01:52,  3.38it/s] 53%|    | 426/805 [02:31<01:49,  3.45it/s] 53%|    | 427/805 [02:31<01:48,  3.50it/s] 53%|    | 428/805 [02:31<01:46,  3.53it/s] 53%|    | 429/805 [02:32<01:45,  3.55it/s] 53%|    | 430/805 [02:32<01:44,  3.58it/s] 54%|    | 431/805 [02:32<01:44,  3.59it/s] 54%|    | 432/805 [02:32<01:43,  3.60it/s] 54%|    | 433/805 [02:33<01:43,  3.61it/s] 54%|    | 434/805 [02:33<01:42,  3.61it/s] 54%|    | 435/805 [02:33<01:42,  3.62it/s] 54%|    | 436/805 [02:34<01:49,  3.37it/s] 54%|    | 437/805 [02:34<01:47,  3.44it/s] 54%|    | 438/805 [02:34<01:44,  3.50it/s] 55%|    | 439/805 [02:34<01:43,  3.53it/s] 55%|    | 440/805 [02:35<01:42,  3.56it/s] 55%|    | 441/805 [02:35<01:41,  3.57it/s] 55%|    | 442/805 [02:35<01:41,  3.59it/s] 55%|    | 443/805 [02:36<01:40,  3.60it/s] 55%|    | 444/805 [02:36<01:40,  3.61it/s] 55%|    | 445/805 [02:36<01:39,  3.61it/s] 55%|    | 446/805 [02:36<01:39,  3.61it/s] 56%|    | 447/805 [02:37<01:41,  3.53it/s] 56%|    | 448/805 [02:37<01:40,  3.56it/s] 56%|    | 449/805 [02:37<01:39,  3.58it/s] 56%|    | 450/805 [02:38<01:38,  3.59it/s] 56%|    | 451/805 [02:38<01:38,  3.60it/s] 56%|    | 452/805 [02:38<01:38,  3.60it/s] 56%|    | 453/805 [02:38<01:37,  3.60it/s] 56%|    | 454/805 [02:39<01:37,  3.61it/s] 57%|    | 455/805 [02:39<01:36,  3.61it/s] 57%|    | 456/805 [02:39<01:36,  3.61it/s] 57%|    | 457/805 [02:39<01:36,  3.61it/s] 57%|    | 458/805 [02:40<01:39,  3.49it/s] 57%|    | 459/805 [02:40<01:38,  3.53it/s] 57%|    | 460/805 [02:40<01:37,  3.55it/s] 57%|    | 461/805 [02:41<01:36,  3.57it/s] 57%|    | 462/805 [02:41<01:35,  3.58it/s] 58%|    | 463/805 [02:41<01:35,  3.59it/s] 58%|    | 464/805 [02:41<01:34,  3.60it/s] 58%|    | 465/805 [02:42<01:34,  3.61it/s] 58%|    | 466/805 [02:42<01:33,  3.61it/s] 58%|    | 467/805 [02:42<01:33,  3.61it/s] 58%|    | 468/805 [02:43<01:33,  3.61it/s] 58%|    | 469/805 [02:43<01:34,  3.54it/s] 58%|    | 470/805 [02:43<01:33,  3.57it/s] 59%|    | 471/805 [02:43<01:33,  3.58it/s] 59%|    | 472/805 [02:44<01:32,  3.59it/s] 59%|    | 473/805 [02:44<01:32,  3.60it/s] 59%|    | 474/805 [02:44<01:31,  3.61it/s] 59%|    | 475/805 [02:45<01:31,  3.61it/s] 59%|    | 476/805 [02:45<01:31,  3.61it/s] 59%|    | 477/805 [02:45<01:30,  3.61it/s] 59%|    | 478/805 [02:45<01:30,  3.61it/s] 60%|    | 479/805 [02:46<01:30,  3.61it/s] 60%|    | 480/805 [02:46<01:33,  3.49it/s] 60%|    | 481/805 [02:46<01:31,  3.53it/s] 60%|    | 482/805 [02:46<01:31,  3.55it/s] 60%|    | 483/805 [02:47<01:30,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 03:24:13,325 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:24:13,326 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:24:13,326 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8955, 'eval_samples_per_second': 352.584, 'eval_steps_per_second': 44.161, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.22it/s][A
  3%|         | 12/437 [00:00<00:08, 49.36it/s][A
  4%|         | 17/437 [00:00<00:08, 47.52it/s][A
  5%|         | 22/437 [00:00<00:08, 46.86it/s][A
  6%|         | 27/437 [00:00<00:08, 46.32it/s][A
  7%|         | 32/437 [00:00<00:08, 45.82it/s][A
  8%|         | 37/437 [00:00<00:08, 45.41it/s][A
 10%|         | 42/437 [00:00<00:08, 45.01it/s][A
 11%|         | 47/437 [00:01<00:08, 45.01it/s][A
 12%|        | 52/437 [00:01<00:08, 45.20it/s][A
 13%|        | 57/437 [00:01<00:08, 45.19it/s][A
 14%|        | 62/437 [00:01<00:08, 45.17it/s][A
 15%|        | 67/437 [00:01<00:08, 45.28it/s][A
 16%|        | 72/437 [00:01<00:08, 45.34it/s][A
 18%|        | 77/437 [00:01<00:07, 45.20it/s][A
 19%|        | 82/437 [00:01<00:07, 44.94it/s][A
 20%|        | 87/437 [00:01<00:08, 43.07it/s][A
 21%|        | 92/437 [00:02<00:07, 43.70it/s][A
 22%|       | 97/437 [00:02<00:07, 44.10it/s][A
 23%|       | 102/437 [00:02<00:07, 44.38it/s][A
 24%|       | 107/437 [00:02<00:07, 44.66it/s][A
 26%|       | 112/437 [00:02<00:07, 44.94it/s][A
 27%|       | 117/437 [00:02<00:07, 45.12it/s][A
 28%|       | 122/437 [00:02<00:06, 45.10it/s][A
 29%|       | 127/437 [00:02<00:06, 44.90it/s][A
 30%|       | 132/437 [00:02<00:06, 44.76it/s][A
 31%|      | 137/437 [00:03<00:06, 44.94it/s][A
 32%|      | 142/437 [00:03<00:06, 45.03it/s][A
 34%|      | 147/437 [00:03<00:06, 45.13it/s][A
 35%|      | 152/437 [00:03<00:06, 45.21it/s][A
 36%|      | 157/437 [00:03<00:06, 45.29it/s][A
 37%|      | 162/437 [00:03<00:06, 45.25it/s][A
 38%|      | 167/437 [00:03<00:05, 45.18it/s][A
 39%|      | 172/437 [00:03<00:05, 44.99it/s][A
 41%|      | 177/437 [00:03<00:05, 44.82it/s][A
 42%|     | 182/437 [00:04<00:05, 44.90it/s][A
 43%|     | 187/437 [00:04<00:05, 44.97it/s][A
 44%|     | 192/437 [00:04<00:05, 45.08it/s][A
 45%|     | 197/437 [00:04<00:05, 45.20it/s][A
 46%|     | 202/437 [00:04<00:05, 45.30it/s][A
 47%|     | 207/437 [00:04<00:05, 45.24it/s][A
 49%|     | 212/437 [00:04<00:04, 45.14it/s][A
 50%|     | 217/437 [00:04<00:04, 45.05it/s][A
 51%|     | 222/437 [00:04<00:04, 43.80it/s][A
 52%|    | 227/437 [00:05<00:04, 44.20it/s][A
 53%|    | 232/437 [00:05<00:04, 44.44it/s][A
 54%|    | 237/437 [00:05<00:04, 44.72it/s][A
 55%|    | 242/437 [00:05<00:04, 44.79it/s][A
 57%|    | 247/437 [00:05<00:04, 44.90it/s][A
 58%|    | 252/437 [00:05<00:04, 45.04it/s][A
 59%|    | 257/437 [00:05<00:03, 45.02it/s][A
 60%|    | 262/437 [00:05<00:03, 44.74it/s][A
 61%|    | 267/437 [00:05<00:03, 44.89it/s][A
 62%|   | 272/437 [00:06<00:03, 45.02it/s][A
 63%|   | 277/437 [00:06<00:03, 45.09it/s][A
 65%|   | 282/437 [00:06<00:03, 45.22it/s][A
 66%|   | 287/437 [00:06<00:03, 45.13it/s][A
 67%|   | 292/437 [00:06<00:03, 45.23it/s][A
 68%|   | 297/437 [00:06<00:03, 45.14it/s][A
 69%|   | 302/437 [00:06<00:03, 45.00it/s][A
 70%|   | 307/437 [00:06<00:02, 44.99it/s][A
 71%|  | 312/437 [00:06<00:02, 44.93it/s][A
 73%|  | 317/437 [00:07<00:02, 44.91it/s][A
 74%|  | 322/437 [00:07<00:02, 45.06it/s][A
 75%|  | 327/437 [00:07<00:02, 45.22it/s][A
 76%|  | 332/437 [00:07<00:02, 45.22it/s][A
 77%|  | 337/437 [00:07<00:02, 45.31it/s][A
 78%|  | 342/437 [00:07<00:02, 45.20it/s][A
 79%|  | 347/437 [00:07<00:01, 45.15it/s][A
 81%|  | 352/437 [00:07<00:01, 45.10it/s][A
 82%| | 357/437 [00:07<00:01, 44.79it/s][A
 83%| | 362/437 [00:08<00:01, 44.88it/s][A
 84%| | 367/437 [00:08<00:01, 44.98it/s][A
 85%| | 372/437 [00:08<00:01, 45.12it/s][A
 86%| | 377/437 [00:08<00:01, 45.23it/s][A
 87%| | 382/437 [00:08<00:01, 45.20it/s][A
 89%| | 387/437 [00:08<00:01, 45.27it/s][A
 90%| | 392/437 [00:08<00:00, 45.21it/s][A
 91%| | 397/437 [00:08<00:00, 45.20it/s][A
 92%|| 402/437 [00:08<00:00, 45.12it/s][A
 93%|| 407/437 [00:09<00:00, 45.07it/s][A
 94%|| 412/437 [00:09<00:00, 44.92it/s][A
 95%|| 417/437 [00:09<00:00, 45.05it/s][A
 97%|| 422/437 [00:09<00:00, 45.19it/s][A
 98%|| 427/437 [00:09<00:00, 45.15it/s][A
 99%|| 432/437 [00:09<00:00, 45.16it/s][A
100%|| 437/437 [00:09<00:00, 45.23it/s][A                                                 
                                                 [A 60%|    | 483/805 [02:57<01:30,  3.56it/s]
100%|| 437/437 [00:09<00:00, 45.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:24:23,266 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 03:24:23,516 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:24:28,110 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:24:28,251 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:24:28,309 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [03:03<27:05,  5.06s/it] 60%|    | 485/805 [03:03<19:21,  3.63s/it] 60%|    | 486/805 [03:04<13:57,  2.62s/it] 60%|    | 487/805 [03:04<10:10,  1.92s/it] 61%|    | 488/805 [03:04<07:32,  1.43s/it] 61%|    | 489/805 [03:04<05:42,  1.08s/it] 61%|    | 490/805 [03:05<04:25,  1.19it/s] 61%|    | 491/805 [03:05<03:33,  1.47it/s] 61%|    | 492/805 [03:05<02:55,  1.79it/s] 61%|    | 493/805 [03:06<02:28,  2.10it/s] 61%|   | 494/805 [03:06<02:09,  2.40it/s] 61%|   | 495/805 [03:06<01:56,  2.66it/s] 62%|   | 496/805 [03:06<01:47,  2.88it/s] 62%|   | 497/805 [03:07<01:40,  3.06it/s] 62%|   | 498/805 [03:07<01:35,  3.20it/s] 62%|   | 499/805 [03:07<01:32,  3.30it/s] 62%|   | 500/805 [03:07<01:30,  3.38it/s]                                                  62%|   | 500/805 [03:07<01:30,  3.38it/s] 62%|   | 501/805 [03:08<01:28,  3.44it/s] 62%|   | 502/805 [03:08<01:29,  3.40it/s] 62%|   | 503/805 [03:08<01:27,  3.45it/s] 63%|   | 504/805 [03:09<01:26,  3.49it/s] 63%|   | 505/805 [03:09<01:25,  3.51it/s] 63%|   | 506/805 [03:09<01:24,  3.53it/s] 63%|   | 507/805 [03:09<01:24,  3.54it/s] 63%|   | 508/805 [03:10<01:23,  3.55it/s] 63%|   | 509/805 [03:10<01:23,  3.56it/s] 63%|   | 510/805 [03:10<01:22,  3.57it/s] 63%|   | 511/805 [03:11<01:22,  3.56it/s] 64%|   | 512/805 [03:11<01:22,  3.57it/s] 64%|   | 513/805 [03:11<01:23,  3.51it/s] 64%|   | 514/805 [03:11<01:22,  3.53it/s] 64%|   | 515/805 [03:12<01:21,  3.54it/s] 64%|   | 516/805 [03:12<01:21,  3.54it/s] 64%|   | 517/805 [03:12<01:21,  3.55it/s] 64%|   | 518/805 [03:13<01:20,  3.56it/s] 64%|   | 519/805 [03:13<01:20,  3.56it/s] 65%|   | 520/805 [03:13<01:19,  3.57it/s] 65%|   | 521/805 [03:13<01:19,  3.58it/s] 65%|   | 522/805 [03:14<01:18,  3.59it/s] 65%|   | 523/805 [03:14<01:18,  3.59it/s] 65%|   | 524/805 [03:14<01:19,  3.52it/s] 65%|   | 525/805 [03:15<01:18,  3.55it/s] 65%|   | 526/805 [03:15<01:18,  3.57it/s] 65%|   | 527/805 [03:15<01:17,  3.59it/s] 66%|   | 528/805 [03:15<01:16,  3.60it/s] 66%|   | 529/805 [03:16<01:16,  3.61it/s] 66%|   | 530/805 [03:16<01:16,  3.62it/s] 66%|   | 531/805 [03:16<01:15,  3.62it/s] 66%|   | 532/805 [03:16<01:15,  3.62it/s] 66%|   | 533/805 [03:17<01:15,  3.62it/s] 66%|   | 534/805 [03:17<01:14,  3.63it/s] 66%|   | 535/805 [03:17<01:17,  3.47it/s] 67%|   | 536/805 [03:18<01:16,  3.51it/s] 67%|   | 537/805 [03:18<01:15,  3.54it/s] 67%|   | 538/805 [03:18<01:14,  3.56it/s] 67%|   | 539/805 [03:18<01:14,  3.58it/s] 67%|   | 540/805 [03:19<01:13,  3.59it/s] 67%|   | 541/805 [03:19<01:13,  3.61it/s] 67%|   | 542/805 [03:19<01:12,  3.61it/s] 67%|   | 543/805 [03:20<01:12,  3.61it/s] 68%|   | 544/805 [03:20<01:12,  3.61it/s] 68%|   | 545/805 [03:20<01:12,  3.61it/s] 68%|   | 546/805 [03:20<01:14,  3.49it/s] 68%|   | 547/805 [03:21<01:13,  3.53it/s] 68%|   | 548/805 [03:21<01:12,  3.55it/s] 68%|   | 549/805 [03:21<01:11,  3.58it/s] 68%|   | 550/805 [03:22<01:10,  3.59it/s] 68%|   | 551/805 [03:22<01:10,  3.60it/s] 69%|   | 552/805 [03:22<01:10,  3.61it/s] 69%|   | 553/805 [03:22<01:09,  3.61it/s] 69%|   | 554/805 [03:23<01:09,  3.62it/s] 69%|   | 555/805 [03:23<01:09,  3.62it/s] 69%|   | 556/805 [03:23<01:08,  3.62it/s] 69%|   | 557/805 [03:23<01:11,  3.46it/s] 69%|   | 558/805 [03:24<01:10,  3.51it/s] 69%|   | 559/805 [03:24<01:09,  3.54it/s] 70%|   | 560/805 [03:24<01:08,  3.57it/s] 70%|   | 561/805 [03:25<01:08,  3.59it/s] 70%|   | 562/805 [03:25<01:07,  3.59it/s] 70%|   | 563/805 [03:25<01:07,  3.60it/s] 70%|   | 564/805 [03:25<01:06,  3.61it/s] 70%|   | 565/805 [03:26<01:06,  3.61it/s] 70%|   | 566/805 [03:26<01:06,  3.62it/s] 70%|   | 567/805 [03:26<01:05,  3.62it/s] 71%|   | 568/805 [03:27<01:08,  3.44it/s] 71%|   | 569/805 [03:27<01:07,  3.49it/s] 71%|   | 570/805 [03:27<01:06,  3.53it/s] 71%|   | 571/805 [03:27<01:05,  3.56it/s] 71%|   | 572/805 [03:28<01:05,  3.58it/s] 71%|   | 573/805 [03:28<01:04,  3.59it/s] 71%|  | 574/805 [03:28<01:04,  3.60it/s] 71%|  | 575/805 [03:28<01:03,  3.61it/s] 72%|  | 576/805 [03:29<01:03,  3.61it/s] 72%|  | 577/805 [03:29<01:03,  3.62it/s] 72%|  | 578/805 [03:29<01:02,  3.62it/s] 72%|  | 579/805 [03:30<01:02,  3.62it/s] 72%|  | 580/805 [03:30<01:02,  3.62it/s] 72%|  | 581/805 [03:30<01:01,  3.62it/s] 72%|  | 582/805 [03:30<01:01,  3.62it/s] 72%|  | 583/805 [03:31<01:01,  3.63it/s] 73%|  | 584/805 [03:31<01:00,  3.63it/s] 73%|  | 585/805 [03:31<01:00,  3.63it/s] 73%|  | 586/805 [03:32<01:00,  3.62it/s] 73%|  | 587/805 [03:32<01:03,  3.42it/s] 73%|  | 588/805 [03:32<01:02,  3.48it/s] 73%|  | 589/805 [03:32<01:01,  3.52it/s] 73%|  | 590/805 [03:33<01:00,  3.55it/s] 73%|  | 591/805 [03:33<00:59,  3.57it/s] 74%|  | 592/805 [03:33<00:59,  3.59it/s] 74%|  | 593/805 [03:34<00:58,  3.60it/s] 74%|  | 594/805 [03:34<00:58,  3.60it/s] 74%|  | 595/805 [03:34<00:58,  3.61it/s] 74%|  | 596/805 [03:34<00:57,  3.61it/s] 74%|  | 597/805 [03:35<00:57,  3.62it/s] 74%|  | 598/805 [03:35<00:57,  3.62it/s] 74%|  | 599/805 [03:35<00:56,  3.62it/s] 75%|  | 600/805 [03:35<00:56,  3.62it/s] 75%|  | 601/805 [03:36<00:56,  3.62it/s] 75%|  | 602/805 [03:36<00:56,  3.62it/s] 75%|  | 603/805 [03:36<00:55,  3.62it/s] 75%|  | 604/805 [03:37<00:55,  3.62it/s] 75%|  | 605/805 [03:37<00:55,  3.62it/s] 75%|  | 606/805 [03:37<00:59,  3.33it/s] 75%|  | 607/805 [03:37<00:58,  3.41it/s] 76%|  | 608/805 [03:38<00:56,  3.48it/s] 76%|  | 609/805 [03:38<00:55,  3.52it/s] 76%|  | 610/805 [03:38<00:54,  3.55it/s] 76%|  | 611/805 [03:39<00:54,  3.57it/s] 76%|  | 612/805 [03:39<00:53,  3.59it/s] 76%|  | 613/805 [03:39<00:53,  3.60it/s] 76%|  | 614/805 [03:39<00:52,  3.61it/s] 76%|  | 615/805 [03:40<00:52,  3.61it/s] 77%|  | 616/805 [03:40<00:52,  3.61it/s] 77%|  | 617/805 [03:40<01:00,  3.10it/s] 77%|  | 618/805 [03:41<00:57,  3.24it/s] 77%|  | 619/805 [03:41<00:55,  3.34it/s] 77%|  | 620/805 [03:41<00:54,  3.42it/s] 77%|  | 621/805 [03:41<00:52,  3.48it/s] 77%|  | 622/805 [03:42<00:51,  3.52it/s] 77%|  | 623/805 [03:42<00:51,  3.55it/s] 78%|  | 624/805 [03:42<00:50,  3.57it/s] 78%|  | 625/805 [03:43<00:50,  3.59it/s] 78%|  | 626/805 [03:43<00:49,  3.60it/s] 78%|  | 627/805 [03:43<00:49,  3.61it/s] 78%|  | 628/805 [03:43<00:50,  3.50it/s] 78%|  | 629/805 [03:44<00:49,  3.54it/s] 78%|  | 630/805 [03:44<00:49,  3.56it/s] 78%|  | 631/805 [03:44<00:48,  3.58it/s] 79%|  | 632/805 [03:45<00:48,  3.59it/s] 79%|  | 633/805 [03:45<00:47,  3.59it/s] 79%|  | 634/805 [03:45<00:47,  3.60it/s] 79%|  | 635/805 [03:45<00:47,  3.61it/s] 79%|  | 636/805 [03:46<00:46,  3.61it/s] 79%|  | 637/805 [03:46<00:46,  3.62it/s] 79%|  | 638/805 [03:46<00:46,  3.61it/s] 79%|  | 639/805 [03:47<00:47,  3.51it/s] 80%|  | 640/805 [03:47<00:46,  3.54it/s] 80%|  | 641/805 [03:47<00:46,  3.56it/s] 80%|  | 642/805 [03:47<00:45,  3.58it/s] 80%|  | 643/805 [03:48<00:45,  3.59it/s] 80%|  | 644/805 [03:48<00:44,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 03:25:14,450 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:25:14,450 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:25:14,450 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.7338, 'eval_samples_per_second': 358.441, 'eval_steps_per_second': 44.895, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.71it/s][A
  3%|         | 12/437 [00:00<00:08, 49.37it/s][A
  4%|         | 17/437 [00:00<00:08, 47.66it/s][A
  5%|         | 22/437 [00:00<00:08, 46.84it/s][A
  6%|         | 27/437 [00:00<00:08, 46.17it/s][A
  7%|         | 32/437 [00:00<00:08, 45.67it/s][A
  8%|         | 37/437 [00:00<00:08, 45.18it/s][A
 10%|         | 42/437 [00:00<00:08, 44.78it/s][A
 11%|         | 47/437 [00:01<00:08, 45.02it/s][A
 12%|        | 52/437 [00:01<00:08, 45.14it/s][A
 13%|        | 57/437 [00:01<00:08, 45.34it/s][A
 14%|        | 62/437 [00:01<00:08, 42.90it/s][A
 15%|        | 67/437 [00:01<00:08, 43.62it/s][A
 16%|        | 72/437 [00:01<00:08, 44.14it/s][A
 18%|        | 77/437 [00:01<00:08, 44.33it/s][A
 19%|        | 82/437 [00:01<00:08, 44.18it/s][A
 20%|        | 87/437 [00:01<00:07, 44.27it/s][A
 21%|        | 92/437 [00:02<00:07, 44.40it/s][A
 22%|       | 97/437 [00:02<00:07, 44.63it/s][A
 23%|       | 102/437 [00:02<00:07, 44.73it/s][A
 24%|       | 107/437 [00:02<00:07, 44.85it/s][A
 26%|       | 112/437 [00:02<00:07, 45.10it/s][A
 27%|       | 117/437 [00:02<00:07, 45.10it/s][A
 28%|       | 122/437 [00:02<00:06, 45.26it/s][A
 29%|       | 127/437 [00:02<00:06, 45.10it/s][A
 30%|       | 132/437 [00:02<00:06, 45.10it/s][A
 31%|      | 137/437 [00:03<00:06, 45.03it/s][A
 32%|      | 142/437 [00:03<00:06, 45.15it/s][A
 34%|      | 147/437 [00:03<00:06, 45.12it/s][A
 35%|      | 152/437 [00:03<00:06, 45.11it/s][A
 36%|      | 157/437 [00:03<00:06, 45.11it/s][A
 37%|      | 162/437 [00:03<00:06, 45.25it/s][A
 38%|      | 167/437 [00:03<00:05, 45.26it/s][A
 39%|      | 172/437 [00:03<00:05, 45.24it/s][A
 41%|      | 177/437 [00:03<00:05, 45.19it/s][A
 42%|     | 182/437 [00:04<00:05, 45.22it/s][A
 43%|     | 187/437 [00:04<00:05, 45.20it/s][A
 44%|     | 192/437 [00:04<00:05, 45.16it/s][A
 45%|     | 197/437 [00:04<00:06, 39.97it/s][A
 46%|     | 202/437 [00:04<00:05, 41.52it/s][A
 47%|     | 207/437 [00:04<00:05, 42.54it/s][A
 49%|     | 212/437 [00:04<00:05, 43.42it/s][A
 50%|     | 217/437 [00:04<00:05, 44.00it/s][A
 51%|     | 222/437 [00:04<00:04, 44.48it/s][A
 52%|    | 227/437 [00:05<00:04, 44.76it/s][A
 53%|    | 232/437 [00:05<00:04, 44.98it/s][A
 54%|    | 237/437 [00:05<00:04, 44.66it/s][A
 55%|    | 242/437 [00:05<00:04, 44.76it/s][A
 57%|    | 247/437 [00:05<00:04, 44.83it/s][A
 58%|    | 252/437 [00:05<00:04, 44.95it/s][A
 59%|    | 257/437 [00:05<00:03, 45.10it/s][A
 60%|    | 262/437 [00:05<00:03, 45.21it/s][A
 61%|    | 267/437 [00:05<00:03, 45.30it/s][A
 62%|   | 272/437 [00:06<00:03, 45.29it/s][A
 63%|   | 277/437 [00:06<00:03, 45.17it/s][A
 65%|   | 282/437 [00:06<00:03, 44.92it/s][A
 66%|   | 287/437 [00:06<00:03, 44.90it/s][A
 67%|   | 292/437 [00:06<00:03, 44.93it/s][A
 68%|   | 297/437 [00:06<00:03, 45.07it/s][A
 69%|   | 302/437 [00:06<00:02, 45.15it/s][A
 70%|   | 307/437 [00:06<00:02, 45.19it/s][A
 71%|  | 312/437 [00:06<00:02, 45.23it/s][A
 73%|  | 317/437 [00:07<00:02, 45.26it/s][A
 74%|  | 322/437 [00:07<00:02, 45.10it/s][A
 75%|  | 327/437 [00:07<00:02, 45.10it/s][A
 76%|  | 332/437 [00:07<00:02, 43.20it/s][A
 77%|  | 337/437 [00:07<00:02, 43.91it/s][A
 78%|  | 342/437 [00:07<00:02, 44.25it/s][A
 79%|  | 347/437 [00:07<00:02, 44.52it/s][A
 81%|  | 352/437 [00:07<00:01, 44.82it/s][A
 82%| | 357/437 [00:07<00:01, 44.99it/s][A
 83%| | 362/437 [00:08<00:01, 45.00it/s][A
 84%| | 367/437 [00:08<00:01, 45.06it/s][A
 85%| | 372/437 [00:08<00:01, 44.81it/s][A
 86%| | 377/437 [00:08<00:01, 44.80it/s][A
 87%| | 382/437 [00:08<00:01, 44.99it/s][A
 89%| | 387/437 [00:08<00:01, 45.05it/s][A
 90%| | 392/437 [00:08<00:00, 45.13it/s][A
 91%| | 397/437 [00:08<00:00, 45.21it/s][A
 92%|| 402/437 [00:08<00:00, 45.26it/s][A
 93%|| 407/437 [00:09<00:00, 45.31it/s][A
 94%|| 412/437 [00:09<00:00, 45.10it/s][A
 95%|| 417/437 [00:09<00:00, 44.94it/s][A
 97%|| 422/437 [00:09<00:00, 44.86it/s][A
 98%|| 427/437 [00:09<00:00, 44.94it/s][A
 99%|| 432/437 [00:09<00:00, 45.09it/s][A
100%|| 437/437 [00:09<00:00, 45.18it/s][A                                                 
                                                 [A 80%|  | 644/805 [03:58<00:44,  3.60it/s]
100%|| 437/437 [00:09<00:00, 45.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:25:24,432 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 03:25:24,664 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:25:28,493 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:25:28,614 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:25:28,677 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [04:03<12:50,  4.82s/it] 80%|  | 646/805 [04:04<09:09,  3.46s/it] 80%|  | 647/805 [04:04<06:35,  2.50s/it] 80%|  | 648/805 [04:04<04:48,  1.84s/it] 81%|  | 649/805 [04:04<03:33,  1.37s/it] 81%|  | 650/805 [04:05<02:41,  1.04s/it] 81%|  | 651/805 [04:05<02:05,  1.23it/s] 81%|  | 652/805 [04:05<01:39,  1.53it/s] 81%|  | 653/805 [04:06<01:22,  1.85it/s] 81%|  | 654/805 [04:06<01:09,  2.17it/s] 81%| | 655/805 [04:06<01:00,  2.47it/s] 81%| | 656/805 [04:06<00:54,  2.73it/s] 82%| | 657/805 [04:07<00:50,  2.95it/s] 82%| | 658/805 [04:07<00:47,  3.12it/s] 82%| | 659/805 [04:07<00:44,  3.26it/s] 82%| | 660/805 [04:07<00:43,  3.36it/s] 82%| | 661/805 [04:08<00:42,  3.40it/s] 82%| | 662/805 [04:08<00:41,  3.46it/s] 82%| | 663/805 [04:08<00:40,  3.51it/s] 82%| | 664/805 [04:09<00:39,  3.54it/s] 83%| | 665/805 [04:09<00:39,  3.57it/s] 83%| | 666/805 [04:09<00:38,  3.58it/s] 83%| | 667/805 [04:09<00:38,  3.60it/s] 83%| | 668/805 [04:10<00:37,  3.61it/s] 83%| | 669/805 [04:10<00:37,  3.61it/s] 83%| | 670/805 [04:10<00:37,  3.62it/s] 83%| | 671/805 [04:11<00:37,  3.62it/s] 83%| | 672/805 [04:11<00:37,  3.53it/s] 84%| | 673/805 [04:11<00:37,  3.55it/s] 84%| | 674/805 [04:11<00:36,  3.57it/s] 84%| | 675/805 [04:12<00:36,  3.59it/s] 84%| | 676/805 [04:12<00:35,  3.60it/s] 84%| | 677/805 [04:12<00:35,  3.60it/s] 84%| | 678/805 [04:12<00:35,  3.61it/s] 84%| | 679/805 [04:13<00:34,  3.62it/s] 84%| | 680/805 [04:13<00:34,  3.62it/s] 85%| | 681/805 [04:13<00:34,  3.62it/s] 85%| | 682/805 [04:14<00:33,  3.62it/s] 85%| | 683/805 [04:14<00:35,  3.43it/s] 85%| | 684/805 [04:14<00:34,  3.49it/s] 85%| | 685/805 [04:14<00:34,  3.52it/s] 85%| | 686/805 [04:15<00:33,  3.55it/s] 85%| | 687/805 [04:15<00:33,  3.48it/s] 85%| | 688/805 [04:15<00:33,  3.51it/s] 86%| | 689/805 [04:16<00:32,  3.54it/s] 86%| | 690/805 [04:16<00:32,  3.57it/s] 86%| | 691/805 [04:16<00:31,  3.58it/s] 86%| | 692/805 [04:16<00:31,  3.59it/s] 86%| | 693/805 [04:17<00:31,  3.60it/s] 86%| | 694/805 [04:17<00:31,  3.50it/s] 86%| | 695/805 [04:17<00:32,  3.40it/s] 86%| | 696/805 [04:18<00:34,  3.16it/s] 87%| | 697/805 [04:18<00:33,  3.27it/s] 87%| | 698/805 [04:18<00:31,  3.36it/s] 87%| | 699/805 [04:19<00:30,  3.44it/s] 87%| | 700/805 [04:19<00:30,  3.49it/s] 87%| | 701/805 [04:19<00:29,  3.53it/s] 87%| | 702/805 [04:19<00:28,  3.55it/s] 87%| | 703/805 [04:20<00:28,  3.57it/s] 87%| | 704/805 [04:20<00:28,  3.59it/s] 88%| | 705/805 [04:20<00:28,  3.45it/s] 88%| | 706/805 [04:20<00:28,  3.50it/s] 88%| | 707/805 [04:21<00:27,  3.54it/s] 88%| | 708/805 [04:21<00:27,  3.56it/s] 88%| | 709/805 [04:21<00:26,  3.58it/s] 88%| | 710/805 [04:22<00:26,  3.59it/s] 88%| | 711/805 [04:22<00:26,  3.60it/s] 88%| | 712/805 [04:22<00:25,  3.61it/s] 89%| | 713/805 [04:22<00:25,  3.61it/s] 89%| | 714/805 [04:23<00:25,  3.61it/s] 89%| | 715/805 [04:23<00:24,  3.61it/s] 89%| | 716/805 [04:23<00:25,  3.49it/s] 89%| | 717/805 [04:24<00:24,  3.53it/s] 89%| | 718/805 [04:24<00:24,  3.56it/s] 89%| | 719/805 [04:24<00:24,  3.57it/s] 89%| | 720/805 [04:24<00:23,  3.59it/s] 90%| | 721/805 [04:25<00:23,  3.60it/s] 90%| | 722/805 [04:25<00:23,  3.60it/s] 90%| | 723/805 [04:25<00:22,  3.60it/s] 90%| | 724/805 [04:25<00:22,  3.61it/s] 90%| | 725/805 [04:26<00:22,  3.61it/s] 90%| | 726/805 [04:26<00:21,  3.62it/s] 90%| | 727/805 [04:26<00:22,  3.50it/s] 90%| | 728/805 [04:27<00:21,  3.54it/s] 91%| | 729/805 [04:27<00:21,  3.56it/s] 91%| | 730/805 [04:27<00:20,  3.58it/s] 91%| | 731/805 [04:27<00:20,  3.59it/s] 91%| | 732/805 [04:28<00:20,  3.60it/s] 91%| | 733/805 [04:28<00:19,  3.60it/s] 91%| | 734/805 [04:28<00:19,  3.61it/s] 91%|| 735/805 [04:29<00:19,  3.61it/s] 91%|| 736/805 [04:29<00:19,  3.61it/s] 92%|| 737/805 [04:29<00:18,  3.61it/s] 92%|| 738/805 [04:29<00:19,  3.40it/s] 92%|| 739/805 [04:30<00:19,  3.46it/s] 92%|| 740/805 [04:30<00:18,  3.51it/s] 92%|| 741/805 [04:30<00:18,  3.54it/s] 92%|| 742/805 [04:31<00:17,  3.56it/s] 92%|| 743/805 [04:31<00:17,  3.58it/s] 92%|| 744/805 [04:31<00:17,  3.59it/s] 93%|| 745/805 [04:31<00:16,  3.59it/s] 93%|| 746/805 [04:32<00:16,  3.60it/s] 93%|| 747/805 [04:32<00:16,  3.61it/s] 93%|| 748/805 [04:32<00:15,  3.60it/s] 93%|| 749/805 [04:33<00:16,  3.33it/s] 93%|| 750/805 [04:33<00:16,  3.41it/s] 93%|| 751/805 [04:33<00:15,  3.48it/s] 93%|| 752/805 [04:33<00:15,  3.51it/s] 94%|| 753/805 [04:34<00:14,  3.55it/s] 94%|| 754/805 [04:34<00:14,  3.57it/s] 94%|| 755/805 [04:34<00:13,  3.58it/s] 94%|| 756/805 [04:35<00:13,  3.59it/s] 94%|| 757/805 [04:35<00:13,  3.60it/s] 94%|| 758/805 [04:35<00:13,  3.61it/s] 94%|| 759/805 [04:35<00:12,  3.61it/s] 94%|| 760/805 [04:36<00:12,  3.61it/s] 95%|| 761/805 [04:36<00:12,  3.61it/s] 95%|| 762/805 [04:36<00:11,  3.60it/s] 95%|| 763/805 [04:36<00:11,  3.61it/s] 95%|| 764/805 [04:37<00:11,  3.61it/s] 95%|| 765/805 [04:37<00:11,  3.61it/s] 95%|| 766/805 [04:37<00:10,  3.61it/s] 95%|| 767/805 [04:38<00:10,  3.60it/s] 95%|| 768/805 [04:38<00:10,  3.61it/s] 96%|| 769/805 [04:38<00:09,  3.60it/s] 96%|| 770/805 [04:38<00:09,  3.52it/s] 96%|| 771/805 [04:39<00:09,  3.55it/s] 96%|| 772/805 [04:39<00:09,  3.57it/s] 96%|| 773/805 [04:39<00:08,  3.58it/s] 96%|| 774/805 [04:40<00:08,  3.59it/s] 96%|| 775/805 [04:40<00:08,  3.60it/s] 96%|| 776/805 [04:40<00:08,  3.60it/s] 97%|| 777/805 [04:40<00:07,  3.60it/s] 97%|| 778/805 [04:41<00:07,  3.61it/s] 97%|| 779/805 [04:41<00:07,  3.61it/s] 97%|| 780/805 [04:41<00:06,  3.61it/s] 97%|| 781/805 [04:41<00:06,  3.51it/s] 97%|| 782/805 [04:42<00:06,  3.54it/s] 97%|| 783/805 [04:42<00:06,  3.56it/s] 97%|| 784/805 [04:42<00:05,  3.58it/s] 98%|| 785/805 [04:43<00:05,  3.59it/s] 98%|| 786/805 [04:43<00:05,  3.60it/s] 98%|| 787/805 [04:43<00:04,  3.61it/s] 98%|| 788/805 [04:43<00:04,  3.61it/s] 98%|| 789/805 [04:44<00:04,  3.61it/s] 98%|| 790/805 [04:44<00:04,  3.62it/s] 98%|| 791/805 [04:44<00:03,  3.62it/s] 98%|| 792/805 [04:45<00:03,  3.50it/s] 99%|| 793/805 [04:45<00:03,  3.54it/s] 99%|| 794/805 [04:45<00:03,  3.57it/s] 99%|| 795/805 [04:45<00:02,  3.58it/s] 99%|| 796/805 [04:46<00:02,  3.59it/s] 99%|| 797/805 [04:46<00:02,  3.60it/s] 99%|| 798/805 [04:46<00:01,  3.61it/s] 99%|| 799/805 [04:46<00:01,  3.62it/s] 99%|| 800/805 [04:47<00:01,  3.62it/s]100%|| 801/805 [04:47<00:01,  3.62it/s]100%|| 802/805 [04:47<00:00,  3.62it/s]100%|| 803/805 [04:48<00:00,  3.50it/s]100%|| 804/805 [04:48<00:00,  3.53it/s]100%|| 805/805 [04:48<00:00,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 03:26:14,687 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:26:14,687 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:26:14,687 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.7872, 'eval_samples_per_second': 356.485, 'eval_steps_per_second': 44.65, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.62it/s][A
  3%|         | 12/437 [00:00<00:08, 49.47it/s][A
  4%|         | 18/437 [00:00<00:08, 47.58it/s][A
  5%|         | 23/437 [00:00<00:08, 47.00it/s][A
  6%|         | 28/437 [00:00<00:08, 46.34it/s][A
  8%|         | 33/437 [00:00<00:08, 45.76it/s][A
  9%|         | 38/437 [00:00<00:08, 45.27it/s][A
 10%|         | 43/437 [00:00<00:08, 44.94it/s][A
 11%|         | 48/437 [00:01<00:08, 45.08it/s][A
 12%|        | 53/437 [00:01<00:08, 45.15it/s][A
 13%|        | 58/437 [00:01<00:08, 45.21it/s][A
 14%|        | 63/437 [00:01<00:08, 45.36it/s][A
 16%|        | 68/437 [00:01<00:08, 45.39it/s][A
 17%|        | 73/437 [00:01<00:08, 45.30it/s][A
 18%|        | 78/437 [00:01<00:07, 45.11it/s][A
 19%|        | 83/437 [00:01<00:07, 44.91it/s][A
 20%|        | 88/437 [00:01<00:07, 44.80it/s][A
 21%|       | 93/437 [00:02<00:07, 44.94it/s][A
 22%|       | 98/437 [00:02<00:07, 45.02it/s][A
 24%|       | 103/437 [00:02<00:08, 41.21it/s][A
 25%|       | 108/437 [00:02<00:07, 42.52it/s][A
 26%|       | 113/437 [00:02<00:07, 43.37it/s][A
 27%|       | 118/437 [00:02<00:07, 44.10it/s][A
 28%|       | 123/437 [00:02<00:07, 44.43it/s][A
 29%|       | 128/437 [00:02<00:06, 44.57it/s][A
 30%|       | 133/437 [00:02<00:06, 44.64it/s][A
 32%|      | 138/437 [00:03<00:06, 44.76it/s][A
 33%|      | 143/437 [00:03<00:06, 44.58it/s][A
 34%|      | 148/437 [00:03<00:06, 44.49it/s][A
 35%|      | 153/437 [00:03<00:06, 44.81it/s][A
 36%|      | 158/437 [00:03<00:06, 44.83it/s][A
 37%|      | 163/437 [00:03<00:06, 45.05it/s][A
 38%|      | 168/437 [00:03<00:05, 45.10it/s][A
 40%|      | 173/437 [00:03<00:05, 45.12it/s][A
 41%|      | 178/437 [00:03<00:05, 45.08it/s][A
 42%|     | 183/437 [00:04<00:05, 44.88it/s][A
 43%|     | 188/437 [00:04<00:05, 44.84it/s][A
 44%|     | 193/437 [00:04<00:05, 44.76it/s][A
 45%|     | 198/437 [00:04<00:05, 44.92it/s][A
 46%|     | 203/437 [00:04<00:05, 45.03it/s][A
 48%|     | 208/437 [00:04<00:05, 45.21it/s][A
 49%|     | 213/437 [00:04<00:04, 45.28it/s][A
 50%|     | 218/437 [00:04<00:04, 45.21it/s][A
 51%|     | 223/437 [00:04<00:04, 45.25it/s][A
 52%|    | 228/437 [00:05<00:04, 45.11it/s][A
 53%|    | 233/437 [00:05<00:04, 45.05it/s][A
 54%|    | 238/437 [00:05<00:04, 42.43it/s][A
 56%|    | 243/437 [00:05<00:04, 43.43it/s][A
 57%|    | 248/437 [00:05<00:04, 44.05it/s][A
 58%|    | 253/437 [00:05<00:04, 44.44it/s][A
 59%|    | 258/437 [00:05<00:04, 44.61it/s][A
 60%|    | 263/437 [00:05<00:03, 44.84it/s][A
 61%|   | 268/437 [00:05<00:03, 44.92it/s][A
 62%|   | 273/437 [00:06<00:03, 44.80it/s][A
 64%|   | 278/437 [00:06<00:03, 44.63it/s][A
 65%|   | 283/437 [00:06<00:03, 44.60it/s][A
 66%|   | 288/437 [00:06<00:03, 44.85it/s][A
 67%|   | 293/437 [00:06<00:03, 45.08it/s][A
 68%|   | 298/437 [00:06<00:03, 45.20it/s][A
 69%|   | 303/437 [00:06<00:02, 45.27it/s][A
 70%|   | 308/437 [00:06<00:02, 45.24it/s][A
 72%|  | 313/437 [00:06<00:02, 45.23it/s][A
 73%|  | 318/437 [00:07<00:02, 45.15it/s][A
 74%|  | 323/437 [00:07<00:02, 45.01it/s][A
 75%|  | 328/437 [00:07<00:02, 45.00it/s][A
 76%|  | 333/437 [00:07<00:02, 45.05it/s][A
 77%|  | 338/437 [00:07<00:02, 45.03it/s][A
 78%|  | 343/437 [00:07<00:02, 45.10it/s][A
 80%|  | 348/437 [00:07<00:01, 45.26it/s][A
 81%|  | 353/437 [00:07<00:01, 45.02it/s][A
 82%| | 358/437 [00:07<00:01, 45.05it/s][A
 83%| | 363/437 [00:08<00:01, 44.99it/s][A
 84%| | 368/437 [00:08<00:01, 44.91it/s][A
 85%| | 373/437 [00:08<00:01, 43.29it/s][A
 86%| | 378/437 [00:08<00:01, 43.91it/s][A
 88%| | 383/437 [00:08<00:01, 44.35it/s][A
 89%| | 388/437 [00:08<00:01, 44.66it/s][A
 90%| | 393/437 [00:08<00:00, 44.83it/s][A
 91%| | 398/437 [00:08<00:00, 44.98it/s][A
 92%|| 403/437 [00:08<00:00, 45.11it/s][A
 93%|| 408/437 [00:09<00:00, 45.07it/s][A
 95%|| 413/437 [00:09<00:00, 44.80it/s][A
 96%|| 418/437 [00:09<00:00, 44.83it/s][A
 97%|| 423/437 [00:09<00:00, 44.90it/s][A
 98%|| 428/437 [00:09<00:00, 45.02it/s][A
 99%|| 433/437 [00:09<00:00, 45.18it/s][A                                                 
                                                 [A100%|| 805/805 [04:58<00:00,  3.56it/s]
100%|| 437/437 [00:09<00:00, 45.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:26:24,591 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 03:26:24,779 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:26:28,174 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:26:28,282 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:26:28,320 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:26:29,426 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:26:29,427 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161 (score: 1.0192089080810547).
                                                 100%|| 805/805 [05:13<00:00,  3.56it/s]100%|| 805/805 [05:13<00:00,  2.57it/s]
[INFO|trainer.py:1894] 2023-08-28 03:26:39,491 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 03:26:39,595 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:26:42,195 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:26:42,280 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:26:42,353 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:26:42,849 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   train_runtime            = 0:05:13.43
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   train_samples            =      10320
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   train_samples_per_second =    164.627
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:42,859 >>   train_steps_per_second   =      2.568
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.7455, 'eval_samples_per_second': 358.012, 'eval_steps_per_second': 44.841, 'epoch': 5.0}
{'train_runtime': 313.4352, 'train_samples_per_second': 164.627, 'train_steps_per_second': 2.568, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:26:43 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:26:43,075 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:26:43,075 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 03:26:43,075 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 55.43it/s]  3%|         | 12/437 [00:00<00:08, 49.25it/s]  4%|         | 17/437 [00:00<00:08, 47.99it/s]  5%|         | 22/437 [00:00<00:09, 45.43it/s]  6%|         | 27/437 [00:00<00:08, 45.78it/s]  7%|         | 32/437 [00:00<00:08, 45.61it/s]  8%|         | 37/437 [00:00<00:10, 37.31it/s] 10%|         | 42/437 [00:00<00:09, 40.11it/s] 11%|         | 47/437 [00:01<00:12, 30.98it/s] 12%|        | 52/437 [00:01<00:11, 34.47it/s] 13%|        | 57/437 [00:01<00:10, 37.30it/s] 14%|        | 62/437 [00:01<00:09, 39.55it/s] 15%|        | 67/437 [00:01<00:08, 41.23it/s] 16%|        | 72/437 [00:01<00:08, 42.47it/s] 18%|        | 77/437 [00:01<00:08, 43.34it/s] 19%|        | 82/437 [00:01<00:08, 44.02it/s] 20%|        | 87/437 [00:02<00:07, 44.30it/s] 21%|        | 92/437 [00:02<00:07, 44.34it/s] 22%|       | 97/437 [00:02<00:07, 44.59it/s] 23%|       | 102/437 [00:02<00:07, 44.85it/s] 24%|       | 107/437 [00:02<00:07, 45.18it/s] 26%|       | 112/437 [00:02<00:07, 45.29it/s] 27%|       | 117/437 [00:02<00:07, 45.36it/s] 28%|       | 122/437 [00:02<00:06, 45.37it/s] 29%|       | 127/437 [00:02<00:06, 44.87it/s] 30%|       | 132/437 [00:03<00:06, 44.84it/s] 31%|      | 137/437 [00:03<00:06, 44.90it/s] 32%|      | 142/437 [00:03<00:06, 44.91it/s] 34%|      | 147/437 [00:03<00:06, 45.17it/s] 35%|      | 152/437 [00:03<00:06, 45.31it/s] 36%|      | 157/437 [00:03<00:06, 45.41it/s] 37%|      | 162/437 [00:03<00:06, 45.49it/s] 38%|      | 167/437 [00:03<00:05, 45.44it/s] 39%|      | 172/437 [00:03<00:05, 45.35it/s] 41%|      | 177/437 [00:04<00:05, 45.27it/s] 42%|     | 182/437 [00:04<00:05, 45.10it/s] 43%|     | 187/437 [00:04<00:05, 44.98it/s] 44%|     | 192/437 [00:04<00:05, 45.15it/s] 45%|     | 197/437 [00:04<00:05, 45.23it/s] 46%|     | 202/437 [00:04<00:05, 45.50it/s] 47%|     | 207/437 [00:04<00:05, 45.50it/s] 49%|     | 212/437 [00:04<00:04, 45.47it/s] 50%|     | 217/437 [00:04<00:04, 45.36it/s] 51%|     | 222/437 [00:05<00:04, 45.28it/s] 52%|    | 227/437 [00:05<00:04, 45.16it/s] 53%|    | 232/437 [00:05<00:04, 45.10it/s] 54%|    | 237/437 [00:05<00:04, 45.13it/s] 55%|    | 242/437 [00:05<00:04, 45.29it/s] 57%|    | 247/437 [00:05<00:04, 45.37it/s] 58%|    | 252/437 [00:05<00:04, 45.46it/s] 59%|    | 257/437 [00:05<00:03, 45.40it/s] 60%|    | 262/437 [00:05<00:03, 45.18it/s] 61%|    | 267/437 [00:06<00:03, 43.62it/s] 62%|   | 272/437 [00:06<00:03, 44.04it/s] 63%|   | 277/437 [00:06<00:03, 44.42it/s] 65%|   | 282/437 [00:06<00:03, 44.64it/s] 66%|   | 287/437 [00:06<00:03, 44.74it/s] 67%|   | 292/437 [00:06<00:03, 45.05it/s] 68%|   | 297/437 [00:06<00:03, 45.23it/s] 69%|   | 302/437 [00:06<00:02, 45.39it/s] 70%|   | 307/437 [00:06<00:02, 45.11it/s] 71%|  | 312/437 [00:07<00:02, 45.07it/s] 73%|  | 317/437 [00:07<00:02, 45.08it/s] 74%|  | 322/437 [00:07<00:02, 45.20it/s] 75%|  | 327/437 [00:07<00:02, 45.19it/s] 76%|  | 332/437 [00:07<00:02, 45.17it/s] 77%|  | 337/437 [00:07<00:02, 45.24it/s] 78%|  | 342/437 [00:07<00:02, 45.35it/s] 79%|  | 347/437 [00:07<00:01, 45.43it/s] 81%|  | 352/437 [00:07<00:01, 45.28it/s] 82%| | 357/437 [00:08<00:01, 45.25it/s] 83%| | 362/437 [00:08<00:01, 45.07it/s] 84%| | 367/437 [00:08<00:01, 45.18it/s] 85%| | 372/437 [00:08<00:01, 45.15it/s] 86%| | 377/437 [00:08<00:01, 45.34it/s] 87%| | 382/437 [00:08<00:01, 45.31it/s] 89%| | 387/437 [00:08<00:01, 45.43it/s] 90%| | 392/437 [00:08<00:00, 45.34it/s] 91%| | 397/437 [00:08<00:00, 45.42it/s] 92%|| 402/437 [00:09<00:00, 45.30it/s] 93%|| 407/437 [00:09<00:00, 45.18it/s] 94%|| 412/437 [00:09<00:00, 45.13it/s] 95%|| 417/437 [00:09<00:00, 45.17it/s] 97%|| 422/437 [00:09<00:00, 45.21it/s] 98%|| 427/437 [00:09<00:00, 45.27it/s] 99%|| 432/437 [00:09<00:00, 45.43it/s]100%|| 437/437 [00:09<00:00, 45.42it/s]100%|| 437/437 [00:09<00:00, 44.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:26:52,933 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   eval_loss               =     1.0192
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   eval_runtime            = 0:00:09.85
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   eval_samples_per_second =    353.922
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   eval_steps_per_second   =     44.329
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:26:52,933 >>   perplexity              =      2.771
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:05,844 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:05,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:05,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:05,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:05,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:27:06,465 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:27:06,501 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:27:07,093 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:27:08,171 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:27:08,171 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:11,167 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:11,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:11,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:11,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:27:11,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:27:11,836 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:27:11,838 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:27:12,473 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:27:12,626 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:27:12,626 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.74it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.71it/s]Extractor Predicting: 17it [00:09,  1.72it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:12,  1.71it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:16,  1.71it/s]Extractor Predicting: 30it [00:17,  1.75it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:18,  1.72it/s]Extractor Predicting: 33it [00:19,  1.78it/s]Extractor Predicting: 34it [00:19,  1.78it/s]Extractor Predicting: 35it [00:20,  1.74it/s]Extractor Predicting: 36it [00:21,  1.70it/s]Extractor Predicting: 37it [00:21,  1.69it/s]Extractor Predicting: 38it [00:22,  1.66it/s]Extractor Predicting: 39it [00:22,  1.67it/s]Extractor Predicting: 40it [00:23,  1.68it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:24,  1.67it/s]Extractor Predicting: 43it [00:25,  1.66it/s]Extractor Predicting: 44it [00:25,  1.65it/s]Extractor Predicting: 45it [00:26,  1.66it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:27,  1.68it/s]Extractor Predicting: 48it [00:28,  1.54it/s]Extractor Predicting: 49it [00:29,  1.57it/s]Extractor Predicting: 50it [00:29,  1.58it/s]Extractor Predicting: 51it [00:30,  1.61it/s]Extractor Predicting: 52it [00:30,  1.65it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:31,  1.66it/s]Extractor Predicting: 55it [00:32,  1.62it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:33,  1.65it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:36,  1.68it/s]Extractor Predicting: 63it [00:37,  1.70it/s]Extractor Predicting: 64it [00:38,  1.69it/s]Extractor Predicting: 65it [00:38,  1.72it/s]Extractor Predicting: 66it [00:39,  1.70it/s]Extractor Predicting: 67it [00:39,  1.72it/s]Extractor Predicting: 68it [00:40,  1.73it/s]Extractor Predicting: 69it [00:40,  1.77it/s]Extractor Predicting: 70it [00:41,  1.79it/s]Extractor Predicting: 71it [00:41,  1.79it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:43,  1.66it/s]Extractor Predicting: 74it [00:43,  1.71it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:45,  1.66it/s]Extractor Predicting: 77it [00:45,  1.67it/s]Extractor Predicting: 78it [00:46,  1.72it/s]Extractor Predicting: 79it [00:46,  1.74it/s]Extractor Predicting: 80it [00:47,  1.75it/s]Extractor Predicting: 81it [00:47,  1.76it/s]Extractor Predicting: 82it [00:48,  1.78it/s]Extractor Predicting: 83it [00:49,  1.74it/s]Extractor Predicting: 84it [00:49,  1.71it/s]Extractor Predicting: 85it [00:50,  1.76it/s]Extractor Predicting: 86it [00:50,  1.77it/s]Extractor Predicting: 87it [00:51,  1.79it/s]Extractor Predicting: 88it [00:51,  1.79it/s]Extractor Predicting: 89it [00:52,  1.77it/s]Extractor Predicting: 90it [00:52,  1.74it/s]Extractor Predicting: 91it [00:53,  1.74it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:54,  1.71it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:55,  1.70it/s]Extractor Predicting: 96it [00:56,  1.68it/s]Extractor Predicting: 97it [00:57,  1.70it/s]Extractor Predicting: 98it [00:57,  1.74it/s]Extractor Predicting: 99it [00:58,  1.71it/s]Extractor Predicting: 100it [00:58,  1.69it/s]Extractor Predicting: 101it [00:59,  1.72it/s]Extractor Predicting: 102it [01:00,  1.68it/s]Extractor Predicting: 103it [01:00,  1.66it/s]Extractor Predicting: 104it [01:01,  1.68it/s]Extractor Predicting: 105it [01:01,  1.68it/s]Extractor Predicting: 106it [01:02,  1.67it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:03,  1.70it/s]Extractor Predicting: 109it [01:04,  1.65it/s]Extractor Predicting: 110it [01:04,  1.65it/s]Extractor Predicting: 111it [01:05,  1.66it/s]Extractor Predicting: 112it [01:06,  1.64it/s]Extractor Predicting: 113it [01:06,  1.66it/s]Extractor Predicting: 114it [01:07,  1.67it/s]Extractor Predicting: 115it [01:07,  1.66it/s]Extractor Predicting: 116it [01:08,  1.70it/s]Extractor Predicting: 117it [01:09,  1.69it/s]Extractor Predicting: 118it [01:09,  1.68it/s]Extractor Predicting: 119it [01:10,  1.73it/s]Extractor Predicting: 120it [01:10,  1.78it/s]Extractor Predicting: 121it [01:11,  1.78it/s]Extractor Predicting: 122it [01:11,  1.79it/s]Extractor Predicting: 123it [01:12,  1.75it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:13,  1.75it/s]Extractor Predicting: 126it [01:14,  1.60it/s]Extractor Predicting: 127it [01:14,  1.64it/s]Extractor Predicting: 128it [01:15,  1.64it/s]Extractor Predicting: 129it [01:16,  1.69it/s]Extractor Predicting: 130it [01:16,  1.66it/s]Extractor Predicting: 131it [01:17,  1.69it/s]Extractor Predicting: 132it [01:17,  1.72it/s]Extractor Predicting: 133it [01:18,  1.71it/s]Extractor Predicting: 134it [01:19,  1.70it/s]Extractor Predicting: 135it [01:19,  1.71it/s]Extractor Predicting: 136it [01:20,  1.73it/s]Extractor Predicting: 137it [01:20,  1.71it/s]Extractor Predicting: 138it [01:21,  1.70it/s]Extractor Predicting: 139it [01:21,  1.74it/s]Extractor Predicting: 140it [01:22,  1.73it/s]Extractor Predicting: 141it [01:23,  1.74it/s]Extractor Predicting: 142it [01:23,  1.72it/s]Extractor Predicting: 143it [01:24,  1.73it/s]Extractor Predicting: 144it [01:24,  1.74it/s]Extractor Predicting: 145it [01:24,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:48,945 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:48,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:48,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:48,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:48,968 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:28:49,599 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:28:49,601 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:28:50,556 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:28:51,631 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:28:51,631 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:54,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:54,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:54,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:54,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:28:54,596 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:28:55,245 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:28:55,246 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:28:55,847 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:28:56,019 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:28:56,020 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.78it/s]Extractor Predicting: 24it [00:14,  1.73it/s]Extractor Predicting: 25it [00:14,  1.68it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:16,  1.58it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:17,  1.58it/s]Extractor Predicting: 31it [00:18,  1.61it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:19,  1.64it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.64it/s]Extractor Predicting: 36it [00:21,  1.69it/s]Extractor Predicting: 37it [00:22,  1.75it/s]Extractor Predicting: 38it [00:22,  1.74it/s]Extractor Predicting: 39it [00:23,  1.75it/s]Extractor Predicting: 40it [00:23,  1.76it/s]Extractor Predicting: 41it [00:24,  1.73it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.72it/s]Extractor Predicting: 44it [00:26,  1.74it/s]Extractor Predicting: 45it [00:26,  1.72it/s]Extractor Predicting: 46it [00:27,  1.74it/s]Extractor Predicting: 47it [00:27,  1.72it/s]Extractor Predicting: 48it [00:28,  1.73it/s]Extractor Predicting: 49it [00:28,  1.73it/s]Extractor Predicting: 50it [00:29,  1.71it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:30,  1.73it/s]Extractor Predicting: 53it [00:31,  1.68it/s]Extractor Predicting: 54it [00:31,  1.71it/s]Extractor Predicting: 55it [00:32,  1.67it/s]Extractor Predicting: 56it [00:33,  1.69it/s]Extractor Predicting: 57it [00:33,  1.73it/s]Extractor Predicting: 58it [00:34,  1.77it/s]Extractor Predicting: 59it [00:34,  1.74it/s]Extractor Predicting: 60it [00:35,  1.84it/s]Extractor Predicting: 61it [00:35,  1.86it/s]Extractor Predicting: 62it [00:36,  1.89it/s]Extractor Predicting: 63it [00:36,  1.90it/s]Extractor Predicting: 64it [00:37,  1.94it/s]Extractor Predicting: 65it [00:37,  1.95it/s]Extractor Predicting: 66it [00:38,  1.94it/s]Extractor Predicting: 67it [00:38,  1.94it/s]Extractor Predicting: 68it [00:39,  1.99it/s]Extractor Predicting: 69it [00:39,  2.04it/s]Extractor Predicting: 70it [00:40,  2.01it/s]Extractor Predicting: 71it [00:40,  2.01it/s]Extractor Predicting: 72it [00:41,  2.02it/s]Extractor Predicting: 73it [00:41,  2.02it/s]Extractor Predicting: 74it [00:42,  1.96it/s]Extractor Predicting: 75it [00:42,  1.95it/s]Extractor Predicting: 76it [00:43,  2.00it/s]Extractor Predicting: 77it [00:43,  2.03it/s]Extractor Predicting: 78it [00:44,  2.01it/s]Extractor Predicting: 79it [00:44,  2.03it/s]Extractor Predicting: 80it [00:45,  2.00it/s]Extractor Predicting: 81it [00:45,  1.99it/s]Extractor Predicting: 82it [00:46,  1.98it/s]Extractor Predicting: 83it [00:46,  1.96it/s]Extractor Predicting: 84it [00:47,  1.95it/s]Extractor Predicting: 85it [00:47,  1.97it/s]Extractor Predicting: 86it [00:48,  1.87it/s]Extractor Predicting: 87it [00:48,  1.82it/s]Extractor Predicting: 88it [00:49,  1.79it/s]Extractor Predicting: 89it [00:50,  1.76it/s]Extractor Predicting: 90it [00:50,  1.70it/s]Extractor Predicting: 91it [00:51,  1.71it/s]Extractor Predicting: 92it [00:52,  1.65it/s]Extractor Predicting: 93it [00:52,  1.70it/s]Extractor Predicting: 94it [00:53,  1.69it/s]Extractor Predicting: 95it [00:53,  1.69it/s]Extractor Predicting: 96it [00:54,  1.68it/s]Extractor Predicting: 97it [00:54,  1.68it/s]Extractor Predicting: 98it [00:55,  1.64it/s]Extractor Predicting: 99it [00:56,  1.68it/s]Extractor Predicting: 100it [00:56,  1.67it/s]Extractor Predicting: 101it [00:57,  1.68it/s]Extractor Predicting: 102it [00:57,  1.67it/s]Extractor Predicting: 103it [00:58,  1.65it/s]Extractor Predicting: 104it [00:59,  1.63it/s]Extractor Predicting: 105it [00:59,  1.65it/s]Extractor Predicting: 106it [01:00,  1.62it/s]Extractor Predicting: 107it [01:01,  1.62it/s]Extractor Predicting: 108it [01:01,  1.58it/s]Extractor Predicting: 109it [01:02,  1.63it/s]Extractor Predicting: 110it [01:02,  1.64it/s]Extractor Predicting: 111it [01:03,  1.63it/s]Extractor Predicting: 112it [01:04,  1.60it/s]Extractor Predicting: 113it [01:04,  1.59it/s]Extractor Predicting: 114it [01:05,  1.66it/s]Extractor Predicting: 115it [01:06,  1.64it/s]Extractor Predicting: 116it [01:06,  1.65it/s]Extractor Predicting: 117it [01:07,  1.68it/s]Extractor Predicting: 118it [01:07,  1.64it/s]Extractor Predicting: 119it [01:08,  1.60it/s]Extractor Predicting: 120it [01:09,  1.63it/s]Extractor Predicting: 121it [01:09,  1.60it/s]Extractor Predicting: 122it [01:10,  1.64it/s]Extractor Predicting: 123it [01:10,  1.66it/s]Extractor Predicting: 124it [01:11,  1.65it/s]Extractor Predicting: 125it [01:12,  1.64it/s]Extractor Predicting: 126it [01:12,  1.60it/s]Extractor Predicting: 127it [01:13,  1.60it/s]Extractor Predicting: 128it [01:14,  1.58it/s]Extractor Predicting: 129it [01:14,  1.60it/s]Extractor Predicting: 130it [01:15,  1.59it/s]Extractor Predicting: 131it [01:16,  1.42it/s]Extractor Predicting: 132it [01:16,  1.48it/s]Extractor Predicting: 133it [01:17,  1.53it/s]Extractor Predicting: 134it [01:17,  1.59it/s]Extractor Predicting: 135it [01:18,  1.59it/s]Extractor Predicting: 136it [01:19,  1.55it/s]Extractor Predicting: 137it [01:19,  1.58it/s]Extractor Predicting: 138it [01:20,  1.59it/s]Extractor Predicting: 139it [01:21,  1.59it/s]Extractor Predicting: 140it [01:21,  1.62it/s]Extractor Predicting: 141it [01:22,  1.57it/s]Extractor Predicting: 142it [01:22,  1.60it/s]Extractor Predicting: 143it [01:23,  1.56it/s]Extractor Predicting: 144it [01:24,  1.58it/s]Extractor Predicting: 145it [01:24,  1.60it/s]Extractor Predicting: 146it [01:25,  1.58it/s]Extractor Predicting: 147it [01:26,  1.62it/s]Extractor Predicting: 148it [01:26,  1.66it/s]Extractor Predicting: 149it [01:27,  1.62it/s]Extractor Predicting: 150it [01:27,  1.63it/s]Extractor Predicting: 151it [01:28,  1.64it/s]Extractor Predicting: 152it [01:29,  1.68it/s]Extractor Predicting: 153it [01:29,  1.70it/s]Extractor Predicting: 154it [01:30,  1.73it/s]Extractor Predicting: 155it [01:30,  1.75it/s]Extractor Predicting: 156it [01:31,  1.77it/s]Extractor Predicting: 157it [01:31,  1.73it/s]Extractor Predicting: 158it [01:32,  1.75it/s]Extractor Predicting: 159it [01:32,  1.82it/s]Extractor Predicting: 160it [01:33,  1.76it/s]Extractor Predicting: 161it [01:34,  1.73it/s]Extractor Predicting: 162it [01:34,  1.70it/s]Extractor Predicting: 163it [01:35,  1.68it/s]Extractor Predicting: 164it [01:36,  1.67it/s]Extractor Predicting: 165it [01:36,  1.67it/s]Extractor Predicting: 166it [01:37,  1.65it/s]Extractor Predicting: 167it [01:37,  1.66it/s]Extractor Predicting: 168it [01:38,  1.66it/s]Extractor Predicting: 169it [01:39,  1.65it/s]Extractor Predicting: 170it [01:39,  1.69it/s]Extractor Predicting: 171it [01:40,  1.68it/s]Extractor Predicting: 172it [01:40,  1.66it/s]Extractor Predicting: 173it [01:41,  1.68it/s]Extractor Predicting: 174it [01:42,  1.68it/s]Extractor Predicting: 175it [01:42,  1.68it/s]Extractor Predicting: 176it [01:43,  1.68it/s]Extractor Predicting: 177it [01:43,  1.70it/s]Extractor Predicting: 178it [01:44,  1.69it/s]Extractor Predicting: 179it [01:44,  1.73it/s]Extractor Predicting: 180it [01:45,  1.75it/s]Extractor Predicting: 181it [01:46,  1.73it/s]Extractor Predicting: 182it [01:46,  1.78it/s]Extractor Predicting: 183it [01:47,  1.77it/s]Extractor Predicting: 184it [01:47,  1.75it/s]Extractor Predicting: 185it [01:48,  1.79it/s]Extractor Predicting: 186it [01:48,  1.79it/s]Extractor Predicting: 187it [01:49,  1.75it/s]Extractor Predicting: 188it [01:49,  1.80it/s]Extractor Predicting: 189it [01:50,  1.78it/s]Extractor Predicting: 190it [01:51,  1.83it/s]Extractor Predicting: 191it [01:51,  1.76it/s]Extractor Predicting: 192it [01:52,  1.80it/s]Extractor Predicting: 193it [01:52,  1.81it/s]Extractor Predicting: 194it [01:53,  1.86it/s]Extractor Predicting: 195it [01:53,  1.79it/s]Extractor Predicting: 196it [01:54,  1.80it/s]Extractor Predicting: 197it [01:54,  1.83it/s]Extractor Predicting: 198it [01:55,  1.80it/s]Extractor Predicting: 199it [01:56,  1.76it/s]Extractor Predicting: 200it [01:56,  1.77it/s]Extractor Predicting: 201it [01:57,  1.74it/s]Extractor Predicting: 202it [01:57,  1.79it/s]Extractor Predicting: 203it [01:58,  1.84it/s]Extractor Predicting: 204it [01:58,  1.85it/s]Extractor Predicting: 205it [01:59,  1.85it/s]Extractor Predicting: 206it [01:59,  1.92it/s]Extractor Predicting: 207it [02:00,  1.81it/s]Extractor Predicting: 208it [02:00,  1.88it/s]Extractor Predicting: 209it [02:01,  1.86it/s]Extractor Predicting: 210it [02:02,  1.87it/s]Extractor Predicting: 211it [02:02,  1.86it/s]Extractor Predicting: 212it [02:03,  1.91it/s]Extractor Predicting: 213it [02:03,  1.74it/s]Extractor Predicting: 214it [02:04,  1.80it/s]Extractor Predicting: 215it [02:04,  1.89it/s]Extractor Predicting: 216it [02:05,  1.90it/s]Extractor Predicting: 217it [02:05,  1.87it/s]Extractor Predicting: 218it [02:06,  1.87it/s]Extractor Predicting: 219it [02:06,  1.83it/s]Extractor Predicting: 220it [02:07,  1.87it/s]Extractor Predicting: 221it [02:07,  1.86it/s]Extractor Predicting: 222it [02:08,  1.95it/s]Extractor Predicting: 223it [02:09,  1.90it/s]Extractor Predicting: 224it [02:09,  1.95it/s]Extractor Predicting: 225it [02:10,  1.93it/s]Extractor Predicting: 226it [02:10,  1.94it/s]Extractor Predicting: 227it [02:11,  1.91it/s]Extractor Predicting: 228it [02:11,  1.91it/s]Extractor Predicting: 229it [02:12,  1.82it/s]Extractor Predicting: 230it [02:12,  1.78it/s]Extractor Predicting: 231it [02:13,  1.70it/s]Extractor Predicting: 232it [02:14,  1.66it/s]Extractor Predicting: 233it [02:14,  1.65it/s]Extractor Predicting: 234it [02:15,  1.64it/s]Extractor Predicting: 235it [02:15,  1.65it/s]Extractor Predicting: 236it [02:16,  1.65it/s]Extractor Predicting: 237it [02:17,  1.61it/s]Extractor Predicting: 238it [02:17,  1.57it/s]Extractor Predicting: 239it [02:18,  1.59it/s]Extractor Predicting: 240it [02:19,  1.60it/s]Extractor Predicting: 241it [02:19,  1.62it/s]Extractor Predicting: 242it [02:20,  1.65it/s]Extractor Predicting: 243it [02:20,  1.57it/s]Extractor Predicting: 244it [02:21,  1.61it/s]Extractor Predicting: 245it [02:22,  1.62it/s]Extractor Predicting: 246it [02:22,  1.64it/s]Extractor Predicting: 247it [02:23,  1.60it/s]Extractor Predicting: 248it [02:24,  1.58it/s]Extractor Predicting: 249it [02:24,  1.38it/s]Extractor Predicting: 250it [02:25,  1.44it/s]Extractor Predicting: 251it [02:26,  1.49it/s]Extractor Predicting: 252it [02:26,  1.53it/s]Extractor Predicting: 253it [02:27,  1.55it/s]Extractor Predicting: 254it [02:28,  1.55it/s]Extractor Predicting: 255it [02:28,  1.57it/s]Extractor Predicting: 256it [02:29,  1.60it/s]Extractor Predicting: 257it [02:29,  1.67it/s]Extractor Predicting: 258it [02:30,  1.66it/s]Extractor Predicting: 259it [02:31,  1.69it/s]Extractor Predicting: 260it [02:31,  1.73it/s]Extractor Predicting: 261it [02:32,  1.73it/s]Extractor Predicting: 262it [02:32,  1.73it/s]Extractor Predicting: 263it [02:33,  1.75it/s]Extractor Predicting: 264it [02:33,  1.73it/s]Extractor Predicting: 265it [02:34,  1.71it/s]Extractor Predicting: 266it [02:35,  1.74it/s]Extractor Predicting: 267it [02:35,  1.78it/s]Extractor Predicting: 268it [02:36,  1.75it/s]Extractor Predicting: 269it [02:36,  1.76it/s]Extractor Predicting: 270it [02:37,  1.71it/s]Extractor Predicting: 271it [02:37,  1.70it/s]Extractor Predicting: 272it [02:38,  1.70it/s]Extractor Predicting: 273it [02:39,  1.71it/s]Extractor Predicting: 274it [02:39,  1.70it/s]Extractor Predicting: 275it [02:40,  1.74it/s]Extractor Predicting: 276it [02:40,  1.74it/s]Extractor Predicting: 277it [02:41,  1.73it/s]Extractor Predicting: 278it [02:42,  1.66it/s]Extractor Predicting: 279it [02:42,  1.70it/s]Extractor Predicting: 280it [02:43,  1.69it/s]Extractor Predicting: 281it [02:43,  1.73it/s]Extractor Predicting: 282it [02:44,  1.72it/s]Extractor Predicting: 283it [02:44,  1.72it/s]Extractor Predicting: 284it [02:45,  1.73it/s]Extractor Predicting: 285it [02:46,  1.70it/s]Extractor Predicting: 286it [02:46,  1.69it/s]Extractor Predicting: 287it [02:47,  1.73it/s]Extractor Predicting: 288it [02:47,  1.73it/s]Extractor Predicting: 289it [02:48,  1.72it/s]Extractor Predicting: 290it [02:49,  1.70it/s]Extractor Predicting: 291it [02:49,  1.70it/s]Extractor Predicting: 292it [02:50,  1.71it/s]Extractor Predicting: 293it [02:50,  1.71it/s]Extractor Predicting: 294it [02:51,  1.71it/s]Extractor Predicting: 295it [02:51,  1.72it/s]Extractor Predicting: 296it [02:52,  1.69it/s]Extractor Predicting: 297it [02:53,  1.72it/s]Extractor Predicting: 298it [02:53,  1.69it/s]Extractor Predicting: 299it [02:54,  1.75it/s]Extractor Predicting: 300it [02:54,  1.74it/s]Extractor Predicting: 301it [02:55,  1.75it/s]Extractor Predicting: 302it [02:56,  1.71it/s]Extractor Predicting: 303it [02:56,  1.77it/s]Extractor Predicting: 304it [02:57,  1.76it/s]Extractor Predicting: 305it [02:57,  1.80it/s]Extractor Predicting: 306it [02:58,  1.78it/s]Extractor Predicting: 307it [02:58,  1.75it/s]Extractor Predicting: 308it [02:59,  1.69it/s]Extractor Predicting: 309it [03:00,  1.73it/s]Extractor Predicting: 310it [03:00,  1.69it/s]Extractor Predicting: 311it [03:01,  1.69it/s]Extractor Predicting: 312it [03:01,  1.70it/s]Extractor Predicting: 313it [03:02,  1.72it/s]Extractor Predicting: 314it [03:03,  1.68it/s]Extractor Predicting: 315it [03:03,  1.72it/s]Extractor Predicting: 316it [03:04,  1.72it/s]Extractor Predicting: 317it [03:04,  1.71it/s]Extractor Predicting: 318it [03:05,  1.78it/s]Extractor Predicting: 319it [03:05,  1.77it/s]Extractor Predicting: 320it [03:06,  1.78it/s]Extractor Predicting: 321it [03:06,  1.75it/s]Extractor Predicting: 322it [03:07,  1.74it/s]Extractor Predicting: 323it [03:08,  1.74it/s]Extractor Predicting: 324it [03:08,  1.72it/s]Extractor Predicting: 325it [03:09,  1.72it/s]Extractor Predicting: 326it [03:09,  1.73it/s]Extractor Predicting: 327it [03:10,  1.71it/s]Extractor Predicting: 328it [03:11,  1.71it/s]Extractor Predicting: 329it [03:11,  1.65it/s]Extractor Predicting: 330it [03:12,  1.67it/s]Extractor Predicting: 331it [03:12,  1.65it/s]Extractor Predicting: 332it [03:13,  1.71it/s]Extractor Predicting: 333it [03:14,  1.72it/s]Extractor Predicting: 334it [03:14,  1.73it/s]Extractor Predicting: 335it [03:15,  1.51it/s]Extractor Predicting: 336it [03:16,  1.55it/s]Extractor Predicting: 337it [03:16,  1.59it/s]Extractor Predicting: 338it [03:17,  1.63it/s]Extractor Predicting: 339it [03:17,  1.63it/s]Extractor Predicting: 340it [03:18,  1.71it/s]Extractor Predicting: 341it [03:18,  1.69it/s]Extractor Predicting: 342it [03:19,  1.71it/s]Extractor Predicting: 343it [03:20,  1.65it/s]Extractor Predicting: 344it [03:20,  1.69it/s]Extractor Predicting: 345it [03:21,  1.66it/s]Extractor Predicting: 346it [03:21,  1.70it/s]Extractor Predicting: 347it [03:22,  1.71it/s]Extractor Predicting: 348it [03:23,  1.70it/s]Extractor Predicting: 349it [03:23,  1.63it/s]Extractor Predicting: 350it [03:24,  1.63it/s]Extractor Predicting: 351it [03:25,  1.62it/s]Extractor Predicting: 352it [03:25,  1.65it/s]Extractor Predicting: 353it [03:26,  1.61it/s]Extractor Predicting: 354it [03:26,  1.64it/s]Extractor Predicting: 355it [03:27,  1.64it/s]Extractor Predicting: 356it [03:28,  1.68it/s]Extractor Predicting: 357it [03:28,  1.66it/s]Extractor Predicting: 358it [03:29,  1.63it/s]Extractor Predicting: 359it [03:29,  1.62it/s]Extractor Predicting: 360it [03:30,  1.59it/s]Extractor Predicting: 361it [03:31,  1.65it/s]Extractor Predicting: 362it [03:31,  1.69it/s]Extractor Predicting: 363it [03:32,  1.66it/s]Extractor Predicting: 364it [03:32,  1.70it/s]Extractor Predicting: 365it [03:33,  1.71it/s]Extractor Predicting: 366it [03:34,  1.69it/s]Extractor Predicting: 367it [03:34,  1.69it/s]Extractor Predicting: 368it [03:35,  1.67it/s]Extractor Predicting: 369it [03:35,  1.67it/s]Extractor Predicting: 370it [03:36,  1.66it/s]Extractor Predicting: 371it [03:37,  1.69it/s]Extractor Predicting: 372it [03:37,  1.71it/s]Extractor Predicting: 373it [03:38,  1.73it/s]Extractor Predicting: 374it [03:38,  1.73it/s]Extractor Predicting: 375it [03:39,  1.70it/s]Extractor Predicting: 376it [03:39,  1.72it/s]Extractor Predicting: 377it [03:40,  1.73it/s]Extractor Predicting: 378it [03:41,  1.75it/s]Extractor Predicting: 379it [03:41,  1.77it/s]Extractor Predicting: 380it [03:42,  1.77it/s]Extractor Predicting: 381it [03:42,  1.76it/s]Extractor Predicting: 382it [03:43,  1.71it/s]Extractor Predicting: 383it [03:43,  1.74it/s]Extractor Predicting: 384it [03:44,  1.76it/s]Extractor Predicting: 385it [03:45,  1.77it/s]Extractor Predicting: 386it [03:45,  1.80it/s]Extractor Predicting: 387it [03:46,  1.82it/s]Extractor Predicting: 388it [03:46,  1.76it/s]Extractor Predicting: 389it [03:47,  1.76it/s]Extractor Predicting: 390it [03:47,  1.76it/s]Extractor Predicting: 391it [03:48,  1.76it/s]Extractor Predicting: 392it [03:48,  1.76it/s]Extractor Predicting: 393it [03:49,  1.78it/s]Extractor Predicting: 394it [03:50,  1.77it/s]Extractor Predicting: 395it [03:50,  1.70it/s]Extractor Predicting: 396it [03:51,  1.74it/s]Extractor Predicting: 397it [03:51,  1.77it/s]Extractor Predicting: 398it [03:52,  1.73it/s]Extractor Predicting: 399it [03:52,  1.73it/s]Extractor Predicting: 400it [03:53,  1.71it/s]Extractor Predicting: 401it [03:54,  1.73it/s]Extractor Predicting: 402it [03:54,  1.79it/s]Extractor Predicting: 403it [03:55,  1.77it/s]Extractor Predicting: 404it [03:55,  1.79it/s]Extractor Predicting: 405it [03:56,  1.79it/s]Extractor Predicting: 406it [03:56,  1.78it/s]Extractor Predicting: 407it [03:57,  1.76it/s]Extractor Predicting: 408it [03:58,  1.80it/s]Extractor Predicting: 409it [03:58,  1.82it/s]Extractor Predicting: 410it [03:59,  1.82it/s]Extractor Predicting: 411it [03:59,  1.78it/s]Extractor Predicting: 412it [04:00,  1.78it/s]Extractor Predicting: 413it [04:00,  1.79it/s]Extractor Predicting: 414it [04:01,  1.79it/s]Extractor Predicting: 415it [04:01,  1.84it/s]Extractor Predicting: 416it [04:02,  1.60it/s]Extractor Predicting: 417it [04:03,  1.63it/s]Extractor Predicting: 418it [04:03,  1.66it/s]Extractor Predicting: 419it [04:04,  1.69it/s]Extractor Predicting: 420it [04:04,  1.72it/s]Extractor Predicting: 421it [04:05,  1.73it/s]Extractor Predicting: 422it [04:06,  1.73it/s]Extractor Predicting: 423it [04:06,  1.75it/s]Extractor Predicting: 424it [04:07,  1.72it/s]Extractor Predicting: 425it [04:07,  1.99it/s]Extractor Predicting: 425it [04:07,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:18,073 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:18,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:18,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:18,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:18,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:33:18,723 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:33:18,724 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:19,324 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:20,413 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:20,413 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:23,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:23,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:23,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:23,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:33:23,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:33:24,117 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:33:24,118 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:33:24,715 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:33:24,887 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:33:24,888 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 7it [00:04,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:33:30,996 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:30,997 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:33:31,053 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:31,054 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:33:31,088 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:33:42,823 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:33:42,824 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:33:43,101 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:33:43,102 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:33:43,187 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:33:43,265 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:33:43,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:44,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:48,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:50,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:53,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:58,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:58,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:15, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:00,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:07,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:09,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:12,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:15,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:16,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:18,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:35<05:23, 17.98s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:19,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:20,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:23,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:25,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:25,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:27,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:28,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:28,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:30,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:33,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:35,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:52<04:57, 17.48s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:36,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:37,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:37,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:42,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:44,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:46,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:50,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:09<04:36, 17.27s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:53,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:53,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:54,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:54,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:58,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:58,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:05,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:07,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:09,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:29<04:32, 18.18s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:12,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:15,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:18,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:20,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:21,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:22,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:24,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<03:59, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:27,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:28,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:31,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:31,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:33,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:35,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:36,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:37,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:38,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:39,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:40,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:41,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:42,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:42,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:43,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:44,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:44,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:45,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:46,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:46,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:47,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:04<03:55, 18.10s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:48,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:48,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:49,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:50,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:51,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:52,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:52,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:53,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:54,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:54,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:55,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:56,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:56,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:57,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:58,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:58,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:59,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:00,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:00,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:01,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:01,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:02,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:03,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:04,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:04,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:05,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:22<03:36, 18.02s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:05,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:06,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:07,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:08,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:09,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:10,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:11,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:12,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:13,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:14,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:15,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:16,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:17,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:18,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:20,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:21,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:22,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:23,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:24,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:41<03:22, 18.43s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:25,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:25,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:26,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:28,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:29,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:30,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:31,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:32,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:33,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:34,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:35,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:36,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:38,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:39,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:39,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:40,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:41,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:42,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:59<03:01, 18.14s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:42,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:43,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:44,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:45,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:46,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:47,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:48,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:49,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:50,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:51,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:52,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:53,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:54,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:56,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:57,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:58,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:36:59,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:16<02:40, 17.88s/it][WARNING|generation_utils.py:914] 2023-08-28 03:36:59,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:00,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:01,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:02,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:03,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:04,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:05,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:06,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:07,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:08,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:09,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:09,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:10,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:11,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:12,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:13,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:14,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:15,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:16,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:17,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:34<02:23, 17.94s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:18,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:18,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:19,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:20,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:21,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:22,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:23,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:23,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:24,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:25,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:26,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:27,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:28,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:29,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:30,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:31,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:32,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:33,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:50<02:01, 17.35s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:34,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:34,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:35,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:36,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:37,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:38,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:39,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:39,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:40,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:41,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:42,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:43,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:44,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:45,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:46,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:47,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:48,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:49,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:50,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:50,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:51,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:52,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:53,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:10<01:48, 18.05s/it][WARNING|generation_utils.py:914] 2023-08-28 03:37:53,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:54,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:55,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:56,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:57,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:58,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:58,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:37:59,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:00,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:00,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:01,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:02,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:03,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:04,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:04,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:05,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:06,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:06,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:07,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:08,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:08,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:09,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:10,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:27<01:28, 17.74s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:10,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:11,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:12,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:12,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:13,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:14,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:15,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:16,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:17,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:18,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:19,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:19,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:20,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:21,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:22,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:23,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:24,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:24,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:25,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:26,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:27,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:28,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:28,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:29,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:30,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:31,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:48<01:14, 18.72s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:31,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:32,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:33,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:34,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:35,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:35,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:36,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:36,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:37,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:38,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:38,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:39,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:40,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:41,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:42,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:43,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:44,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:45,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:46,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:46,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:47,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:04<00:53, 17.97s/it][WARNING|generation_utils.py:914] 2023-08-28 03:38:47,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:48,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:49,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:50,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:50,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:51,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:52,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:53,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:53,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:54,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:55,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:55,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:56,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:57,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:57,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:58,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:59,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:38:59,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:00,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:01,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:01,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:02,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:03,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:20<00:34, 17.41s/it][WARNING|generation_utils.py:914] 2023-08-28 03:39:04,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:04,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:05,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:06,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:06,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:07,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:08,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:09,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:09,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:10,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:11,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:11,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:12,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:13,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:13,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:14,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:15,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:15,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:16,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:16,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:17,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:18,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:18,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:19,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:20,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:20,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:21,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:22,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:23,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:23,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:24,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:25,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:25,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:26,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:43<00:19, 19.06s/it][WARNING|generation_utils.py:914] 2023-08-28 03:39:26,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:27,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:28,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:29,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:29,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:30,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:31,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:31,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:32,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:33,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:33,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:34,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:35,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:36,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:37,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:37,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:38,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:39,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:40,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:41,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:41,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:42,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:39:42,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:59<00:00, 18.30s/it]Generating: 100%|| 20/20 [05:59<00:00, 17.99s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:51,670 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:51,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:51,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:51,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:51,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:39:52,313 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:39:52,314 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:52,905 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:54,007 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:54,007 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:56,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:56,891 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:56,891 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:56,891 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:39:56,891 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:39:57,540 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:39:57,541 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:39:58,118 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:39:58,292 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:39:58,292 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.53it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:05,  1.52it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:08,  1.48it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.46it/s]Extractor Estimating: 16it [00:10,  1.46it/s]Extractor Estimating: 17it [00:11,  1.46it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:14,  1.48it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:16,  1.46it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.50it/s]Extractor Estimating: 27it [00:18,  1.55it/s]Extractor Estimating: 28it [00:18,  1.54it/s]Extractor Estimating: 29it [00:19,  1.58it/s]Extractor Estimating: 30it [00:19,  1.64it/s]Extractor Estimating: 31it [00:20,  1.55it/s]Extractor Estimating: 32it [00:21,  1.59it/s]Extractor Estimating: 33it [00:21,  1.53it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:23,  1.59it/s]Extractor Estimating: 36it [00:23,  1.54it/s]Extractor Estimating: 37it [00:24,  1.52it/s]Extractor Estimating: 38it [00:25,  1.49it/s]Extractor Estimating: 39it [00:25,  1.55it/s]Extractor Estimating: 40it [00:26,  1.54it/s]Extractor Estimating: 41it [00:27,  1.52it/s]Extractor Estimating: 42it [00:27,  1.56it/s]Extractor Estimating: 43it [00:28,  1.55it/s]Extractor Estimating: 44it [00:29,  1.54it/s]Extractor Estimating: 45it [00:29,  1.57it/s]Extractor Estimating: 46it [00:30,  1.50it/s]Extractor Estimating: 47it [00:31,  1.48it/s]Extractor Estimating: 48it [00:31,  1.46it/s]Extractor Estimating: 49it [00:32,  1.48it/s]Extractor Estimating: 50it [00:33,  1.45it/s]Extractor Estimating: 51it [00:33,  1.48it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:35,  1.53it/s]Extractor Estimating: 54it [00:35,  1.59it/s]Extractor Estimating: 55it [00:36,  1.62it/s]Extractor Estimating: 56it [00:36,  1.63it/s]Extractor Estimating: 57it [00:37,  1.49it/s]Extractor Estimating: 58it [00:38,  1.57it/s]Extractor Estimating: 59it [00:38,  1.57it/s]Extractor Estimating: 60it [00:39,  1.54it/s]Extractor Estimating: 61it [00:40,  1.51it/s]Extractor Estimating: 62it [00:40,  1.53it/s]Extractor Estimating: 63it [00:41,  1.55it/s]Extractor Estimating: 64it [00:42,  1.54it/s]Extractor Estimating: 65it [00:42,  1.51it/s]Extractor Estimating: 66it [00:43,  1.57it/s]Extractor Estimating: 67it [00:43,  1.58it/s]Extractor Estimating: 68it [00:44,  1.60it/s]Extractor Estimating: 69it [00:45,  1.59it/s]Extractor Estimating: 70it [00:45,  1.54it/s]Extractor Estimating: 71it [00:46,  1.53it/s]Extractor Estimating: 72it [00:47,  1.52it/s]Extractor Estimating: 73it [00:47,  1.50it/s]Extractor Estimating: 74it [00:48,  1.55it/s]Extractor Estimating: 75it [00:49,  1.57it/s]Extractor Estimating: 76it [00:49,  1.60it/s]Extractor Estimating: 77it [00:50,  1.60it/s]Extractor Estimating: 78it [00:51,  1.59it/s]Extractor Estimating: 79it [00:51,  1.60it/s]Extractor Estimating: 80it [00:52,  1.57it/s]Extractor Estimating: 81it [00:52,  1.57it/s]Extractor Estimating: 82it [00:53,  1.57it/s]Extractor Estimating: 83it [00:54,  1.54it/s]Extractor Estimating: 84it [00:54,  1.50it/s]Extractor Estimating: 85it [00:55,  1.48it/s]Extractor Estimating: 86it [00:56,  1.29it/s]Extractor Estimating: 87it [00:57,  1.36it/s]Extractor Estimating: 88it [00:57,  1.40it/s]Extractor Estimating: 89it [00:58,  1.46it/s]Extractor Estimating: 90it [00:59,  1.42it/s]Extractor Estimating: 91it [00:59,  1.45it/s]Extractor Estimating: 92it [01:00,  1.51it/s]Extractor Estimating: 93it [01:01,  1.51it/s]Extractor Estimating: 94it [01:01,  1.56it/s]Extractor Estimating: 95it [01:02,  1.59it/s]Extractor Estimating: 96it [01:03,  1.53it/s]Extractor Estimating: 97it [01:03,  1.52it/s]Extractor Estimating: 98it [01:04,  1.56it/s]Extractor Estimating: 99it [01:05,  1.53it/s]Extractor Estimating: 100it [01:05,  1.55it/s]Extractor Estimating: 101it [01:06,  1.54it/s]Extractor Estimating: 102it [01:06,  1.57it/s]Extractor Estimating: 103it [01:07,  1.55it/s]Extractor Estimating: 104it [01:08,  1.53it/s]Extractor Estimating: 105it [01:08,  1.57it/s]Extractor Estimating: 106it [01:09,  1.57it/s]Extractor Estimating: 107it [01:10,  1.57it/s]Extractor Estimating: 108it [01:10,  1.60it/s]Extractor Estimating: 109it [01:11,  1.61it/s]Extractor Estimating: 110it [01:12,  1.57it/s]Extractor Estimating: 111it [01:12,  1.59it/s]Extractor Estimating: 112it [01:13,  1.58it/s]Extractor Estimating: 113it [01:13,  1.57it/s]Extractor Estimating: 114it [01:14,  1.58it/s]Extractor Estimating: 115it [01:15,  1.58it/s]Extractor Estimating: 116it [01:15,  1.60it/s]Extractor Estimating: 117it [01:16,  1.63it/s]Extractor Estimating: 118it [01:17,  1.60it/s]Extractor Estimating: 119it [01:17,  1.60it/s]Extractor Estimating: 120it [01:18,  1.57it/s]Extractor Estimating: 121it [01:19,  1.57it/s]Extractor Estimating: 122it [01:19,  1.58it/s]Extractor Estimating: 123it [01:20,  1.55it/s]Extractor Estimating: 124it [01:20,  1.56it/s]Extractor Estimating: 125it [01:21,  1.60it/s]Extractor Estimating: 126it [01:22,  1.55it/s]Extractor Estimating: 127it [01:22,  1.54it/s]Extractor Estimating: 128it [01:23,  1.51it/s]Extractor Estimating: 129it [01:24,  1.53it/s]Extractor Estimating: 130it [01:24,  1.49it/s]Extractor Estimating: 131it [01:25,  1.49it/s]Extractor Estimating: 132it [01:26,  1.46it/s]Extractor Estimating: 133it [01:26,  1.48it/s]Extractor Estimating: 134it [01:27,  1.46it/s]Extractor Estimating: 135it [01:28,  1.45it/s]Extractor Estimating: 136it [01:29,  1.42it/s]Extractor Estimating: 137it [01:29,  1.44it/s]Extractor Estimating: 138it [01:30,  1.45it/s]Extractor Estimating: 139it [01:31,  1.43it/s]Extractor Estimating: 140it [01:31,  1.46it/s]Extractor Estimating: 141it [01:32,  1.47it/s]Extractor Estimating: 142it [01:33,  1.50it/s]Extractor Estimating: 143it [01:33,  1.55it/s]Extractor Estimating: 144it [01:34,  1.50it/s]Extractor Estimating: 145it [01:35,  1.46it/s]Extractor Estimating: 146it [01:35,  1.41it/s]Extractor Estimating: 147it [01:36,  1.44it/s]Extractor Estimating: 148it [01:37,  1.42it/s]Extractor Estimating: 149it [01:37,  1.47it/s]Extractor Estimating: 150it [01:38,  1.49it/s]Extractor Estimating: 151it [01:39,  1.52it/s]Extractor Estimating: 152it [01:39,  1.56it/s]Extractor Estimating: 153it [01:40,  1.63it/s]Extractor Estimating: 154it [01:41,  1.62it/s]Extractor Estimating: 155it [01:41,  1.62it/s]Extractor Estimating: 156it [01:42,  1.61it/s]Extractor Estimating: 157it [01:42,  1.64it/s]Extractor Estimating: 158it [01:43,  1.66it/s]Extractor Estimating: 159it [01:43,  1.71it/s]Extractor Estimating: 160it [01:44,  1.69it/s]Extractor Estimating: 161it [01:45,  1.65it/s]Extractor Estimating: 162it [01:45,  1.55it/s]Extractor Estimating: 163it [01:46,  1.52it/s]Extractor Estimating: 164it [01:47,  1.56it/s]Extractor Estimating: 165it [01:47,  1.60it/s]Extractor Estimating: 166it [01:48,  1.55it/s]Extractor Estimating: 167it [01:49,  1.55it/s]Extractor Estimating: 168it [01:49,  1.56it/s]Extractor Estimating: 169it [01:50,  1.58it/s]Extractor Estimating: 170it [01:51,  1.58it/s]Extractor Estimating: 171it [01:51,  1.58it/s]Extractor Estimating: 172it [01:52,  1.43it/s]Extractor Estimating: 173it [01:53,  1.51it/s]Extractor Estimating: 174it [01:53,  1.55it/s]Extractor Estimating: 175it [01:54,  1.58it/s]Extractor Estimating: 176it [01:54,  1.60it/s]Extractor Estimating: 177it [01:55,  1.59it/s]Extractor Estimating: 178it [01:56,  1.56it/s]Extractor Estimating: 179it [01:56,  1.48it/s]Extractor Estimating: 180it [01:57,  1.54it/s]Extractor Estimating: 181it [01:58,  1.59it/s]Extractor Estimating: 182it [01:58,  1.60it/s]Extractor Estimating: 183it [01:59,  1.55it/s]Extractor Estimating: 184it [02:00,  1.58it/s]Extractor Estimating: 185it [02:00,  1.62it/s]Extractor Estimating: 186it [02:01,  1.60it/s]Extractor Estimating: 187it [02:01,  1.63it/s]Extractor Estimating: 188it [02:02,  1.55it/s]Extractor Estimating: 189it [02:03,  1.57it/s]Extractor Estimating: 190it [02:03,  1.58it/s]Extractor Estimating: 191it [02:04,  1.61it/s]Extractor Estimating: 192it [02:04,  1.66it/s]Extractor Estimating: 193it [02:05,  1.68it/s]Extractor Estimating: 194it [02:06,  1.65it/s]Extractor Estimating: 195it [02:06,  1.62it/s]Extractor Estimating: 196it [02:07,  1.53it/s]Extractor Estimating: 197it [02:08,  1.61it/s]Extractor Estimating: 198it [02:08,  1.62it/s]Extractor Estimating: 199it [02:09,  1.65it/s]Extractor Estimating: 200it [02:09,  1.67it/s]Extractor Estimating: 201it [02:10,  1.67it/s]Extractor Estimating: 202it [02:11,  1.62it/s]Extractor Estimating: 203it [02:11,  1.58it/s]Extractor Estimating: 204it [02:12,  1.57it/s]Extractor Estimating: 205it [02:13,  1.57it/s]Extractor Estimating: 206it [02:13,  1.53it/s]Extractor Estimating: 207it [02:14,  1.44it/s]Extractor Estimating: 208it [02:15,  1.39it/s]Extractor Estimating: 209it [02:15,  1.46it/s]Extractor Estimating: 210it [02:16,  1.48it/s]Extractor Estimating: 211it [02:17,  1.41it/s]Extractor Estimating: 212it [02:17,  1.49it/s]Extractor Estimating: 213it [02:18,  1.45it/s]Extractor Estimating: 214it [02:19,  1.47it/s]Extractor Estimating: 215it [02:20,  1.48it/s]Extractor Estimating: 216it [02:20,  1.56it/s]Extractor Estimating: 217it [02:21,  1.59it/s]Extractor Estimating: 218it [02:21,  1.51it/s]Extractor Estimating: 219it [02:22,  1.49it/s]Extractor Estimating: 220it [02:23,  1.46it/s]Extractor Estimating: 221it [02:24,  1.49it/s]Extractor Estimating: 222it [02:24,  1.51it/s]Extractor Estimating: 223it [02:25,  1.56it/s]Extractor Estimating: 224it [02:25,  1.56it/s]Extractor Estimating: 225it [02:26,  1.49it/s]Extractor Estimating: 226it [02:27,  1.47it/s]Extractor Estimating: 227it [02:28,  1.47it/s]Extractor Estimating: 228it [02:28,  1.52it/s]Extractor Estimating: 229it [02:29,  1.51it/s]Extractor Estimating: 230it [02:29,  1.56it/s]Extractor Estimating: 231it [02:30,  1.57it/s]Extractor Estimating: 232it [02:31,  1.55it/s]Extractor Estimating: 233it [02:31,  1.58it/s]Extractor Estimating: 234it [02:32,  1.52it/s]Extractor Estimating: 235it [02:33,  1.55it/s]Extractor Estimating: 236it [02:33,  1.54it/s]Extractor Estimating: 237it [02:34,  1.56it/s]Extractor Estimating: 238it [02:34,  1.59it/s]Extractor Estimating: 239it [02:35,  1.55it/s]Extractor Estimating: 240it [02:36,  1.50it/s]Extractor Estimating: 241it [02:37,  1.49it/s]Extractor Estimating: 242it [02:37,  1.49it/s]Extractor Estimating: 243it [02:38,  1.49it/s]Extractor Estimating: 244it [02:39,  1.45it/s]Extractor Estimating: 245it [02:39,  1.44it/s]Extractor Estimating: 246it [02:40,  1.49it/s]Extractor Estimating: 247it [02:41,  1.45it/s]Extractor Estimating: 248it [02:41,  1.46it/s]Extractor Estimating: 249it [02:42,  1.47it/s]Extractor Estimating: 250it [02:43,  1.47it/s]Extractor Estimating: 251it [02:43,  1.53it/s]Extractor Estimating: 252it [02:44,  1.57it/s]Extractor Estimating: 253it [02:45,  1.51it/s]Extractor Estimating: 254it [02:45,  1.50it/s]Extractor Estimating: 255it [02:46,  1.51it/s]Extractor Estimating: 256it [02:47,  1.52it/s]Extractor Estimating: 257it [02:47,  1.52it/s]Extractor Estimating: 258it [02:48,  1.40it/s]Extractor Estimating: 259it [02:49,  1.47it/s]Extractor Estimating: 260it [02:49,  1.52it/s]Extractor Estimating: 261it [02:50,  1.47it/s]Extractor Estimating: 262it [02:51,  1.52it/s]Extractor Estimating: 263it [02:51,  1.48it/s]Extractor Estimating: 264it [02:52,  1.50it/s]Extractor Estimating: 265it [02:53,  1.51it/s]Extractor Estimating: 266it [02:53,  1.50it/s]Extractor Estimating: 267it [02:54,  1.52it/s]Extractor Estimating: 268it [02:55,  1.51it/s]Extractor Estimating: 269it [02:55,  1.48it/s]Extractor Estimating: 270it [02:56,  1.44it/s]Extractor Estimating: 271it [02:57,  1.45it/s]Extractor Estimating: 272it [02:57,  1.47it/s]Extractor Estimating: 273it [02:58,  1.47it/s]Extractor Estimating: 274it [02:59,  1.44it/s]Extractor Estimating: 275it [02:59,  1.47it/s]Extractor Estimating: 276it [03:00,  1.51it/s]Extractor Estimating: 277it [03:01,  1.46it/s]Extractor Estimating: 278it [03:01,  1.49it/s]Extractor Estimating: 279it [03:02,  1.46it/s]Extractor Estimating: 280it [03:03,  1.42it/s]Extractor Estimating: 281it [03:04,  1.38it/s]Extractor Estimating: 282it [03:04,  1.41it/s]Extractor Estimating: 283it [03:05,  1.46it/s]Extractor Estimating: 284it [03:06,  1.49it/s]Extractor Estimating: 285it [03:06,  1.48it/s]Extractor Estimating: 286it [03:07,  1.50it/s]Extractor Estimating: 287it [03:08,  1.51it/s]Extractor Estimating: 288it [03:08,  1.53it/s]Extractor Estimating: 289it [03:09,  1.57it/s]Extractor Estimating: 290it [03:10,  1.49it/s]Extractor Estimating: 291it [03:10,  1.51it/s]Extractor Estimating: 292it [03:11,  1.46it/s]Extractor Estimating: 293it [03:12,  1.50it/s]Extractor Estimating: 294it [03:12,  1.48it/s]Extractor Estimating: 295it [03:13,  1.51it/s]Extractor Estimating: 296it [03:14,  1.49it/s]Extractor Estimating: 297it [03:14,  1.51it/s]Extractor Estimating: 298it [03:15,  1.51it/s]Extractor Estimating: 299it [03:16,  1.51it/s]Extractor Estimating: 300it [03:16,  1.51it/s]Extractor Estimating: 301it [03:17,  1.49it/s]Extractor Estimating: 302it [03:18,  1.47it/s]Extractor Estimating: 303it [03:18,  1.45it/s]Extractor Estimating: 304it [03:19,  1.45it/s]Extractor Estimating: 305it [03:20,  1.44it/s]Extractor Estimating: 306it [03:20,  1.49it/s]Extractor Estimating: 307it [03:21,  1.46it/s]Extractor Estimating: 308it [03:22,  1.44it/s]Extractor Estimating: 309it [03:22,  1.49it/s]Extractor Estimating: 310it [03:23,  1.47it/s]Extractor Estimating: 311it [03:24,  1.51it/s]Extractor Estimating: 312it [03:24,  1.50it/s]Extractor Estimating: 313it [03:25,  1.49it/s]Extractor Estimating: 314it [03:26,  1.53it/s]Extractor Estimating: 315it [03:26,  1.58it/s]Extractor Estimating: 316it [03:27,  1.54it/s]Extractor Estimating: 317it [03:28,  1.57it/s]Extractor Estimating: 318it [03:28,  1.52it/s]Extractor Estimating: 319it [03:29,  1.56it/s]Extractor Estimating: 320it [03:30,  1.60it/s]Extractor Estimating: 321it [03:30,  1.51it/s]Extractor Estimating: 322it [03:31,  1.54it/s]Extractor Estimating: 323it [03:32,  1.56it/s]Extractor Estimating: 324it [03:32,  1.56it/s]Extractor Estimating: 325it [03:33,  1.57it/s]Extractor Estimating: 326it [03:33,  1.61it/s]Extractor Estimating: 327it [03:34,  1.62it/s]Extractor Estimating: 328it [03:35,  1.62it/s]Extractor Estimating: 329it [03:35,  1.63it/s]Extractor Estimating: 330it [03:36,  1.60it/s]Extractor Estimating: 331it [03:37,  1.57it/s]Extractor Estimating: 332it [03:37,  1.53it/s]Extractor Estimating: 333it [03:38,  1.55it/s]Extractor Estimating: 334it [03:38,  1.54it/s]Extractor Estimating: 335it [03:39,  1.58it/s]Extractor Estimating: 336it [03:40,  1.64it/s]Extractor Estimating: 337it [03:40,  1.68it/s]Extractor Estimating: 338it [03:41,  1.64it/s]Extractor Estimating: 339it [03:42,  1.55it/s]Extractor Estimating: 340it [03:42,  1.59it/s]Extractor Estimating: 341it [03:43,  1.66it/s]Extractor Estimating: 342it [03:43,  1.65it/s]Extractor Estimating: 343it [03:44,  1.65it/s]Extractor Estimating: 344it [03:45,  1.63it/s]Extractor Estimating: 345it [03:45,  1.68it/s]Extractor Estimating: 346it [03:46,  1.60it/s]Extractor Estimating: 347it [03:46,  1.57it/s]Extractor Estimating: 348it [03:47,  1.54it/s]Extractor Estimating: 349it [03:48,  1.48it/s]Extractor Estimating: 350it [03:49,  1.50it/s]Extractor Estimating: 351it [03:49,  1.50it/s]Extractor Estimating: 352it [03:50,  1.41it/s]Extractor Estimating: 353it [03:51,  1.43it/s]Extractor Estimating: 354it [03:51,  1.42it/s]Extractor Estimating: 355it [03:52,  1.47it/s]Extractor Estimating: 356it [03:53,  1.54it/s]Extractor Estimating: 357it [03:53,  1.53it/s]Extractor Estimating: 358it [03:54,  1.53it/s]Extractor Estimating: 359it [03:55,  1.55it/s]Extractor Estimating: 360it [03:55,  1.53it/s]Extractor Estimating: 361it [03:56,  1.45it/s]Extractor Estimating: 362it [03:57,  1.50it/s]Extractor Estimating: 363it [03:57,  1.47it/s]Extractor Estimating: 364it [03:58,  1.48it/s]Extractor Estimating: 365it [03:59,  1.46it/s]Extractor Estimating: 366it [03:59,  1.46it/s]Extractor Estimating: 367it [04:00,  1.46it/s]Extractor Estimating: 368it [04:01,  1.46it/s]Extractor Estimating: 369it [04:01,  1.51it/s]Extractor Estimating: 370it [04:02,  1.45it/s]Extractor Estimating: 371it [04:03,  1.49it/s]Extractor Estimating: 372it [04:03,  1.54it/s]Extractor Estimating: 373it [04:04,  1.53it/s]Extractor Estimating: 374it [04:05,  1.59it/s]Extractor Estimating: 375it [04:05,  1.57it/s]Extractor Estimating: 376it [04:06,  1.51it/s]Extractor Estimating: 377it [04:07,  1.50it/s]Extractor Estimating: 378it [04:07,  1.48it/s]Extractor Estimating: 379it [04:08,  1.49it/s]Extractor Estimating: 380it [04:09,  1.53it/s]Extractor Estimating: 381it [04:09,  1.56it/s]Extractor Estimating: 382it [04:10,  1.51it/s]Extractor Estimating: 383it [04:11,  1.52it/s]Extractor Estimating: 384it [04:11,  1.58it/s]Extractor Estimating: 385it [04:12,  1.52it/s]Extractor Estimating: 386it [04:12,  1.53it/s]Extractor Estimating: 387it [04:13,  1.49it/s]Extractor Estimating: 388it [04:14,  1.54it/s]Extractor Estimating: 389it [04:15,  1.50it/s]Extractor Estimating: 390it [04:15,  1.55it/s]Extractor Estimating: 391it [04:16,  1.53it/s]Extractor Estimating: 392it [04:16,  1.53it/s]Extractor Estimating: 393it [04:17,  1.56it/s]Extractor Estimating: 394it [04:18,  1.55it/s]Extractor Estimating: 395it [04:18,  1.50it/s]Extractor Estimating: 396it [04:19,  1.50it/s]Extractor Estimating: 397it [04:20,  1.54it/s]Extractor Estimating: 398it [04:20,  1.54it/s]Extractor Estimating: 399it [04:21,  1.54it/s]Extractor Estimating: 400it [04:22,  1.51it/s]Extractor Estimating: 401it [04:22,  1.49it/s]Extractor Estimating: 402it [04:23,  1.54it/s]Extractor Estimating: 403it [04:24,  1.52it/s]Extractor Estimating: 404it [04:24,  1.54it/s]Extractor Estimating: 405it [04:25,  1.55it/s]Extractor Estimating: 406it [04:26,  1.54it/s]Extractor Estimating: 407it [04:26,  1.54it/s]Extractor Estimating: 408it [04:27,  1.59it/s]Extractor Estimating: 409it [04:27,  1.60it/s]Extractor Estimating: 410it [04:28,  1.58it/s]Extractor Estimating: 411it [04:29,  1.55it/s]Extractor Estimating: 412it [04:30,  1.44it/s]Extractor Estimating: 413it [04:30,  1.48it/s]Extractor Estimating: 414it [04:31,  1.55it/s]Extractor Estimating: 415it [04:31,  1.59it/s]Extractor Estimating: 416it [04:32,  1.57it/s]Extractor Estimating: 417it [04:33,  1.56it/s]Extractor Estimating: 418it [04:33,  1.54it/s]Extractor Estimating: 419it [04:34,  1.52it/s]Extractor Estimating: 420it [04:35,  1.58it/s]Extractor Estimating: 421it [04:35,  1.48it/s]Extractor Estimating: 422it [04:36,  1.44it/s]Extractor Estimating: 423it [04:37,  1.48it/s]Extractor Estimating: 424it [04:37,  1.51it/s]Extractor Estimating: 425it [04:38,  1.59it/s]Extractor Estimating: 426it [04:39,  1.54it/s]Extractor Estimating: 427it [04:39,  1.50it/s]Extractor Estimating: 428it [04:40,  1.46it/s]Extractor Estimating: 429it [04:41,  1.33it/s]Extractor Estimating: 430it [04:42,  1.35it/s]Extractor Estimating: 431it [04:42,  1.41it/s]Extractor Estimating: 432it [04:43,  1.36it/s]Extractor Estimating: 433it [04:44,  1.44it/s]Extractor Estimating: 434it [04:44,  1.46it/s]Extractor Estimating: 435it [04:45,  1.43it/s]Extractor Estimating: 436it [04:46,  1.38it/s]Extractor Estimating: 437it [04:47,  1.37it/s]Extractor Estimating: 438it [04:47,  1.44it/s]Extractor Estimating: 439it [04:48,  1.44it/s]Extractor Estimating: 440it [04:49,  1.50it/s]Extractor Estimating: 441it [04:49,  1.45it/s]Extractor Estimating: 442it [04:50,  1.48it/s]Extractor Estimating: 443it [04:51,  1.53it/s]Extractor Estimating: 444it [04:51,  1.52it/s]Extractor Estimating: 445it [04:52,  1.55it/s]Extractor Estimating: 446it [04:52,  1.52it/s]Extractor Estimating: 447it [04:53,  1.52it/s]Extractor Estimating: 448it [04:54,  1.52it/s]Extractor Estimating: 449it [04:55,  1.49it/s]Extractor Estimating: 450it [04:55,  1.50it/s]Extractor Estimating: 451it [04:56,  1.50it/s]Extractor Estimating: 452it [04:56,  1.52it/s]Extractor Estimating: 453it [04:57,  1.58it/s]Extractor Estimating: 454it [04:58,  1.58it/s]Extractor Estimating: 455it [04:58,  1.67it/s]Extractor Estimating: 456it [04:59,  1.59it/s]Extractor Estimating: 457it [04:59,  1.61it/s]Extractor Estimating: 458it [05:00,  1.61it/s]Extractor Estimating: 459it [05:01,  1.64it/s]Extractor Estimating: 460it [05:01,  1.62it/s]Extractor Estimating: 461it [05:02,  1.65it/s]Extractor Estimating: 462it [05:02,  1.67it/s]Extractor Estimating: 463it [05:03,  1.69it/s]Extractor Estimating: 464it [05:04,  1.71it/s]Extractor Estimating: 465it [05:04,  1.70it/s]Extractor Estimating: 466it [05:05,  1.70it/s]Extractor Estimating: 467it [05:05,  1.72it/s]Extractor Estimating: 468it [05:06,  1.71it/s]Extractor Estimating: 469it [05:07,  1.69it/s]Extractor Estimating: 470it [05:07,  1.68it/s]Extractor Estimating: 471it [05:08,  1.68it/s]Extractor Estimating: 472it [05:08,  1.63it/s]Extractor Estimating: 473it [05:09,  1.60it/s]Extractor Estimating: 474it [05:10,  1.61it/s]Extractor Estimating: 475it [05:10,  1.55it/s]Extractor Estimating: 476it [05:11,  1.60it/s]Extractor Estimating: 477it [05:12,  1.51it/s]Extractor Estimating: 478it [05:12,  1.54it/s]Extractor Estimating: 479it [05:13,  1.46it/s]Extractor Estimating: 480it [05:14,  1.49it/s]Extractor Estimating: 481it [05:14,  1.53it/s]Extractor Estimating: 482it [05:15,  1.54it/s]Extractor Estimating: 483it [05:16,  1.52it/s]Extractor Estimating: 484it [05:16,  1.51it/s]Extractor Estimating: 485it [05:17,  1.51it/s]Extractor Estimating: 486it [05:18,  1.55it/s]Extractor Estimating: 487it [05:18,  1.58it/s]Extractor Estimating: 488it [05:19,  1.60it/s]Extractor Estimating: 489it [05:20,  1.51it/s]Extractor Estimating: 490it [05:20,  1.43it/s]Extractor Estimating: 491it [05:21,  1.44it/s]Extractor Estimating: 492it [05:22,  1.46it/s]Extractor Estimating: 493it [05:22,  1.47it/s]Extractor Estimating: 494it [05:23,  1.45it/s]Extractor Estimating: 495it [05:24,  1.50it/s]Extractor Estimating: 496it [05:24,  1.46it/s]Extractor Estimating: 497it [05:25,  1.48it/s]Extractor Estimating: 498it [05:26,  1.46it/s]Extractor Estimating: 499it [05:26,  1.51it/s]Extractor Estimating: 500it [05:27,  1.85it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:43,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:43,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:43,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:43,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:43,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:45:44,598 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:45:44,599 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:45:45,235 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:45:46,321 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:45:46,321 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:50,197 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:50,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:50,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:50,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:45:50,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:45:51,119 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:45:51,120 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:45:51,782 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:45:51,955 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:45:51,955 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:49:47,833 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:49:47,883 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10479 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 29880
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29980, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29980, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.044, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.064, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.053, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 63, avg_time 1.049, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 163, avg_time 2.143, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 263, avg_time 1.047, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 363, avg_time 1.062, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 26, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 126, avg_time 1.053, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 226, avg_time 2.117, loss:nan
g_step 1200, step 326, avg_time 1.066, loss:nan
g_step 1300, step 426, avg_time 1.061, loss:nan
g_step 1400, step 89, avg_time 1.044, loss:nan
g_step 1500, step 189, avg_time 1.058, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 289, avg_time 2.116, loss:nan
g_step 1700, step 389, avg_time 1.054, loss:nan
g_step 1800, step 52, avg_time 1.049, loss:nan
g_step 1900, step 152, avg_time 1.051, loss:nan
g_step 2000, step 252, avg_time 1.052, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 352, avg_time 2.129, loss:nan
g_step 2200, step 15, avg_time 1.051, loss:nan
g_step 2300, step 115, avg_time 1.069, loss:nan
g_step 2400, step 215, avg_time 1.047, loss:nan
g_step 2500, step 315, avg_time 1.050, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 415, avg_time 2.116, loss:nan
g_step 2700, step 78, avg_time 1.050, loss:nan
g_step 2800, step 178, avg_time 1.048, loss:nan
g_step 2900, step 278, avg_time 1.080, loss:nan
g_step 3000, step 378, avg_time 1.042, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 41, avg_time 2.111, loss:nan
g_step 3200, step 141, avg_time 1.057, loss:nan
g_step 3300, step 241, avg_time 1.064, loss:nan
g_step 3400, step 341, avg_time 1.055, loss:nan
g_step 3500, step 4, avg_time 1.047, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 104, avg_time 2.120, loss:nan
g_step 3700, step 204, avg_time 1.052, loss:nan
g_step 3800, step 304, avg_time 1.057, loss:nan
g_step 3900, step 404, avg_time 1.046, loss:nan
g_step 4000, step 67, avg_time 1.055, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 167, avg_time 2.133, loss:nan
g_step 4200, step 267, avg_time 1.042, loss:nan
g_step 4300, step 367, avg_time 1.051, loss:nan
g_step 4400, step 30, avg_time 1.052, loss:nan
g_step 4500, step 130, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 230, avg_time 2.118, loss:nan
g_step 4700, step 330, avg_time 1.055, loss:nan
g_step 4800, step 430, avg_time 1.058, loss:nan
g_step 4900, step 93, avg_time 1.048, loss:nan
g_step 5000, step 193, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 293, avg_time 2.112, loss:nan
g_step 5200, step 393, avg_time 1.063, loss:nan
g_step 5300, step 56, avg_time 1.070, loss:nan
g_step 5400, step 156, avg_time 1.068, loss:nan
g_step 5500, step 256, avg_time 1.072, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 356, avg_time 2.108, loss:nan
g_step 5700, step 19, avg_time 1.044, loss:nan
g_step 5800, step 119, avg_time 1.049, loss:nan
g_step 5900, step 219, avg_time 1.053, loss:nan
g_step 6000, step 319, avg_time 1.059, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 419, avg_time 2.127, loss:nan
g_step 6200, step 82, avg_time 1.047, loss:nan
g_step 6300, step 182, avg_time 1.051, loss:nan
g_step 6400, step 282, avg_time 1.053, loss:nan
g_step 6500, step 382, avg_time 1.060, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 45, avg_time 2.106, loss:nan
g_step 6700, step 145, avg_time 1.059, loss:nan
g_step 6800, step 245, avg_time 1.053, loss:nan
g_step 6900, step 345, avg_time 1.052, loss:nan
g_step 7000, step 8, avg_time 1.064, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 108, avg_time 2.124, loss:nan
g_step 7200, step 208, avg_time 1.054, loss:nan
g_step 7300, step 308, avg_time 1.049, loss:nan
g_step 7400, step 408, avg_time 1.059, loss:nan
g_step 7500, step 71, avg_time 1.034, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 171, avg_time 2.125, loss:nan
g_step 7700, step 271, avg_time 1.056, loss:nan
g_step 7800, step 371, avg_time 1.073, loss:nan
g_step 7900, step 34, avg_time 1.057, loss:nan
g_step 8000, step 134, avg_time 1.048, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 234, avg_time 2.103, loss:nan
g_step 8200, step 334, avg_time 1.047, loss:nan
g_step 8300, step 434, avg_time 1.068, loss:nan
g_step 8400, step 97, avg_time 1.050, loss:nan
g_step 8500, step 197, avg_time 1.064, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 297, avg_time 2.117, loss:nan
g_step 8700, step 397, avg_time 1.068, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:49:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:49:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-49-47_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:49:48 - WARNING - datasets.builder -   Using custom data configuration default-40e4b71058f369f3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-40e4b71058f369f3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:49:51,244 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:51,273 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:49:51,274 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:51,275 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:49:51,399 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:51,460 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:49:51,900 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:49:54,970 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:49:54,979 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-40e4b71058f369f3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.98ba/s] 18%|        | 2/11 [00:00<00:02,  3.83ba/s] 27%|       | 3/11 [00:00<00:01,  4.09ba/s] 36%|      | 4/11 [00:00<00:01,  4.27ba/s] 45%|     | 5/11 [00:01<00:01,  4.37ba/s] 55%|    | 6/11 [00:01<00:01,  4.43ba/s] 64%|   | 7/11 [00:01<00:00,  4.47ba/s] 73%|  | 8/11 [00:01<00:00,  4.48ba/s] 82%| | 9/11 [00:02<00:00,  4.50ba/s] 91%| | 10/11 [00:02<00:00,  3.78ba/s]100%|| 11/11 [00:02<00:00,  4.56ba/s]100%|| 11/11 [00:02<00:00,  4.29ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.98ba/s] 50%|     | 2/4 [00:00<00:00,  3.44ba/s] 75%|  | 3/4 [00:00<00:00,  3.84ba/s]100%|| 4/4 [00:00<00:00,  4.98ba/s]100%|| 4/4 [00:00<00:00,  4.31ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.64ba/s] 27%|       | 3/11 [00:00<00:00,  8.65ba/s] 45%|     | 5/11 [00:00<00:00,  9.54ba/s] 64%|   | 7/11 [00:00<00:00,  9.95ba/s] 82%| | 9/11 [00:00<00:00, 10.14ba/s]100%|| 11/11 [00:01<00:00, 11.17ba/s]100%|| 11/11 [00:01<00:00, 10.18ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.85ba/s] 75%|  | 3/4 [00:00<00:00,  8.67ba/s]100%|| 4/4 [00:00<00:00,  9.74ba/s]
[INFO|trainer.py:414] 2023-08-28 06:50:00,998 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:50:01,007 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:50:01,008 >>   Num examples = 10520
[INFO|trainer.py:1149] 2023-08-28 06:50:01,008 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:50:01,008 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:50:01,008 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:50:01,008 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:50:01,008 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:54,  3.50it/s]  0%|          | 2/820 [00:00<03:57,  3.44it/s]  0%|          | 3/820 [00:00<03:51,  3.53it/s]  0%|          | 4/820 [00:01<03:48,  3.58it/s]  1%|          | 5/820 [00:01<03:47,  3.58it/s]  1%|          | 6/820 [00:01<03:47,  3.58it/s]  1%|          | 7/820 [00:01<03:46,  3.58it/s]  1%|          | 8/820 [00:02<03:46,  3.58it/s]  1%|          | 9/820 [00:02<03:46,  3.59it/s]  1%|          | 10/820 [00:02<03:45,  3.59it/s]  1%|         | 11/820 [00:03<03:45,  3.59it/s]  1%|         | 12/820 [00:03<03:45,  3.59it/s]  2%|         | 13/820 [00:03<03:49,  3.51it/s]  2%|         | 14/820 [00:03<03:48,  3.53it/s]  2%|         | 15/820 [00:04<03:46,  3.55it/s]  2%|         | 16/820 [00:04<03:45,  3.56it/s]  2%|         | 17/820 [00:04<03:45,  3.57it/s]  2%|         | 18/820 [00:05<03:44,  3.57it/s]  2%|         | 19/820 [00:05<03:44,  3.57it/s]  2%|         | 20/820 [00:05<03:43,  3.58it/s]  3%|         | 21/820 [00:05<03:43,  3.58it/s]  3%|         | 22/820 [00:06<03:43,  3.58it/s]  3%|         | 23/820 [00:06<03:42,  3.58it/s]  3%|         | 24/820 [00:06<03:48,  3.48it/s]  3%|         | 25/820 [00:07<03:46,  3.51it/s]  3%|         | 26/820 [00:07<03:44,  3.53it/s]  3%|         | 27/820 [00:07<03:43,  3.55it/s]  3%|         | 28/820 [00:07<03:42,  3.56it/s]  4%|         | 29/820 [00:08<03:41,  3.56it/s]  4%|         | 30/820 [00:08<03:41,  3.57it/s]  4%|         | 31/820 [00:08<03:40,  3.57it/s]  4%|         | 32/820 [00:08<03:40,  3.57it/s]  4%|         | 33/820 [00:09<03:46,  3.47it/s]  4%|         | 34/820 [00:09<03:44,  3.50it/s]  4%|         | 35/820 [00:09<03:48,  3.44it/s]  4%|         | 36/820 [00:10<03:49,  3.42it/s]  5%|         | 37/820 [00:10<03:45,  3.48it/s]  5%|         | 38/820 [00:10<03:41,  3.52it/s]  5%|         | 39/820 [00:11<03:39,  3.55it/s]  5%|         | 40/820 [00:11<03:38,  3.58it/s]  5%|         | 41/820 [00:11<03:36,  3.59it/s]  5%|         | 42/820 [00:11<03:35,  3.61it/s]  5%|         | 43/820 [00:12<03:35,  3.61it/s]  5%|         | 44/820 [00:12<03:34,  3.62it/s]  5%|         | 45/820 [00:12<03:33,  3.63it/s]  6%|         | 46/820 [00:12<03:38,  3.55it/s]  6%|         | 47/820 [00:13<03:36,  3.57it/s]  6%|         | 48/820 [00:13<03:35,  3.59it/s]  6%|         | 49/820 [00:13<03:34,  3.60it/s]  6%|         | 50/820 [00:14<03:33,  3.61it/s]  6%|         | 51/820 [00:14<03:32,  3.62it/s]  6%|         | 52/820 [00:14<03:31,  3.62it/s]  6%|         | 53/820 [00:14<03:31,  3.63it/s]  7%|         | 54/820 [00:15<03:31,  3.63it/s]  7%|         | 55/820 [00:15<03:30,  3.63it/s]  7%|         | 56/820 [00:15<03:30,  3.63it/s]  7%|         | 57/820 [00:16<03:35,  3.53it/s]  7%|         | 58/820 [00:16<03:33,  3.56it/s]  7%|         | 59/820 [00:16<03:32,  3.58it/s]  7%|         | 60/820 [00:16<03:31,  3.60it/s]  7%|         | 61/820 [00:17<03:30,  3.61it/s]  8%|         | 62/820 [00:17<03:29,  3.62it/s]  8%|         | 63/820 [00:17<03:28,  3.62it/s]  8%|         | 64/820 [00:17<03:28,  3.63it/s]  8%|         | 65/820 [00:18<03:27,  3.63it/s]  8%|         | 66/820 [00:18<03:27,  3.63it/s]  8%|         | 67/820 [00:18<03:27,  3.63it/s]  8%|         | 68/820 [00:19<03:31,  3.55it/s]  8%|         | 69/820 [00:19<03:30,  3.57it/s]  9%|         | 70/820 [00:19<03:28,  3.59it/s]  9%|         | 71/820 [00:19<03:27,  3.60it/s]  9%|         | 72/820 [00:20<03:27,  3.61it/s]  9%|         | 73/820 [00:20<03:26,  3.62it/s]  9%|         | 74/820 [00:20<03:25,  3.62it/s]  9%|         | 75/820 [00:20<03:25,  3.62it/s]  9%|         | 76/820 [00:21<03:25,  3.62it/s]  9%|         | 77/820 [00:21<03:24,  3.62it/s] 10%|         | 78/820 [00:21<03:24,  3.63it/s] 10%|         | 79/820 [00:22<03:24,  3.63it/s] 10%|         | 80/820 [00:22<03:24,  3.62it/s] 10%|         | 81/820 [00:22<03:23,  3.62it/s] 10%|         | 82/820 [00:22<03:23,  3.63it/s] 10%|         | 83/820 [00:23<03:34,  3.44it/s] 10%|         | 84/820 [00:23<03:30,  3.49it/s] 10%|         | 85/820 [00:23<03:28,  3.53it/s] 10%|         | 86/820 [00:24<03:26,  3.56it/s] 11%|         | 87/820 [00:24<03:25,  3.57it/s] 11%|         | 88/820 [00:24<03:23,  3.59it/s] 11%|         | 89/820 [00:24<03:22,  3.60it/s] 11%|         | 90/820 [00:25<03:22,  3.61it/s] 11%|         | 91/820 [00:25<03:21,  3.62it/s] 11%|         | 92/820 [00:25<03:21,  3.62it/s] 11%|        | 93/820 [00:25<03:20,  3.62it/s] 11%|        | 94/820 [00:26<03:28,  3.49it/s] 12%|        | 95/820 [00:26<03:25,  3.53it/s] 12%|        | 96/820 [00:26<03:23,  3.56it/s] 12%|        | 97/820 [00:27<03:22,  3.58it/s] 12%|        | 98/820 [00:27<03:20,  3.59it/s] 12%|        | 99/820 [00:27<03:20,  3.60it/s] 12%|        | 100/820 [00:27<03:19,  3.61it/s] 12%|        | 101/820 [00:28<03:19,  3.61it/s] 12%|        | 102/820 [00:28<03:18,  3.62it/s] 13%|        | 103/820 [00:28<03:18,  3.62it/s] 13%|        | 104/820 [00:29<03:17,  3.62it/s] 13%|        | 105/820 [00:29<03:20,  3.57it/s] 13%|        | 106/820 [00:29<03:18,  3.59it/s] 13%|        | 107/820 [00:29<03:18,  3.60it/s] 13%|        | 108/820 [00:30<03:17,  3.61it/s] 13%|        | 109/820 [00:30<03:16,  3.61it/s] 13%|        | 110/820 [00:30<03:16,  3.61it/s] 14%|        | 111/820 [00:31<03:15,  3.62it/s] 14%|        | 112/820 [00:31<03:15,  3.62it/s] 14%|        | 113/820 [00:31<03:15,  3.62it/s] 14%|        | 114/820 [00:31<03:15,  3.62it/s] 14%|        | 115/820 [00:32<03:14,  3.62it/s] 14%|        | 116/820 [00:32<03:19,  3.53it/s] 14%|        | 117/820 [00:32<03:17,  3.56it/s] 14%|        | 118/820 [00:32<03:15,  3.58it/s] 15%|        | 119/820 [00:33<03:15,  3.59it/s] 15%|        | 120/820 [00:33<03:14,  3.60it/s] 15%|        | 121/820 [00:33<03:13,  3.61it/s] 15%|        | 122/820 [00:34<03:12,  3.62it/s] 15%|        | 123/820 [00:34<03:12,  3.62it/s] 15%|        | 124/820 [00:34<03:12,  3.62it/s] 15%|        | 125/820 [00:34<03:11,  3.62it/s] 15%|        | 126/820 [00:35<03:11,  3.62it/s] 15%|        | 127/820 [00:35<03:17,  3.52it/s] 16%|        | 128/820 [00:35<03:14,  3.55it/s] 16%|        | 129/820 [00:36<03:13,  3.57it/s] 16%|        | 130/820 [00:36<03:12,  3.59it/s] 16%|        | 131/820 [00:36<03:11,  3.60it/s] 16%|        | 132/820 [00:36<03:10,  3.60it/s] 16%|        | 133/820 [00:37<03:10,  3.61it/s] 16%|        | 134/820 [00:37<03:09,  3.62it/s] 16%|        | 135/820 [00:37<03:09,  3.62it/s] 17%|        | 136/820 [00:37<03:09,  3.62it/s] 17%|        | 137/820 [00:38<03:08,  3.62it/s] 17%|        | 138/820 [00:38<03:13,  3.52it/s] 17%|        | 139/820 [00:38<03:11,  3.55it/s] 17%|        | 140/820 [00:39<03:27,  3.28it/s] 17%|        | 141/820 [00:39<04:01,  2.81it/s] 17%|        | 142/820 [00:39<03:45,  3.01it/s] 17%|        | 143/820 [00:40<03:33,  3.17it/s] 18%|        | 144/820 [00:40<03:25,  3.29it/s] 18%|        | 145/820 [00:40<03:19,  3.39it/s] 18%|        | 146/820 [00:41<03:14,  3.46it/s] 18%|        | 147/820 [00:41<03:11,  3.51it/s] 18%|        | 148/820 [00:41<03:17,  3.40it/s] 18%|        | 149/820 [00:41<03:13,  3.46it/s] 18%|        | 150/820 [00:42<03:10,  3.51it/s] 18%|        | 151/820 [00:42<03:08,  3.55it/s] 19%|        | 152/820 [00:42<03:07,  3.57it/s] 19%|        | 153/820 [00:42<03:06,  3.58it/s] 19%|        | 154/820 [00:43<03:05,  3.60it/s] 19%|        | 155/820 [00:43<03:04,  3.61it/s] 19%|        | 156/820 [00:43<03:03,  3.62it/s] 19%|        | 157/820 [00:44<03:03,  3.62it/s] 19%|        | 158/820 [00:44<03:02,  3.62it/s] 19%|        | 159/820 [00:44<03:11,  3.46it/s] 20%|        | 160/820 [00:44<03:08,  3.50it/s] 20%|        | 161/820 [00:45<03:06,  3.54it/s] 20%|        | 162/820 [00:45<03:04,  3.57it/s] 20%|        | 163/820 [00:45<03:03,  3.58it/s] 20%|        | 164/820 [00:46<03:02,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 06:50:47,124 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:50:47,124 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:50:47,124 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.03it/s][A
  3%|         | 12/437 [00:00<00:10, 41.81it/s][A
  4%|         | 17/437 [00:00<00:09, 43.28it/s][A
  5%|         | 22/437 [00:00<00:09, 44.11it/s][A
  6%|         | 27/437 [00:00<00:09, 44.65it/s][A
  7%|         | 32/437 [00:00<00:09, 44.88it/s][A
  8%|         | 37/437 [00:00<00:08, 44.83it/s][A
 10%|         | 42/437 [00:00<00:08, 44.85it/s][A
 11%|         | 47/437 [00:01<00:08, 44.90it/s][A
 12%|        | 52/437 [00:01<00:08, 44.75it/s][A
 13%|        | 57/437 [00:01<00:08, 44.84it/s][A
 14%|        | 62/437 [00:01<00:09, 41.29it/s][A
 15%|        | 67/437 [00:01<00:08, 42.58it/s][A
 16%|        | 72/437 [00:01<00:08, 43.42it/s][A
 18%|        | 77/437 [00:01<00:08, 44.15it/s][A
 19%|        | 82/437 [00:01<00:07, 44.58it/s][A
 20%|        | 87/437 [00:01<00:07, 44.85it/s][A
 21%|        | 92/437 [00:02<00:07, 44.79it/s][A
 22%|       | 97/437 [00:02<00:07, 44.85it/s][A
 23%|       | 102/437 [00:02<00:07, 44.55it/s][A
 24%|       | 107/437 [00:02<00:07, 44.50it/s][A
 26%|       | 112/437 [00:02<00:07, 44.84it/s][A
 27%|       | 117/437 [00:02<00:07, 45.05it/s][A
 28%|       | 122/437 [00:02<00:06, 45.34it/s][A
 29%|       | 127/437 [00:02<00:07, 40.82it/s][A
 30%|       | 132/437 [00:03<00:07, 40.54it/s][A
 31%|      | 137/437 [00:03<00:07, 42.00it/s][A
 32%|      | 142/437 [00:03<00:06, 43.06it/s][A
 34%|      | 147/437 [00:03<00:06, 43.69it/s][A
 35%|      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|      | 157/437 [00:03<00:06, 44.51it/s][A
 37%|      | 162/437 [00:03<00:06, 44.73it/s][A
 38%|      | 167/437 [00:03<00:06, 44.95it/s][A
 39%|      | 172/437 [00:03<00:05, 44.66it/s][A
 41%|      | 177/437 [00:04<00:05, 44.67it/s][A
 42%|     | 182/437 [00:04<00:05, 44.88it/s][A
 43%|     | 187/437 [00:04<00:05, 45.07it/s][A
 44%|     | 192/437 [00:04<00:05, 43.06it/s][A
 45%|     | 197/437 [00:04<00:05, 43.84it/s][A
 46%|     | 202/437 [00:04<00:05, 44.42it/s][A
 47%|     | 207/437 [00:04<00:05, 44.59it/s][A
 49%|     | 212/437 [00:04<00:05, 44.75it/s][A
 50%|     | 217/437 [00:04<00:04, 44.60it/s][A
 51%|     | 222/437 [00:05<00:04, 44.73it/s][A
 52%|    | 227/437 [00:05<00:04, 44.80it/s][A
 53%|    | 232/437 [00:05<00:04, 44.70it/s][A
 54%|    | 237/437 [00:05<00:04, 44.95it/s][A
 55%|    | 242/437 [00:05<00:04, 45.07it/s][A
 57%|    | 247/437 [00:05<00:04, 45.30it/s][A
 58%|    | 252/437 [00:05<00:04, 45.27it/s][A
 59%|    | 257/437 [00:05<00:03, 45.19it/s][A
 60%|    | 262/437 [00:05<00:03, 45.07it/s][A
 61%|    | 267/437 [00:06<00:03, 44.97it/s][A
 62%|   | 272/437 [00:06<00:03, 45.01it/s][A
 63%|   | 277/437 [00:06<00:03, 44.96it/s][A
 65%|   | 282/437 [00:06<00:03, 45.09it/s][A
 66%|   | 287/437 [00:06<00:03, 45.20it/s][A
 67%|   | 292/437 [00:06<00:03, 45.35it/s][A
 68%|   | 297/437 [00:06<00:03, 45.33it/s][A
 69%|   | 302/437 [00:06<00:02, 45.22it/s][A
 70%|   | 307/437 [00:06<00:02, 45.07it/s][A
 71%|  | 312/437 [00:07<00:02, 44.87it/s][A
 73%|  | 317/437 [00:07<00:02, 44.86it/s][A
 74%|  | 322/437 [00:07<00:02, 44.94it/s][A
 75%|  | 327/437 [00:07<00:02, 42.54it/s][A
 76%|  | 332/437 [00:07<00:02, 43.51it/s][A
 77%|  | 337/437 [00:07<00:02, 44.09it/s][A
 78%|  | 342/437 [00:07<00:02, 44.61it/s][A
 79%|  | 347/437 [00:07<00:02, 44.75it/s][A
 81%|  | 352/437 [00:07<00:01, 44.85it/s][A
 82%| | 357/437 [00:08<00:01, 44.88it/s][A
 83%| | 362/437 [00:08<00:01, 44.73it/s][A
 84%| | 367/437 [00:08<00:01, 44.64it/s][A
 85%| | 372/437 [00:08<00:01, 44.59it/s][A
 86%| | 377/437 [00:08<00:01, 44.86it/s][A
 87%| | 382/437 [00:08<00:01, 44.98it/s][A
 89%| | 387/437 [00:08<00:01, 45.19it/s][A
 90%| | 392/437 [00:08<00:00, 45.29it/s][A
 91%| | 397/437 [00:08<00:00, 45.28it/s][A
 92%|| 402/437 [00:09<00:00, 45.27it/s][A
 93%|| 407/437 [00:09<00:00, 45.05it/s][A
 94%|| 412/437 [00:09<00:00, 44.94it/s][A
 95%|| 417/437 [00:09<00:00, 44.84it/s][A
 97%|| 422/437 [00:09<00:00, 44.94it/s][A
 98%|| 427/437 [00:09<00:00, 45.06it/s][A
 99%|| 432/437 [00:09<00:00, 45.23it/s][A
100%|| 437/437 [00:09<00:00, 45.32it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.32it/s][A 20%|        | 164/820 [00:55<03:02,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:50:57,166 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:50:57,298 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:51:00,563 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:51:00,720 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:51:00,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:01<51:10,  4.69s/it] 20%|        | 166/820 [01:01<36:41,  3.37s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:01<26:33,  2.44s/it] 20%|        | 168/820 [01:01<19:28,  1.79s/it] 21%|        | 169/820 [01:02<14:31,  1.34s/it] 21%|        | 170/820 [01:02<11:03,  1.02s/it] 21%|        | 171/820 [01:02<08:38,  1.25it/s] 21%|        | 172/820 [01:03<06:57,  1.55it/s] 21%|        | 173/820 [01:03<05:46,  1.87it/s] 21%|        | 174/820 [01:03<04:56,  2.18it/s] 21%|       | 175/820 [01:03<04:21,  2.47it/s] 21%|       | 176/820 [01:04<03:56,  2.72it/s] 22%|       | 177/820 [01:04<03:39,  2.93it/s] 22%|       | 178/820 [01:04<03:27,  3.10it/s] 22%|       | 179/820 [01:04<03:18,  3.23it/s] 22%|       | 180/820 [01:05<03:12,  3.32it/s] 22%|       | 181/820 [01:05<03:08,  3.40it/s] 22%|       | 182/820 [01:05<03:05,  3.45it/s] 22%|       | 183/820 [01:06<03:04,  3.45it/s] 22%|       | 184/820 [01:06<03:02,  3.49it/s] 23%|       | 185/820 [01:06<03:00,  3.51it/s] 23%|       | 186/820 [01:06<02:59,  3.53it/s] 23%|       | 187/820 [01:07<02:58,  3.55it/s] 23%|       | 188/820 [01:07<02:57,  3.56it/s] 23%|       | 189/820 [01:07<02:57,  3.56it/s] 23%|       | 190/820 [01:08<02:56,  3.57it/s] 23%|       | 191/820 [01:08<02:56,  3.57it/s] 23%|       | 192/820 [01:08<02:55,  3.57it/s] 24%|       | 193/820 [01:08<02:55,  3.58it/s] 24%|       | 194/820 [01:09<02:59,  3.49it/s] 24%|       | 195/820 [01:09<02:57,  3.52it/s] 24%|       | 196/820 [01:09<02:56,  3.53it/s] 24%|       | 197/820 [01:10<02:55,  3.55it/s] 24%|       | 198/820 [01:10<02:54,  3.56it/s] 24%|       | 199/820 [01:10<02:54,  3.57it/s] 24%|       | 200/820 [01:10<02:53,  3.57it/s] 25%|       | 201/820 [01:11<02:53,  3.57it/s] 25%|       | 202/820 [01:11<02:52,  3.58it/s] 25%|       | 203/820 [01:11<02:52,  3.58it/s] 25%|       | 204/820 [01:11<02:52,  3.58it/s] 25%|       | 205/820 [01:12<02:57,  3.46it/s] 25%|       | 206/820 [01:12<02:55,  3.50it/s] 25%|       | 207/820 [01:12<02:54,  3.52it/s] 25%|       | 208/820 [01:13<02:53,  3.54it/s] 25%|       | 209/820 [01:13<02:52,  3.55it/s] 26%|       | 210/820 [01:13<02:51,  3.55it/s] 26%|       | 211/820 [01:13<02:51,  3.56it/s] 26%|       | 212/820 [01:14<02:50,  3.56it/s] 26%|       | 213/820 [01:14<02:50,  3.57it/s] 26%|       | 214/820 [01:14<02:49,  3.57it/s] 26%|       | 215/820 [01:15<02:49,  3.57it/s] 26%|       | 216/820 [01:15<02:55,  3.45it/s] 26%|       | 217/820 [01:15<02:53,  3.48it/s] 27%|       | 218/820 [01:15<02:51,  3.51it/s] 27%|       | 219/820 [01:16<02:50,  3.53it/s] 27%|       | 220/820 [01:16<02:49,  3.54it/s] 27%|       | 221/820 [01:16<02:48,  3.56it/s] 27%|       | 222/820 [01:17<02:47,  3.56it/s] 27%|       | 223/820 [01:17<02:47,  3.56it/s] 27%|       | 224/820 [01:17<02:47,  3.56it/s] 27%|       | 225/820 [01:17<02:46,  3.57it/s] 28%|       | 226/820 [01:18<02:46,  3.57it/s] 28%|       | 227/820 [01:18<02:49,  3.50it/s] 28%|       | 228/820 [01:18<02:48,  3.52it/s] 28%|       | 229/820 [01:19<02:47,  3.54it/s] 28%|       | 230/820 [01:19<02:46,  3.55it/s] 28%|       | 231/820 [01:19<02:45,  3.55it/s] 28%|       | 232/820 [01:19<02:45,  3.55it/s] 28%|       | 233/820 [01:20<02:44,  3.56it/s] 29%|       | 234/820 [01:20<02:44,  3.57it/s] 29%|       | 235/820 [01:20<02:43,  3.57it/s] 29%|       | 236/820 [01:21<02:43,  3.56it/s] 29%|       | 237/820 [01:21<02:43,  3.57it/s] 29%|       | 238/820 [01:21<02:42,  3.57it/s] 29%|       | 239/820 [01:21<02:42,  3.57it/s] 29%|       | 240/820 [01:22<02:42,  3.57it/s] 29%|       | 241/820 [01:22<02:42,  3.57it/s] 30%|       | 242/820 [01:22<02:41,  3.57it/s] 30%|       | 243/820 [01:22<02:41,  3.57it/s] 30%|       | 244/820 [01:23<02:41,  3.57it/s] 30%|       | 245/820 [01:23<02:40,  3.58it/s] 30%|       | 246/820 [01:23<02:40,  3.58it/s] 30%|       | 247/820 [01:24<02:40,  3.58it/s] 30%|       | 248/820 [01:24<02:47,  3.41it/s] 30%|       | 249/820 [01:24<02:44,  3.46it/s] 30%|       | 250/820 [01:24<02:42,  3.50it/s] 31%|       | 251/820 [01:25<02:41,  3.52it/s] 31%|       | 252/820 [01:25<02:40,  3.53it/s] 31%|       | 253/820 [01:25<02:39,  3.55it/s] 31%|       | 254/820 [01:26<02:39,  3.56it/s] 31%|       | 255/820 [01:26<02:38,  3.56it/s] 31%|       | 256/820 [01:26<02:37,  3.57it/s] 31%|      | 257/820 [01:26<02:37,  3.57it/s] 31%|      | 258/820 [01:27<02:37,  3.57it/s] 32%|      | 259/820 [01:27<02:43,  3.43it/s] 32%|      | 260/820 [01:27<02:41,  3.47it/s] 32%|      | 261/820 [01:28<02:39,  3.50it/s] 32%|      | 262/820 [01:28<02:38,  3.52it/s] 32%|      | 263/820 [01:28<02:37,  3.54it/s] 32%|      | 264/820 [01:28<02:36,  3.55it/s] 32%|      | 265/820 [01:29<02:36,  3.55it/s] 32%|      | 266/820 [01:29<02:35,  3.56it/s] 33%|      | 267/820 [01:29<02:35,  3.56it/s] 33%|      | 268/820 [01:30<02:34,  3.57it/s] 33%|      | 269/820 [01:30<02:34,  3.57it/s] 33%|      | 270/820 [01:30<02:37,  3.49it/s] 33%|      | 271/820 [01:30<02:36,  3.51it/s] 33%|      | 272/820 [01:31<02:35,  3.53it/s] 33%|      | 273/820 [01:31<02:34,  3.54it/s] 33%|      | 274/820 [01:31<02:33,  3.55it/s] 34%|      | 275/820 [01:32<02:33,  3.56it/s] 34%|      | 276/820 [01:32<02:32,  3.56it/s] 34%|      | 277/820 [01:32<02:32,  3.57it/s] 34%|      | 278/820 [01:32<02:31,  3.57it/s] 34%|      | 279/820 [01:33<02:31,  3.57it/s] 34%|      | 280/820 [01:33<02:31,  3.57it/s] 34%|      | 281/820 [01:33<02:33,  3.52it/s] 34%|      | 282/820 [01:34<02:32,  3.54it/s] 35%|      | 283/820 [01:34<02:31,  3.54it/s] 35%|      | 284/820 [01:34<02:31,  3.55it/s] 35%|      | 285/820 [01:34<02:30,  3.55it/s] 35%|      | 286/820 [01:35<02:30,  3.55it/s] 35%|      | 287/820 [01:35<02:29,  3.56it/s] 35%|      | 288/820 [01:35<02:29,  3.56it/s] 35%|      | 289/820 [01:35<02:28,  3.57it/s] 35%|      | 290/820 [01:36<02:28,  3.57it/s] 35%|      | 291/820 [01:36<02:28,  3.57it/s] 36%|      | 292/820 [01:36<02:34,  3.42it/s] 36%|      | 293/820 [01:37<02:32,  3.46it/s] 36%|      | 294/820 [01:37<02:30,  3.50it/s] 36%|      | 295/820 [01:37<02:28,  3.52it/s] 36%|      | 296/820 [01:37<02:27,  3.55it/s] 36%|      | 297/820 [01:38<02:26,  3.57it/s] 36%|      | 298/820 [01:38<02:25,  3.59it/s] 36%|      | 299/820 [01:38<02:24,  3.60it/s] 37%|      | 300/820 [01:39<02:24,  3.61it/s] 37%|      | 301/820 [01:39<02:23,  3.61it/s] 37%|      | 302/820 [01:39<02:23,  3.62it/s] 37%|      | 303/820 [01:39<02:28,  3.49it/s] 37%|      | 304/820 [01:40<02:26,  3.53it/s] 37%|      | 305/820 [01:40<02:24,  3.56it/s] 37%|      | 306/820 [01:40<02:23,  3.58it/s] 37%|      | 307/820 [01:41<02:22,  3.60it/s] 38%|      | 308/820 [01:41<02:21,  3.61it/s] 38%|      | 309/820 [01:41<02:21,  3.61it/s] 38%|      | 310/820 [01:41<02:21,  3.61it/s] 38%|      | 311/820 [01:42<02:20,  3.62it/s] 38%|      | 312/820 [01:42<02:20,  3.62it/s] 38%|      | 313/820 [01:42<02:19,  3.62it/s] 38%|      | 314/820 [01:43<02:27,  3.43it/s] 38%|      | 315/820 [01:43<02:24,  3.49it/s] 39%|      | 316/820 [01:43<02:22,  3.53it/s] 39%|      | 317/820 [01:43<02:21,  3.55it/s] 39%|      | 318/820 [01:44<02:20,  3.57it/s] 39%|      | 319/820 [01:44<02:19,  3.59it/s] 39%|      | 320/820 [01:44<02:18,  3.60it/s] 39%|      | 321/820 [01:44<02:18,  3.61it/s] 39%|      | 322/820 [01:45<02:17,  3.61it/s] 39%|      | 323/820 [01:45<02:17,  3.62it/s] 40%|      | 324/820 [01:45<02:17,  3.62it/s] 40%|      | 325/820 [01:46<02:20,  3.52it/s] 40%|      | 326/820 [01:46<02:23,  3.43it/s] 40%|      | 327/820 [01:46<02:21,  3.47it/s] 40%|      | 328/820 [01:46<02:19,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 06:51:48,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:51:48,004 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:51:48,004 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8795, 'eval_samples_per_second': 353.156, 'eval_steps_per_second': 44.233, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.68it/s][A
  3%|         | 12/437 [00:00<00:08, 49.54it/s][A
  4%|         | 18/437 [00:00<00:08, 47.61it/s][A
  5%|         | 23/437 [00:00<00:08, 46.71it/s][A
  6%|         | 28/437 [00:00<00:08, 45.93it/s][A
  8%|         | 33/437 [00:00<00:08, 45.54it/s][A
  9%|         | 38/437 [00:00<00:08, 45.16it/s][A
 10%|         | 43/437 [00:00<00:08, 44.92it/s][A
 11%|         | 48/437 [00:01<00:08, 45.06it/s][A
 12%|        | 53/437 [00:01<00:08, 45.19it/s][A
 13%|        | 58/437 [00:01<00:08, 45.37it/s][A
 14%|        | 63/437 [00:01<00:08, 45.43it/s][A
 16%|        | 68/437 [00:01<00:08, 45.50it/s][A
 17%|        | 73/437 [00:01<00:08, 45.24it/s][A
 18%|        | 78/437 [00:01<00:07, 45.06it/s][A
 19%|        | 83/437 [00:01<00:07, 44.83it/s][A
 20%|        | 88/437 [00:01<00:08, 41.47it/s][A
 21%|       | 93/437 [00:02<00:09, 34.85it/s][A
 22%|       | 98/437 [00:02<00:09, 37.58it/s][A
 24%|       | 103/437 [00:02<00:08, 39.62it/s][A
 25%|       | 108/437 [00:02<00:07, 41.19it/s][A
 26%|       | 113/437 [00:02<00:07, 42.32it/s][A
 27%|       | 118/437 [00:02<00:07, 43.26it/s][A
 28%|       | 123/437 [00:02<00:07, 43.78it/s][A
 29%|       | 128/437 [00:02<00:06, 44.37it/s][A
 30%|       | 133/437 [00:03<00:06, 44.38it/s][A
 32%|      | 138/437 [00:03<00:06, 44.28it/s][A
 33%|      | 143/437 [00:03<00:06, 44.52it/s][A
 34%|      | 148/437 [00:03<00:06, 44.77it/s][A
 35%|      | 153/437 [00:03<00:06, 44.98it/s][A
 36%|      | 158/437 [00:03<00:06, 45.08it/s][A
 37%|      | 163/437 [00:03<00:06, 45.07it/s][A
 38%|      | 168/437 [00:03<00:05, 45.21it/s][A
 40%|      | 173/437 [00:03<00:05, 45.14it/s][A
 41%|      | 178/437 [00:04<00:05, 44.95it/s][A
 42%|     | 183/437 [00:04<00:05, 44.80it/s][A
 43%|     | 188/437 [00:04<00:05, 44.84it/s][A
 44%|     | 193/437 [00:04<00:05, 44.90it/s][A
 45%|     | 198/437 [00:04<00:05, 45.08it/s][A
 46%|     | 203/437 [00:04<00:05, 45.15it/s][A
 48%|     | 208/437 [00:04<00:05, 45.26it/s][A
 49%|     | 213/437 [00:04<00:04, 45.26it/s][A
 50%|     | 218/437 [00:04<00:04, 45.21it/s][A
 51%|     | 223/437 [00:05<00:04, 44.91it/s][A
 52%|    | 228/437 [00:05<00:04, 44.98it/s][A
 53%|    | 233/437 [00:05<00:04, 44.89it/s][A
 54%|    | 238/437 [00:05<00:04, 44.89it/s][A
 56%|    | 243/437 [00:05<00:04, 45.01it/s][A
 57%|    | 248/437 [00:05<00:04, 45.19it/s][A
 58%|    | 253/437 [00:05<00:04, 45.28it/s][A
 59%|    | 258/437 [00:05<00:03, 45.32it/s][A
 60%|    | 263/437 [00:05<00:03, 45.24it/s][A
 61%|   | 268/437 [00:06<00:03, 45.06it/s][A
 62%|   | 273/437 [00:06<00:03, 45.05it/s][A
 64%|   | 278/437 [00:06<00:03, 44.92it/s][A
 65%|   | 283/437 [00:06<00:03, 45.01it/s][A
 66%|   | 288/437 [00:06<00:03, 44.99it/s][A
 67%|   | 293/437 [00:06<00:03, 45.11it/s][A
 68%|   | 298/437 [00:06<00:03, 45.27it/s][A
 69%|   | 303/437 [00:06<00:02, 45.23it/s][A
 70%|   | 308/437 [00:06<00:02, 45.30it/s][A
 72%|  | 313/437 [00:07<00:02, 45.16it/s][A
 73%|  | 318/437 [00:07<00:02, 45.14it/s][A
 74%|  | 323/437 [00:07<00:02, 45.03it/s][A
 75%|  | 328/437 [00:07<00:02, 45.07it/s][A
 76%|  | 333/437 [00:07<00:02, 45.01it/s][A
 77%|  | 338/437 [00:07<00:02, 45.18it/s][A
 78%|  | 343/437 [00:07<00:02, 45.14it/s][A
 80%|  | 348/437 [00:07<00:01, 45.20it/s][A
 81%|  | 353/437 [00:07<00:01, 44.02it/s][A
 82%| | 358/437 [00:08<00:01, 44.40it/s][A
 83%| | 363/437 [00:08<00:01, 44.55it/s][A
 84%| | 368/437 [00:08<00:01, 44.64it/s][A
 85%| | 373/437 [00:08<00:01, 44.70it/s][A
 86%| | 378/437 [00:08<00:01, 44.83it/s][A
 88%| | 383/437 [00:08<00:01, 44.92it/s][A
 89%| | 388/437 [00:08<00:01, 45.01it/s][A
 90%| | 393/437 [00:08<00:00, 44.91it/s][A
 91%| | 398/437 [00:08<00:00, 44.95it/s][A
 92%|| 403/437 [00:09<00:00, 45.10it/s][A
 93%|| 408/437 [00:09<00:00, 45.06it/s][A
 95%|| 413/437 [00:09<00:00, 45.14it/s][A
 96%|| 418/437 [00:09<00:00, 45.08it/s][A
 97%|| 423/437 [00:09<00:00, 45.02it/s][A
 98%|| 428/437 [00:09<00:00, 45.17it/s][A
 99%|| 433/437 [00:09<00:00, 45.09it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.09it/s][A 40%|      | 328/820 [01:56<02:19,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:51:58,004 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:51:58,176 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:52:01,543 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:52:01,718 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:52:01,828 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:02<38:59,  4.76s/it] 40%|      | 330/820 [02:02<27:55,  3.42s/it] 40%|      | 331/820 [02:02<20:11,  2.48s/it] 40%|      | 332/820 [02:03<14:47,  1.82s/it] 41%|      | 333/820 [02:03<11:00,  1.36s/it] 41%|      | 334/820 [02:03<08:22,  1.03s/it] 41%|      | 335/820 [02:03<06:33,  1.23it/s] 41%|      | 336/820 [02:04<05:15,  1.54it/s] 41%|      | 337/820 [02:04<04:20,  1.85it/s] 41%|      | 338/820 [02:04<03:42,  2.17it/s] 41%|     | 339/820 [02:04<03:15,  2.46it/s] 41%|     | 340/820 [02:05<02:57,  2.71it/s] 42%|     | 341/820 [02:05<02:43,  2.92it/s] 42%|     | 342/820 [02:05<02:34,  3.09it/s] 42%|     | 343/820 [02:06<02:27,  3.23it/s] 42%|     | 344/820 [02:06<02:22,  3.34it/s] 42%|     | 345/820 [02:06<02:18,  3.42it/s] 42%|     | 346/820 [02:06<02:20,  3.37it/s] 42%|     | 347/820 [02:07<02:17,  3.44it/s] 42%|     | 348/820 [02:07<02:15,  3.49it/s] 43%|     | 349/820 [02:07<02:13,  3.53it/s] 43%|     | 350/820 [02:08<02:11,  3.56it/s] 43%|     | 351/820 [02:08<02:10,  3.58it/s] 43%|     | 352/820 [02:08<02:10,  3.59it/s] 43%|     | 353/820 [02:08<02:09,  3.60it/s] 43%|     | 354/820 [02:09<02:08,  3.61it/s] 43%|     | 355/820 [02:09<02:08,  3.62it/s] 43%|     | 356/820 [02:09<02:08,  3.62it/s] 44%|     | 357/820 [02:10<02:13,  3.46it/s] 44%|     | 358/820 [02:10<02:15,  3.40it/s] 44%|     | 359/820 [02:10<02:13,  3.45it/s] 44%|     | 360/820 [02:10<02:11,  3.50it/s] 44%|     | 361/820 [02:11<02:09,  3.54it/s] 44%|     | 362/820 [02:11<02:08,  3.56it/s] 44%|     | 363/820 [02:11<02:07,  3.58it/s] 44%|     | 364/820 [02:11<02:06,  3.60it/s] 45%|     | 365/820 [02:12<02:06,  3.61it/s] 45%|     | 366/820 [02:12<02:05,  3.61it/s] 45%|     | 367/820 [02:12<02:05,  3.62it/s] 45%|     | 368/820 [02:13<02:07,  3.55it/s] 45%|     | 369/820 [02:13<02:06,  3.57it/s] 45%|     | 370/820 [02:13<02:05,  3.59it/s] 45%|     | 371/820 [02:13<02:04,  3.60it/s] 45%|     | 372/820 [02:14<02:04,  3.61it/s] 45%|     | 373/820 [02:14<02:03,  3.62it/s] 46%|     | 374/820 [02:14<02:03,  3.62it/s] 46%|     | 375/820 [02:15<02:03,  3.61it/s] 46%|     | 376/820 [02:15<02:02,  3.62it/s] 46%|     | 377/820 [02:15<02:02,  3.62it/s] 46%|     | 378/820 [02:15<02:01,  3.62it/s] 46%|     | 379/820 [02:16<02:05,  3.51it/s] 46%|     | 380/820 [02:16<02:04,  3.55it/s] 46%|     | 381/820 [02:16<02:02,  3.57it/s] 47%|     | 382/820 [02:17<02:02,  3.59it/s] 47%|     | 383/820 [02:17<02:01,  3.60it/s] 47%|     | 384/820 [02:17<02:00,  3.61it/s] 47%|     | 385/820 [02:17<02:00,  3.61it/s] 47%|     | 386/820 [02:18<02:00,  3.61it/s] 47%|     | 387/820 [02:18<01:59,  3.62it/s] 47%|     | 388/820 [02:18<01:59,  3.62it/s] 47%|     | 389/820 [02:18<01:59,  3.62it/s] 48%|     | 390/820 [02:19<02:01,  3.54it/s] 48%|     | 391/820 [02:19<02:00,  3.56it/s] 48%|     | 392/820 [02:19<01:59,  3.58it/s] 48%|     | 393/820 [02:20<01:58,  3.59it/s] 48%|     | 394/820 [02:20<01:58,  3.60it/s] 48%|     | 395/820 [02:20<01:57,  3.60it/s] 48%|     | 396/820 [02:20<01:57,  3.61it/s] 48%|     | 397/820 [02:21<01:57,  3.61it/s] 49%|     | 398/820 [02:21<01:56,  3.61it/s] 49%|     | 399/820 [02:21<01:56,  3.61it/s] 49%|     | 400/820 [02:21<01:56,  3.60it/s] 49%|     | 401/820 [02:22<02:00,  3.48it/s] 49%|     | 402/820 [02:22<01:58,  3.52it/s] 49%|     | 403/820 [02:22<02:11,  3.18it/s] 49%|     | 404/820 [02:23<02:06,  3.30it/s] 49%|     | 405/820 [02:23<02:02,  3.39it/s] 50%|     | 406/820 [02:23<01:59,  3.45it/s] 50%|     | 407/820 [02:24<01:57,  3.50it/s] 50%|     | 408/820 [02:24<01:56,  3.53it/s] 50%|     | 409/820 [02:24<01:55,  3.56it/s] 50%|     | 410/820 [02:24<01:54,  3.58it/s] 50%|     | 411/820 [02:25<01:53,  3.59it/s] 50%|     | 412/820 [02:25<01:53,  3.60it/s] 50%|     | 413/820 [02:25<01:52,  3.60it/s] 50%|     | 414/820 [02:26<01:55,  3.50it/s] 51%|     | 415/820 [02:26<01:54,  3.54it/s] 51%|     | 416/820 [02:26<01:53,  3.56it/s] 51%|     | 417/820 [02:26<01:52,  3.58it/s] 51%|     | 418/820 [02:27<01:51,  3.60it/s] 51%|     | 419/820 [02:27<01:51,  3.60it/s] 51%|     | 420/820 [02:27<01:50,  3.61it/s] 51%|    | 421/820 [02:27<01:50,  3.61it/s] 51%|    | 422/820 [02:28<01:49,  3.62it/s] 52%|    | 423/820 [02:28<01:49,  3.62it/s] 52%|    | 424/820 [02:28<01:49,  3.62it/s] 52%|    | 425/820 [02:29<01:51,  3.55it/s] 52%|    | 426/820 [02:29<01:50,  3.57it/s] 52%|    | 427/820 [02:29<01:49,  3.59it/s] 52%|    | 428/820 [02:29<01:48,  3.60it/s] 52%|    | 429/820 [02:30<01:48,  3.61it/s] 52%|    | 430/820 [02:30<01:47,  3.61it/s] 53%|    | 431/820 [02:30<01:47,  3.62it/s] 53%|    | 432/820 [02:31<01:47,  3.62it/s] 53%|    | 433/820 [02:31<01:46,  3.62it/s] 53%|    | 434/820 [02:31<01:46,  3.62it/s] 53%|    | 435/820 [02:31<01:46,  3.63it/s] 53%|    | 436/820 [02:32<01:48,  3.52it/s] 53%|    | 437/820 [02:32<01:47,  3.56it/s] 53%|    | 438/820 [02:32<01:46,  3.58it/s] 54%|    | 439/820 [02:32<01:46,  3.59it/s] 54%|    | 440/820 [02:33<01:45,  3.60it/s] 54%|    | 441/820 [02:33<01:45,  3.61it/s] 54%|    | 442/820 [02:33<01:44,  3.61it/s] 54%|    | 443/820 [02:34<01:44,  3.62it/s] 54%|    | 444/820 [02:34<01:43,  3.62it/s] 54%|    | 445/820 [02:34<01:43,  3.62it/s] 54%|    | 446/820 [02:34<01:43,  3.62it/s] 55%|    | 447/820 [02:35<01:47,  3.47it/s] 55%|    | 448/820 [02:35<01:45,  3.51it/s] 55%|    | 449/820 [02:35<01:44,  3.54it/s] 55%|    | 450/820 [02:36<01:43,  3.57it/s] 55%|    | 451/820 [02:36<01:42,  3.58it/s] 55%|    | 452/820 [02:36<01:42,  3.60it/s] 55%|    | 453/820 [02:36<01:41,  3.61it/s] 55%|    | 454/820 [02:37<01:41,  3.61it/s] 55%|    | 455/820 [02:37<01:41,  3.61it/s] 56%|    | 456/820 [02:37<01:40,  3.61it/s] 56%|    | 457/820 [02:37<01:40,  3.62it/s] 56%|    | 458/820 [02:38<01:43,  3.49it/s] 56%|    | 459/820 [02:38<01:42,  3.53it/s] 56%|    | 460/820 [02:38<01:41,  3.56it/s] 56%|    | 461/820 [02:39<01:49,  3.27it/s] 56%|    | 462/820 [02:39<01:49,  3.27it/s] 56%|    | 463/820 [02:39<01:46,  3.36it/s] 57%|    | 464/820 [02:40<01:43,  3.43it/s] 57%|    | 465/820 [02:40<01:41,  3.49it/s] 57%|    | 466/820 [02:40<01:40,  3.53it/s] 57%|    | 467/820 [02:40<01:39,  3.56it/s] 57%|    | 468/820 [02:41<01:38,  3.58it/s] 57%|    | 469/820 [02:41<01:41,  3.46it/s] 57%|    | 470/820 [02:41<01:39,  3.51it/s] 57%|    | 471/820 [02:42<01:38,  3.54it/s] 58%|    | 472/820 [02:42<01:37,  3.57it/s] 58%|    | 473/820 [02:42<01:36,  3.58it/s] 58%|    | 474/820 [02:42<01:36,  3.60it/s] 58%|    | 475/820 [02:43<01:35,  3.61it/s] 58%|    | 476/820 [02:43<01:35,  3.61it/s] 58%|    | 477/820 [02:43<01:34,  3.62it/s] 58%|    | 478/820 [02:43<01:34,  3.62it/s] 58%|    | 479/820 [02:44<01:34,  3.62it/s] 59%|    | 480/820 [02:44<01:38,  3.45it/s] 59%|    | 481/820 [02:44<01:36,  3.50it/s] 59%|    | 482/820 [02:45<01:35,  3.53it/s] 59%|    | 483/820 [02:45<01:34,  3.56it/s] 59%|    | 484/820 [02:45<01:34,  3.57it/s] 59%|    | 485/820 [02:45<01:33,  3.58it/s] 59%|    | 486/820 [02:46<01:33,  3.59it/s] 59%|    | 487/820 [02:46<01:32,  3.60it/s] 60%|    | 488/820 [02:46<01:31,  3.61it/s] 60%|    | 489/820 [02:47<01:31,  3.61it/s] 60%|    | 490/820 [02:47<01:31,  3.62it/s] 60%|    | 491/820 [02:47<01:34,  3.46it/s] 60%|    | 492/820 [02:47<01:33,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 06:52:48,981 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:52:48,981 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:52:48,981 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8572, 'eval_samples_per_second': 353.954, 'eval_steps_per_second': 44.333, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.43it/s][A
  3%|         | 12/437 [00:00<00:08, 49.39it/s][A
  4%|         | 17/437 [00:00<00:08, 47.81it/s][A
  5%|         | 22/437 [00:00<00:08, 46.65it/s][A
  6%|         | 27/437 [00:00<00:08, 45.89it/s][A
  7%|         | 32/437 [00:00<00:08, 45.41it/s][A
  8%|         | 37/437 [00:00<00:08, 45.10it/s][A
 10%|         | 42/437 [00:00<00:08, 45.08it/s][A
 11%|         | 47/437 [00:01<00:12, 32.37it/s][A
 12%|        | 53/437 [00:01<00:10, 36.83it/s][A
 13%|        | 58/437 [00:01<00:09, 39.02it/s][A
 14%|        | 63/437 [00:01<00:09, 40.68it/s][A
 16%|        | 68/437 [00:01<00:08, 41.94it/s][A
 17%|        | 73/437 [00:01<00:08, 42.94it/s][A
 18%|        | 78/437 [00:01<00:08, 43.66it/s][A
 19%|        | 83/437 [00:01<00:08, 44.23it/s][A
 20%|        | 88/437 [00:02<00:07, 44.44it/s][A
 21%|       | 93/437 [00:02<00:07, 44.28it/s][A
 22%|       | 98/437 [00:02<00:07, 44.44it/s][A
 24%|       | 103/437 [00:02<00:07, 44.73it/s][A
 25%|       | 108/437 [00:02<00:07, 42.64it/s][A
 26%|       | 113/437 [00:02<00:07, 43.52it/s][A
 27%|       | 118/437 [00:02<00:07, 43.95it/s][A
 28%|       | 123/437 [00:02<00:07, 44.48it/s][A
 29%|       | 128/437 [00:02<00:06, 44.68it/s][A
 30%|       | 133/437 [00:03<00:06, 44.86it/s][A
 32%|      | 138/437 [00:03<00:06, 44.85it/s][A
 33%|      | 143/437 [00:03<00:06, 44.93it/s][A
 34%|      | 148/437 [00:03<00:06, 44.85it/s][A
 35%|      | 153/437 [00:03<00:06, 44.78it/s][A
 36%|      | 158/437 [00:03<00:06, 45.02it/s][A
 37%|      | 163/437 [00:03<00:06, 45.10it/s][A
 38%|      | 168/437 [00:03<00:05, 45.28it/s][A
 40%|      | 173/437 [00:03<00:05, 45.18it/s][A
 41%|      | 178/437 [00:04<00:05, 45.08it/s][A
 42%|     | 183/437 [00:04<00:05, 45.00it/s][A
 43%|     | 188/437 [00:04<00:05, 45.05it/s][A
 44%|     | 193/437 [00:04<00:05, 44.97it/s][A
 45%|     | 198/437 [00:04<00:05, 44.91it/s][A
 46%|     | 203/437 [00:04<00:05, 45.01it/s][A
 48%|     | 208/437 [00:04<00:05, 45.17it/s][A
 49%|     | 213/437 [00:04<00:04, 45.32it/s][A
 50%|     | 218/437 [00:04<00:04, 45.27it/s][A
 51%|     | 223/437 [00:05<00:04, 45.10it/s][A
 52%|    | 228/437 [00:05<00:04, 44.92it/s][A
 53%|    | 233/437 [00:05<00:04, 45.01it/s][A
 54%|    | 238/437 [00:05<00:04, 44.85it/s][A
 56%|    | 243/437 [00:05<00:04, 44.91it/s][A
 57%|    | 248/437 [00:05<00:04, 45.01it/s][A
 58%|    | 253/437 [00:05<00:04, 45.13it/s][A
 59%|    | 258/437 [00:05<00:03, 45.23it/s][A
 60%|    | 263/437 [00:05<00:03, 45.28it/s][A
 61%|   | 268/437 [00:06<00:03, 45.26it/s][A
 62%|   | 273/437 [00:06<00:03, 45.15it/s][A
 64%|   | 278/437 [00:06<00:03, 45.01it/s][A
 65%|   | 283/437 [00:06<00:03, 44.85it/s][A
 66%|   | 288/437 [00:06<00:03, 44.91it/s][A
 67%|   | 293/437 [00:06<00:03, 45.02it/s][A
 68%|   | 298/437 [00:06<00:03, 45.07it/s][A
 69%|   | 303/437 [00:06<00:02, 45.20it/s][A
 70%|   | 308/437 [00:06<00:02, 45.12it/s][A
 72%|  | 313/437 [00:07<00:02, 45.22it/s][A
 73%|  | 318/437 [00:07<00:02, 45.23it/s][A
 74%|  | 323/437 [00:07<00:02, 45.13it/s][A
 75%|  | 328/437 [00:07<00:02, 45.01it/s][A
 76%|  | 333/437 [00:07<00:02, 45.00it/s][A
 77%|  | 338/437 [00:07<00:02, 45.05it/s][A
 78%|  | 343/437 [00:07<00:02, 45.14it/s][A
 80%|  | 348/437 [00:07<00:01, 45.12it/s][A
 81%|  | 353/437 [00:07<00:01, 45.18it/s][A
 82%| | 358/437 [00:08<00:01, 45.20it/s][A
 83%| | 363/437 [00:08<00:01, 45.11it/s][A
 84%| | 368/437 [00:08<00:01, 45.09it/s][A
 85%| | 373/437 [00:08<00:01, 44.79it/s][A
 86%| | 378/437 [00:08<00:01, 44.92it/s][A
 88%| | 383/437 [00:08<00:01, 44.93it/s][A
 89%| | 388/437 [00:08<00:01, 44.97it/s][A
 90%| | 393/437 [00:08<00:00, 45.03it/s][A
 91%| | 398/437 [00:08<00:00, 45.17it/s][A
 92%|| 403/437 [00:09<00:00, 45.06it/s][A
 93%|| 408/437 [00:09<00:00, 45.08it/s][A
 95%|| 413/437 [00:09<00:00, 45.08it/s][A
 96%|| 418/437 [00:09<00:00, 44.98it/s][A
 97%|| 423/437 [00:09<00:00, 45.02it/s][A
 98%|| 428/437 [00:09<00:00, 44.94it/s][A
 99%|| 433/437 [00:09<00:00, 45.06it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.06it/s][A 60%|    | 492/820 [02:57<01:33,  3.51it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:52:59,018 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 06:52:59,188 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:53:01,475 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:53:01,630 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:53:01,705 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:01<23:35,  4.33s/it] 60%|    | 494/820 [03:01<16:57,  3.12s/it] 60%|    | 495/820 [03:02<12:17,  2.27s/it] 60%|    | 496/820 [03:02<09:01,  1.67s/it] 61%|    | 497/820 [03:02<06:45,  1.25s/it] 61%|    | 498/820 [03:03<05:09,  1.04it/s] 61%|    | 499/820 [03:03<04:03,  1.32it/s] 61%|    | 500/820 [03:03<03:16,  1.63it/s]                                                  61%|    | 500/820 [03:03<03:16,  1.63it/s] 61%|    | 501/820 [03:03<02:43,  1.95it/s] 61%|    | 502/820 [03:04<02:21,  2.25it/s] 61%|   | 503/820 [03:04<02:05,  2.53it/s] 61%|   | 504/820 [03:04<01:53,  2.78it/s] 62%|   | 505/820 [03:05<01:47,  2.92it/s] 62%|   | 506/820 [03:05<01:41,  3.09it/s] 62%|   | 507/820 [03:05<01:37,  3.22it/s] 62%|   | 508/820 [03:05<01:33,  3.32it/s] 62%|   | 509/820 [03:06<01:31,  3.39it/s] 62%|   | 510/820 [03:06<01:29,  3.44it/s] 62%|   | 511/820 [03:06<01:28,  3.48it/s] 62%|   | 512/820 [03:07<01:27,  3.51it/s] 63%|   | 513/820 [03:07<01:27,  3.53it/s] 63%|   | 514/820 [03:07<01:26,  3.54it/s] 63%|   | 515/820 [03:07<01:25,  3.55it/s] 63%|   | 516/820 [03:08<01:28,  3.44it/s] 63%|   | 517/820 [03:08<01:27,  3.48it/s] 63%|   | 518/820 [03:08<01:26,  3.51it/s] 63%|   | 519/820 [03:09<01:25,  3.52it/s] 63%|   | 520/820 [03:09<01:24,  3.54it/s] 64%|   | 521/820 [03:09<01:25,  3.48it/s] 64%|   | 522/820 [03:09<01:25,  3.49it/s] 64%|   | 523/820 [03:10<01:24,  3.51it/s] 64%|   | 524/820 [03:10<01:26,  3.40it/s] 64%|   | 525/820 [03:10<01:25,  3.44it/s] 64%|   | 526/820 [03:11<01:24,  3.48it/s] 64%|   | 527/820 [03:11<01:26,  3.38it/s] 64%|   | 528/820 [03:11<01:25,  3.43it/s] 65%|   | 529/820 [03:11<01:23,  3.47it/s] 65%|   | 530/820 [03:12<01:22,  3.50it/s] 65%|   | 531/820 [03:12<01:22,  3.51it/s] 65%|   | 532/820 [03:12<01:21,  3.52it/s] 65%|   | 533/820 [03:13<01:21,  3.54it/s] 65%|   | 534/820 [03:13<01:20,  3.55it/s] 65%|   | 535/820 [03:13<01:20,  3.55it/s] 65%|   | 536/820 [03:13<01:19,  3.56it/s] 65%|   | 537/820 [03:14<01:19,  3.56it/s] 66%|   | 538/820 [03:14<01:20,  3.49it/s] 66%|   | 539/820 [03:14<01:19,  3.51it/s] 66%|   | 540/820 [03:15<01:19,  3.53it/s] 66%|   | 541/820 [03:15<01:18,  3.54it/s] 66%|   | 542/820 [03:15<01:18,  3.55it/s] 66%|   | 543/820 [03:15<01:17,  3.55it/s] 66%|   | 544/820 [03:16<01:17,  3.56it/s] 66%|   | 545/820 [03:16<01:17,  3.56it/s] 67%|   | 546/820 [03:16<01:16,  3.57it/s] 67%|   | 547/820 [03:17<01:16,  3.57it/s] 67%|   | 548/820 [03:17<01:16,  3.57it/s] 67%|   | 549/820 [03:17<01:17,  3.50it/s] 67%|   | 550/820 [03:17<01:16,  3.53it/s] 67%|   | 551/820 [03:18<01:15,  3.55it/s] 67%|   | 552/820 [03:18<01:14,  3.57it/s] 67%|   | 553/820 [03:18<01:14,  3.59it/s] 68%|   | 554/820 [03:18<01:13,  3.60it/s] 68%|   | 555/820 [03:19<01:13,  3.61it/s] 68%|   | 556/820 [03:19<01:13,  3.61it/s] 68%|   | 557/820 [03:19<01:12,  3.62it/s] 68%|   | 558/820 [03:20<01:12,  3.62it/s] 68%|   | 559/820 [03:20<01:12,  3.62it/s] 68%|   | 560/820 [03:20<01:12,  3.57it/s] 68%|   | 561/820 [03:20<01:12,  3.59it/s] 69%|   | 562/820 [03:21<01:11,  3.60it/s] 69%|   | 563/820 [03:21<01:11,  3.60it/s] 69%|   | 564/820 [03:21<01:10,  3.61it/s] 69%|   | 565/820 [03:22<01:10,  3.61it/s] 69%|   | 566/820 [03:22<01:10,  3.62it/s] 69%|   | 567/820 [03:22<01:09,  3.62it/s] 69%|   | 568/820 [03:22<01:09,  3.62it/s] 69%|   | 569/820 [03:23<01:09,  3.62it/s] 70%|   | 570/820 [03:23<01:09,  3.62it/s] 70%|   | 571/820 [03:23<01:10,  3.54it/s] 70%|   | 572/820 [03:23<01:09,  3.56it/s] 70%|   | 573/820 [03:24<01:08,  3.58it/s] 70%|   | 574/820 [03:24<01:08,  3.59it/s] 70%|   | 575/820 [03:24<01:08,  3.60it/s] 70%|   | 576/820 [03:25<01:07,  3.60it/s] 70%|   | 577/820 [03:25<01:07,  3.61it/s] 70%|   | 578/820 [03:25<01:06,  3.62it/s] 71%|   | 579/820 [03:25<01:06,  3.62it/s] 71%|   | 580/820 [03:26<01:06,  3.61it/s] 71%|   | 581/820 [03:26<01:06,  3.61it/s] 71%|   | 582/820 [03:26<01:07,  3.53it/s] 71%|   | 583/820 [03:27<01:06,  3.55it/s] 71%|   | 584/820 [03:27<01:06,  3.57it/s] 71%|  | 585/820 [03:27<01:05,  3.59it/s] 71%|  | 586/820 [03:27<01:05,  3.60it/s] 72%|  | 587/820 [03:28<01:04,  3.61it/s] 72%|  | 588/820 [03:28<01:04,  3.61it/s] 72%|  | 589/820 [03:28<01:03,  3.62it/s] 72%|  | 590/820 [03:28<01:03,  3.62it/s] 72%|  | 591/820 [03:29<01:03,  3.62it/s] 72%|  | 592/820 [03:29<01:03,  3.62it/s] 72%|  | 593/820 [03:29<01:02,  3.62it/s] 72%|  | 594/820 [03:30<01:02,  3.62it/s] 73%|  | 595/820 [03:30<01:02,  3.62it/s] 73%|  | 596/820 [03:30<01:01,  3.62it/s] 73%|  | 597/820 [03:30<01:01,  3.62it/s] 73%|  | 598/820 [03:31<01:01,  3.62it/s] 73%|  | 599/820 [03:31<01:00,  3.62it/s] 73%|  | 600/820 [03:31<01:00,  3.63it/s] 73%|  | 601/820 [03:32<01:00,  3.62it/s] 73%|  | 602/820 [03:32<01:00,  3.62it/s] 74%|  | 603/820 [03:32<01:01,  3.52it/s] 74%|  | 604/820 [03:32<01:00,  3.55it/s] 74%|  | 605/820 [03:33<01:00,  3.57it/s] 74%|  | 606/820 [03:33<00:59,  3.59it/s] 74%|  | 607/820 [03:33<00:59,  3.60it/s] 74%|  | 608/820 [03:33<00:58,  3.61it/s] 74%|  | 609/820 [03:34<00:58,  3.61it/s] 74%|  | 610/820 [03:34<00:58,  3.62it/s] 75%|  | 611/820 [03:34<00:57,  3.62it/s] 75%|  | 612/820 [03:35<00:57,  3.62it/s] 75%|  | 613/820 [03:35<00:57,  3.62it/s] 75%|  | 614/820 [03:35<00:58,  3.52it/s] 75%|  | 615/820 [03:35<00:57,  3.54it/s] 75%|  | 616/820 [03:36<00:57,  3.57it/s] 75%|  | 617/820 [03:36<00:56,  3.58it/s] 75%|  | 618/820 [03:36<00:56,  3.60it/s] 75%|  | 619/820 [03:37<00:55,  3.61it/s] 76%|  | 620/820 [03:37<00:55,  3.61it/s] 76%|  | 621/820 [03:37<00:55,  3.62it/s] 76%|  | 622/820 [03:37<00:54,  3.62it/s] 76%|  | 623/820 [03:38<00:54,  3.62it/s] 76%|  | 624/820 [03:38<00:54,  3.63it/s] 76%|  | 625/820 [03:38<00:59,  3.26it/s] 76%|  | 626/820 [03:39<00:57,  3.36it/s] 76%|  | 627/820 [03:39<00:56,  3.44it/s] 77%|  | 628/820 [03:39<01:00,  3.20it/s] 77%|  | 629/820 [03:39<00:58,  3.29it/s] 77%|  | 630/820 [03:40<00:56,  3.38it/s] 77%|  | 631/820 [03:40<00:54,  3.45it/s] 77%|  | 632/820 [03:40<00:53,  3.51it/s] 77%|  | 633/820 [03:41<00:52,  3.54it/s] 77%|  | 634/820 [03:41<00:52,  3.57it/s] 77%|  | 635/820 [03:41<00:51,  3.59it/s] 78%|  | 636/820 [03:41<00:51,  3.54it/s] 78%|  | 637/820 [03:42<00:51,  3.57it/s] 78%|  | 638/820 [03:42<00:50,  3.58it/s] 78%|  | 639/820 [03:42<00:50,  3.59it/s] 78%|  | 640/820 [03:43<00:49,  3.61it/s] 78%|  | 641/820 [03:43<00:49,  3.61it/s] 78%|  | 642/820 [03:43<00:49,  3.62it/s] 78%|  | 643/820 [03:43<00:48,  3.62it/s] 79%|  | 644/820 [03:44<00:48,  3.62it/s] 79%|  | 645/820 [03:44<00:48,  3.62it/s] 79%|  | 646/820 [03:44<00:47,  3.63it/s] 79%|  | 647/820 [03:44<00:49,  3.48it/s] 79%|  | 648/820 [03:45<00:48,  3.52it/s] 79%|  | 649/820 [03:45<00:48,  3.56it/s] 79%|  | 650/820 [03:45<00:47,  3.57it/s] 79%|  | 651/820 [03:46<00:47,  3.59it/s] 80%|  | 652/820 [03:46<00:46,  3.60it/s] 80%|  | 653/820 [03:46<00:46,  3.61it/s] 80%|  | 654/820 [03:46<00:45,  3.62it/s] 80%|  | 655/820 [03:47<00:45,  3.62it/s] 80%|  | 656/820 [03:47<00:45,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 06:53:48,528 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:53:48,529 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:53:48,529 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8822, 'eval_samples_per_second': 353.061, 'eval_steps_per_second': 44.221, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.40it/s][A
  3%|         | 12/437 [00:00<00:09, 45.21it/s][A
  4%|         | 17/437 [00:00<00:09, 45.34it/s][A
  5%|         | 22/437 [00:00<00:09, 45.33it/s][A
  6%|         | 27/437 [00:00<00:09, 45.25it/s][A
  7%|         | 32/437 [00:00<00:08, 45.16it/s][A
  8%|         | 37/437 [00:00<00:08, 44.95it/s][A
 10%|         | 42/437 [00:00<00:08, 44.91it/s][A
 11%|         | 47/437 [00:01<00:08, 44.98it/s][A
 12%|        | 52/437 [00:01<00:08, 44.75it/s][A
 13%|        | 57/437 [00:01<00:08, 44.83it/s][A
 14%|        | 62/437 [00:01<00:08, 44.87it/s][A
 15%|        | 67/437 [00:01<00:08, 45.11it/s][A
 16%|        | 72/437 [00:01<00:09, 38.19it/s][A
 18%|        | 77/437 [00:01<00:08, 40.20it/s][A
 19%|        | 82/437 [00:01<00:08, 41.65it/s][A
 20%|        | 87/437 [00:01<00:08, 42.67it/s][A
 21%|        | 92/437 [00:02<00:07, 43.53it/s][A
 22%|       | 97/437 [00:02<00:07, 43.98it/s][A
 23%|       | 102/437 [00:02<00:07, 44.49it/s][A
 24%|       | 107/437 [00:02<00:07, 44.74it/s][A
 26%|       | 112/437 [00:02<00:07, 44.53it/s][A
 27%|       | 117/437 [00:02<00:07, 44.50it/s][A
 28%|       | 122/437 [00:02<00:07, 44.60it/s][A
 29%|       | 127/437 [00:02<00:06, 44.89it/s][A
 30%|       | 132/437 [00:02<00:06, 45.08it/s][A
 31%|      | 137/437 [00:03<00:06, 45.22it/s][A
 32%|      | 142/437 [00:03<00:06, 45.22it/s][A
 34%|      | 147/437 [00:03<00:06, 43.03it/s][A
 35%|      | 152/437 [00:03<00:06, 43.67it/s][A
 36%|      | 157/437 [00:03<00:06, 43.99it/s][A
 37%|      | 162/437 [00:03<00:06, 44.21it/s][A
 38%|      | 167/437 [00:03<00:06, 44.47it/s][A
 39%|      | 172/437 [00:03<00:05, 44.71it/s][A
 41%|      | 177/437 [00:03<00:05, 44.86it/s][A
 42%|     | 182/437 [00:04<00:05, 45.05it/s][A
 43%|     | 187/437 [00:04<00:05, 44.91it/s][A
 44%|     | 192/437 [00:04<00:05, 44.98it/s][A
 45%|     | 197/437 [00:04<00:05, 45.09it/s][A
 46%|     | 202/437 [00:04<00:05, 45.04it/s][A
 47%|     | 207/437 [00:04<00:05, 44.89it/s][A
 49%|     | 212/437 [00:04<00:05, 44.93it/s][A
 50%|     | 217/437 [00:04<00:04, 45.03it/s][A
 51%|     | 222/437 [00:04<00:04, 45.14it/s][A
 52%|    | 227/437 [00:05<00:04, 45.14it/s][A
 53%|    | 232/437 [00:05<00:04, 45.16it/s][A
 54%|    | 237/437 [00:05<00:04, 44.89it/s][A
 55%|    | 242/437 [00:05<00:04, 45.09it/s][A
 57%|    | 247/437 [00:05<00:04, 45.05it/s][A
 58%|    | 252/437 [00:05<00:04, 44.98it/s][A
 59%|    | 257/437 [00:05<00:04, 44.95it/s][A
 60%|    | 262/437 [00:05<00:03, 44.87it/s][A
 61%|    | 267/437 [00:05<00:03, 44.92it/s][A
 62%|   | 272/437 [00:06<00:03, 44.79it/s][A
 63%|   | 277/437 [00:06<00:03, 44.95it/s][A
 65%|   | 282/437 [00:06<00:03, 44.41it/s][A
 66%|   | 287/437 [00:06<00:03, 44.50it/s][A
 67%|   | 292/437 [00:06<00:03, 44.54it/s][A
 68%|   | 297/437 [00:06<00:03, 44.65it/s][A
 69%|   | 302/437 [00:06<00:03, 44.77it/s][A
 70%|   | 307/437 [00:06<00:02, 44.87it/s][A
 71%|  | 312/437 [00:07<00:02, 44.99it/s][A
 73%|  | 317/437 [00:07<00:02, 44.97it/s][A
 74%|  | 322/437 [00:07<00:02, 44.82it/s][A
 75%|  | 327/437 [00:07<00:02, 45.00it/s][A
 76%|  | 332/437 [00:07<00:02, 44.98it/s][A
 77%|  | 337/437 [00:07<00:02, 45.08it/s][A
 78%|  | 342/437 [00:07<00:02, 45.13it/s][A
 79%|  | 347/437 [00:07<00:01, 45.16it/s][A
 81%|  | 352/437 [00:07<00:01, 45.05it/s][A
 82%| | 357/437 [00:08<00:01, 45.06it/s][A
 83%| | 362/437 [00:08<00:01, 45.11it/s][A
 84%| | 367/437 [00:08<00:01, 45.06it/s][A
 85%| | 372/437 [00:08<00:01, 45.05it/s][A
 86%| | 377/437 [00:08<00:01, 44.91it/s][A
 87%| | 382/437 [00:08<00:01, 44.93it/s][A
 89%| | 387/437 [00:08<00:01, 45.04it/s][A
 90%| | 392/437 [00:08<00:00, 45.13it/s][A
 91%| | 397/437 [00:08<00:00, 45.08it/s][A
 92%|| 402/437 [00:09<00:00, 45.04it/s][A
 93%|| 407/437 [00:09<00:00, 44.98it/s][A
 94%|| 412/437 [00:09<00:00, 45.07it/s][A
 95%|| 417/437 [00:09<00:00, 44.73it/s][A
 97%|| 422/437 [00:09<00:00, 44.73it/s][A
 98%|| 427/437 [00:09<00:00, 44.85it/s][A
 99%|| 432/437 [00:09<00:00, 44.93it/s][A
100%|| 437/437 [00:09<00:00, 45.08it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.08it/s][A 80%|  | 656/820 [03:57<00:45,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:53:58,421 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 06:53:58,509 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:01,051 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:01,195 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:01,256 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:01<11:43,  4.32s/it] 80%|  | 658/820 [04:01<08:23,  3.11s/it] 80%|  | 659/820 [04:01<06:03,  2.26s/it] 80%|  | 660/820 [04:02<04:26,  1.67s/it] 81%|  | 661/820 [04:02<03:18,  1.25s/it] 81%|  | 662/820 [04:02<02:31,  1.04it/s] 81%|  | 663/820 [04:02<01:58,  1.32it/s] 81%|  | 664/820 [04:03<01:37,  1.60it/s] 81%|  | 665/820 [04:03<01:20,  1.91it/s] 81%|  | 666/820 [04:03<01:09,  2.22it/s] 81%| | 667/820 [04:04<01:01,  2.51it/s] 81%| | 668/820 [04:04<00:55,  2.75it/s] 82%| | 669/820 [04:04<00:51,  2.95it/s] 82%| | 670/820 [04:04<00:48,  3.11it/s] 82%| | 671/820 [04:05<00:46,  3.24it/s] 82%| | 672/820 [04:05<00:44,  3.33it/s] 82%| | 673/820 [04:05<00:43,  3.40it/s] 82%| | 674/820 [04:06<00:42,  3.46it/s] 82%| | 675/820 [04:06<00:42,  3.42it/s] 82%| | 676/820 [04:06<00:41,  3.48it/s] 83%| | 677/820 [04:06<00:40,  3.52it/s] 83%| | 678/820 [04:07<00:39,  3.56it/s] 83%| | 679/820 [04:07<00:39,  3.58it/s] 83%| | 680/820 [04:07<00:38,  3.59it/s] 83%| | 681/820 [04:07<00:38,  3.60it/s] 83%| | 682/820 [04:08<00:38,  3.60it/s] 83%| | 683/820 [04:08<00:37,  3.61it/s] 83%| | 684/820 [04:08<00:37,  3.62it/s] 84%| | 685/820 [04:09<00:37,  3.62it/s] 84%| | 686/820 [04:09<00:37,  3.57it/s] 84%| | 687/820 [04:09<00:37,  3.53it/s] 84%| | 688/820 [04:09<00:37,  3.55it/s] 84%| | 689/820 [04:10<00:36,  3.57it/s] 84%| | 690/820 [04:10<00:36,  3.53it/s] 84%| | 691/820 [04:10<00:36,  3.56it/s] 84%| | 692/820 [04:11<00:35,  3.58it/s] 85%| | 693/820 [04:11<00:35,  3.59it/s] 85%| | 694/820 [04:11<00:34,  3.60it/s] 85%| | 695/820 [04:11<00:34,  3.61it/s] 85%| | 696/820 [04:12<00:34,  3.61it/s] 85%| | 697/820 [04:12<00:34,  3.53it/s] 85%| | 698/820 [04:12<00:34,  3.56it/s] 85%| | 699/820 [04:13<00:33,  3.58it/s] 85%| | 700/820 [04:13<00:33,  3.60it/s] 85%| | 701/820 [04:13<00:33,  3.61it/s] 86%| | 702/820 [04:13<00:32,  3.61it/s] 86%| | 703/820 [04:14<00:32,  3.62it/s] 86%| | 704/820 [04:14<00:32,  3.61it/s] 86%| | 705/820 [04:14<00:31,  3.62it/s] 86%| | 706/820 [04:14<00:31,  3.62it/s] 86%| | 707/820 [04:15<00:31,  3.62it/s] 86%| | 708/820 [04:15<00:31,  3.53it/s] 86%| | 709/820 [04:15<00:31,  3.56it/s] 87%| | 710/820 [04:16<00:30,  3.58it/s] 87%| | 711/820 [04:16<00:30,  3.60it/s] 87%| | 712/820 [04:16<00:29,  3.61it/s] 87%| | 713/820 [04:16<00:29,  3.61it/s] 87%| | 714/820 [04:17<00:29,  3.62it/s] 87%| | 715/820 [04:17<00:29,  3.62it/s] 87%| | 716/820 [04:17<00:28,  3.62it/s] 87%| | 717/820 [04:18<00:28,  3.62it/s] 88%| | 718/820 [04:18<00:28,  3.62it/s] 88%| | 719/820 [04:18<00:28,  3.53it/s] 88%| | 720/820 [04:18<00:28,  3.56it/s] 88%| | 721/820 [04:19<00:27,  3.58it/s] 88%| | 722/820 [04:19<00:27,  3.59it/s] 88%| | 723/820 [04:19<00:26,  3.60it/s] 88%| | 724/820 [04:19<00:26,  3.61it/s] 88%| | 725/820 [04:20<00:26,  3.61it/s] 89%| | 726/820 [04:20<00:25,  3.62it/s] 89%| | 727/820 [04:20<00:25,  3.62it/s] 89%| | 728/820 [04:21<00:25,  3.62it/s] 89%| | 729/820 [04:21<00:25,  3.62it/s] 89%| | 730/820 [04:21<00:25,  3.56it/s] 89%| | 731/820 [04:21<00:24,  3.58it/s] 89%| | 732/820 [04:22<00:24,  3.60it/s] 89%| | 733/820 [04:22<00:24,  3.60it/s] 90%| | 734/820 [04:22<00:23,  3.61it/s] 90%| | 735/820 [04:23<00:23,  3.62it/s] 90%| | 736/820 [04:23<00:23,  3.62it/s] 90%| | 737/820 [04:23<00:22,  3.62it/s] 90%| | 738/820 [04:23<00:22,  3.62it/s] 90%| | 739/820 [04:24<00:22,  3.63it/s] 90%| | 740/820 [04:24<00:22,  3.63it/s] 90%| | 741/820 [04:24<00:22,  3.50it/s] 90%| | 742/820 [04:24<00:22,  3.54it/s] 91%| | 743/820 [04:25<00:21,  3.56it/s] 91%| | 744/820 [04:25<00:21,  3.58it/s] 91%| | 745/820 [04:25<00:20,  3.59it/s] 91%| | 746/820 [04:26<00:20,  3.60it/s] 91%| | 747/820 [04:26<00:20,  3.61it/s] 91%| | 748/820 [04:26<00:19,  3.62it/s] 91%|| 749/820 [04:26<00:19,  3.62it/s] 91%|| 750/820 [04:27<00:19,  3.62it/s] 92%|| 751/820 [04:27<00:19,  3.62it/s] 92%|| 752/820 [04:27<00:19,  3.52it/s] 92%|| 753/820 [04:28<00:18,  3.55it/s] 92%|| 754/820 [04:28<00:18,  3.57it/s] 92%|| 755/820 [04:28<00:18,  3.59it/s] 92%|| 756/820 [04:28<00:17,  3.60it/s] 92%|| 757/820 [04:29<00:17,  3.61it/s] 92%|| 758/820 [04:29<00:17,  3.61it/s] 93%|| 759/820 [04:29<00:16,  3.62it/s] 93%|| 760/820 [04:29<00:16,  3.62it/s] 93%|| 761/820 [04:30<00:16,  3.62it/s] 93%|| 762/820 [04:30<00:16,  3.62it/s] 93%|| 763/820 [04:30<00:15,  3.58it/s] 93%|| 764/820 [04:31<00:15,  3.59it/s] 93%|| 765/820 [04:31<00:15,  3.60it/s] 93%|| 766/820 [04:31<00:14,  3.61it/s] 94%|| 767/820 [04:31<00:14,  3.61it/s] 94%|| 768/820 [04:32<00:14,  3.61it/s] 94%|| 769/820 [04:32<00:14,  3.62it/s] 94%|| 770/820 [04:32<00:13,  3.62it/s] 94%|| 771/820 [04:33<00:13,  3.62it/s] 94%|| 772/820 [04:33<00:13,  3.62it/s] 94%|| 773/820 [04:33<00:12,  3.62it/s] 94%|| 774/820 [04:33<00:13,  3.51it/s] 95%|| 775/820 [04:34<00:12,  3.55it/s] 95%|| 776/820 [04:34<00:12,  3.57it/s] 95%|| 777/820 [04:34<00:11,  3.58it/s] 95%|| 778/820 [04:34<00:11,  3.60it/s] 95%|| 779/820 [04:35<00:11,  3.61it/s] 95%|| 780/820 [04:35<00:11,  3.61it/s] 95%|| 781/820 [04:35<00:10,  3.61it/s] 95%|| 782/820 [04:36<00:10,  3.61it/s] 95%|| 783/820 [04:36<00:10,  3.62it/s] 96%|| 784/820 [04:36<00:09,  3.62it/s] 96%|| 785/820 [04:36<00:09,  3.55it/s] 96%|| 786/820 [04:37<00:09,  3.57it/s] 96%|| 787/820 [04:37<00:09,  3.59it/s] 96%|| 788/820 [04:37<00:08,  3.59it/s] 96%|| 789/820 [04:38<00:08,  3.60it/s] 96%|| 790/820 [04:38<00:08,  3.61it/s] 96%|| 791/820 [04:38<00:08,  3.61it/s] 97%|| 792/820 [04:38<00:07,  3.61it/s] 97%|| 793/820 [04:39<00:07,  3.62it/s] 97%|| 794/820 [04:39<00:08,  3.20it/s] 97%|| 795/820 [04:39<00:08,  2.90it/s] 97%|| 796/820 [04:40<00:08,  2.98it/s] 97%|| 797/820 [04:40<00:07,  3.15it/s] 97%|| 798/820 [04:40<00:06,  3.27it/s] 97%|| 799/820 [04:41<00:06,  3.37it/s] 98%|| 800/820 [04:41<00:05,  3.44it/s] 98%|| 801/820 [04:41<00:05,  3.50it/s] 98%|| 802/820 [04:41<00:05,  3.53it/s] 98%|| 803/820 [04:42<00:04,  3.56it/s] 98%|| 804/820 [04:42<00:04,  3.58it/s] 98%|| 805/820 [04:42<00:04,  3.59it/s] 98%|| 806/820 [04:43<00:03,  3.60it/s] 98%|| 807/820 [04:43<00:03,  3.53it/s] 99%|| 808/820 [04:43<00:03,  3.56it/s] 99%|| 809/820 [04:43<00:03,  3.58it/s] 99%|| 810/820 [04:44<00:02,  3.59it/s] 99%|| 811/820 [04:44<00:02,  3.60it/s] 99%|| 812/820 [04:44<00:02,  3.61it/s] 99%|| 813/820 [04:44<00:01,  3.61it/s] 99%|| 814/820 [04:45<00:01,  3.62it/s] 99%|| 815/820 [04:45<00:01,  3.62it/s]100%|| 816/820 [04:45<00:01,  3.62it/s]100%|| 817/820 [04:46<00:00,  3.62it/s]100%|| 818/820 [04:46<00:00,  3.46it/s]100%|| 819/820 [04:46<00:00,  3.51it/s]100%|| 820/820 [04:46<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 06:54:47,961 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:47,961 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:54:47,961 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8501, 'eval_samples_per_second': 354.208, 'eval_steps_per_second': 44.365, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.89it/s][A
  3%|         | 12/437 [00:00<00:08, 49.59it/s][A
  4%|         | 18/437 [00:00<00:08, 47.56it/s][A
  5%|         | 23/437 [00:00<00:08, 46.51it/s][A
  6%|         | 28/437 [00:00<00:08, 45.97it/s][A
  8%|         | 33/437 [00:00<00:08, 45.50it/s][A
  9%|         | 38/437 [00:00<00:08, 45.12it/s][A
 10%|         | 43/437 [00:00<00:08, 45.05it/s][A
 11%|         | 48/437 [00:01<00:08, 45.11it/s][A
 12%|        | 53/437 [00:01<00:08, 45.23it/s][A
 13%|        | 58/437 [00:01<00:08, 45.42it/s][A
 14%|        | 63/437 [00:01<00:08, 45.45it/s][A
 16%|        | 68/437 [00:01<00:08, 45.21it/s][A
 17%|        | 73/437 [00:01<00:08, 45.06it/s][A
 18%|        | 78/437 [00:01<00:07, 44.95it/s][A
 19%|        | 83/437 [00:01<00:07, 44.74it/s][A
 20%|        | 88/437 [00:01<00:07, 44.81it/s][A
 21%|       | 93/437 [00:02<00:07, 44.87it/s][A
 22%|       | 98/437 [00:02<00:08, 42.22it/s][A
 24%|       | 103/437 [00:02<00:08, 38.02it/s][A
 25%|       | 108/437 [00:02<00:08, 40.06it/s][A
 26%|       | 113/437 [00:02<00:07, 41.55it/s][A
 27%|       | 118/437 [00:02<00:07, 42.63it/s][A
 28%|       | 123/437 [00:02<00:07, 43.40it/s][A
 29%|       | 128/437 [00:02<00:07, 44.07it/s][A
 30%|       | 133/437 [00:02<00:06, 44.45it/s][A
 32%|      | 138/437 [00:03<00:06, 44.67it/s][A
 33%|      | 143/437 [00:03<00:06, 44.35it/s][A
 34%|      | 148/437 [00:03<00:06, 44.44it/s][A
 35%|      | 153/437 [00:03<00:06, 44.73it/s][A
 36%|      | 158/437 [00:03<00:06, 44.87it/s][A
 37%|      | 163/437 [00:03<00:06, 45.07it/s][A
 38%|      | 168/437 [00:03<00:05, 45.03it/s][A
 40%|      | 173/437 [00:03<00:05, 45.05it/s][A
 41%|      | 178/437 [00:03<00:05, 45.13it/s][A
 42%|     | 183/437 [00:04<00:05, 45.08it/s][A
 43%|     | 188/437 [00:04<00:05, 44.74it/s][A
 44%|     | 193/437 [00:04<00:05, 44.64it/s][A
 45%|     | 198/437 [00:04<00:05, 44.94it/s][A
 46%|     | 203/437 [00:04<00:05, 44.87it/s][A
 48%|     | 208/437 [00:04<00:05, 45.13it/s][A
 49%|     | 213/437 [00:04<00:04, 45.10it/s][A
 50%|     | 218/437 [00:04<00:04, 45.28it/s][A
 51%|     | 223/437 [00:04<00:04, 45.19it/s][A
 52%|    | 228/437 [00:05<00:04, 44.98it/s][A
 53%|    | 233/437 [00:05<00:04, 44.72it/s][A
 54%|    | 238/437 [00:05<00:04, 41.77it/s][A
 56%|    | 243/437 [00:05<00:04, 42.82it/s][A
 57%|    | 248/437 [00:05<00:04, 43.54it/s][A
 58%|    | 253/437 [00:05<00:04, 44.04it/s][A
 59%|    | 258/437 [00:05<00:04, 44.41it/s][A
 60%|    | 263/437 [00:05<00:03, 44.80it/s][A
 61%|   | 268/437 [00:06<00:03, 44.98it/s][A
 62%|   | 273/437 [00:06<00:03, 45.06it/s][A
 64%|   | 278/437 [00:06<00:03, 44.61it/s][A
 65%|   | 283/437 [00:06<00:03, 44.51it/s][A
 66%|   | 288/437 [00:06<00:03, 44.72it/s][A
 67%|   | 293/437 [00:06<00:03, 44.78it/s][A
 68%|   | 298/437 [00:06<00:03, 45.05it/s][A
 69%|   | 303/437 [00:06<00:02, 45.11it/s][A
 70%|   | 308/437 [00:06<00:02, 45.21it/s][A
 72%|  | 313/437 [00:07<00:02, 45.10it/s][A
 73%|  | 318/437 [00:07<00:02, 45.11it/s][A
 74%|  | 323/437 [00:07<00:02, 44.94it/s][A
 75%|  | 328/437 [00:07<00:02, 44.75it/s][A
 76%|  | 333/437 [00:07<00:02, 44.86it/s][A
 77%|  | 338/437 [00:07<00:02, 44.93it/s][A
 78%|  | 343/437 [00:07<00:02, 45.05it/s][A
 80%|  | 348/437 [00:07<00:01, 45.23it/s][A
 81%|  | 353/437 [00:07<00:01, 45.26it/s][A
 82%| | 358/437 [00:08<00:01, 45.28it/s][A
 83%| | 363/437 [00:08<00:01, 45.03it/s][A
 84%| | 368/437 [00:08<00:01, 44.94it/s][A
 85%| | 373/437 [00:08<00:01, 43.67it/s][A
 86%| | 378/437 [00:08<00:01, 44.05it/s][A
 88%| | 383/437 [00:08<00:01, 44.40it/s][A
 89%| | 388/437 [00:08<00:01, 44.62it/s][A
 90%| | 393/437 [00:08<00:00, 44.89it/s][A
 91%| | 398/437 [00:08<00:00, 45.05it/s][A
 92%|| 403/437 [00:09<00:00, 45.07it/s][A
 93%|| 408/437 [00:09<00:00, 45.02it/s][A
 95%|| 413/437 [00:09<00:00, 44.75it/s][A
 96%|| 418/437 [00:09<00:00, 44.78it/s][A
 97%|| 423/437 [00:09<00:00, 44.76it/s][A
 98%|| 428/437 [00:09<00:00, 44.92it/s][A
 99%|| 433/437 [00:09<00:00, 45.01it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.01it/s][A100%|| 820/820 [04:56<00:00,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:54:57,848 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 06:54:57,949 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:55:00,454 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:55:00,590 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:55:00,650 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:55:01,251 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:55:01,251 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164 (score: 1.0192089080810547).
                                                 100%|| 820/820 [05:08<00:00,  3.55it/s]100%|| 820/820 [05:08<00:00,  2.66it/s]
[INFO|trainer.py:1894] 2023-08-28 06:55:09,312 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 06:55:09,414 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:55:11,869 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:55:12,015 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:55:12,089 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:55:12,485 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   train_runtime            = 0:05:08.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   train_samples            =      10520
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   train_samples_per_second =    170.634
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:12,502 >>   train_steps_per_second   =       2.66
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.798, 'eval_samples_per_second': 356.092, 'eval_steps_per_second': 44.601, 'epoch': 5.0}
{'train_runtime': 308.263, 'train_samples_per_second': 170.634, 'train_steps_per_second': 2.66, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:55:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:55:12,707 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:55:12,707 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 06:55:12,707 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 55.53it/s]  3%|         | 12/437 [00:00<00:08, 49.47it/s]  4%|         | 17/437 [00:00<00:08, 47.86it/s]  5%|         | 22/437 [00:00<00:08, 46.94it/s]  6%|         | 27/437 [00:00<00:08, 46.45it/s]  7%|         | 32/437 [00:00<00:08, 46.10it/s]  8%|         | 37/437 [00:00<00:08, 46.06it/s] 10%|         | 42/437 [00:00<00:08, 45.62it/s] 11%|         | 47/437 [00:01<00:08, 45.30it/s] 12%|        | 52/437 [00:01<00:08, 45.24it/s] 13%|        | 57/437 [00:01<00:08, 45.28it/s] 14%|        | 62/437 [00:01<00:08, 45.28it/s] 15%|        | 67/437 [00:01<00:08, 45.36it/s] 16%|        | 72/437 [00:01<00:08, 45.51it/s] 18%|        | 77/437 [00:01<00:07, 45.56it/s] 19%|        | 82/437 [00:01<00:07, 45.49it/s] 20%|        | 87/437 [00:01<00:07, 45.32it/s] 21%|        | 92/437 [00:02<00:07, 45.12it/s] 22%|       | 97/437 [00:02<00:07, 45.04it/s] 23%|       | 102/437 [00:02<00:07, 45.17it/s] 24%|       | 107/437 [00:02<00:07, 45.30it/s] 26%|       | 112/437 [00:02<00:07, 45.25it/s] 27%|       | 117/437 [00:02<00:07, 45.25it/s] 28%|       | 122/437 [00:02<00:06, 45.37it/s] 29%|       | 127/437 [00:02<00:06, 45.47it/s] 30%|       | 132/437 [00:02<00:06, 44.35it/s] 31%|      | 137/437 [00:03<00:06, 44.59it/s] 32%|      | 142/437 [00:03<00:06, 44.48it/s] 34%|      | 147/437 [00:03<00:06, 44.83it/s] 35%|      | 152/437 [00:03<00:06, 44.90it/s] 36%|      | 157/437 [00:03<00:06, 45.18it/s] 37%|      | 162/437 [00:03<00:06, 45.26it/s] 38%|      | 167/437 [00:03<00:05, 45.31it/s] 39%|      | 172/437 [00:03<00:05, 45.19it/s] 41%|      | 177/437 [00:03<00:05, 45.20it/s] 42%|     | 182/437 [00:04<00:05, 45.17it/s] 43%|     | 187/437 [00:04<00:05, 45.07it/s] 44%|     | 192/437 [00:04<00:05, 45.17it/s] 45%|     | 197/437 [00:04<00:05, 45.04it/s] 46%|     | 202/437 [00:04<00:05, 45.14it/s] 47%|     | 207/437 [00:04<00:05, 45.20it/s] 49%|     | 212/437 [00:04<00:04, 45.37it/s] 50%|     | 217/437 [00:04<00:04, 45.27it/s] 51%|     | 222/437 [00:04<00:04, 45.19it/s] 52%|    | 227/437 [00:04<00:04, 45.12it/s] 53%|    | 232/437 [00:05<00:04, 45.02it/s] 54%|    | 237/437 [00:05<00:04, 45.14it/s] 55%|    | 242/437 [00:05<00:04, 45.22it/s] 57%|    | 247/437 [00:05<00:04, 45.22it/s] 58%|    | 252/437 [00:05<00:04, 45.30it/s] 59%|    | 257/437 [00:05<00:03, 45.31it/s] 60%|    | 262/437 [00:05<00:03, 45.29it/s] 61%|    | 267/437 [00:05<00:03, 45.29it/s] 62%|   | 272/437 [00:05<00:03, 44.25it/s] 63%|   | 277/437 [00:06<00:03, 44.66it/s] 65%|   | 282/437 [00:06<00:03, 44.81it/s] 66%|   | 287/437 [00:06<00:03, 44.98it/s] 67%|   | 292/437 [00:06<00:03, 45.07it/s] 68%|   | 297/437 [00:06<00:03, 45.21it/s] 69%|   | 302/437 [00:06<00:02, 45.25it/s] 70%|   | 307/437 [00:06<00:02, 45.15it/s] 71%|  | 312/437 [00:06<00:02, 44.98it/s] 73%|  | 317/437 [00:06<00:02, 44.83it/s] 74%|  | 322/437 [00:07<00:02, 44.94it/s] 75%|  | 327/437 [00:07<00:02, 44.95it/s] 76%|  | 332/437 [00:07<00:02, 45.31it/s] 77%|  | 337/437 [00:07<00:02, 45.34it/s] 78%|  | 342/437 [00:07<00:02, 45.42it/s] 79%|  | 347/437 [00:07<00:01, 45.24it/s] 81%|  | 352/437 [00:07<00:01, 45.24it/s] 82%| | 357/437 [00:07<00:01, 45.16it/s] 83%| | 362/437 [00:07<00:01, 45.05it/s] 84%| | 367/437 [00:08<00:01, 45.03it/s] 85%| | 372/437 [00:08<00:01, 44.87it/s] 86%| | 377/437 [00:08<00:01, 45.16it/s] 87%| | 382/437 [00:08<00:01, 45.31it/s] 89%| | 387/437 [00:08<00:01, 45.45it/s] 90%| | 392/437 [00:08<00:00, 45.27it/s] 91%| | 397/437 [00:08<00:00, 45.22it/s] 92%|| 402/437 [00:08<00:00, 45.12it/s] 93%|| 407/437 [00:08<00:00, 45.14it/s] 94%|| 412/437 [00:09<00:00, 44.40it/s] 95%|| 417/437 [00:09<00:00, 44.54it/s] 97%|| 422/437 [00:09<00:00, 44.80it/s] 98%|| 427/437 [00:09<00:00, 45.06it/s] 99%|| 432/437 [00:09<00:00, 45.21it/s]100%|| 437/437 [00:09<00:00, 45.37it/s]100%|| 437/437 [00:09<00:00, 45.26it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:55:22,379 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   eval_loss               =     1.0192
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   eval_runtime            = 0:00:09.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   eval_samples_per_second =    360.762
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   eval_steps_per_second   =     45.186
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:55:22,379 >>   perplexity              =      2.771
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:30,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:30,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:30,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:30,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:30,492 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:55:31,088 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:55:31,089 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:55:31,661 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:55:32,709 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:55:32,709 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:35,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:35,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:35,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:35,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:55:35,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:55:36,253 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:55:36,271 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:55:36,867 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:55:37,034 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:55:37,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.71it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:07,  1.75it/s]Extractor Predicting: 13it [00:07,  1.74it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.70it/s]Extractor Predicting: 16it [00:09,  1.71it/s]Extractor Predicting: 17it [00:09,  1.72it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.70it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:14,  1.65it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:16,  1.58it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:17,  1.64it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.74it/s]Extractor Predicting: 34it [00:20,  1.75it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:21,  1.66it/s]Extractor Predicting: 38it [00:22,  1.66it/s]Extractor Predicting: 39it [00:23,  1.66it/s]Extractor Predicting: 40it [00:23,  1.67it/s]Extractor Predicting: 41it [00:24,  1.65it/s]Extractor Predicting: 42it [00:24,  1.66it/s]Extractor Predicting: 43it [00:25,  1.67it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:26,  1.66it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:28,  1.63it/s]Extractor Predicting: 49it [00:29,  1.65it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:31,  1.65it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:32,  1.63it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:35,  1.61it/s]Extractor Predicting: 61it [00:36,  1.62it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:37,  1.67it/s]Extractor Predicting: 64it [00:38,  1.67it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:39,  1.70it/s]Extractor Predicting: 67it [00:40,  1.73it/s]Extractor Predicting: 68it [00:40,  1.72it/s]Extractor Predicting: 69it [00:41,  1.75it/s]Extractor Predicting: 70it [00:41,  1.78it/s]Extractor Predicting: 71it [00:42,  1.77it/s]Extractor Predicting: 72it [00:42,  1.76it/s]Extractor Predicting: 73it [00:43,  1.76it/s]Extractor Predicting: 74it [00:44,  1.76it/s]Extractor Predicting: 75it [00:44,  1.71it/s]Extractor Predicting: 76it [00:45,  1.69it/s]Extractor Predicting: 77it [00:45,  1.72it/s]Extractor Predicting: 78it [00:46,  1.76it/s]Extractor Predicting: 79it [00:46,  1.76it/s]Extractor Predicting: 80it [00:47,  1.76it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:48,  1.78it/s]Extractor Predicting: 83it [00:49,  1.77it/s]Extractor Predicting: 84it [00:49,  1.74it/s]Extractor Predicting: 85it [00:50,  1.77it/s]Extractor Predicting: 86it [00:50,  1.77it/s]Extractor Predicting: 87it [00:51,  1.78it/s]Extractor Predicting: 88it [00:52,  1.78it/s]Extractor Predicting: 89it [00:52,  1.76it/s]Extractor Predicting: 90it [00:53,  1.77it/s]Extractor Predicting: 91it [00:53,  1.75it/s]Extractor Predicting: 92it [00:54,  1.69it/s]Extractor Predicting: 93it [00:54,  1.71it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:56,  1.70it/s]Extractor Predicting: 96it [00:56,  1.70it/s]Extractor Predicting: 97it [00:57,  1.71it/s]Extractor Predicting: 98it [00:57,  1.75it/s]Extractor Predicting: 99it [00:58,  1.72it/s]Extractor Predicting: 100it [00:59,  1.66it/s]Extractor Predicting: 101it [00:59,  1.69it/s]Extractor Predicting: 102it [01:00,  1.67it/s]Extractor Predicting: 103it [01:00,  1.65it/s]Extractor Predicting: 104it [01:01,  1.68it/s]Extractor Predicting: 105it [01:02,  1.52it/s]Extractor Predicting: 106it [01:02,  1.56it/s]Extractor Predicting: 107it [01:03,  1.62it/s]Extractor Predicting: 108it [01:03,  1.65it/s]Extractor Predicting: 109it [01:04,  1.60it/s]Extractor Predicting: 110it [01:05,  1.59it/s]Extractor Predicting: 111it [01:05,  1.61it/s]Extractor Predicting: 112it [01:06,  1.64it/s]Extractor Predicting: 113it [01:07,  1.65it/s]Extractor Predicting: 114it [01:07,  1.67it/s]Extractor Predicting: 115it [01:08,  1.65it/s]Extractor Predicting: 116it [01:08,  1.69it/s]Extractor Predicting: 117it [01:09,  1.70it/s]Extractor Predicting: 118it [01:10,  1.70it/s]Extractor Predicting: 119it [01:10,  1.74it/s]Extractor Predicting: 120it [01:11,  1.79it/s]Extractor Predicting: 121it [01:11,  1.77it/s]Extractor Predicting: 122it [01:12,  1.77it/s]Extractor Predicting: 123it [01:12,  1.75it/s]Extractor Predicting: 124it [01:13,  1.75it/s]Extractor Predicting: 125it [01:13,  1.77it/s]Extractor Predicting: 126it [01:14,  1.77it/s]Extractor Predicting: 127it [01:15,  1.72it/s]Extractor Predicting: 128it [01:15,  1.70it/s]Extractor Predicting: 129it [01:16,  1.75it/s]Extractor Predicting: 130it [01:16,  1.71it/s]Extractor Predicting: 131it [01:17,  1.72it/s]Extractor Predicting: 132it [01:18,  1.74it/s]Extractor Predicting: 133it [01:18,  1.67it/s]Extractor Predicting: 134it [01:19,  1.67it/s]Extractor Predicting: 135it [01:19,  1.70it/s]Extractor Predicting: 136it [01:20,  1.72it/s]Extractor Predicting: 137it [01:20,  1.70it/s]Extractor Predicting: 138it [01:21,  1.68it/s]Extractor Predicting: 139it [01:22,  1.70it/s]Extractor Predicting: 140it [01:22,  1.70it/s]Extractor Predicting: 141it [01:23,  1.71it/s]Extractor Predicting: 142it [01:23,  1.72it/s]Extractor Predicting: 143it [01:24,  1.74it/s]Extractor Predicting: 144it [01:25,  1.74it/s]Extractor Predicting: 145it [01:25,  2.26it/s]Extractor Predicting: 145it [01:25,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:13,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:13,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:13,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:13,942 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:13,942 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:57:14,547 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:57:14,548 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:57:15,126 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:57:16,161 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:57:16,161 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:19,128 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:19,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:19,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:19,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:57:19,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:57:19,823 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:57:19,824 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:57:20,406 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:57:20,574 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:57:20,574 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.72it/s]Extractor Predicting: 15it [00:08,  1.71it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.74it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:12,  1.74it/s]Extractor Predicting: 22it [00:12,  1.78it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:14,  1.69it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:16,  1.72it/s]Extractor Predicting: 30it [00:17,  1.66it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:18,  1.66it/s]Extractor Predicting: 33it [00:19,  1.65it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:20,  1.69it/s]Extractor Predicting: 36it [00:21,  1.71it/s]Extractor Predicting: 37it [00:21,  1.77it/s]Extractor Predicting: 38it [00:22,  1.76it/s]Extractor Predicting: 39it [00:22,  1.76it/s]Extractor Predicting: 40it [00:23,  1.77it/s]Extractor Predicting: 41it [00:23,  1.77it/s]Extractor Predicting: 42it [00:24,  1.73it/s]Extractor Predicting: 43it [00:25,  1.73it/s]Extractor Predicting: 44it [00:25,  1.74it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:26,  1.73it/s]Extractor Predicting: 47it [00:27,  1.73it/s]Extractor Predicting: 48it [00:28,  1.74it/s]Extractor Predicting: 49it [00:28,  1.74it/s]Extractor Predicting: 50it [00:29,  1.71it/s]Extractor Predicting: 51it [00:29,  1.69it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:30,  1.73it/s]Extractor Predicting: 54it [00:31,  1.73it/s]Extractor Predicting: 55it [00:32,  1.69it/s]Extractor Predicting: 56it [00:32,  1.70it/s]Extractor Predicting: 57it [00:33,  1.74it/s]Extractor Predicting: 58it [00:33,  1.77it/s]Extractor Predicting: 59it [00:34,  1.77it/s]Extractor Predicting: 60it [00:34,  1.83it/s]Extractor Predicting: 61it [00:35,  1.86it/s]Extractor Predicting: 62it [00:35,  1.89it/s]Extractor Predicting: 63it [00:36,  1.90it/s]Extractor Predicting: 64it [00:36,  1.93it/s]Extractor Predicting: 65it [00:37,  1.94it/s]Extractor Predicting: 66it [00:37,  1.93it/s]Extractor Predicting: 67it [00:38,  1.95it/s]Extractor Predicting: 68it [00:38,  1.99it/s]Extractor Predicting: 69it [00:39,  2.03it/s]Extractor Predicting: 70it [00:39,  2.01it/s]Extractor Predicting: 71it [00:40,  2.01it/s]Extractor Predicting: 72it [00:40,  2.01it/s]Extractor Predicting: 73it [00:41,  2.01it/s]Extractor Predicting: 74it [00:41,  1.98it/s]Extractor Predicting: 75it [00:42,  1.96it/s]Extractor Predicting: 76it [00:42,  2.01it/s]Extractor Predicting: 77it [00:43,  2.04it/s]Extractor Predicting: 78it [00:43,  2.01it/s]Extractor Predicting: 79it [00:44,  2.00it/s]Extractor Predicting: 80it [00:44,  2.01it/s]Extractor Predicting: 81it [00:45,  1.99it/s]Extractor Predicting: 82it [00:45,  1.98it/s]Extractor Predicting: 83it [00:46,  1.96it/s]Extractor Predicting: 84it [00:46,  1.94it/s]Extractor Predicting: 85it [00:47,  1.95it/s]Extractor Predicting: 86it [00:48,  1.90it/s]Extractor Predicting: 87it [00:48,  1.84it/s]Extractor Predicting: 88it [00:49,  1.81it/s]Extractor Predicting: 89it [00:49,  1.77it/s]Extractor Predicting: 90it [00:50,  1.52it/s]Extractor Predicting: 91it [00:51,  1.56it/s]Extractor Predicting: 92it [00:51,  1.58it/s]Extractor Predicting: 93it [00:52,  1.65it/s]Extractor Predicting: 94it [00:53,  1.65it/s]Extractor Predicting: 95it [00:53,  1.67it/s]Extractor Predicting: 96it [00:54,  1.66it/s]Extractor Predicting: 97it [00:54,  1.67it/s]Extractor Predicting: 98it [00:55,  1.64it/s]Extractor Predicting: 99it [00:56,  1.67it/s]Extractor Predicting: 100it [00:56,  1.67it/s]Extractor Predicting: 101it [00:57,  1.68it/s]Extractor Predicting: 102it [00:57,  1.67it/s]Extractor Predicting: 103it [00:58,  1.70it/s]Extractor Predicting: 104it [00:59,  1.66it/s]Extractor Predicting: 105it [00:59,  1.66it/s]Extractor Predicting: 106it [01:00,  1.61it/s]Extractor Predicting: 107it [01:00,  1.61it/s]Extractor Predicting: 108it [01:01,  1.62it/s]Extractor Predicting: 109it [01:02,  1.66it/s]Extractor Predicting: 110it [01:02,  1.65it/s]Extractor Predicting: 111it [01:03,  1.64it/s]Extractor Predicting: 112it [01:03,  1.60it/s]Extractor Predicting: 113it [01:04,  1.62it/s]Extractor Predicting: 114it [01:05,  1.65it/s]Extractor Predicting: 115it [01:05,  1.64it/s]Extractor Predicting: 116it [01:06,  1.67it/s]Extractor Predicting: 117it [01:06,  1.71it/s]Extractor Predicting: 118it [01:07,  1.66it/s]Extractor Predicting: 119it [01:08,  1.62it/s]Extractor Predicting: 120it [01:08,  1.64it/s]Extractor Predicting: 121it [01:09,  1.63it/s]Extractor Predicting: 122it [01:10,  1.65it/s]Extractor Predicting: 123it [01:10,  1.65it/s]Extractor Predicting: 124it [01:11,  1.65it/s]Extractor Predicting: 125it [01:11,  1.64it/s]Extractor Predicting: 126it [01:12,  1.60it/s]Extractor Predicting: 127it [01:13,  1.61it/s]Extractor Predicting: 128it [01:13,  1.58it/s]Extractor Predicting: 129it [01:14,  1.59it/s]Extractor Predicting: 130it [01:15,  1.59it/s]Extractor Predicting: 131it [01:15,  1.62it/s]Extractor Predicting: 132it [01:16,  1.63it/s]Extractor Predicting: 133it [01:16,  1.64it/s]Extractor Predicting: 134it [01:17,  1.67it/s]Extractor Predicting: 135it [01:18,  1.65it/s]Extractor Predicting: 136it [01:18,  1.61it/s]Extractor Predicting: 137it [01:19,  1.62it/s]Extractor Predicting: 138it [01:19,  1.61it/s]Extractor Predicting: 139it [01:20,  1.61it/s]Extractor Predicting: 140it [01:21,  1.62it/s]Extractor Predicting: 141it [01:21,  1.63it/s]Extractor Predicting: 142it [01:22,  1.64it/s]Extractor Predicting: 143it [01:23,  1.60it/s]Extractor Predicting: 144it [01:23,  1.60it/s]Extractor Predicting: 145it [01:24,  1.61it/s]Extractor Predicting: 146it [01:24,  1.62it/s]Extractor Predicting: 147it [01:25,  1.65it/s]Extractor Predicting: 148it [01:26,  1.66it/s]Extractor Predicting: 149it [01:26,  1.62it/s]Extractor Predicting: 150it [01:27,  1.62it/s]Extractor Predicting: 151it [01:27,  1.66it/s]Extractor Predicting: 152it [01:28,  1.69it/s]Extractor Predicting: 153it [01:29,  1.71it/s]Extractor Predicting: 154it [01:29,  1.73it/s]Extractor Predicting: 155it [01:30,  1.75it/s]Extractor Predicting: 156it [01:30,  1.77it/s]Extractor Predicting: 157it [01:31,  1.71it/s]Extractor Predicting: 158it [01:31,  1.74it/s]Extractor Predicting: 159it [01:32,  1.80it/s]Extractor Predicting: 160it [01:33,  1.70it/s]Extractor Predicting: 161it [01:33,  1.69it/s]Extractor Predicting: 162it [01:34,  1.67it/s]Extractor Predicting: 163it [01:34,  1.69it/s]Extractor Predicting: 164it [01:35,  1.68it/s]Extractor Predicting: 165it [01:36,  1.65it/s]Extractor Predicting: 166it [01:36,  1.66it/s]Extractor Predicting: 167it [01:37,  1.67it/s]Extractor Predicting: 168it [01:37,  1.67it/s]Extractor Predicting: 169it [01:38,  1.66it/s]Extractor Predicting: 170it [01:39,  1.70it/s]Extractor Predicting: 171it [01:39,  1.69it/s]Extractor Predicting: 172it [01:40,  1.67it/s]Extractor Predicting: 173it [01:40,  1.69it/s]Extractor Predicting: 174it [01:41,  1.68it/s]Extractor Predicting: 175it [01:42,  1.68it/s]Extractor Predicting: 176it [01:42,  1.68it/s]Extractor Predicting: 177it [01:43,  1.73it/s]Extractor Predicting: 178it [01:43,  1.72it/s]Extractor Predicting: 179it [01:44,  1.74it/s]Extractor Predicting: 180it [01:44,  1.76it/s]Extractor Predicting: 181it [01:45,  1.74it/s]Extractor Predicting: 182it [01:45,  1.79it/s]Extractor Predicting: 183it [01:46,  1.75it/s]Extractor Predicting: 184it [01:47,  1.74it/s]Extractor Predicting: 185it [01:47,  1.77it/s]Extractor Predicting: 186it [01:48,  1.75it/s]Extractor Predicting: 187it [01:48,  1.72it/s]Extractor Predicting: 188it [01:49,  1.78it/s]Extractor Predicting: 189it [01:49,  1.80it/s]Extractor Predicting: 190it [01:50,  1.85it/s]Extractor Predicting: 191it [01:51,  1.78it/s]Extractor Predicting: 192it [01:51,  1.79it/s]Extractor Predicting: 193it [01:52,  1.81it/s]Extractor Predicting: 194it [01:52,  1.85it/s]Extractor Predicting: 195it [01:53,  1.81it/s]Extractor Predicting: 196it [01:53,  1.81it/s]Extractor Predicting: 197it [01:54,  1.83it/s]Extractor Predicting: 198it [01:54,  1.80it/s]Extractor Predicting: 199it [01:55,  1.76it/s]Extractor Predicting: 200it [01:56,  1.77it/s]Extractor Predicting: 201it [01:56,  1.75it/s]Extractor Predicting: 202it [01:57,  1.79it/s]Extractor Predicting: 203it [01:57,  1.84it/s]Extractor Predicting: 204it [01:58,  1.86it/s]Extractor Predicting: 205it [01:58,  1.85it/s]Extractor Predicting: 206it [01:59,  1.92it/s]Extractor Predicting: 207it [01:59,  1.88it/s]Extractor Predicting: 208it [02:00,  1.93it/s]Extractor Predicting: 209it [02:00,  1.91it/s]Extractor Predicting: 210it [02:01,  1.65it/s]Extractor Predicting: 211it [02:02,  1.68it/s]Extractor Predicting: 212it [02:02,  1.77it/s]Extractor Predicting: 213it [02:03,  1.83it/s]Extractor Predicting: 214it [02:03,  1.87it/s]Extractor Predicting: 215it [02:04,  1.93it/s]Extractor Predicting: 216it [02:04,  1.94it/s]Extractor Predicting: 217it [02:05,  1.89it/s]Extractor Predicting: 218it [02:05,  1.88it/s]Extractor Predicting: 219it [02:06,  1.91it/s]Extractor Predicting: 220it [02:06,  1.89it/s]Extractor Predicting: 221it [02:07,  1.90it/s]Extractor Predicting: 222it [02:07,  1.97it/s]Extractor Predicting: 223it [02:08,  1.92it/s]Extractor Predicting: 224it [02:08,  1.96it/s]Extractor Predicting: 225it [02:09,  1.94it/s]Extractor Predicting: 226it [02:09,  1.93it/s]Extractor Predicting: 227it [02:10,  1.97it/s]Extractor Predicting: 228it [02:10,  1.96it/s]Extractor Predicting: 229it [02:11,  1.85it/s]Extractor Predicting: 230it [02:12,  1.78it/s]Extractor Predicting: 231it [02:12,  1.70it/s]Extractor Predicting: 232it [02:13,  1.66it/s]Extractor Predicting: 233it [02:13,  1.67it/s]Extractor Predicting: 234it [02:14,  1.64it/s]Extractor Predicting: 235it [02:15,  1.65it/s]Extractor Predicting: 236it [02:15,  1.64it/s]Extractor Predicting: 237it [02:16,  1.61it/s]Extractor Predicting: 238it [02:17,  1.58it/s]Extractor Predicting: 239it [02:17,  1.59it/s]Extractor Predicting: 240it [02:18,  1.60it/s]Extractor Predicting: 241it [02:18,  1.62it/s]Extractor Predicting: 242it [02:19,  1.65it/s]Extractor Predicting: 243it [02:20,  1.59it/s]Extractor Predicting: 244it [02:20,  1.62it/s]Extractor Predicting: 245it [02:21,  1.63it/s]Extractor Predicting: 246it [02:22,  1.63it/s]Extractor Predicting: 247it [02:22,  1.59it/s]Extractor Predicting: 248it [02:23,  1.59it/s]Extractor Predicting: 249it [02:24,  1.55it/s]Extractor Predicting: 250it [02:24,  1.57it/s]Extractor Predicting: 251it [02:25,  1.58it/s]Extractor Predicting: 252it [02:25,  1.61it/s]Extractor Predicting: 253it [02:26,  1.62it/s]Extractor Predicting: 254it [02:27,  1.61it/s]Extractor Predicting: 255it [02:27,  1.59it/s]Extractor Predicting: 256it [02:28,  1.61it/s]Extractor Predicting: 257it [02:28,  1.68it/s]Extractor Predicting: 258it [02:29,  1.70it/s]Extractor Predicting: 259it [02:30,  1.71it/s]Extractor Predicting: 260it [02:30,  1.75it/s]Extractor Predicting: 261it [02:31,  1.74it/s]Extractor Predicting: 262it [02:31,  1.74it/s]Extractor Predicting: 263it [02:32,  1.73it/s]Extractor Predicting: 264it [02:32,  1.75it/s]Extractor Predicting: 265it [02:33,  1.72it/s]Extractor Predicting: 266it [02:34,  1.75it/s]Extractor Predicting: 267it [02:34,  1.79it/s]Extractor Predicting: 268it [02:35,  1.75it/s]Extractor Predicting: 269it [02:35,  1.76it/s]Extractor Predicting: 270it [02:36,  1.74it/s]Extractor Predicting: 271it [02:36,  1.73it/s]Extractor Predicting: 272it [02:37,  1.66it/s]Extractor Predicting: 273it [02:38,  1.67it/s]Extractor Predicting: 274it [02:38,  1.68it/s]Extractor Predicting: 275it [02:39,  1.72it/s]Extractor Predicting: 276it [02:39,  1.73it/s]Extractor Predicting: 277it [02:40,  1.71it/s]Extractor Predicting: 278it [02:41,  1.68it/s]Extractor Predicting: 279it [02:41,  1.71it/s]Extractor Predicting: 280it [02:42,  1.70it/s]Extractor Predicting: 281it [02:42,  1.71it/s]Extractor Predicting: 282it [02:43,  1.70it/s]Extractor Predicting: 283it [02:43,  1.71it/s]Extractor Predicting: 284it [02:44,  1.74it/s]Extractor Predicting: 285it [02:45,  1.70it/s]Extractor Predicting: 286it [02:45,  1.70it/s]Extractor Predicting: 287it [02:46,  1.73it/s]Extractor Predicting: 288it [02:46,  1.73it/s]Extractor Predicting: 289it [02:47,  1.71it/s]Extractor Predicting: 290it [02:48,  1.70it/s]Extractor Predicting: 291it [02:48,  1.70it/s]Extractor Predicting: 292it [02:49,  1.72it/s]Extractor Predicting: 293it [02:49,  1.71it/s]Extractor Predicting: 294it [02:50,  1.72it/s]Extractor Predicting: 295it [02:50,  1.72it/s]Extractor Predicting: 296it [02:51,  1.72it/s]Extractor Predicting: 297it [02:52,  1.74it/s]Extractor Predicting: 298it [02:52,  1.69it/s]Extractor Predicting: 299it [02:53,  1.72it/s]Extractor Predicting: 300it [02:53,  1.71it/s]Extractor Predicting: 301it [02:54,  1.72it/s]Extractor Predicting: 302it [02:55,  1.71it/s]Extractor Predicting: 303it [02:55,  1.77it/s]Extractor Predicting: 304it [02:56,  1.76it/s]Extractor Predicting: 305it [02:56,  1.80it/s]Extractor Predicting: 306it [02:57,  1.78it/s]Extractor Predicting: 307it [02:57,  1.75it/s]Extractor Predicting: 308it [02:58,  1.72it/s]Extractor Predicting: 309it [02:58,  1.74it/s]Extractor Predicting: 310it [02:59,  1.71it/s]Extractor Predicting: 311it [03:00,  1.69it/s]Extractor Predicting: 312it [03:00,  1.70it/s]Extractor Predicting: 313it [03:01,  1.73it/s]Extractor Predicting: 314it [03:01,  1.73it/s]Extractor Predicting: 315it [03:02,  1.76it/s]Extractor Predicting: 316it [03:03,  1.73it/s]Extractor Predicting: 317it [03:03,  1.71it/s]Extractor Predicting: 318it [03:04,  1.78it/s]Extractor Predicting: 319it [03:04,  1.77it/s]Extractor Predicting: 320it [03:05,  1.81it/s]Extractor Predicting: 321it [03:05,  1.77it/s]Extractor Predicting: 322it [03:06,  1.76it/s]Extractor Predicting: 323it [03:07,  1.75it/s]Extractor Predicting: 324it [03:07,  1.73it/s]Extractor Predicting: 325it [03:08,  1.71it/s]Extractor Predicting: 326it [03:08,  1.73it/s]Extractor Predicting: 327it [03:09,  1.70it/s]Extractor Predicting: 328it [03:09,  1.70it/s]Extractor Predicting: 329it [03:10,  1.65it/s]Extractor Predicting: 330it [03:11,  1.67it/s]Extractor Predicting: 331it [03:11,  1.68it/s]Extractor Predicting: 332it [03:12,  1.73it/s]Extractor Predicting: 333it [03:12,  1.73it/s]Extractor Predicting: 334it [03:13,  1.70it/s]Extractor Predicting: 335it [03:14,  1.70it/s]Extractor Predicting: 336it [03:14,  1.71it/s]Extractor Predicting: 337it [03:15,  1.49it/s]Extractor Predicting: 338it [03:16,  1.55it/s]Extractor Predicting: 339it [03:16,  1.58it/s]Extractor Predicting: 340it [03:17,  1.66it/s]Extractor Predicting: 341it [03:17,  1.65it/s]Extractor Predicting: 342it [03:18,  1.68it/s]Extractor Predicting: 343it [03:19,  1.64it/s]Extractor Predicting: 344it [03:19,  1.68it/s]Extractor Predicting: 345it [03:20,  1.65it/s]Extractor Predicting: 346it [03:20,  1.69it/s]Extractor Predicting: 347it [03:21,  1.70it/s]Extractor Predicting: 348it [03:21,  1.73it/s]Extractor Predicting: 349it [03:22,  1.65it/s]Extractor Predicting: 350it [03:23,  1.64it/s]Extractor Predicting: 351it [03:23,  1.62it/s]Extractor Predicting: 352it [03:24,  1.64it/s]Extractor Predicting: 353it [03:25,  1.64it/s]Extractor Predicting: 354it [03:25,  1.66it/s]Extractor Predicting: 355it [03:26,  1.66it/s]Extractor Predicting: 356it [03:26,  1.70it/s]Extractor Predicting: 357it [03:27,  1.67it/s]Extractor Predicting: 358it [03:28,  1.66it/s]Extractor Predicting: 359it [03:28,  1.61it/s]Extractor Predicting: 360it [03:29,  1.58it/s]Extractor Predicting: 361it [03:29,  1.64it/s]Extractor Predicting: 362it [03:30,  1.68it/s]Extractor Predicting: 363it [03:31,  1.67it/s]Extractor Predicting: 364it [03:31,  1.71it/s]Extractor Predicting: 365it [03:32,  1.72it/s]Extractor Predicting: 366it [03:32,  1.69it/s]Extractor Predicting: 367it [03:33,  1.69it/s]Extractor Predicting: 368it [03:34,  1.64it/s]Extractor Predicting: 369it [03:34,  1.67it/s]Extractor Predicting: 370it [03:35,  1.66it/s]Extractor Predicting: 371it [03:35,  1.69it/s]Extractor Predicting: 372it [03:36,  1.71it/s]Extractor Predicting: 373it [03:37,  1.74it/s]Extractor Predicting: 374it [03:37,  1.74it/s]Extractor Predicting: 375it [03:38,  1.72it/s]Extractor Predicting: 376it [03:38,  1.74it/s]Extractor Predicting: 377it [03:39,  1.70it/s]Extractor Predicting: 378it [03:39,  1.73it/s]Extractor Predicting: 379it [03:40,  1.75it/s]Extractor Predicting: 380it [03:41,  1.75it/s]Extractor Predicting: 381it [03:41,  1.76it/s]Extractor Predicting: 382it [03:42,  1.73it/s]Extractor Predicting: 383it [03:42,  1.76it/s]Extractor Predicting: 384it [03:43,  1.77it/s]Extractor Predicting: 385it [03:43,  1.77it/s]Extractor Predicting: 386it [03:44,  1.77it/s]Extractor Predicting: 387it [03:44,  1.80it/s]Extractor Predicting: 388it [03:45,  1.76it/s]Extractor Predicting: 389it [03:46,  1.77it/s]Extractor Predicting: 390it [03:46,  1.76it/s]Extractor Predicting: 391it [03:47,  1.76it/s]Extractor Predicting: 392it [03:47,  1.77it/s]Extractor Predicting: 393it [03:48,  1.77it/s]Extractor Predicting: 394it [03:48,  1.79it/s]Extractor Predicting: 395it [03:49,  1.68it/s]Extractor Predicting: 396it [03:50,  1.73it/s]Extractor Predicting: 397it [03:50,  1.75it/s]Extractor Predicting: 398it [03:51,  1.73it/s]Extractor Predicting: 399it [03:51,  1.73it/s]Extractor Predicting: 400it [03:52,  1.73it/s]Extractor Predicting: 401it [03:53,  1.74it/s]Extractor Predicting: 402it [03:53,  1.79it/s]Extractor Predicting: 403it [03:54,  1.77it/s]Extractor Predicting: 404it [03:54,  1.77it/s]Extractor Predicting: 405it [03:55,  1.78it/s]Extractor Predicting: 406it [03:55,  1.79it/s]Extractor Predicting: 407it [03:56,  1.77it/s]Extractor Predicting: 408it [03:56,  1.81it/s]Extractor Predicting: 409it [03:57,  1.83it/s]Extractor Predicting: 410it [03:57,  1.82it/s]Extractor Predicting: 411it [03:58,  1.77it/s]Extractor Predicting: 412it [03:59,  1.79it/s]Extractor Predicting: 413it [03:59,  1.77it/s]Extractor Predicting: 414it [04:00,  1.78it/s]Extractor Predicting: 415it [04:00,  1.82it/s]Extractor Predicting: 416it [04:01,  1.59it/s]Extractor Predicting: 417it [04:02,  1.62it/s]Extractor Predicting: 418it [04:02,  1.69it/s]Extractor Predicting: 419it [04:03,  1.71it/s]Extractor Predicting: 420it [04:03,  1.73it/s]Extractor Predicting: 421it [04:04,  1.71it/s]Extractor Predicting: 422it [04:05,  1.72it/s]Extractor Predicting: 423it [04:05,  1.73it/s]Extractor Predicting: 424it [04:06,  1.75it/s]Extractor Predicting: 425it [04:06,  2.01it/s]Extractor Predicting: 425it [04:06,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:39,607 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:39,624 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:39,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:39,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:39,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:01:40,224 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:01:40,225 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:40,800 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:41,881 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:41,881 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:44,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:44,859 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:44,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:44,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:01:44,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:01:45,504 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:01:45,505 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:01:46,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:01:46,326 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:01:46,326 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.89it/s]Extractor Predicting: 7it [00:04,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:01:51,532 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:51,533 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:01:51,572 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:51,573 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:01:51,588 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:02:05,490 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:02:05,542 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:02:05,776 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:02:05,776 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:02:05,894 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:02:05,970 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:02:06,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:10,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:13,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:15,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:19,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:21,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:15, 16.62s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:23,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:23,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:24,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:27,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:28,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:32,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:32,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:37,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:41,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:35<05:23, 17.95s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:41,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:42,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:47,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:52,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:55,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:55,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:57,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:58,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:52<04:56, 17.43s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:58,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:59,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:02,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:04,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:06,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:10,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:12,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:14,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:09<04:35, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:15,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:17,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:19,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:21,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:21,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:23,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:23,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:24,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:27,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:33,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:29<04:33, 18.24s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:35,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:37,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:39,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:50,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<04:00, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:50,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:53,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:53,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:55,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:55,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:57,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:59,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:01,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:03,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:04,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:05,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:05,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:06,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:07,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:07,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:08,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:08,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:09,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:10,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:04<03:55, 18.14s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:10,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:12,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:12,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:13,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:14,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:15,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:15,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:16,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:17,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:17,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:18,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:19,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:19,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:20,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:21,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:21,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:22,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:23,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:23,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:25,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:26,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:27,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:28,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:22<03:37, 18.10s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:28,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:29,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:30,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:32,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:33,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:34,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:34,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:35,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:36,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:36,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:37,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:38,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:39,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:39,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:40,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:41,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:41,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:42,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:43,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:44,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:45,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:46,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:47,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:41<03:22, 18.40s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:47,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:48,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:49,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:49,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:50,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:51,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:51,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:52,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:53,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:53,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:54,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:55,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:55,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:56,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:57,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:57,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:58,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:59,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:59,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:00,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:01,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:02,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:02,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:03,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:04,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:58<02:59, 17.99s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:05,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:05,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:06,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:07,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:08,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:08,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:09,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:10,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:10,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:12,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:12,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:13,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:14,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:14,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:15,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:16,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:17,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:17,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:18,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:19,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:19,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:20,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:21,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:21,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:16<02:40, 17.84s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:22,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:23,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:24,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:24,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:25,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:26,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:27,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:27,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:28,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:29,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:29,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:30,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:31,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:31,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:32,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:33,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:34,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:34,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:35,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:36,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:36,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:37,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:38,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:38,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:39,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:34<02:23, 17.93s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:40,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:41,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:41,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:42,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:43,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:43,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:44,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:45,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:45,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:46,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:47,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:47,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:48,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:49,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:49,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:50,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:50,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:51,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:52,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:52,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:53,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:54,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:55,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:56,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:50<02:01, 17.36s/it][WARNING|generation_utils.py:914] 2023-08-28 07:05:56,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:57,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:57,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:58,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:05:59,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:00,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:00,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:01,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:02,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:03,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:04,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:05,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:05,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:06,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:07,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:08,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:09,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:09,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:10,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:11,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:12,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:12,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:13,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:14,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:15,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:15,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:10<01:48, 18.07s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:16,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:17,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:17,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:18,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:19,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:20,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:20,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:21,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:22,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:22,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:23,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:24,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:24,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:25,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:26,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:26,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:27,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:28,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:28,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:29,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:30,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:30,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:31,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:32,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:32,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:27<01:28, 17.76s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:33,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:34,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:34,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:35,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:36,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:38,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:39,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:40,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:41,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:41,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:42,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:43,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:44,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:45,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:46,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:47,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:47,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:48,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:49,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:49,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:50,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:51,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:51,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:52,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:53,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:54,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:54,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:49<01:16, 19.04s/it][WARNING|generation_utils.py:914] 2023-08-28 07:06:55,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:56,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:56,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:57,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:58,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:58,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:06:59,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:00,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:00,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:01,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:01,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:02,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:03,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:04,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:04,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:05,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:06,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:06,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:07,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:08,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:09,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:09,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:10,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:11,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:05<00:54, 18.17s/it][WARNING|generation_utils.py:914] 2023-08-28 07:07:11,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:12,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:13,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:13,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:14,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:15,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:16,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:16,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:17,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:18,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:18,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:19,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:20,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:20,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:21,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:22,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:23,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:23,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:24,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:25,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:25,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:26,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:27,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:21<00:35, 17.59s/it][WARNING|generation_utils.py:914] 2023-08-28 07:07:27,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:28,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:29,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:29,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:30,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:30,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:31,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:32,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:33,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:33,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:34,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:34,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:35,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:36,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:36,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:37,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:38,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:38,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:39,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:40,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:41,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:41,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:42,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:43,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:44,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:44,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:45,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:46,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:46,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:47,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:48,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:48,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:49,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:50,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:44<00:19, 19.17s/it][WARNING|generation_utils.py:914] 2023-08-28 07:07:50,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:51,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:52,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:52,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:53,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:54,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:54,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:55,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:56,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:56,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:57,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:58,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:58,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:59,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:07:59,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:01,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:01,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:02,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:03,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:04,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:04,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:05,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:06,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:08:06,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [06:00<00:00, 18.36s/it]Generating: 100%|| 20/20 [06:00<00:00, 18.04s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:15,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:15,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:15,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:15,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:15,994 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:08:16,611 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:08:16,612 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:08:17,202 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:08:18,291 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:08:18,291 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:21,219 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:21,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:21,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:21,230 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:21,231 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:08:21,894 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:08:21,895 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:08:22,489 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:08:22,666 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:08:22,666 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.53it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:05,  1.52it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:07,  1.50it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.46it/s]Extractor Estimating: 16it [00:10,  1.48it/s]Extractor Estimating: 17it [00:11,  1.47it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.46it/s]Extractor Estimating: 25it [00:16,  1.52it/s]Extractor Estimating: 26it [00:17,  1.53it/s]Extractor Estimating: 27it [00:17,  1.59it/s]Extractor Estimating: 28it [00:18,  1.57it/s]Extractor Estimating: 29it [00:19,  1.59it/s]Extractor Estimating: 30it [00:19,  1.65it/s]Extractor Estimating: 31it [00:20,  1.60it/s]Extractor Estimating: 32it [00:21,  1.63it/s]Extractor Estimating: 33it [00:21,  1.56it/s]Extractor Estimating: 34it [00:22,  1.58it/s]Extractor Estimating: 35it [00:22,  1.60it/s]Extractor Estimating: 36it [00:23,  1.56it/s]Extractor Estimating: 37it [00:24,  1.53it/s]Extractor Estimating: 38it [00:24,  1.50it/s]Extractor Estimating: 39it [00:25,  1.55it/s]Extractor Estimating: 40it [00:26,  1.55it/s]Extractor Estimating: 41it [00:26,  1.53it/s]Extractor Estimating: 42it [00:27,  1.59it/s]Extractor Estimating: 43it [00:28,  1.57it/s]Extractor Estimating: 44it [00:28,  1.56it/s]Extractor Estimating: 45it [00:29,  1.58it/s]Extractor Estimating: 46it [00:30,  1.50it/s]Extractor Estimating: 47it [00:30,  1.49it/s]Extractor Estimating: 48it [00:31,  1.48it/s]Extractor Estimating: 49it [00:32,  1.49it/s]Extractor Estimating: 50it [00:32,  1.46it/s]Extractor Estimating: 51it [00:33,  1.49it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:34,  1.54it/s]Extractor Estimating: 54it [00:35,  1.60it/s]Extractor Estimating: 55it [00:35,  1.63it/s]Extractor Estimating: 56it [00:36,  1.64it/s]Extractor Estimating: 57it [00:37,  1.57it/s]Extractor Estimating: 58it [00:37,  1.63it/s]Extractor Estimating: 59it [00:38,  1.61it/s]Extractor Estimating: 60it [00:39,  1.58it/s]Extractor Estimating: 61it [00:39,  1.52it/s]Extractor Estimating: 62it [00:40,  1.58it/s]Extractor Estimating: 63it [00:41,  1.58it/s]Extractor Estimating: 64it [00:41,  1.56it/s]Extractor Estimating: 65it [00:42,  1.53it/s]Extractor Estimating: 66it [00:42,  1.59it/s]Extractor Estimating: 67it [00:43,  1.62it/s]Extractor Estimating: 68it [00:44,  1.63it/s]Extractor Estimating: 69it [00:44,  1.58it/s]Extractor Estimating: 70it [00:45,  1.54it/s]Extractor Estimating: 71it [00:46,  1.53it/s]Extractor Estimating: 72it [00:46,  1.54it/s]Extractor Estimating: 73it [00:47,  1.52it/s]Extractor Estimating: 74it [00:48,  1.53it/s]Extractor Estimating: 75it [00:48,  1.56it/s]Extractor Estimating: 76it [00:49,  1.59it/s]Extractor Estimating: 77it [00:49,  1.61it/s]Extractor Estimating: 78it [00:50,  1.60it/s]Extractor Estimating: 79it [00:51,  1.56it/s]Extractor Estimating: 80it [00:51,  1.55it/s]Extractor Estimating: 81it [00:52,  1.56it/s]Extractor Estimating: 82it [00:53,  1.57it/s]Extractor Estimating: 83it [00:53,  1.54it/s]Extractor Estimating: 84it [00:54,  1.50it/s]Extractor Estimating: 85it [00:55,  1.48it/s]Extractor Estimating: 86it [00:56,  1.28it/s]Extractor Estimating: 87it [00:56,  1.35it/s]Extractor Estimating: 88it [00:57,  1.42it/s]Extractor Estimating: 89it [00:58,  1.46it/s]Extractor Estimating: 90it [00:58,  1.42it/s]Extractor Estimating: 91it [00:59,  1.45it/s]Extractor Estimating: 92it [01:00,  1.48it/s]Extractor Estimating: 93it [01:00,  1.49it/s]Extractor Estimating: 94it [01:01,  1.54it/s]Extractor Estimating: 95it [01:02,  1.58it/s]Extractor Estimating: 96it [01:02,  1.52it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:04,  1.57it/s]Extractor Estimating: 99it [01:04,  1.54it/s]Extractor Estimating: 100it [01:05,  1.54it/s]Extractor Estimating: 101it [01:06,  1.53it/s]Extractor Estimating: 102it [01:06,  1.56it/s]Extractor Estimating: 103it [01:07,  1.55it/s]Extractor Estimating: 104it [01:07,  1.54it/s]Extractor Estimating: 105it [01:08,  1.57it/s]Extractor Estimating: 106it [01:09,  1.58it/s]Extractor Estimating: 107it [01:09,  1.58it/s]Extractor Estimating: 108it [01:10,  1.60it/s]Extractor Estimating: 109it [01:11,  1.61it/s]Extractor Estimating: 110it [01:11,  1.58it/s]Extractor Estimating: 111it [01:12,  1.59it/s]Extractor Estimating: 112it [01:12,  1.57it/s]Extractor Estimating: 113it [01:13,  1.61it/s]Extractor Estimating: 114it [01:14,  1.60it/s]Extractor Estimating: 115it [01:14,  1.60it/s]Extractor Estimating: 116it [01:15,  1.58it/s]Extractor Estimating: 117it [01:16,  1.61it/s]Extractor Estimating: 118it [01:16,  1.63it/s]Extractor Estimating: 119it [01:17,  1.63it/s]Extractor Estimating: 120it [01:17,  1.58it/s]Extractor Estimating: 121it [01:18,  1.58it/s]Extractor Estimating: 122it [01:19,  1.59it/s]Extractor Estimating: 123it [01:19,  1.61it/s]Extractor Estimating: 124it [01:20,  1.58it/s]Extractor Estimating: 125it [01:21,  1.61it/s]Extractor Estimating: 126it [01:21,  1.55it/s]Extractor Estimating: 127it [01:22,  1.55it/s]Extractor Estimating: 128it [01:23,  1.53it/s]Extractor Estimating: 129it [01:23,  1.54it/s]Extractor Estimating: 130it [01:24,  1.50it/s]Extractor Estimating: 131it [01:25,  1.51it/s]Extractor Estimating: 132it [01:25,  1.47it/s]Extractor Estimating: 133it [01:26,  1.48it/s]Extractor Estimating: 134it [01:27,  1.46it/s]Extractor Estimating: 135it [01:27,  1.45it/s]Extractor Estimating: 136it [01:28,  1.44it/s]Extractor Estimating: 137it [01:29,  1.45it/s]Extractor Estimating: 138it [01:29,  1.46it/s]Extractor Estimating: 139it [01:30,  1.44it/s]Extractor Estimating: 140it [01:31,  1.46it/s]Extractor Estimating: 141it [01:31,  1.49it/s]Extractor Estimating: 142it [01:32,  1.50it/s]Extractor Estimating: 143it [01:33,  1.55it/s]Extractor Estimating: 144it [01:33,  1.50it/s]Extractor Estimating: 145it [01:34,  1.47it/s]Extractor Estimating: 146it [01:35,  1.43it/s]Extractor Estimating: 147it [01:36,  1.44it/s]Extractor Estimating: 148it [01:36,  1.43it/s]Extractor Estimating: 149it [01:37,  1.48it/s]Extractor Estimating: 150it [01:38,  1.50it/s]Extractor Estimating: 151it [01:38,  1.54it/s]Extractor Estimating: 152it [01:39,  1.56it/s]Extractor Estimating: 153it [01:39,  1.62it/s]Extractor Estimating: 154it [01:40,  1.62it/s]Extractor Estimating: 155it [01:41,  1.61it/s]Extractor Estimating: 156it [01:41,  1.63it/s]Extractor Estimating: 157it [01:42,  1.61it/s]Extractor Estimating: 158it [01:42,  1.64it/s]Extractor Estimating: 159it [01:43,  1.70it/s]Extractor Estimating: 160it [01:44,  1.68it/s]Extractor Estimating: 161it [01:44,  1.65it/s]Extractor Estimating: 162it [01:45,  1.53it/s]Extractor Estimating: 163it [01:46,  1.51it/s]Extractor Estimating: 164it [01:46,  1.56it/s]Extractor Estimating: 165it [01:47,  1.60it/s]Extractor Estimating: 166it [01:48,  1.56it/s]Extractor Estimating: 167it [01:48,  1.52it/s]Extractor Estimating: 168it [01:49,  1.54it/s]Extractor Estimating: 169it [01:49,  1.56it/s]Extractor Estimating: 170it [01:50,  1.57it/s]Extractor Estimating: 171it [01:51,  1.56it/s]Extractor Estimating: 172it [01:52,  1.41it/s]Extractor Estimating: 173it [01:52,  1.49it/s]Extractor Estimating: 174it [01:53,  1.49it/s]Extractor Estimating: 175it [01:53,  1.53it/s]Extractor Estimating: 176it [01:54,  1.56it/s]Extractor Estimating: 177it [01:55,  1.56it/s]Extractor Estimating: 178it [01:55,  1.56it/s]Extractor Estimating: 179it [01:56,  1.47it/s]Extractor Estimating: 180it [01:57,  1.53it/s]Extractor Estimating: 181it [01:57,  1.58it/s]Extractor Estimating: 182it [01:58,  1.59it/s]Extractor Estimating: 183it [01:59,  1.56it/s]Extractor Estimating: 184it [01:59,  1.58it/s]Extractor Estimating: 185it [02:00,  1.62it/s]Extractor Estimating: 186it [02:00,  1.61it/s]Extractor Estimating: 187it [02:01,  1.61it/s]Extractor Estimating: 188it [02:02,  1.55it/s]Extractor Estimating: 189it [02:02,  1.57it/s]Extractor Estimating: 190it [02:03,  1.58it/s]Extractor Estimating: 191it [02:04,  1.62it/s]Extractor Estimating: 192it [02:04,  1.67it/s]Extractor Estimating: 193it [02:05,  1.71it/s]Extractor Estimating: 194it [02:05,  1.67it/s]Extractor Estimating: 195it [02:06,  1.64it/s]Extractor Estimating: 196it [02:07,  1.53it/s]Extractor Estimating: 197it [02:07,  1.61it/s]Extractor Estimating: 198it [02:08,  1.65it/s]Extractor Estimating: 199it [02:08,  1.66it/s]Extractor Estimating: 200it [02:09,  1.68it/s]Extractor Estimating: 201it [02:10,  1.69it/s]Extractor Estimating: 202it [02:10,  1.63it/s]Extractor Estimating: 203it [02:11,  1.62it/s]Extractor Estimating: 204it [02:12,  1.58it/s]Extractor Estimating: 205it [02:12,  1.58it/s]Extractor Estimating: 206it [02:13,  1.55it/s]Extractor Estimating: 207it [02:14,  1.45it/s]Extractor Estimating: 208it [02:14,  1.45it/s]Extractor Estimating: 209it [02:15,  1.50it/s]Extractor Estimating: 210it [02:16,  1.52it/s]Extractor Estimating: 211it [02:16,  1.43it/s]Extractor Estimating: 212it [02:17,  1.50it/s]Extractor Estimating: 213it [02:18,  1.50it/s]Extractor Estimating: 214it [02:18,  1.50it/s]Extractor Estimating: 215it [02:19,  1.51it/s]Extractor Estimating: 216it [02:19,  1.58it/s]Extractor Estimating: 217it [02:20,  1.62it/s]Extractor Estimating: 218it [02:21,  1.57it/s]Extractor Estimating: 219it [02:21,  1.52it/s]Extractor Estimating: 220it [02:22,  1.48it/s]Extractor Estimating: 221it [02:23,  1.52it/s]Extractor Estimating: 222it [02:23,  1.53it/s]Extractor Estimating: 223it [02:24,  1.58it/s]Extractor Estimating: 224it [02:25,  1.58it/s]Extractor Estimating: 225it [02:25,  1.51it/s]Extractor Estimating: 226it [02:26,  1.50it/s]Extractor Estimating: 227it [02:27,  1.47it/s]Extractor Estimating: 228it [02:27,  1.52it/s]Extractor Estimating: 229it [02:28,  1.51it/s]Extractor Estimating: 230it [02:29,  1.56it/s]Extractor Estimating: 231it [02:29,  1.58it/s]Extractor Estimating: 232it [02:30,  1.56it/s]Extractor Estimating: 233it [02:31,  1.59it/s]Extractor Estimating: 234it [02:31,  1.53it/s]Extractor Estimating: 235it [02:32,  1.55it/s]Extractor Estimating: 236it [02:32,  1.57it/s]Extractor Estimating: 237it [02:33,  1.59it/s]Extractor Estimating: 238it [02:34,  1.60it/s]Extractor Estimating: 239it [02:34,  1.57it/s]Extractor Estimating: 240it [02:35,  1.51it/s]Extractor Estimating: 241it [02:36,  1.52it/s]Extractor Estimating: 242it [02:36,  1.51it/s]Extractor Estimating: 243it [02:37,  1.48it/s]Extractor Estimating: 244it [02:38,  1.44it/s]Extractor Estimating: 245it [02:39,  1.44it/s]Extractor Estimating: 246it [02:39,  1.51it/s]Extractor Estimating: 247it [02:40,  1.47it/s]Extractor Estimating: 248it [02:41,  1.47it/s]Extractor Estimating: 249it [02:41,  1.48it/s]Extractor Estimating: 250it [02:42,  1.48it/s]Extractor Estimating: 251it [02:43,  1.51it/s]Extractor Estimating: 252it [02:43,  1.56it/s]Extractor Estimating: 253it [02:44,  1.51it/s]Extractor Estimating: 254it [02:45,  1.49it/s]Extractor Estimating: 255it [02:45,  1.51it/s]Extractor Estimating: 256it [02:46,  1.54it/s]Extractor Estimating: 257it [02:46,  1.53it/s]Extractor Estimating: 258it [02:47,  1.39it/s]Extractor Estimating: 259it [02:48,  1.46it/s]Extractor Estimating: 260it [02:49,  1.52it/s]Extractor Estimating: 261it [02:49,  1.44it/s]Extractor Estimating: 262it [02:50,  1.50it/s]Extractor Estimating: 263it [02:51,  1.47it/s]Extractor Estimating: 264it [02:51,  1.49it/s]Extractor Estimating: 265it [02:52,  1.51it/s]Extractor Estimating: 266it [02:53,  1.49it/s]Extractor Estimating: 267it [02:53,  1.51it/s]Extractor Estimating: 268it [02:54,  1.50it/s]Extractor Estimating: 269it [02:55,  1.48it/s]Extractor Estimating: 270it [02:55,  1.45it/s]Extractor Estimating: 271it [02:56,  1.46it/s]Extractor Estimating: 272it [02:57,  1.49it/s]Extractor Estimating: 273it [02:57,  1.47it/s]Extractor Estimating: 274it [02:58,  1.45it/s]Extractor Estimating: 275it [02:59,  1.48it/s]Extractor Estimating: 276it [02:59,  1.52it/s]Extractor Estimating: 277it [03:00,  1.49it/s]Extractor Estimating: 278it [03:01,  1.52it/s]Extractor Estimating: 279it [03:01,  1.49it/s]Extractor Estimating: 280it [03:02,  1.44it/s]Extractor Estimating: 281it [03:03,  1.38it/s]Extractor Estimating: 282it [03:04,  1.44it/s]Extractor Estimating: 283it [03:04,  1.48it/s]Extractor Estimating: 284it [03:05,  1.51it/s]Extractor Estimating: 285it [03:05,  1.49it/s]Extractor Estimating: 286it [03:06,  1.52it/s]Extractor Estimating: 287it [03:07,  1.54it/s]Extractor Estimating: 288it [03:07,  1.56it/s]Extractor Estimating: 289it [03:08,  1.57it/s]Extractor Estimating: 290it [03:09,  1.49it/s]Extractor Estimating: 291it [03:09,  1.51it/s]Extractor Estimating: 292it [03:10,  1.49it/s]Extractor Estimating: 293it [03:11,  1.53it/s]Extractor Estimating: 294it [03:11,  1.50it/s]Extractor Estimating: 295it [03:12,  1.53it/s]Extractor Estimating: 296it [03:13,  1.48it/s]Extractor Estimating: 297it [03:13,  1.52it/s]Extractor Estimating: 298it [03:14,  1.51it/s]Extractor Estimating: 299it [03:15,  1.52it/s]Extractor Estimating: 300it [03:15,  1.52it/s]Extractor Estimating: 301it [03:16,  1.50it/s]Extractor Estimating: 302it [03:17,  1.51it/s]Extractor Estimating: 303it [03:17,  1.48it/s]Extractor Estimating: 304it [03:18,  1.47it/s]Extractor Estimating: 305it [03:19,  1.45it/s]Extractor Estimating: 306it [03:19,  1.50it/s]Extractor Estimating: 307it [03:20,  1.49it/s]Extractor Estimating: 308it [03:21,  1.46it/s]Extractor Estimating: 309it [03:21,  1.52it/s]Extractor Estimating: 310it [03:22,  1.49it/s]Extractor Estimating: 311it [03:23,  1.53it/s]Extractor Estimating: 312it [03:23,  1.52it/s]Extractor Estimating: 313it [03:24,  1.51it/s]Extractor Estimating: 314it [03:25,  1.54it/s]Extractor Estimating: 315it [03:25,  1.58it/s]Extractor Estimating: 316it [03:26,  1.55it/s]Extractor Estimating: 317it [03:27,  1.58it/s]Extractor Estimating: 318it [03:27,  1.52it/s]Extractor Estimating: 319it [03:28,  1.56it/s]Extractor Estimating: 320it [03:28,  1.59it/s]Extractor Estimating: 321it [03:29,  1.50it/s]Extractor Estimating: 322it [03:30,  1.53it/s]Extractor Estimating: 323it [03:30,  1.57it/s]Extractor Estimating: 324it [03:31,  1.57it/s]Extractor Estimating: 325it [03:32,  1.58it/s]Extractor Estimating: 326it [03:32,  1.62it/s]Extractor Estimating: 327it [03:33,  1.63it/s]Extractor Estimating: 328it [03:33,  1.63it/s]Extractor Estimating: 329it [03:34,  1.64it/s]Extractor Estimating: 330it [03:35,  1.61it/s]Extractor Estimating: 331it [03:35,  1.57it/s]Extractor Estimating: 332it [03:36,  1.53it/s]Extractor Estimating: 333it [03:37,  1.58it/s]Extractor Estimating: 334it [03:37,  1.56it/s]Extractor Estimating: 335it [03:38,  1.59it/s]Extractor Estimating: 336it [03:39,  1.61it/s]Extractor Estimating: 337it [03:39,  1.66it/s]Extractor Estimating: 338it [03:40,  1.65it/s]Extractor Estimating: 339it [03:40,  1.52it/s]Extractor Estimating: 340it [03:41,  1.57it/s]Extractor Estimating: 341it [03:42,  1.65it/s]Extractor Estimating: 342it [03:42,  1.65it/s]Extractor Estimating: 343it [03:43,  1.68it/s]Extractor Estimating: 344it [03:43,  1.63it/s]Extractor Estimating: 345it [03:44,  1.67it/s]Extractor Estimating: 346it [03:45,  1.60it/s]Extractor Estimating: 347it [03:45,  1.57it/s]Extractor Estimating: 348it [03:46,  1.58it/s]Extractor Estimating: 349it [03:47,  1.50it/s]Extractor Estimating: 350it [03:47,  1.52it/s]Extractor Estimating: 351it [03:48,  1.51it/s]Extractor Estimating: 352it [03:49,  1.39it/s]Extractor Estimating: 353it [03:49,  1.47it/s]Extractor Estimating: 354it [03:50,  1.45it/s]Extractor Estimating: 355it [03:51,  1.49it/s]Extractor Estimating: 356it [03:51,  1.56it/s]Extractor Estimating: 357it [03:52,  1.55it/s]Extractor Estimating: 358it [03:53,  1.55it/s]Extractor Estimating: 359it [03:53,  1.57it/s]Extractor Estimating: 360it [03:54,  1.41it/s]Extractor Estimating: 361it [03:55,  1.37it/s]Extractor Estimating: 362it [03:56,  1.44it/s]Extractor Estimating: 363it [03:56,  1.43it/s]Extractor Estimating: 364it [03:57,  1.45it/s]Extractor Estimating: 365it [03:58,  1.46it/s]Extractor Estimating: 366it [03:58,  1.47it/s]Extractor Estimating: 367it [03:59,  1.44it/s]Extractor Estimating: 368it [04:00,  1.45it/s]Extractor Estimating: 369it [04:00,  1.50it/s]Extractor Estimating: 370it [04:01,  1.47it/s]Extractor Estimating: 371it [04:02,  1.51it/s]Extractor Estimating: 372it [04:02,  1.55it/s]Extractor Estimating: 373it [04:03,  1.54it/s]Extractor Estimating: 374it [04:03,  1.60it/s]Extractor Estimating: 375it [04:04,  1.59it/s]Extractor Estimating: 376it [04:05,  1.52it/s]Extractor Estimating: 377it [04:05,  1.51it/s]Extractor Estimating: 378it [04:06,  1.50it/s]Extractor Estimating: 379it [04:07,  1.49it/s]Extractor Estimating: 380it [04:07,  1.55it/s]Extractor Estimating: 381it [04:08,  1.58it/s]Extractor Estimating: 382it [04:09,  1.53it/s]Extractor Estimating: 383it [04:09,  1.52it/s]Extractor Estimating: 384it [04:10,  1.58it/s]Extractor Estimating: 385it [04:11,  1.54it/s]Extractor Estimating: 386it [04:11,  1.54it/s]Extractor Estimating: 387it [04:12,  1.51it/s]Extractor Estimating: 388it [04:13,  1.54it/s]Extractor Estimating: 389it [04:13,  1.51it/s]Extractor Estimating: 390it [04:14,  1.58it/s]Extractor Estimating: 391it [04:15,  1.53it/s]Extractor Estimating: 392it [04:15,  1.52it/s]Extractor Estimating: 393it [04:16,  1.56it/s]Extractor Estimating: 394it [04:17,  1.55it/s]Extractor Estimating: 395it [04:17,  1.55it/s]Extractor Estimating: 396it [04:18,  1.52it/s]Extractor Estimating: 397it [04:18,  1.55it/s]Extractor Estimating: 398it [04:19,  1.55it/s]Extractor Estimating: 399it [04:20,  1.52it/s]Extractor Estimating: 400it [04:20,  1.54it/s]Extractor Estimating: 401it [04:21,  1.51it/s]Extractor Estimating: 402it [04:22,  1.56it/s]Extractor Estimating: 403it [04:22,  1.54it/s]Extractor Estimating: 404it [04:23,  1.55it/s]Extractor Estimating: 405it [04:24,  1.59it/s]Extractor Estimating: 406it [04:24,  1.58it/s]Extractor Estimating: 407it [04:25,  1.55it/s]Extractor Estimating: 408it [04:26,  1.59it/s]Extractor Estimating: 409it [04:26,  1.60it/s]Extractor Estimating: 410it [04:27,  1.58it/s]Extractor Estimating: 411it [04:27,  1.54it/s]Extractor Estimating: 412it [04:28,  1.45it/s]Extractor Estimating: 413it [04:29,  1.50it/s]Extractor Estimating: 414it [04:29,  1.56it/s]Extractor Estimating: 415it [04:30,  1.59it/s]Extractor Estimating: 416it [04:31,  1.56it/s]Extractor Estimating: 417it [04:31,  1.59it/s]Extractor Estimating: 418it [04:32,  1.56it/s]Extractor Estimating: 419it [04:33,  1.54it/s]Extractor Estimating: 420it [04:33,  1.59it/s]Extractor Estimating: 421it [04:34,  1.48it/s]Extractor Estimating: 422it [04:35,  1.47it/s]Extractor Estimating: 423it [04:35,  1.49it/s]Extractor Estimating: 424it [04:36,  1.52it/s]Extractor Estimating: 425it [04:37,  1.60it/s]Extractor Estimating: 426it [04:37,  1.55it/s]Extractor Estimating: 427it [04:38,  1.52it/s]Extractor Estimating: 428it [04:39,  1.48it/s]Extractor Estimating: 429it [04:40,  1.34it/s]Extractor Estimating: 430it [04:40,  1.34it/s]Extractor Estimating: 431it [04:41,  1.42it/s]Extractor Estimating: 432it [04:42,  1.37it/s]Extractor Estimating: 433it [04:42,  1.45it/s]Extractor Estimating: 434it [04:43,  1.47it/s]Extractor Estimating: 435it [04:44,  1.43it/s]Extractor Estimating: 436it [04:44,  1.40it/s]Extractor Estimating: 437it [04:45,  1.39it/s]Extractor Estimating: 438it [04:46,  1.45it/s]Extractor Estimating: 439it [04:46,  1.45it/s]Extractor Estimating: 440it [04:47,  1.49it/s]Extractor Estimating: 441it [04:48,  1.46it/s]Extractor Estimating: 442it [04:48,  1.49it/s]Extractor Estimating: 443it [04:49,  1.55it/s]Extractor Estimating: 444it [04:50,  1.54it/s]Extractor Estimating: 445it [04:50,  1.52it/s]Extractor Estimating: 446it [04:51,  1.53it/s]Extractor Estimating: 447it [04:52,  1.53it/s]Extractor Estimating: 448it [04:52,  1.54it/s]Extractor Estimating: 449it [04:53,  1.50it/s]Extractor Estimating: 450it [04:54,  1.50it/s]Extractor Estimating: 451it [04:54,  1.51it/s]Extractor Estimating: 452it [04:55,  1.53it/s]Extractor Estimating: 453it [04:56,  1.60it/s]Extractor Estimating: 454it [04:56,  1.59it/s]Extractor Estimating: 455it [04:57,  1.67it/s]Extractor Estimating: 456it [04:57,  1.59it/s]Extractor Estimating: 457it [04:58,  1.61it/s]Extractor Estimating: 458it [04:59,  1.65it/s]Extractor Estimating: 459it [04:59,  1.68it/s]Extractor Estimating: 460it [05:00,  1.64it/s]Extractor Estimating: 461it [05:00,  1.66it/s]Extractor Estimating: 462it [05:01,  1.68it/s]Extractor Estimating: 463it [05:02,  1.72it/s]Extractor Estimating: 464it [05:02,  1.74it/s]Extractor Estimating: 465it [05:03,  1.72it/s]Extractor Estimating: 466it [05:03,  1.72it/s]Extractor Estimating: 467it [05:04,  1.72it/s]Extractor Estimating: 468it [05:04,  1.72it/s]Extractor Estimating: 469it [05:05,  1.72it/s]Extractor Estimating: 470it [05:06,  1.71it/s]Extractor Estimating: 471it [05:06,  1.70it/s]Extractor Estimating: 472it [05:07,  1.65it/s]Extractor Estimating: 473it [05:07,  1.61it/s]Extractor Estimating: 474it [05:08,  1.64it/s]Extractor Estimating: 475it [05:09,  1.58it/s]Extractor Estimating: 476it [05:09,  1.63it/s]Extractor Estimating: 477it [05:10,  1.53it/s]Extractor Estimating: 478it [05:11,  1.52it/s]Extractor Estimating: 479it [05:11,  1.47it/s]Extractor Estimating: 480it [05:12,  1.50it/s]Extractor Estimating: 481it [05:13,  1.53it/s]Extractor Estimating: 482it [05:13,  1.53it/s]Extractor Estimating: 483it [05:14,  1.52it/s]Extractor Estimating: 484it [05:15,  1.52it/s]Extractor Estimating: 485it [05:15,  1.53it/s]Extractor Estimating: 486it [05:16,  1.54it/s]Extractor Estimating: 487it [05:17,  1.57it/s]Extractor Estimating: 488it [05:17,  1.59it/s]Extractor Estimating: 489it [05:18,  1.53it/s]Extractor Estimating: 490it [05:19,  1.44it/s]Extractor Estimating: 491it [05:19,  1.45it/s]Extractor Estimating: 492it [05:20,  1.46it/s]Extractor Estimating: 493it [05:21,  1.48it/s]Extractor Estimating: 494it [05:21,  1.48it/s]Extractor Estimating: 495it [05:22,  1.52it/s]Extractor Estimating: 496it [05:23,  1.47it/s]Extractor Estimating: 497it [05:23,  1.49it/s]Extractor Estimating: 498it [05:24,  1.47it/s]Extractor Estimating: 499it [05:25,  1.55it/s]Extractor Estimating: 500it [05:25,  1.88it/s]Extractor Estimating: 500it [05:25,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:06,292 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:06,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:06,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:06,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:06,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:14:07,021 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:14:07,022 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:14:07,608 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:14:08,682 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:14:08,682 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:11,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:11,621 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:11,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:11,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:14:11,622 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:14:12,257 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:14:12,258 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:14:12,845 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:14:13,020 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:14:13,020 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:24:45,481 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:24:45,525 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 10570 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 27731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27831, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.072, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.097, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.115, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.091, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 59, avg_time 1.088, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 159, avg_time 2.146, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 259, avg_time 1.089, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 359, avg_time 1.085, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 18, avg_time 1.094, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 118, avg_time 1.088, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 218, avg_time 2.152, loss:nan
g_step 1200, step 318, avg_time 1.101, loss:nan
g_step 1300, step 418, avg_time 1.096, loss:nan
g_step 1400, step 77, avg_time 1.093, loss:nan
g_step 1500, step 177, avg_time 1.091, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 277, avg_time 2.173, loss:nan
g_step 1700, step 377, avg_time 1.078, loss:nan
g_step 1800, step 36, avg_time 1.086, loss:nan
g_step 1900, step 136, avg_time 1.098, loss:nan
g_step 2000, step 236, avg_time 1.089, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 336, avg_time 2.158, loss:nan
g_step 2200, step 436, avg_time 1.091, loss:nan
g_step 2300, step 95, avg_time 1.093, loss:nan
g_step 2400, step 195, avg_time 1.071, loss:nan
g_step 2500, step 295, avg_time 1.089, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 395, avg_time 2.157, loss:nan
g_step 2700, step 54, avg_time 1.112, loss:nan
g_step 2800, step 154, avg_time 1.084, loss:nan
g_step 2900, step 254, avg_time 1.082, loss:nan
g_step 3000, step 354, avg_time 1.101, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 13, avg_time 2.167, loss:nan
g_step 3200, step 113, avg_time 1.090, loss:nan
g_step 3300, step 213, avg_time 1.116, loss:nan
g_step 3400, step 313, avg_time 1.079, loss:nan
g_step 3500, step 413, avg_time 1.090, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 72, avg_time 2.179, loss:nan
g_step 3700, step 172, avg_time 1.094, loss:nan
g_step 3800, step 272, avg_time 1.080, loss:nan
g_step 3900, step 372, avg_time 1.080, loss:nan
g_step 4000, step 31, avg_time 1.079, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 131, avg_time 2.127, loss:nan
g_step 4200, step 231, avg_time 1.097, loss:nan
g_step 4300, step 331, avg_time 1.107, loss:nan
g_step 4400, step 431, avg_time 1.091, loss:nan
g_step 4500, step 90, avg_time 1.076, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 190, avg_time 2.158, loss:nan
g_step 4700, step 290, avg_time 1.097, loss:nan
g_step 4800, step 390, avg_time 1.077, loss:nan
g_step 4900, step 49, avg_time 1.093, loss:nan
g_step 5000, step 149, avg_time 1.094, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 249, avg_time 2.169, loss:nan
g_step 5200, step 349, avg_time 1.083, loss:nan
g_step 5300, step 8, avg_time 1.081, loss:nan
g_step 5400, step 108, avg_time 1.080, loss:nan
g_step 5500, step 208, avg_time 1.097, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 308, avg_time 2.163, loss:nan
g_step 5700, step 408, avg_time 1.091, loss:nan
g_step 5800, step 67, avg_time 1.084, loss:nan
g_step 5900, step 167, avg_time 1.088, loss:nan
g_step 6000, step 267, avg_time 1.095, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 367, avg_time 2.141, loss:nan
g_step 6200, step 26, avg_time 1.105, loss:nan
g_step 6300, step 126, avg_time 1.096, loss:nan
g_step 6400, step 226, avg_time 1.083, loss:nan
g_step 6500, step 326, avg_time 1.099, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 426, avg_time 2.150, loss:nan
g_step 6700, step 85, avg_time 1.075, loss:nan
g_step 6800, step 185, avg_time 1.088, loss:nan
g_step 6900, step 285, avg_time 1.080, loss:nan
g_step 7000, step 385, avg_time 1.104, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 44, avg_time 2.149, loss:nan
g_step 7200, step 144, avg_time 1.089, loss:nan
g_step 7300, step 244, avg_time 1.107, loss:nan
g_step 7400, step 344, avg_time 1.078, loss:nan
g_step 7500, step 3, avg_time 1.086, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 103, avg_time 2.168, loss:nan
g_step 7700, step 203, avg_time 1.083, loss:nan
g_step 7800, step 303, avg_time 1.072, loss:nan
g_step 7900, step 403, avg_time 1.096, loss:nan
g_step 8000, step 62, avg_time 1.087, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 162, avg_time 2.153, loss:nan
g_step 8200, step 262, avg_time 1.102, loss:nan
g_step 8300, step 362, avg_time 1.082, loss:nan
g_step 8400, step 21, avg_time 1.083, loss:nan
g_step 8500, step 121, avg_time 1.088, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 221, avg_time 2.136, loss:nan
g_step 8700, step 321, avg_time 1.083, loss:nan
g_step 8800, step 421, avg_time 1.091, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:24:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:24:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-24-45_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:24:46 - WARNING - datasets.builder -   Using custom data configuration default-fe8ff58dee5fa165
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-fe8ff58dee5fa165/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:24:49,271 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:24:49,299 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:24:49,300 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:24:49,301 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:24:49,403 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:24:49,479 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:24:49,981 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:24:53,119 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:24:53,160 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-fe8ff58dee5fa165/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.54ba/s] 18%|        | 2/11 [00:00<00:02,  3.50ba/s] 27%|       | 3/11 [00:00<00:02,  3.93ba/s] 36%|      | 4/11 [00:01<00:01,  4.15ba/s] 45%|     | 5/11 [00:01<00:01,  4.29ba/s] 55%|    | 6/11 [00:01<00:01,  4.38ba/s] 64%|   | 7/11 [00:01<00:00,  4.45ba/s] 73%|  | 8/11 [00:01<00:00,  4.48ba/s] 82%| | 9/11 [00:02<00:00,  4.47ba/s] 91%| | 10/11 [00:02<00:00,  4.47ba/s]100%|| 11/11 [00:02<00:00,  5.08ba/s]100%|| 11/11 [00:02<00:00,  4.40ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.34ba/s] 50%|     | 2/4 [00:00<00:00,  2.89ba/s] 75%|  | 3/4 [00:00<00:00,  3.46ba/s]100%|| 4/4 [00:01<00:00,  4.55ba/s]100%|| 4/4 [00:01<00:00,  3.95ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:01,  5.72ba/s] 27%|       | 3/11 [00:00<00:00,  8.49ba/s] 45%|     | 5/11 [00:00<00:00,  9.30ba/s] 64%|   | 7/11 [00:00<00:00,  9.64ba/s] 82%| | 9/11 [00:00<00:00,  9.85ba/s]100%|| 11/11 [00:01<00:00, 10.66ba/s]100%|| 11/11 [00:01<00:00,  9.83ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.03ba/s] 75%|  | 3/4 [00:00<00:00,  8.97ba/s]100%|| 4/4 [00:00<00:00, 10.05ba/s]
[INFO|trainer.py:414] 2023-08-28 10:24:59,094 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:24:59,148 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:24:59,173 >>   Num examples = 10600
[INFO|trainer.py:1149] 2023-08-28 10:24:59,173 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:24:59,173 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:24:59,173 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:24:59,173 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:24:59,173 >>   Total optimization steps = 830
  0%|          | 0/830 [00:00<?, ?it/s]  0%|          | 1/830 [00:00<04:03,  3.41it/s]  0%|          | 2/830 [00:00<03:53,  3.54it/s]  0%|          | 3/830 [00:00<03:50,  3.58it/s]  0%|          | 4/830 [00:01<03:49,  3.60it/s]  1%|          | 5/830 [00:01<03:48,  3.61it/s]  1%|          | 6/830 [00:01<03:48,  3.61it/s]  1%|          | 7/830 [00:01<03:48,  3.60it/s]  1%|          | 8/830 [00:02<03:48,  3.59it/s]  1%|          | 9/830 [00:02<03:48,  3.59it/s]  1%|          | 10/830 [00:02<03:48,  3.59it/s]  1%|         | 11/830 [00:03<03:48,  3.59it/s]  1%|         | 12/830 [00:03<03:54,  3.49it/s]  2%|         | 13/830 [00:03<03:52,  3.52it/s]  2%|         | 14/830 [00:03<03:50,  3.54it/s]  2%|         | 15/830 [00:04<03:49,  3.55it/s]  2%|         | 16/830 [00:04<03:48,  3.56it/s]  2%|         | 17/830 [00:04<03:47,  3.57it/s]  2%|         | 18/830 [00:05<03:47,  3.57it/s]  2%|         | 19/830 [00:05<03:46,  3.57it/s]  2%|         | 20/830 [00:05<03:46,  3.58it/s]  3%|         | 21/830 [00:05<03:46,  3.58it/s]  3%|         | 22/830 [00:06<03:45,  3.58it/s]  3%|         | 23/830 [00:06<03:51,  3.49it/s]  3%|         | 24/830 [00:06<03:49,  3.51it/s]  3%|         | 25/830 [00:07<03:47,  3.53it/s]  3%|         | 26/830 [00:07<03:46,  3.55it/s]  3%|         | 27/830 [00:07<03:45,  3.56it/s]  3%|         | 28/830 [00:07<03:45,  3.56it/s]  3%|         | 29/830 [00:08<03:44,  3.57it/s]  4%|         | 30/830 [00:08<03:44,  3.57it/s]  4%|         | 31/830 [00:08<03:43,  3.57it/s]  4%|         | 32/830 [00:08<03:43,  3.57it/s]  4%|         | 33/830 [00:09<03:43,  3.57it/s]  4%|         | 34/830 [00:09<03:47,  3.50it/s]  4%|         | 35/830 [00:09<03:45,  3.52it/s]  4%|         | 36/830 [00:10<03:44,  3.54it/s]  4%|         | 37/830 [00:10<03:43,  3.54it/s]  5%|         | 38/830 [00:10<03:42,  3.55it/s]  5%|         | 39/830 [00:10<03:42,  3.56it/s]  5%|         | 40/830 [00:11<03:41,  3.56it/s]  5%|         | 41/830 [00:11<03:41,  3.57it/s]  5%|         | 42/830 [00:11<03:40,  3.57it/s]  5%|         | 43/830 [00:12<03:40,  3.57it/s]  5%|         | 44/830 [00:12<03:40,  3.57it/s]  5%|         | 45/830 [00:12<03:45,  3.48it/s]  6%|         | 46/830 [00:12<03:43,  3.51it/s]  6%|         | 47/830 [00:13<03:41,  3.53it/s]  6%|         | 48/830 [00:13<03:40,  3.54it/s]  6%|         | 49/830 [00:13<03:39,  3.55it/s]  6%|         | 50/830 [00:14<03:39,  3.56it/s]  6%|         | 51/830 [00:14<03:38,  3.57it/s]  6%|         | 52/830 [00:14<03:37,  3.57it/s]  6%|         | 53/830 [00:14<03:37,  3.57it/s]  7%|         | 54/830 [00:15<03:37,  3.57it/s]  7%|         | 55/830 [00:15<03:36,  3.57it/s]  7%|         | 56/830 [00:15<03:41,  3.50it/s]  7%|         | 57/830 [00:16<03:40,  3.51it/s]  7%|         | 58/830 [00:16<03:38,  3.53it/s]  7%|         | 59/830 [00:16<03:37,  3.55it/s]  7%|         | 60/830 [00:16<03:36,  3.55it/s]  7%|         | 61/830 [00:17<03:35,  3.57it/s]  7%|         | 62/830 [00:17<03:34,  3.59it/s]  8%|         | 63/830 [00:17<03:33,  3.60it/s]  8%|         | 64/830 [00:17<03:32,  3.61it/s]  8%|         | 65/830 [00:18<03:31,  3.62it/s]  8%|         | 66/830 [00:18<03:31,  3.62it/s]  8%|         | 67/830 [00:18<03:36,  3.53it/s]  8%|         | 68/830 [00:19<03:34,  3.56it/s]  8%|         | 69/830 [00:19<03:32,  3.58it/s]  8%|         | 70/830 [00:19<03:31,  3.59it/s]  9%|         | 71/830 [00:19<03:30,  3.60it/s]  9%|         | 72/830 [00:20<03:29,  3.61it/s]  9%|         | 73/830 [00:20<03:29,  3.62it/s]  9%|         | 74/830 [00:20<03:28,  3.62it/s]  9%|         | 75/830 [00:21<03:28,  3.62it/s]  9%|         | 76/830 [00:21<03:27,  3.63it/s]  9%|         | 77/830 [00:21<03:27,  3.63it/s]  9%|         | 78/830 [00:21<03:27,  3.63it/s] 10%|         | 79/830 [00:22<03:27,  3.63it/s] 10%|         | 80/830 [00:22<03:26,  3.63it/s] 10%|         | 81/830 [00:22<03:26,  3.63it/s] 10%|         | 82/830 [00:22<03:26,  3.63it/s] 10%|         | 83/830 [00:23<03:25,  3.63it/s] 10%|         | 84/830 [00:23<03:25,  3.63it/s] 10%|         | 85/830 [00:23<03:34,  3.48it/s] 10%|         | 86/830 [00:24<03:31,  3.52it/s] 10%|         | 87/830 [00:24<03:29,  3.55it/s] 11%|         | 88/830 [00:24<03:27,  3.57it/s] 11%|         | 89/830 [00:24<03:26,  3.59it/s] 11%|         | 90/830 [00:25<03:25,  3.60it/s] 11%|         | 91/830 [00:25<03:25,  3.60it/s] 11%|         | 92/830 [00:25<03:24,  3.61it/s] 11%|         | 93/830 [00:26<03:24,  3.61it/s] 11%|        | 94/830 [00:26<03:23,  3.61it/s] 11%|        | 95/830 [00:26<03:23,  3.62it/s] 12%|        | 96/830 [00:26<03:28,  3.53it/s] 12%|        | 97/830 [00:27<03:26,  3.55it/s] 12%|        | 98/830 [00:27<03:24,  3.57it/s] 12%|        | 99/830 [00:27<03:23,  3.59it/s] 12%|        | 100/830 [00:28<03:22,  3.60it/s] 12%|        | 101/830 [00:28<03:22,  3.61it/s] 12%|        | 102/830 [00:28<03:21,  3.61it/s] 12%|        | 103/830 [00:28<03:21,  3.61it/s] 13%|        | 104/830 [00:29<03:20,  3.61it/s] 13%|        | 105/830 [00:29<03:20,  3.61it/s] 13%|        | 106/830 [00:29<03:20,  3.61it/s] 13%|        | 107/830 [00:29<03:23,  3.55it/s] 13%|        | 108/830 [00:30<03:22,  3.57it/s] 13%|        | 109/830 [00:30<03:21,  3.58it/s] 13%|        | 110/830 [00:30<03:20,  3.59it/s] 13%|        | 111/830 [00:31<03:19,  3.60it/s] 13%|        | 112/830 [00:31<03:18,  3.61it/s] 14%|        | 113/830 [00:31<03:18,  3.61it/s] 14%|        | 114/830 [00:31<03:18,  3.61it/s] 14%|        | 115/830 [00:32<03:17,  3.61it/s] 14%|        | 116/830 [00:32<03:17,  3.61it/s] 14%|        | 117/830 [00:32<03:17,  3.62it/s] 14%|        | 118/830 [00:33<03:19,  3.56it/s] 14%|        | 119/830 [00:33<03:19,  3.57it/s] 14%|        | 120/830 [00:33<03:18,  3.58it/s] 15%|        | 121/830 [00:33<03:17,  3.59it/s] 15%|        | 122/830 [00:34<03:16,  3.60it/s] 15%|        | 123/830 [00:34<03:15,  3.61it/s] 15%|        | 124/830 [00:34<03:15,  3.61it/s] 15%|        | 125/830 [00:34<03:14,  3.62it/s] 15%|        | 126/830 [00:35<03:14,  3.62it/s] 15%|        | 127/830 [00:35<03:14,  3.62it/s] 15%|        | 128/830 [00:35<03:13,  3.62it/s] 16%|        | 129/830 [00:36<03:19,  3.51it/s] 16%|        | 130/830 [00:36<03:17,  3.54it/s] 16%|        | 131/830 [00:36<03:15,  3.57it/s] 16%|        | 132/830 [00:36<03:14,  3.58it/s] 16%|        | 133/830 [00:37<03:14,  3.59it/s] 16%|        | 134/830 [00:37<03:13,  3.59it/s] 16%|        | 135/830 [00:37<03:12,  3.60it/s] 16%|        | 136/830 [00:38<03:12,  3.61it/s] 17%|        | 137/830 [00:38<03:11,  3.61it/s] 17%|        | 138/830 [00:38<03:11,  3.61it/s] 17%|        | 139/830 [00:38<03:11,  3.62it/s] 17%|        | 140/830 [00:39<03:16,  3.51it/s] 17%|        | 141/830 [00:39<03:14,  3.54it/s] 17%|        | 142/830 [00:39<03:13,  3.56it/s] 17%|        | 143/830 [00:39<03:11,  3.58it/s] 17%|        | 144/830 [00:40<03:10,  3.59it/s] 17%|        | 145/830 [00:40<03:10,  3.60it/s] 18%|        | 146/830 [00:40<03:09,  3.60it/s] 18%|        | 147/830 [00:41<03:09,  3.61it/s] 18%|        | 148/830 [00:41<03:08,  3.61it/s] 18%|        | 149/830 [00:41<03:08,  3.61it/s] 18%|        | 150/830 [00:41<03:08,  3.62it/s] 18%|        | 151/830 [00:42<03:12,  3.54it/s] 18%|        | 152/830 [00:42<03:10,  3.56it/s] 18%|        | 153/830 [00:42<03:09,  3.58it/s] 19%|        | 154/830 [00:43<03:08,  3.59it/s] 19%|        | 155/830 [00:43<03:07,  3.60it/s] 19%|        | 156/830 [00:43<03:06,  3.61it/s] 19%|        | 157/830 [00:43<03:06,  3.61it/s] 19%|        | 158/830 [00:44<03:06,  3.61it/s] 19%|        | 159/830 [00:44<03:05,  3.61it/s] 19%|        | 160/830 [00:44<03:05,  3.62it/s] 19%|        | 161/830 [00:44<03:04,  3.62it/s] 20%|        | 162/830 [00:45<03:12,  3.47it/s] 20%|        | 163/830 [00:45<03:09,  3.51it/s] 20%|        | 164/830 [00:45<03:08,  3.54it/s] 20%|        | 165/830 [00:46<03:06,  3.57it/s] 20%|        | 166/830 [00:46<02:47,  3.97it/s][INFO|trainer.py:2140] 2023-08-28 10:25:45,482 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:25:45,482 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:25:45,482 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.45it/s][A
  3%|         | 12/437 [00:00<00:08, 49.63it/s][A
  4%|         | 18/437 [00:00<00:08, 47.52it/s][A
  5%|         | 23/437 [00:00<00:08, 46.55it/s][A
  6%|         | 28/437 [00:00<00:08, 45.88it/s][A
  8%|         | 33/437 [00:00<00:08, 45.44it/s][A
  9%|         | 38/437 [00:00<00:08, 45.24it/s][A
 10%|         | 43/437 [00:00<00:08, 45.06it/s][A
 11%|         | 48/437 [00:01<00:08, 45.17it/s][A
 12%|        | 53/437 [00:01<00:08, 45.31it/s][A
 13%|        | 58/437 [00:01<00:08, 45.42it/s][A
 14%|        | 63/437 [00:01<00:08, 45.39it/s][A
 16%|        | 68/437 [00:01<00:08, 45.32it/s][A
 17%|        | 73/437 [00:01<00:08, 45.12it/s][A
 18%|        | 78/437 [00:01<00:07, 44.94it/s][A
 19%|        | 83/437 [00:01<00:08, 41.25it/s][A
 20%|        | 88/437 [00:01<00:08, 42.42it/s][A
 21%|       | 93/437 [00:02<00:07, 43.26it/s][A
 22%|       | 98/437 [00:02<00:07, 43.88it/s][A
 24%|       | 103/437 [00:02<00:07, 44.28it/s][A
 25%|       | 108/437 [00:02<00:07, 44.65it/s][A
 26%|       | 113/437 [00:02<00:07, 44.85it/s][A
 27%|       | 118/437 [00:02<00:07, 45.00it/s][A
 28%|       | 123/437 [00:02<00:07, 44.78it/s][A
 29%|       | 128/437 [00:02<00:06, 44.69it/s][A
 30%|       | 133/437 [00:02<00:06, 44.76it/s][A
 32%|      | 138/437 [00:03<00:06, 45.03it/s][A
 33%|      | 143/437 [00:03<00:06, 45.17it/s][A
 34%|      | 148/437 [00:03<00:06, 45.20it/s][A
 35%|      | 153/437 [00:03<00:06, 45.15it/s][A
 36%|      | 158/437 [00:03<00:06, 44.93it/s][A
 37%|      | 163/437 [00:03<00:06, 45.14it/s][A
 38%|      | 168/437 [00:03<00:06, 39.21it/s][A
 40%|      | 173/437 [00:03<00:06, 41.00it/s][A
 41%|      | 178/437 [00:04<00:06, 42.16it/s][A
 42%|     | 183/437 [00:04<00:05, 43.14it/s][A
 43%|     | 188/437 [00:04<00:05, 43.80it/s][A
 44%|     | 193/437 [00:04<00:05, 44.28it/s][A
 45%|     | 198/437 [00:04<00:05, 44.55it/s][A
 46%|     | 203/437 [00:04<00:05, 44.84it/s][A
 48%|     | 208/437 [00:04<00:05, 44.64it/s][A
 49%|     | 213/437 [00:04<00:05, 44.57it/s][A
 50%|     | 218/437 [00:04<00:05, 39.71it/s][A
 51%|     | 223/437 [00:05<00:05, 41.23it/s][A
 52%|    | 228/437 [00:05<00:04, 42.52it/s][A
 53%|    | 233/437 [00:05<00:04, 43.44it/s][A
 54%|    | 238/437 [00:05<00:04, 44.01it/s][A
 56%|    | 243/437 [00:05<00:04, 44.53it/s][A
 57%|    | 248/437 [00:05<00:04, 44.83it/s][A
 58%|    | 253/437 [00:05<00:04, 44.81it/s][A
 59%|    | 258/437 [00:05<00:04, 44.60it/s][A
 60%|    | 263/437 [00:05<00:03, 44.37it/s][A
 61%|   | 268/437 [00:06<00:05, 32.24it/s][A
 62%|   | 273/437 [00:06<00:04, 35.42it/s][A
 63%|   | 277/437 [00:06<00:07, 21.81it/s][A
 65%|   | 282/437 [00:06<00:05, 26.41it/s][A
 66%|   | 287/437 [00:06<00:04, 30.40it/s][A
 67%|   | 292/437 [00:07<00:04, 33.90it/s][A
 68%|   | 297/437 [00:07<00:03, 36.80it/s][A
 69%|   | 302/437 [00:07<00:03, 39.07it/s][A
 70%|   | 307/437 [00:07<00:03, 40.82it/s][A
 71%|  | 312/437 [00:07<00:02, 42.22it/s][A
 73%|  | 317/437 [00:07<00:02, 43.11it/s][A
 74%|  | 322/437 [00:07<00:02, 43.32it/s][A
 75%|  | 327/437 [00:07<00:02, 43.41it/s][A
 76%|  | 332/437 [00:07<00:02, 42.27it/s][A
 77%|  | 337/437 [00:08<00:02, 43.30it/s][A
 78%|  | 342/437 [00:08<00:02, 43.97it/s][A
 79%|  | 347/437 [00:08<00:02, 44.57it/s][A
 81%|  | 352/437 [00:08<00:01, 44.85it/s][A
 82%| | 357/437 [00:08<00:01, 45.14it/s][A
 83%| | 362/437 [00:08<00:01, 45.01it/s][A
 84%| | 367/437 [00:08<00:01, 44.59it/s][A
 85%| | 372/437 [00:08<00:01, 44.49it/s][A
 86%| | 377/437 [00:08<00:01, 44.45it/s][A
 87%| | 382/437 [00:09<00:01, 44.58it/s][A
 89%| | 387/437 [00:09<00:01, 44.82it/s][A
 90%| | 392/437 [00:09<00:00, 45.11it/s][A
 91%| | 397/437 [00:09<00:00, 45.20it/s][A
 92%|| 402/437 [00:09<00:00, 45.31it/s][A
 93%|| 407/437 [00:09<00:00, 45.21it/s][A
 94%|| 412/437 [00:09<00:00, 45.19it/s][A
 95%|| 417/437 [00:09<00:00, 45.06it/s][A
 97%|| 422/437 [00:09<00:00, 44.93it/s][A
 98%|| 427/437 [00:10<00:00, 44.94it/s][A
 99%|| 432/437 [00:10<00:00, 45.02it/s][A
100%|| 437/437 [00:10<00:00, 45.15it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 45.15it/s][A 20%|        | 166/830 [00:56<02:47,  3.97it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:25:55,937 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 10:25:56,139 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:25:58,742 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:25:58,898 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:25:58,955 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/830 [01:01<51:17,  4.64s/it] 20%|        | 168/830 [01:01<36:46,  3.33s/it] 20%|        | 169/830 [01:01<26:37,  2.42s/it] 20%|        | 170/830 [01:02<19:32,  1.78s/it] 21%|        | 171/830 [01:02<14:34,  1.33s/it] 21%|        | 172/830 [01:02<11:06,  1.01s/it] 21%|        | 173/830 [01:02<08:46,  1.25it/s] 21%|        | 174/830 [01:03<07:02,  1.55it/s] 21%|        | 175/830 [01:03<05:50,  1.87it/s] 21%|        | 176/830 [01:03<04:59,  2.18it/s] 21%|       | 177/830 [01:04<04:23,  2.48it/s] 21%|       | 178/830 [01:04<03:58,  2.74it/s] 22%|       | 179/830 [01:04<03:40,  2.95it/s] 22%|       | 180/830 [01:04<03:28,  3.12it/s] 22%|       | 181/830 [01:05<03:19,  3.26it/s] 22%|       | 182/830 [01:05<03:12,  3.36it/s] 22%|       | 183/830 [01:05<03:08,  3.43it/s] 22%|       | 184/830 [01:06<03:17,  3.27it/s] 22%|       | 185/830 [01:06<03:11,  3.36it/s] 22%|       | 186/830 [01:06<03:07,  3.43it/s] 23%|       | 187/830 [01:06<03:04,  3.48it/s] 23%|       | 188/830 [01:07<03:02,  3.52it/s] 23%|       | 189/830 [01:07<03:00,  3.56it/s] 23%|       | 190/830 [01:07<02:59,  3.58it/s] 23%|       | 191/830 [01:07<02:58,  3.59it/s] 23%|       | 192/830 [01:08<02:57,  3.59it/s] 23%|       | 193/830 [01:08<02:56,  3.60it/s] 23%|       | 194/830 [01:08<02:56,  3.61it/s] 23%|       | 195/830 [01:09<03:03,  3.46it/s] 24%|       | 196/830 [01:09<03:00,  3.51it/s] 24%|       | 197/830 [01:09<02:58,  3.54it/s] 24%|       | 198/830 [01:09<02:57,  3.56it/s] 24%|       | 199/830 [01:10<02:56,  3.57it/s] 24%|       | 200/830 [01:10<02:55,  3.59it/s] 24%|       | 201/830 [01:10<02:54,  3.60it/s] 24%|       | 202/830 [01:11<02:54,  3.60it/s] 24%|       | 203/830 [01:11<02:53,  3.61it/s] 25%|       | 204/830 [01:11<02:53,  3.61it/s] 25%|       | 205/830 [01:11<02:52,  3.61it/s] 25%|       | 206/830 [01:12<02:59,  3.48it/s] 25%|       | 207/830 [01:12<02:56,  3.52it/s] 25%|       | 208/830 [01:12<02:55,  3.55it/s] 25%|       | 209/830 [01:12<02:54,  3.57it/s] 25%|       | 210/830 [01:13<02:52,  3.58it/s] 25%|       | 211/830 [01:13<02:52,  3.60it/s] 26%|       | 212/830 [01:13<02:51,  3.60it/s] 26%|       | 213/830 [01:14<02:51,  3.61it/s] 26%|       | 214/830 [01:14<02:50,  3.61it/s] 26%|       | 215/830 [01:14<02:50,  3.61it/s] 26%|       | 216/830 [01:14<02:49,  3.62it/s] 26%|       | 217/830 [01:15<02:58,  3.43it/s] 26%|       | 218/830 [01:15<02:55,  3.49it/s] 26%|       | 219/830 [01:15<02:53,  3.53it/s] 27%|       | 220/830 [01:16<02:51,  3.56it/s] 27%|       | 221/830 [01:16<02:50,  3.57it/s] 27%|       | 222/830 [01:16<02:49,  3.58it/s] 27%|       | 223/830 [01:16<02:48,  3.59it/s] 27%|       | 224/830 [01:17<02:48,  3.60it/s] 27%|       | 225/830 [01:17<02:47,  3.61it/s] 27%|       | 226/830 [01:17<02:47,  3.61it/s] 27%|       | 227/830 [01:18<02:46,  3.61it/s] 27%|       | 228/830 [01:18<02:50,  3.53it/s] 28%|       | 229/830 [01:18<02:48,  3.56it/s] 28%|       | 230/830 [01:18<02:47,  3.58it/s] 28%|       | 231/830 [01:19<02:46,  3.59it/s] 28%|       | 232/830 [01:19<02:46,  3.60it/s] 28%|       | 233/830 [01:19<02:45,  3.61it/s] 28%|       | 234/830 [01:19<02:45,  3.61it/s] 28%|       | 235/830 [01:20<02:44,  3.61it/s] 28%|       | 236/830 [01:20<02:44,  3.62it/s] 29%|       | 237/830 [01:20<02:43,  3.62it/s] 29%|       | 238/830 [01:21<02:43,  3.62it/s] 29%|       | 239/830 [01:21<02:47,  3.52it/s] 29%|       | 240/830 [01:21<02:46,  3.55it/s] 29%|       | 241/830 [01:21<02:45,  3.57it/s] 29%|       | 242/830 [01:22<02:44,  3.58it/s] 29%|       | 243/830 [01:22<02:43,  3.59it/s] 29%|       | 244/830 [01:22<02:42,  3.60it/s] 30%|       | 245/830 [01:23<02:42,  3.61it/s] 30%|       | 246/830 [01:23<02:41,  3.61it/s] 30%|       | 247/830 [01:23<02:41,  3.61it/s] 30%|       | 248/830 [01:23<02:41,  3.61it/s] 30%|       | 249/830 [01:24<02:40,  3.62it/s] 30%|       | 250/830 [01:24<02:40,  3.62it/s] 30%|       | 251/830 [01:24<02:40,  3.62it/s] 30%|       | 252/830 [01:24<02:39,  3.62it/s] 30%|       | 253/830 [01:25<02:41,  3.56it/s] 31%|       | 254/830 [01:25<02:40,  3.58it/s] 31%|       | 255/830 [01:25<02:40,  3.59it/s] 31%|       | 256/830 [01:26<02:39,  3.60it/s] 31%|       | 257/830 [01:26<02:38,  3.61it/s] 31%|       | 258/830 [01:26<02:38,  3.61it/s] 31%|       | 259/830 [01:26<02:37,  3.62it/s] 31%|      | 260/830 [01:27<02:37,  3.61it/s] 31%|      | 261/830 [01:27<02:37,  3.61it/s] 32%|      | 262/830 [01:27<02:37,  3.61it/s] 32%|      | 263/830 [01:28<02:36,  3.61it/s] 32%|      | 264/830 [01:28<02:45,  3.41it/s] 32%|      | 265/830 [01:28<02:42,  3.47it/s] 32%|      | 266/830 [01:28<02:40,  3.51it/s] 32%|      | 267/830 [01:29<02:38,  3.54it/s] 32%|      | 268/830 [01:29<02:37,  3.57it/s] 32%|      | 269/830 [01:29<02:36,  3.59it/s] 33%|      | 270/830 [01:30<02:35,  3.59it/s] 33%|      | 271/830 [01:30<02:35,  3.60it/s] 33%|      | 272/830 [01:30<02:34,  3.60it/s] 33%|      | 273/830 [01:30<02:34,  3.61it/s] 33%|      | 274/830 [01:31<02:33,  3.61it/s] 33%|      | 275/830 [01:31<02:38,  3.51it/s] 33%|      | 276/830 [01:31<02:36,  3.54it/s] 33%|      | 277/830 [01:31<02:35,  3.56it/s] 33%|      | 278/830 [01:32<02:34,  3.58it/s] 34%|      | 279/830 [01:32<02:33,  3.59it/s] 34%|      | 280/830 [01:32<02:32,  3.60it/s] 34%|      | 281/830 [01:33<02:32,  3.60it/s] 34%|      | 282/830 [01:33<02:31,  3.61it/s] 34%|      | 283/830 [01:33<02:31,  3.61it/s] 34%|      | 284/830 [01:33<02:30,  3.62it/s] 34%|      | 285/830 [01:34<02:30,  3.61it/s] 34%|      | 286/830 [01:34<02:35,  3.49it/s] 35%|      | 287/830 [01:34<02:33,  3.53it/s] 35%|      | 288/830 [01:35<02:32,  3.56it/s] 35%|      | 289/830 [01:35<02:31,  3.57it/s] 35%|      | 290/830 [01:35<02:30,  3.58it/s] 35%|      | 291/830 [01:35<02:30,  3.59it/s] 35%|      | 292/830 [01:36<02:29,  3.60it/s] 35%|      | 293/830 [01:36<02:28,  3.61it/s] 35%|      | 294/830 [01:36<02:28,  3.61it/s] 36%|      | 295/830 [01:36<02:28,  3.61it/s] 36%|      | 296/830 [01:37<02:27,  3.61it/s] 36%|      | 297/830 [01:37<02:31,  3.51it/s] 36%|      | 298/830 [01:37<02:30,  3.54it/s] 36%|      | 299/830 [01:38<02:28,  3.57it/s] 36%|      | 300/830 [01:38<02:27,  3.58it/s] 36%|      | 301/830 [01:38<02:27,  3.60it/s] 36%|      | 302/830 [01:38<02:26,  3.60it/s] 37%|      | 303/830 [01:39<02:26,  3.60it/s] 37%|      | 304/830 [01:39<02:25,  3.61it/s] 37%|      | 305/830 [01:39<02:25,  3.62it/s] 37%|      | 306/830 [01:40<02:24,  3.61it/s] 37%|      | 307/830 [01:40<02:24,  3.61it/s] 37%|      | 308/830 [01:40<02:28,  3.52it/s] 37%|      | 309/830 [01:40<02:26,  3.55it/s] 37%|      | 310/830 [01:41<02:25,  3.56it/s] 37%|      | 311/830 [01:41<02:25,  3.58it/s] 38%|      | 312/830 [01:41<02:24,  3.59it/s] 38%|      | 313/830 [01:42<02:23,  3.60it/s] 38%|      | 314/830 [01:42<02:23,  3.60it/s] 38%|      | 315/830 [01:42<02:22,  3.61it/s] 38%|      | 316/830 [01:42<02:22,  3.61it/s] 38%|      | 317/830 [01:43<02:22,  3.61it/s] 38%|      | 318/830 [01:43<02:21,  3.61it/s] 38%|      | 319/830 [01:43<02:26,  3.49it/s] 39%|      | 320/830 [01:43<02:24,  3.53it/s] 39%|      | 321/830 [01:44<02:23,  3.56it/s] 39%|      | 322/830 [01:44<02:22,  3.57it/s] 39%|      | 323/830 [01:44<02:21,  3.59it/s] 39%|      | 324/830 [01:45<02:20,  3.59it/s] 39%|      | 325/830 [01:45<02:20,  3.60it/s] 39%|      | 326/830 [01:45<02:19,  3.60it/s] 39%|      | 327/830 [01:45<02:19,  3.61it/s] 40%|      | 328/830 [01:46<02:18,  3.61it/s] 40%|      | 329/830 [01:46<02:18,  3.62it/s] 40%|      | 330/830 [01:46<02:24,  3.46it/s] 40%|      | 331/830 [01:47<02:22,  3.51it/s] 40%|      | 332/830 [01:47<02:07,  3.92it/s][INFO|trainer.py:2140] 2023-08-28 10:26:46,430 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:26:46,430 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:26:46,430 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.2493, 'eval_samples_per_second': 340.414, 'eval_steps_per_second': 42.637, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.69it/s][A
  3%|         | 12/437 [00:00<00:08, 49.54it/s][A
  4%|         | 18/437 [00:00<00:08, 47.53it/s][A
  5%|         | 23/437 [00:00<00:08, 46.68it/s][A
  6%|         | 28/437 [00:00<00:08, 46.11it/s][A
  8%|         | 33/437 [00:00<00:08, 45.71it/s][A
  9%|         | 38/437 [00:00<00:08, 45.33it/s][A
 10%|         | 43/437 [00:00<00:08, 44.98it/s][A
 11%|         | 48/437 [00:01<00:08, 44.94it/s][A
 12%|        | 53/437 [00:01<00:08, 45.12it/s][A
 13%|        | 58/437 [00:01<00:08, 45.24it/s][A
 14%|        | 63/437 [00:01<00:08, 45.31it/s][A
 16%|        | 68/437 [00:01<00:08, 45.26it/s][A
 17%|        | 73/437 [00:01<00:08, 45.17it/s][A
 18%|        | 78/437 [00:01<00:07, 45.11it/s][A
 19%|        | 83/437 [00:01<00:07, 44.93it/s][A
 20%|        | 88/437 [00:01<00:07, 44.77it/s][A
 21%|       | 93/437 [00:02<00:07, 44.72it/s][A
 22%|       | 98/437 [00:02<00:07, 44.92it/s][A
 24%|       | 103/437 [00:02<00:07, 45.02it/s][A
 25%|       | 108/437 [00:02<00:07, 41.67it/s][A
 26%|       | 113/437 [00:02<00:07, 42.86it/s][A
 27%|       | 118/437 [00:02<00:07, 43.59it/s][A
 28%|       | 123/437 [00:02<00:07, 39.53it/s][A
 29%|       | 128/437 [00:02<00:07, 41.88it/s][A
 30%|       | 133/437 [00:02<00:07, 42.99it/s][A
 32%|      | 138/437 [00:03<00:06, 43.73it/s][A
 33%|      | 143/437 [00:03<00:06, 44.11it/s][A
 34%|      | 148/437 [00:03<00:06, 44.14it/s][A
 35%|      | 153/437 [00:03<00:06, 44.44it/s][A
 36%|      | 158/437 [00:03<00:06, 44.68it/s][A
 37%|      | 163/437 [00:03<00:06, 44.82it/s][A
 38%|      | 168/437 [00:03<00:06, 44.61it/s][A
 40%|      | 173/437 [00:03<00:05, 44.69it/s][A
 41%|      | 178/437 [00:03<00:05, 44.91it/s][A
 42%|     | 183/437 [00:04<00:05, 45.08it/s][A
 43%|     | 188/437 [00:04<00:05, 45.14it/s][A
 44%|     | 193/437 [00:04<00:05, 44.98it/s][A
 45%|     | 198/437 [00:04<00:05, 44.96it/s][A
 46%|     | 203/437 [00:04<00:05, 45.05it/s][A
 48%|     | 208/437 [00:04<00:05, 45.04it/s][A
 49%|     | 213/437 [00:04<00:04, 44.93it/s][A
 50%|     | 218/437 [00:04<00:04, 44.93it/s][A
 51%|     | 223/437 [00:04<00:04, 44.97it/s][A
 52%|    | 228/437 [00:05<00:06, 32.82it/s][A
 53%|    | 232/437 [00:05<00:06, 31.69it/s][A
 54%|    | 236/437 [00:05<00:10, 19.97it/s][A
 55%|    | 241/437 [00:05<00:07, 24.59it/s][A
 56%|    | 246/437 [00:05<00:06, 28.79it/s][A
 57%|    | 251/437 [00:06<00:05, 32.46it/s][A
 59%|    | 256/437 [00:06<00:05, 35.59it/s][A
 60%|    | 261/437 [00:06<00:04, 38.08it/s][A
 61%|    | 266/437 [00:06<00:04, 40.09it/s][A
 62%|   | 271/437 [00:06<00:03, 41.55it/s][A
 63%|   | 276/437 [00:06<00:03, 42.43it/s][A
 64%|   | 281/437 [00:06<00:03, 42.94it/s][A
 65%|   | 286/437 [00:06<00:03, 43.44it/s][A
 67%|   | 291/437 [00:06<00:03, 44.06it/s][A
 68%|   | 296/437 [00:07<00:03, 44.37it/s][A
 69%|   | 301/437 [00:07<00:03, 44.64it/s][A
 70%|   | 306/437 [00:07<00:02, 44.74it/s][A
 71%|   | 311/437 [00:07<00:02, 44.97it/s][A
 72%|  | 316/437 [00:07<00:02, 45.03it/s][A
 73%|  | 321/437 [00:07<00:02, 44.90it/s][A
 75%|  | 326/437 [00:07<00:02, 44.34it/s][A
 76%|  | 331/437 [00:07<00:02, 44.81it/s][A
 77%|  | 336/437 [00:07<00:02, 44.95it/s][A
 78%|  | 341/437 [00:08<00:02, 45.00it/s][A
 79%|  | 346/437 [00:08<00:02, 45.06it/s][A
 80%|  | 351/437 [00:08<00:02, 40.77it/s][A
 81%| | 356/437 [00:08<00:01, 42.15it/s][A
 83%| | 361/437 [00:08<00:01, 43.12it/s][A
 84%| | 366/437 [00:08<00:01, 43.83it/s][A
 85%| | 371/437 [00:08<00:01, 44.22it/s][A
 86%| | 376/437 [00:08<00:01, 44.44it/s][A
 87%| | 381/437 [00:09<00:01, 44.54it/s][A
 88%| | 386/437 [00:09<00:01, 44.78it/s][A
 89%| | 391/437 [00:09<00:01, 44.58it/s][A
 91%| | 396/437 [00:09<00:00, 44.53it/s][A
 92%|| 401/437 [00:09<00:00, 44.75it/s][A
 93%|| 406/437 [00:09<00:00, 44.86it/s][A
 94%|| 411/437 [00:09<00:00, 45.02it/s][A
 95%|| 416/437 [00:09<00:00, 45.12it/s][A
 96%|| 421/437 [00:09<00:00, 45.21it/s][A
 97%|| 426/437 [00:10<00:00, 45.17it/s][A
 99%|| 431/437 [00:10<00:00, 45.09it/s][A
100%|| 436/437 [00:10<00:00, 45.00it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 45.00it/s][A 40%|      | 332/830 [01:57<02:07,  3.92it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:26:56,838 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 10:26:57,032 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:27:00,032 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:27:00,195 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:27:00,276 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/special_tokens_map.json
 40%|      | 333/830 [02:02<39:37,  4.78s/it] 40%|      | 334/830 [02:02<28:22,  3.43s/it] 40%|      | 335/830 [02:03<20:30,  2.49s/it] 40%|      | 336/830 [02:03<15:00,  1.82s/it] 41%|      | 337/830 [02:03<11:09,  1.36s/it] 41%|      | 338/830 [02:03<08:28,  1.03s/it] 41%|      | 339/830 [02:04<06:41,  1.22it/s] 41%|      | 340/830 [02:04<05:20,  1.53it/s] 41%|      | 341/830 [02:04<04:24,  1.85it/s] 41%|      | 342/830 [02:05<03:45,  2.17it/s] 41%|     | 343/830 [02:05<03:17,  2.46it/s] 41%|     | 344/830 [02:05<02:58,  2.72it/s] 42%|     | 345/830 [02:05<02:44,  2.94it/s] 42%|     | 346/830 [02:06<02:35,  3.11it/s] 42%|     | 347/830 [02:06<02:28,  3.25it/s] 42%|     | 348/830 [02:06<02:23,  3.35it/s] 42%|     | 349/830 [02:07<02:20,  3.43it/s] 42%|     | 350/830 [02:07<02:25,  3.29it/s] 42%|     | 351/830 [02:07<02:21,  3.39it/s] 42%|     | 352/830 [02:07<02:18,  3.45it/s] 43%|     | 353/830 [02:08<02:16,  3.50it/s] 43%|     | 354/830 [02:08<02:14,  3.54it/s] 43%|     | 355/830 [02:08<02:13,  3.56it/s] 43%|     | 356/830 [02:09<02:12,  3.58it/s] 43%|     | 357/830 [02:09<02:11,  3.59it/s] 43%|     | 358/830 [02:09<02:11,  3.60it/s] 43%|     | 359/830 [02:09<02:10,  3.60it/s] 43%|     | 360/830 [02:10<02:10,  3.61it/s] 43%|     | 361/830 [02:10<02:13,  3.51it/s] 44%|     | 362/830 [02:10<02:12,  3.54it/s] 44%|     | 363/830 [02:11<02:11,  3.56it/s] 44%|     | 364/830 [02:11<02:10,  3.58it/s] 44%|     | 365/830 [02:11<02:09,  3.60it/s] 44%|     | 366/830 [02:11<02:08,  3.60it/s] 44%|     | 367/830 [02:12<02:08,  3.60it/s] 44%|     | 368/830 [02:12<02:08,  3.60it/s] 44%|     | 369/830 [02:12<02:07,  3.61it/s] 45%|     | 370/830 [02:12<02:07,  3.61it/s] 45%|     | 371/830 [02:13<02:07,  3.61it/s] 45%|     | 372/830 [02:13<02:12,  3.47it/s] 45%|     | 373/830 [02:13<02:10,  3.51it/s] 45%|     | 374/830 [02:14<02:08,  3.54it/s] 45%|     | 375/830 [02:14<02:07,  3.57it/s] 45%|     | 376/830 [02:14<02:06,  3.58it/s] 45%|     | 377/830 [02:14<02:06,  3.59it/s] 46%|     | 378/830 [02:15<02:05,  3.60it/s] 46%|     | 379/830 [02:15<02:05,  3.61it/s] 46%|     | 380/830 [02:15<02:04,  3.61it/s] 46%|     | 381/830 [02:16<02:04,  3.62it/s] 46%|     | 382/830 [02:16<02:04,  3.61it/s] 46%|     | 383/830 [02:16<02:07,  3.51it/s] 46%|     | 384/830 [02:16<02:06,  3.54it/s] 46%|     | 385/830 [02:17<02:05,  3.55it/s] 47%|     | 386/830 [02:17<02:04,  3.57it/s] 47%|     | 387/830 [02:17<02:03,  3.58it/s] 47%|     | 388/830 [02:17<02:03,  3.59it/s] 47%|     | 389/830 [02:18<02:02,  3.60it/s] 47%|     | 390/830 [02:18<02:01,  3.61it/s] 47%|     | 391/830 [02:18<02:01,  3.60it/s] 47%|     | 392/830 [02:19<02:01,  3.61it/s] 47%|     | 393/830 [02:19<02:01,  3.61it/s] 47%|     | 394/830 [02:19<02:02,  3.55it/s] 48%|     | 395/830 [02:19<02:01,  3.57it/s] 48%|     | 396/830 [02:20<02:01,  3.58it/s] 48%|     | 397/830 [02:20<02:00,  3.59it/s] 48%|     | 398/830 [02:20<01:59,  3.60it/s] 48%|     | 399/830 [02:21<01:59,  3.61it/s] 48%|     | 400/830 [02:21<01:59,  3.60it/s] 48%|     | 401/830 [02:21<01:59,  3.60it/s] 48%|     | 402/830 [02:21<01:58,  3.60it/s] 49%|     | 403/830 [02:22<01:58,  3.61it/s] 49%|     | 404/830 [02:22<01:58,  3.60it/s] 49%|     | 405/830 [02:22<02:01,  3.50it/s] 49%|     | 406/830 [02:23<02:00,  3.53it/s] 49%|     | 407/830 [02:23<01:58,  3.56it/s] 49%|     | 408/830 [02:23<01:58,  3.56it/s] 49%|     | 409/830 [02:23<01:57,  3.57it/s] 49%|     | 410/830 [02:24<01:57,  3.58it/s] 50%|     | 411/830 [02:24<01:56,  3.59it/s] 50%|     | 412/830 [02:24<01:56,  3.60it/s] 50%|     | 413/830 [02:24<01:55,  3.60it/s] 50%|     | 414/830 [02:25<01:55,  3.60it/s] 50%|     | 415/830 [02:25<01:55,  3.61it/s] 50%|     | 416/830 [02:25<01:54,  3.61it/s] 50%|     | 417/830 [02:26<01:54,  3.61it/s] 50%|     | 418/830 [02:26<01:53,  3.61it/s] 50%|     | 419/830 [02:26<01:56,  3.54it/s] 51%|     | 420/830 [02:26<01:55,  3.56it/s] 51%|     | 421/830 [02:27<01:54,  3.58it/s] 51%|     | 422/830 [02:27<01:53,  3.59it/s] 51%|     | 423/830 [02:27<01:53,  3.59it/s] 51%|     | 424/830 [02:28<01:53,  3.59it/s] 51%|     | 425/830 [02:28<01:52,  3.60it/s] 51%|    | 426/830 [02:28<01:52,  3.61it/s] 51%|    | 427/830 [02:28<01:51,  3.61it/s] 52%|    | 428/830 [02:29<01:51,  3.61it/s] 52%|    | 429/830 [02:29<01:51,  3.61it/s] 52%|    | 430/830 [02:29<01:55,  3.46it/s] 52%|    | 431/830 [02:30<01:53,  3.51it/s] 52%|    | 432/830 [02:30<01:52,  3.54it/s] 52%|    | 433/830 [02:30<01:51,  3.56it/s] 52%|    | 434/830 [02:30<01:50,  3.58it/s] 52%|    | 435/830 [02:31<01:50,  3.59it/s] 53%|    | 436/830 [02:31<01:49,  3.60it/s] 53%|    | 437/830 [02:31<01:49,  3.61it/s] 53%|    | 438/830 [02:31<01:48,  3.61it/s] 53%|    | 439/830 [02:32<01:48,  3.61it/s] 53%|    | 440/830 [02:32<01:47,  3.62it/s] 53%|    | 441/830 [02:32<01:51,  3.50it/s] 53%|    | 442/830 [02:33<01:49,  3.54it/s] 53%|    | 443/830 [02:33<01:48,  3.56it/s] 53%|    | 444/830 [02:33<01:48,  3.57it/s] 54%|    | 445/830 [02:33<01:47,  3.59it/s] 54%|    | 446/830 [02:34<01:46,  3.60it/s] 54%|    | 447/830 [02:34<01:46,  3.60it/s] 54%|    | 448/830 [02:34<01:45,  3.61it/s] 54%|    | 449/830 [02:35<01:45,  3.61it/s] 54%|    | 450/830 [02:35<01:45,  3.61it/s] 54%|    | 451/830 [02:35<01:44,  3.61it/s] 54%|    | 452/830 [02:35<01:49,  3.44it/s] 55%|    | 453/830 [02:36<01:48,  3.49it/s] 55%|    | 454/830 [02:36<01:46,  3.53it/s] 55%|    | 455/830 [02:36<01:45,  3.55it/s] 55%|    | 456/830 [02:36<01:45,  3.56it/s] 55%|    | 457/830 [02:37<01:44,  3.56it/s] 55%|    | 458/830 [02:37<01:44,  3.56it/s] 55%|    | 459/830 [02:37<01:44,  3.56it/s] 55%|    | 460/830 [02:38<01:43,  3.57it/s] 56%|    | 461/830 [02:38<01:43,  3.57it/s] 56%|    | 462/830 [02:38<01:43,  3.57it/s] 56%|    | 463/830 [02:38<01:46,  3.46it/s] 56%|    | 464/830 [02:39<01:44,  3.49it/s] 56%|    | 465/830 [02:39<01:44,  3.51it/s] 56%|    | 466/830 [02:39<01:43,  3.52it/s] 56%|    | 467/830 [02:40<01:42,  3.53it/s] 56%|    | 468/830 [02:40<01:42,  3.54it/s] 57%|    | 469/830 [02:40<01:41,  3.54it/s] 57%|    | 470/830 [02:40<01:41,  3.55it/s] 57%|    | 471/830 [02:41<01:40,  3.56it/s] 57%|    | 472/830 [02:41<01:40,  3.56it/s] 57%|    | 473/830 [02:41<01:40,  3.56it/s] 57%|    | 474/830 [02:42<01:42,  3.47it/s] 57%|    | 475/830 [02:42<01:41,  3.50it/s] 57%|    | 476/830 [02:42<01:40,  3.52it/s] 57%|    | 477/830 [02:42<01:40,  3.53it/s] 58%|    | 478/830 [02:43<01:39,  3.54it/s] 58%|    | 479/830 [02:43<01:39,  3.54it/s] 58%|    | 480/830 [02:43<01:38,  3.55it/s] 58%|    | 481/830 [02:44<01:38,  3.55it/s] 58%|    | 482/830 [02:44<01:37,  3.55it/s] 58%|    | 483/830 [02:44<01:37,  3.55it/s] 58%|    | 484/830 [02:44<01:37,  3.56it/s] 58%|    | 485/830 [02:45<01:38,  3.51it/s] 59%|    | 486/830 [02:45<01:37,  3.52it/s] 59%|    | 487/830 [02:45<01:37,  3.53it/s] 59%|    | 488/830 [02:46<01:36,  3.54it/s] 59%|    | 489/830 [02:46<01:36,  3.54it/s] 59%|    | 490/830 [02:46<01:35,  3.55it/s] 59%|    | 491/830 [02:46<01:35,  3.55it/s] 59%|    | 492/830 [02:47<01:35,  3.56it/s] 59%|    | 493/830 [02:47<01:34,  3.56it/s] 60%|    | 494/830 [02:47<01:34,  3.56it/s] 60%|    | 495/830 [02:48<01:33,  3.56it/s] 60%|    | 496/830 [02:48<01:39,  3.36it/s] 60%|    | 497/830 [02:48<01:37,  3.42it/s] 60%|    | 498/830 [02:48<01:26,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 10:27:48,005 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:27:48,005 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:27:48,005 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.2796, 'eval_samples_per_second': 339.409, 'eval_steps_per_second': 42.511, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.09it/s][A
  3%|         | 12/437 [00:00<00:08, 49.24it/s][A
  4%|         | 17/437 [00:00<00:08, 47.35it/s][A
  5%|         | 22/437 [00:00<00:08, 46.54it/s][A
  6%|         | 27/437 [00:00<00:08, 45.99it/s][A
  7%|         | 32/437 [00:00<00:08, 45.52it/s][A
  8%|         | 37/437 [00:00<00:08, 45.24it/s][A
 10%|         | 42/437 [00:00<00:08, 44.86it/s][A
 11%|         | 47/437 [00:01<00:08, 44.90it/s][A
 12%|        | 52/437 [00:01<00:09, 39.57it/s][A
 13%|        | 58/437 [00:01<00:08, 42.74it/s][A
 14%|        | 63/437 [00:01<00:08, 43.54it/s][A
 16%|        | 68/437 [00:01<00:08, 44.14it/s][A
 17%|        | 73/437 [00:01<00:08, 44.49it/s][A
 18%|        | 78/437 [00:01<00:08, 44.70it/s][A
 19%|        | 83/437 [00:01<00:07, 44.72it/s][A
 20%|        | 88/437 [00:01<00:07, 44.66it/s][A
 21%|       | 93/437 [00:02<00:07, 44.68it/s][A
 22%|       | 98/437 [00:02<00:07, 44.45it/s][A
 24%|       | 103/437 [00:02<00:08, 40.65it/s][A
 25%|       | 108/437 [00:02<00:07, 42.05it/s][A
 26%|       | 113/437 [00:02<00:07, 43.05it/s][A
 27%|       | 118/437 [00:02<00:07, 43.82it/s][A
 28%|       | 123/437 [00:02<00:07, 44.33it/s][A
 29%|       | 128/437 [00:02<00:06, 44.59it/s][A
 30%|       | 133/437 [00:02<00:06, 44.77it/s][A
 32%|      | 138/437 [00:03<00:06, 44.65it/s][A
 33%|      | 143/437 [00:03<00:06, 44.43it/s][A
 34%|      | 148/437 [00:03<00:06, 44.35it/s][A
 35%|      | 153/437 [00:03<00:06, 44.51it/s][A
 36%|      | 158/437 [00:03<00:09, 29.56it/s][A
 37%|      | 163/437 [00:03<00:08, 30.59it/s][A
 38%|      | 167/437 [00:04<00:13, 19.84it/s][A
 39%|      | 172/437 [00:04<00:10, 24.31it/s][A
 41%|      | 177/437 [00:04<00:09, 28.41it/s][A
 42%|     | 182/437 [00:04<00:07, 32.04it/s][A
 43%|     | 187/437 [00:04<00:07, 35.16it/s][A
 44%|     | 192/437 [00:04<00:06, 37.64it/s][A
 45%|     | 197/437 [00:04<00:06, 39.62it/s][A
 46%|     | 202/437 [00:05<00:05, 41.20it/s][A
 47%|     | 207/437 [00:05<00:06, 35.18it/s][A
 49%|     | 212/437 [00:05<00:05, 37.79it/s][A
 50%|     | 217/437 [00:05<00:05, 39.82it/s][A
 51%|     | 222/437 [00:05<00:05, 41.39it/s][A
 52%|    | 227/437 [00:05<00:04, 42.45it/s][A
 53%|    | 232/437 [00:05<00:04, 43.34it/s][A
 54%|    | 237/437 [00:05<00:04, 43.89it/s][A
 55%|    | 242/437 [00:06<00:04, 44.29it/s][A
 57%|    | 247/437 [00:06<00:04, 43.93it/s][A
 58%|    | 252/437 [00:06<00:04, 44.28it/s][A
 59%|    | 257/437 [00:06<00:04, 44.52it/s][A
 60%|    | 262/437 [00:06<00:03, 44.72it/s][A
 61%|    | 267/437 [00:06<00:03, 44.98it/s][A
 62%|   | 272/437 [00:06<00:03, 45.13it/s][A
 63%|   | 277/437 [00:06<00:03, 45.24it/s][A
 65%|   | 282/437 [00:06<00:03, 45.12it/s][A
 66%|   | 287/437 [00:07<00:03, 45.02it/s][A
 67%|   | 292/437 [00:07<00:03, 44.80it/s][A
 68%|   | 297/437 [00:07<00:03, 44.74it/s][A
 69%|   | 302/437 [00:07<00:03, 44.91it/s][A
 70%|   | 307/437 [00:07<00:02, 44.97it/s][A
 71%|  | 312/437 [00:07<00:02, 45.01it/s][A
 73%|  | 317/437 [00:07<00:02, 45.13it/s][A
 74%|  | 322/437 [00:07<00:02, 45.26it/s][A
 75%|  | 327/437 [00:07<00:02, 45.28it/s][A
 76%|  | 332/437 [00:08<00:02, 45.14it/s][A
 77%|  | 337/437 [00:08<00:02, 44.95it/s][A
 78%|  | 342/437 [00:08<00:02, 39.17it/s][A
 79%|  | 347/437 [00:08<00:02, 40.85it/s][A
 81%|  | 352/437 [00:08<00:02, 42.10it/s][A
 82%| | 357/437 [00:08<00:01, 43.00it/s][A
 83%| | 362/437 [00:08<00:01, 43.67it/s][A
 84%| | 367/437 [00:08<00:01, 44.18it/s][A
 85%| | 372/437 [00:08<00:01, 44.57it/s][A
 86%| | 377/437 [00:09<00:01, 44.79it/s][A
 87%| | 382/437 [00:09<00:01, 44.53it/s][A
 89%| | 387/437 [00:09<00:01, 44.47it/s][A
 90%| | 392/437 [00:09<00:01, 44.70it/s][A
 91%| | 397/437 [00:09<00:00, 44.91it/s][A
 92%|| 402/437 [00:09<00:00, 45.08it/s][A
 93%|| 407/437 [00:09<00:00, 44.62it/s][A
 94%|| 412/437 [00:09<00:00, 44.80it/s][A
 95%|| 417/437 [00:09<00:00, 44.99it/s][A
 97%|| 422/437 [00:10<00:00, 44.91it/s][A
 98%|| 427/437 [00:10<00:00, 44.70it/s][A
 99%|| 432/437 [00:10<00:00, 44.65it/s][A
100%|| 437/437 [00:10<00:00, 44.69it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 44.69it/s][A 60%|    | 498/830 [02:59<01:26,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:27:58,715 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498
[INFO|configuration_utils.py:351] 2023-08-28 10:27:59,064 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:28:03,594 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:28:03,701 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:28:03,755 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/special_tokens_map.json
 60%|    | 499/830 [03:06<29:47,  5.40s/it] 60%|    | 500/830 [03:06<21:14,  3.86s/it]                                                  60%|    | 500/830 [03:06<21:14,  3.86s/it] 60%|    | 501/830 [03:06<15:17,  2.79s/it] 60%|    | 502/830 [03:07<11:07,  2.04s/it] 61%|    | 503/830 [03:07<08:13,  1.51s/it] 61%|    | 504/830 [03:07<06:11,  1.14s/it] 61%|    | 505/830 [03:07<04:56,  1.10it/s] 61%|    | 506/830 [03:08<03:54,  1.38it/s] 61%|    | 507/830 [03:08<03:10,  1.70it/s] 61%|    | 508/830 [03:08<02:39,  2.01it/s] 61%|   | 509/830 [03:09<02:18,  2.32it/s] 61%|   | 510/830 [03:09<02:03,  2.59it/s] 62%|   | 511/830 [03:09<01:52,  2.83it/s] 62%|   | 512/830 [03:09<01:45,  3.02it/s] 62%|   | 513/830 [03:10<01:40,  3.17it/s] 62%|   | 514/830 [03:10<01:36,  3.28it/s] 62%|   | 515/830 [03:10<01:33,  3.36it/s] 62%|   | 516/830 [03:11<01:34,  3.32it/s] 62%|   | 517/830 [03:11<01:32,  3.39it/s] 62%|   | 518/830 [03:11<01:30,  3.44it/s] 63%|   | 519/830 [03:11<01:29,  3.48it/s] 63%|   | 520/830 [03:12<01:28,  3.50it/s] 63%|   | 521/830 [03:12<01:27,  3.52it/s] 63%|   | 522/830 [03:12<01:27,  3.53it/s] 63%|   | 523/830 [03:13<01:26,  3.54it/s] 63%|   | 524/830 [03:13<01:26,  3.55it/s] 63%|   | 525/830 [03:13<01:25,  3.56it/s] 63%|   | 526/830 [03:13<01:25,  3.56it/s] 63%|   | 527/830 [03:14<01:27,  3.45it/s] 64%|   | 528/830 [03:14<01:26,  3.49it/s] 64%|   | 529/830 [03:14<01:25,  3.51it/s] 64%|   | 530/830 [03:15<01:25,  3.53it/s] 64%|   | 531/830 [03:15<01:24,  3.54it/s] 64%|   | 532/830 [03:15<01:24,  3.55it/s] 64%|   | 533/830 [03:15<01:23,  3.55it/s] 64%|   | 534/830 [03:16<01:23,  3.56it/s] 64%|   | 535/830 [03:16<01:22,  3.56it/s] 65%|   | 536/830 [03:16<01:22,  3.57it/s] 65%|   | 537/830 [03:17<01:22,  3.57it/s] 65%|   | 538/830 [03:17<01:24,  3.45it/s] 65%|   | 539/830 [03:17<01:23,  3.48it/s] 65%|   | 540/830 [03:17<01:22,  3.50it/s] 65%|   | 541/830 [03:18<01:22,  3.52it/s] 65%|   | 542/830 [03:18<01:21,  3.54it/s] 65%|   | 543/830 [03:18<01:20,  3.55it/s] 66%|   | 544/830 [03:19<01:20,  3.56it/s] 66%|   | 545/830 [03:19<01:20,  3.56it/s] 66%|   | 546/830 [03:19<01:19,  3.56it/s] 66%|   | 547/830 [03:19<01:19,  3.57it/s] 66%|   | 548/830 [03:20<01:19,  3.57it/s] 66%|   | 549/830 [03:20<01:20,  3.49it/s] 66%|   | 550/830 [03:20<01:19,  3.51it/s] 66%|   | 551/830 [03:20<01:19,  3.53it/s] 67%|   | 552/830 [03:21<01:18,  3.54it/s] 67%|   | 553/830 [03:21<01:18,  3.55it/s] 67%|   | 554/830 [03:21<01:17,  3.56it/s] 67%|   | 555/830 [03:22<01:17,  3.56it/s] 67%|   | 556/830 [03:22<01:16,  3.57it/s] 67%|   | 557/830 [03:22<01:16,  3.57it/s] 67%|   | 558/830 [03:22<01:16,  3.57it/s] 67%|   | 559/830 [03:23<01:15,  3.57it/s] 67%|   | 560/830 [03:23<01:16,  3.54it/s] 68%|   | 561/830 [03:23<01:15,  3.54it/s] 68%|   | 562/830 [03:24<01:15,  3.55it/s] 68%|   | 563/830 [03:24<01:15,  3.55it/s] 68%|   | 564/830 [03:24<01:14,  3.55it/s] 68%|   | 565/830 [03:24<01:16,  3.47it/s] 68%|   | 566/830 [03:25<01:15,  3.50it/s] 68%|   | 567/830 [03:25<01:14,  3.52it/s] 68%|   | 568/830 [03:25<01:14,  3.54it/s] 69%|   | 569/830 [03:26<01:13,  3.54it/s] 69%|   | 570/830 [03:26<01:13,  3.55it/s] 69%|   | 571/830 [03:26<01:12,  3.56it/s] 69%|   | 572/830 [03:26<01:12,  3.56it/s] 69%|   | 573/830 [03:27<01:12,  3.57it/s] 69%|   | 574/830 [03:27<01:11,  3.57it/s] 69%|   | 575/830 [03:27<01:11,  3.56it/s] 69%|   | 576/830 [03:28<01:12,  3.49it/s] 70%|   | 577/830 [03:28<01:11,  3.52it/s] 70%|   | 578/830 [03:28<01:11,  3.54it/s] 70%|   | 579/830 [03:28<01:10,  3.54it/s] 70%|   | 580/830 [03:29<01:10,  3.55it/s] 70%|   | 581/830 [03:29<01:10,  3.56it/s] 70%|   | 582/830 [03:29<01:09,  3.56it/s] 70%|   | 583/830 [03:30<01:09,  3.57it/s] 70%|   | 584/830 [03:30<01:08,  3.57it/s] 70%|   | 585/830 [03:30<01:08,  3.57it/s] 71%|   | 586/830 [03:30<01:08,  3.57it/s] 71%|   | 587/830 [03:31<01:10,  3.44it/s] 71%|   | 588/830 [03:31<01:09,  3.47it/s] 71%|   | 589/830 [03:31<01:08,  3.50it/s] 71%|   | 590/830 [03:32<01:08,  3.52it/s] 71%|   | 591/830 [03:32<01:07,  3.54it/s] 71%|  | 592/830 [03:32<01:07,  3.55it/s] 71%|  | 593/830 [03:32<01:06,  3.56it/s] 72%|  | 594/830 [03:33<01:06,  3.56it/s] 72%|  | 595/830 [03:33<01:05,  3.56it/s] 72%|  | 596/830 [03:33<01:05,  3.56it/s] 72%|  | 597/830 [03:33<01:05,  3.56it/s] 72%|  | 598/830 [03:34<01:07,  3.46it/s] 72%|  | 599/830 [03:34<01:06,  3.49it/s] 72%|  | 600/830 [03:34<01:05,  3.51it/s] 72%|  | 601/830 [03:35<01:04,  3.52it/s] 73%|  | 602/830 [03:35<01:04,  3.53it/s] 73%|  | 603/830 [03:35<01:04,  3.54it/s] 73%|  | 604/830 [03:35<01:03,  3.55it/s] 73%|  | 605/830 [03:36<01:03,  3.55it/s] 73%|  | 606/830 [03:36<01:02,  3.56it/s] 73%|  | 607/830 [03:36<01:02,  3.56it/s] 73%|  | 608/830 [03:37<01:02,  3.56it/s] 73%|  | 609/830 [03:37<01:02,  3.53it/s] 73%|  | 610/830 [03:37<01:02,  3.54it/s] 74%|  | 611/830 [03:37<01:01,  3.54it/s] 74%|  | 612/830 [03:38<01:01,  3.55it/s] 74%|  | 613/830 [03:38<01:01,  3.55it/s] 74%|  | 614/830 [03:38<01:00,  3.55it/s] 74%|  | 615/830 [03:39<01:00,  3.55it/s] 74%|  | 616/830 [03:39<01:00,  3.56it/s] 74%|  | 617/830 [03:39<00:59,  3.56it/s] 74%|  | 618/830 [03:39<00:59,  3.56it/s] 75%|  | 619/830 [03:40<00:59,  3.56it/s] 75%|  | 620/830 [03:40<01:00,  3.45it/s] 75%|  | 621/830 [03:40<01:00,  3.48it/s] 75%|  | 622/830 [03:41<00:59,  3.51it/s] 75%|  | 623/830 [03:41<00:58,  3.52it/s] 75%|  | 624/830 [03:41<00:58,  3.53it/s] 75%|  | 625/830 [03:41<00:57,  3.54it/s] 75%|  | 626/830 [03:42<00:57,  3.55it/s] 76%|  | 627/830 [03:42<00:57,  3.55it/s] 76%|  | 628/830 [03:42<00:56,  3.55it/s] 76%|  | 629/830 [03:43<00:56,  3.56it/s] 76%|  | 630/830 [03:43<00:56,  3.56it/s] 76%|  | 631/830 [03:43<00:56,  3.50it/s] 76%|  | 632/830 [03:43<00:56,  3.52it/s] 76%|  | 633/830 [03:44<00:55,  3.53it/s] 76%|  | 634/830 [03:44<00:55,  3.54it/s] 77%|  | 635/830 [03:44<00:54,  3.55it/s] 77%|  | 636/830 [03:44<00:54,  3.56it/s] 77%|  | 637/830 [03:45<00:53,  3.58it/s] 77%|  | 638/830 [03:45<00:53,  3.59it/s] 77%|  | 639/830 [03:45<00:53,  3.60it/s] 77%|  | 640/830 [03:46<00:52,  3.61it/s] 77%|  | 641/830 [03:46<00:52,  3.61it/s] 77%|  | 642/830 [03:46<00:54,  3.46it/s] 77%|  | 643/830 [03:46<00:53,  3.51it/s] 78%|  | 644/830 [03:47<00:52,  3.54it/s] 78%|  | 645/830 [03:47<00:51,  3.56it/s] 78%|  | 646/830 [03:47<00:51,  3.58it/s] 78%|  | 647/830 [03:48<00:50,  3.60it/s] 78%|  | 648/830 [03:48<00:50,  3.60it/s] 78%|  | 649/830 [03:48<00:50,  3.61it/s] 78%|  | 650/830 [03:48<00:49,  3.61it/s] 78%|  | 651/830 [03:49<00:49,  3.61it/s] 79%|  | 652/830 [03:49<00:49,  3.62it/s] 79%|  | 653/830 [03:49<00:51,  3.41it/s] 79%|  | 654/830 [03:50<00:50,  3.47it/s] 79%|  | 655/830 [03:50<00:49,  3.51it/s] 79%|  | 656/830 [03:50<00:49,  3.54it/s] 79%|  | 657/830 [03:50<00:48,  3.57it/s] 79%|  | 658/830 [03:51<00:47,  3.58it/s] 79%|  | 659/830 [03:51<00:47,  3.59it/s] 80%|  | 660/830 [03:51<00:47,  3.60it/s] 80%|  | 661/830 [03:52<00:46,  3.61it/s] 80%|  | 662/830 [03:52<00:46,  3.61it/s] 80%|  | 663/830 [03:52<00:46,  3.62it/s] 80%|  | 664/830 [03:52<00:43,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 10:28:51,951 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:28:51,952 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:28:51,952 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.4417, 'eval_samples_per_second': 334.14, 'eval_steps_per_second': 41.851, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.2409638554216866e-05, 'epoch': 3.01}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|          | 4/437 [00:00<00:30, 14.31it/s][A
  2%|         | 8/437 [00:00<00:19, 21.65it/s][A
  3%|         | 13/437 [00:00<00:14, 29.71it/s][A
  4%|         | 18/437 [00:00<00:12, 34.74it/s][A
  5%|         | 23/437 [00:00<00:10, 38.13it/s][A
  6%|         | 28/437 [00:00<00:10, 40.34it/s][A
  8%|         | 33/437 [00:00<00:09, 41.97it/s][A
  9%|         | 38/437 [00:01<00:09, 43.01it/s][A
 10%|         | 43/437 [00:01<00:09, 43.76it/s][A
 11%|         | 48/437 [00:01<00:08, 43.76it/s][A
 12%|        | 53/437 [00:01<00:08, 44.08it/s][A
 13%|        | 58/437 [00:01<00:08, 44.28it/s][A
 14%|        | 63/437 [00:01<00:08, 44.73it/s][A
 16%|        | 68/437 [00:01<00:08, 44.87it/s][A
 17%|        | 73/437 [00:01<00:08, 45.03it/s][A
 18%|        | 78/437 [00:01<00:07, 45.04it/s][A
 19%|        | 83/437 [00:02<00:07, 45.06it/s][A
 20%|        | 88/437 [00:02<00:07, 44.98it/s][A
 21%|       | 93/437 [00:02<00:07, 44.77it/s][A
 22%|       | 98/437 [00:02<00:07, 44.75it/s][A
 24%|       | 103/437 [00:02<00:07, 44.87it/s][A
 25%|       | 108/437 [00:02<00:07, 44.87it/s][A
 26%|       | 113/437 [00:02<00:07, 41.57it/s][A
 27%|       | 118/437 [00:02<00:07, 42.65it/s][A
 28%|       | 123/437 [00:02<00:07, 43.45it/s][A
 29%|       | 128/437 [00:03<00:07, 44.04it/s][A
 30%|       | 133/437 [00:03<00:06, 44.42it/s][A
 32%|      | 138/437 [00:03<00:06, 44.56it/s][A
 33%|      | 143/437 [00:03<00:06, 44.61it/s][A
 34%|      | 148/437 [00:03<00:06, 44.76it/s][A
 35%|      | 153/437 [00:03<00:06, 44.47it/s][A
 36%|      | 158/437 [00:03<00:06, 44.63it/s][A
 37%|      | 163/437 [00:03<00:06, 44.76it/s][A
 38%|      | 168/437 [00:03<00:05, 44.98it/s][A
 40%|      | 173/437 [00:04<00:05, 45.08it/s][A
 41%|      | 178/437 [00:04<00:05, 45.18it/s][A
 42%|     | 183/437 [00:04<00:05, 45.28it/s][A
 43%|     | 188/437 [00:04<00:05, 45.16it/s][A
 44%|     | 193/437 [00:04<00:05, 45.12it/s][A
 45%|     | 198/437 [00:04<00:05, 45.01it/s][A
 46%|     | 203/437 [00:04<00:05, 44.97it/s][A
 48%|     | 208/437 [00:04<00:05, 44.97it/s][A
 49%|     | 213/437 [00:04<00:04, 44.98it/s][A
 50%|     | 218/437 [00:05<00:04, 45.05it/s][A
 51%|     | 223/437 [00:05<00:04, 45.23it/s][A
 52%|    | 228/437 [00:05<00:04, 45.20it/s][A
 53%|    | 233/437 [00:05<00:04, 45.19it/s][A
 54%|    | 238/437 [00:05<00:04, 45.09it/s][A
 56%|    | 243/437 [00:05<00:04, 44.91it/s][A
 57%|    | 248/437 [00:05<00:04, 41.07it/s][A
 58%|    | 253/437 [00:05<00:04, 42.36it/s][A
 59%|    | 258/437 [00:06<00:04, 43.18it/s][A
 60%|    | 263/437 [00:06<00:03, 43.90it/s][A
 61%|   | 268/437 [00:06<00:03, 44.28it/s][A
 62%|   | 273/437 [00:06<00:03, 44.62it/s][A
 64%|   | 278/437 [00:06<00:03, 44.88it/s][A
 65%|   | 283/437 [00:06<00:03, 44.97it/s][A
 66%|   | 288/437 [00:06<00:03, 44.68it/s][A
 67%|   | 293/437 [00:06<00:03, 44.56it/s][A
 68%|   | 298/437 [00:06<00:03, 44.79it/s][A
 69%|   | 303/437 [00:07<00:02, 44.93it/s][A
 70%|   | 308/437 [00:07<00:02, 45.01it/s][A
 72%|  | 313/437 [00:07<00:02, 45.12it/s][A
 73%|  | 318/437 [00:07<00:02, 45.22it/s][A
 74%|  | 323/437 [00:07<00:02, 45.18it/s][A
 75%|  | 328/437 [00:07<00:02, 45.12it/s][A
 76%|  | 333/437 [00:07<00:02, 45.00it/s][A
 77%|  | 338/437 [00:07<00:02, 44.97it/s][A
 78%|  | 343/437 [00:07<00:02, 44.96it/s][A
 80%|  | 348/437 [00:08<00:01, 44.85it/s][A
 81%|  | 353/437 [00:08<00:01, 44.76it/s][A
 82%| | 358/437 [00:08<00:01, 44.90it/s][A
 83%| | 363/437 [00:08<00:01, 44.90it/s][A
 84%| | 368/437 [00:08<00:01, 44.89it/s][A
 85%| | 373/437 [00:08<00:01, 44.73it/s][A
 86%| | 378/437 [00:08<00:01, 44.77it/s][A
 88%| | 383/437 [00:08<00:01, 43.80it/s][A
 89%| | 388/437 [00:08<00:01, 44.29it/s][A
 90%| | 393/437 [00:09<00:00, 44.57it/s][A
 91%| | 398/437 [00:09<00:00, 44.69it/s][A
 92%|| 403/437 [00:09<00:00, 44.98it/s][A
 93%|| 408/437 [00:09<00:00, 45.07it/s][A
 95%|| 413/437 [00:09<00:00, 45.05it/s][A
 96%|| 418/437 [00:09<00:00, 45.01it/s][A
 97%|| 423/437 [00:09<00:00, 44.72it/s][A
 98%|| 428/437 [00:09<00:00, 44.80it/s][A
 99%|| 433/437 [00:09<00:00, 44.92it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.92it/s][A 80%|  | 664/830 [04:02<00:43,  3.85it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:29:02,129 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664
[INFO|configuration_utils.py:351] 2023-08-28 10:29:02,320 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:29:05,983 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:29:06,259 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:29:06,372 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/special_tokens_map.json
 80%|  | 665/830 [04:08<13:45,  5.01s/it] 80%|  | 666/830 [04:09<09:48,  3.59s/it] 80%|  | 667/830 [04:09<07:03,  2.60s/it] 80%|  | 668/830 [04:09<05:07,  1.90s/it] 81%|  | 669/830 [04:09<03:47,  1.41s/it] 81%|  | 670/830 [04:10<02:53,  1.09s/it] 81%|  | 671/830 [04:10<02:14,  1.18it/s] 81%|  | 672/830 [04:10<01:46,  1.48it/s] 81%|  | 673/830 [04:11<01:27,  1.80it/s] 81%|  | 674/830 [04:11<01:13,  2.11it/s] 81%| | 675/830 [04:11<01:04,  2.41it/s] 81%| | 676/830 [04:11<00:57,  2.67it/s] 82%| | 677/830 [04:12<00:52,  2.89it/s] 82%| | 678/830 [04:12<00:49,  3.06it/s] 82%| | 679/830 [04:12<00:47,  3.20it/s] 82%| | 680/830 [04:13<00:45,  3.30it/s] 82%| | 681/830 [04:13<00:45,  3.27it/s] 82%| | 682/830 [04:13<00:44,  3.35it/s] 82%| | 683/830 [04:13<00:43,  3.41it/s] 82%| | 684/830 [04:14<00:42,  3.46it/s] 83%| | 685/830 [04:14<00:41,  3.49it/s] 83%| | 686/830 [04:14<00:41,  3.51it/s] 83%| | 687/830 [04:15<00:40,  3.53it/s] 83%| | 688/830 [04:15<00:40,  3.54it/s] 83%| | 689/830 [04:15<00:39,  3.55it/s] 83%| | 690/830 [04:15<00:39,  3.56it/s] 83%| | 691/830 [04:16<00:38,  3.58it/s] 83%| | 692/830 [04:16<00:39,  3.47it/s] 83%| | 693/830 [04:16<00:38,  3.52it/s] 84%| | 694/830 [04:17<00:38,  3.54it/s] 84%| | 695/830 [04:17<00:37,  3.57it/s] 84%| | 696/830 [04:17<00:37,  3.58it/s] 84%| | 697/830 [04:17<00:36,  3.60it/s] 84%| | 698/830 [04:18<00:36,  3.61it/s] 84%| | 699/830 [04:18<00:36,  3.61it/s] 84%| | 700/830 [04:18<00:35,  3.61it/s] 84%| | 701/830 [04:18<00:35,  3.61it/s] 85%| | 702/830 [04:19<00:35,  3.62it/s] 85%| | 703/830 [04:19<00:36,  3.50it/s] 85%| | 704/830 [04:19<00:35,  3.53it/s] 85%| | 705/830 [04:20<00:35,  3.56it/s] 85%| | 706/830 [04:20<00:34,  3.58it/s] 85%| | 707/830 [04:20<00:34,  3.59it/s] 85%| | 708/830 [04:20<00:33,  3.60it/s] 85%| | 709/830 [04:21<00:33,  3.60it/s] 86%| | 710/830 [04:21<00:33,  3.60it/s] 86%| | 711/830 [04:21<00:32,  3.61it/s] 86%| | 712/830 [04:22<00:32,  3.61it/s] 86%| | 713/830 [04:22<00:32,  3.61it/s] 86%| | 714/830 [04:22<00:33,  3.51it/s] 86%| | 715/830 [04:22<00:32,  3.54it/s] 86%| | 716/830 [04:23<00:31,  3.56it/s] 86%| | 717/830 [04:23<00:31,  3.58it/s] 87%| | 718/830 [04:23<00:31,  3.59it/s] 87%| | 719/830 [04:24<00:30,  3.60it/s] 87%| | 720/830 [04:24<00:30,  3.61it/s] 87%| | 721/830 [04:24<00:30,  3.61it/s] 87%| | 722/830 [04:24<00:29,  3.60it/s] 87%| | 723/830 [04:25<00:29,  3.61it/s] 87%| | 724/830 [04:25<00:29,  3.61it/s] 87%| | 725/830 [04:25<00:29,  3.51it/s] 87%| | 726/830 [04:25<00:29,  3.54it/s] 88%| | 727/830 [04:26<00:28,  3.56it/s] 88%| | 728/830 [04:26<00:28,  3.58it/s] 88%| | 729/830 [04:26<00:28,  3.59it/s] 88%| | 730/830 [04:27<00:27,  3.60it/s] 88%| | 731/830 [04:27<00:27,  3.60it/s] 88%| | 732/830 [04:27<00:27,  3.61it/s] 88%| | 733/830 [04:27<00:26,  3.61it/s] 88%| | 734/830 [04:28<00:26,  3.61it/s] 89%| | 735/830 [04:28<00:26,  3.61it/s] 89%| | 736/830 [04:28<00:26,  3.61it/s] 89%| | 737/830 [04:29<00:25,  3.61it/s] 89%| | 738/830 [04:29<00:25,  3.62it/s] 89%| | 739/830 [04:29<00:26,  3.47it/s] 89%| | 740/830 [04:29<00:25,  3.51it/s] 89%| | 741/830 [04:30<00:25,  3.54it/s] 89%| | 742/830 [04:30<00:24,  3.56it/s] 90%| | 743/830 [04:30<00:24,  3.58it/s] 90%| | 744/830 [04:31<00:23,  3.59it/s] 90%| | 745/830 [04:31<00:23,  3.60it/s] 90%| | 746/830 [04:31<00:23,  3.60it/s] 90%| | 747/830 [04:31<00:23,  3.61it/s] 90%| | 748/830 [04:32<00:22,  3.61it/s] 90%| | 749/830 [04:32<00:22,  3.61it/s] 90%| | 750/830 [04:32<00:22,  3.50it/s] 90%| | 751/830 [04:32<00:22,  3.53it/s] 91%| | 752/830 [04:33<00:21,  3.55it/s] 91%| | 753/830 [04:33<00:21,  3.57it/s] 91%| | 754/830 [04:33<00:21,  3.59it/s] 91%| | 755/830 [04:34<00:20,  3.60it/s] 91%| | 756/830 [04:34<00:20,  3.60it/s] 91%| | 757/830 [04:34<00:20,  3.61it/s] 91%|| 758/830 [04:34<00:19,  3.61it/s] 91%|| 759/830 [04:35<00:19,  3.62it/s] 92%|| 760/830 [04:35<00:19,  3.62it/s] 92%|| 761/830 [04:35<00:19,  3.52it/s] 92%|| 762/830 [04:36<00:19,  3.55it/s] 92%|| 763/830 [04:36<00:18,  3.57it/s] 92%|| 764/830 [04:36<00:18,  3.58it/s] 92%|| 765/830 [04:36<00:18,  3.59it/s] 92%|| 766/830 [04:37<00:17,  3.60it/s] 92%|| 767/830 [04:37<00:17,  3.61it/s] 93%|| 768/830 [04:37<00:17,  3.61it/s] 93%|| 769/830 [04:37<00:16,  3.61it/s] 93%|| 770/830 [04:38<00:16,  3.61it/s] 93%|| 771/830 [04:38<00:16,  3.61it/s] 93%|| 772/830 [04:38<00:16,  3.54it/s] 93%|| 773/830 [04:39<00:15,  3.57it/s] 93%|| 774/830 [04:39<00:15,  3.58it/s] 93%|| 775/830 [04:39<00:15,  3.59it/s] 93%|| 776/830 [04:39<00:15,  3.60it/s] 94%|| 777/830 [04:40<00:14,  3.61it/s] 94%|| 778/830 [04:40<00:14,  3.61it/s] 94%|| 779/830 [04:40<00:14,  3.61it/s] 94%|| 780/830 [04:41<00:13,  3.61it/s] 94%|| 781/830 [04:41<00:13,  3.61it/s] 94%|| 782/830 [04:41<00:13,  3.62it/s] 94%|| 783/830 [04:41<00:13,  3.45it/s] 94%|| 784/830 [04:42<00:13,  3.50it/s] 95%|| 785/830 [04:42<00:12,  3.53it/s] 95%|| 786/830 [04:42<00:12,  3.56it/s] 95%|| 787/830 [04:43<00:12,  3.58it/s] 95%|| 788/830 [04:43<00:11,  3.59it/s] 95%|| 789/830 [04:43<00:11,  3.60it/s] 95%|| 790/830 [04:43<00:11,  3.61it/s] 95%|| 791/830 [04:44<00:10,  3.61it/s] 95%|| 792/830 [04:44<00:10,  3.61it/s] 96%|| 793/830 [04:44<00:10,  3.61it/s] 96%|| 794/830 [04:44<00:10,  3.54it/s] 96%|| 795/830 [04:45<00:09,  3.56it/s] 96%|| 796/830 [04:45<00:09,  3.58it/s] 96%|| 797/830 [04:45<00:09,  3.59it/s] 96%|| 798/830 [04:46<00:08,  3.60it/s] 96%|| 799/830 [04:46<00:08,  3.60it/s] 96%|| 800/830 [04:46<00:08,  3.60it/s] 97%|| 801/830 [04:46<00:08,  3.61it/s] 97%|| 802/830 [04:47<00:07,  3.61it/s] 97%|| 803/830 [04:47<00:07,  3.62it/s] 97%|| 804/830 [04:47<00:07,  3.62it/s] 97%|| 805/830 [04:48<00:06,  3.58it/s] 97%|| 806/830 [04:48<00:06,  3.59it/s] 97%|| 807/830 [04:48<00:06,  3.60it/s] 97%|| 808/830 [04:48<00:06,  3.60it/s] 97%|| 809/830 [04:49<00:05,  3.61it/s] 98%|| 810/830 [04:49<00:05,  3.61it/s] 98%|| 811/830 [04:49<00:05,  3.61it/s] 98%|| 812/830 [04:49<00:04,  3.62it/s] 98%|| 813/830 [04:50<00:04,  3.62it/s] 98%|| 814/830 [04:50<00:04,  3.62it/s] 98%|| 815/830 [04:50<00:04,  3.62it/s] 98%|| 816/830 [04:51<00:04,  3.43it/s] 98%|| 817/830 [04:51<00:03,  3.48it/s] 99%|| 818/830 [04:51<00:03,  3.52it/s] 99%|| 819/830 [04:51<00:03,  3.55it/s] 99%|| 820/830 [04:52<00:02,  3.57it/s] 99%|| 821/830 [04:52<00:03,  2.74it/s] 99%|| 822/830 [04:53<00:03,  2.26it/s] 99%|| 823/830 [04:53<00:02,  2.54it/s] 99%|| 824/830 [04:53<00:02,  2.79it/s] 99%|| 825/830 [04:54<00:01,  2.81it/s]100%|| 826/830 [04:54<00:01,  3.01it/s]100%|| 827/830 [04:54<00:00,  3.17it/s]100%|| 828/830 [04:55<00:00,  3.30it/s]100%|| 829/830 [04:55<00:00,  3.39it/s]100%|| 830/830 [04:55<00:00,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 10:29:54,779 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:29:54,779 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:29:54,779 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.0034, 'eval_samples_per_second': 348.781, 'eval_steps_per_second': 43.685, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.56it/s][A
  3%|         | 12/437 [00:00<00:08, 49.52it/s][A
  4%|         | 18/437 [00:00<00:08, 47.65it/s][A
  5%|         | 23/437 [00:00<00:08, 46.71it/s][A
  6%|         | 28/437 [00:00<00:08, 46.11it/s][A
  8%|         | 33/437 [00:00<00:08, 45.73it/s][A
  9%|         | 38/437 [00:00<00:08, 45.36it/s][A
 10%|         | 43/437 [00:00<00:08, 45.06it/s][A
 11%|         | 48/437 [00:01<00:08, 45.06it/s][A
 12%|        | 53/437 [00:01<00:08, 45.21it/s][A
 13%|        | 58/437 [00:01<00:08, 45.29it/s][A
 14%|        | 63/437 [00:01<00:08, 44.56it/s][A
 16%|        | 68/437 [00:01<00:08, 44.75it/s][A
 17%|        | 73/437 [00:01<00:08, 44.74it/s][A
 18%|        | 78/437 [00:01<00:08, 44.74it/s][A
 19%|        | 83/437 [00:01<00:07, 44.57it/s][A
 20%|        | 88/437 [00:01<00:07, 44.48it/s][A
 21%|       | 93/437 [00:02<00:07, 44.57it/s][A
 22%|       | 98/437 [00:02<00:07, 44.76it/s][A
 24%|       | 103/437 [00:02<00:07, 45.11it/s][A
 25%|       | 108/437 [00:02<00:07, 45.18it/s][A
 26%|       | 113/437 [00:02<00:07, 45.23it/s][A
 27%|       | 118/437 [00:02<00:07, 45.16it/s][A
 28%|       | 123/437 [00:02<00:06, 45.04it/s][A
 29%|       | 128/437 [00:02<00:06, 44.84it/s][A
 30%|       | 133/437 [00:02<00:06, 44.68it/s][A
 32%|      | 138/437 [00:03<00:06, 44.75it/s][A
 33%|      | 143/437 [00:03<00:06, 44.88it/s][A
 34%|      | 148/437 [00:03<00:06, 45.06it/s][A
 35%|      | 153/437 [00:03<00:06, 45.26it/s][A
 36%|      | 158/437 [00:03<00:06, 45.36it/s][A
 37%|      | 163/437 [00:03<00:06, 45.28it/s][A
 38%|      | 168/437 [00:03<00:05, 45.09it/s][A
 40%|      | 173/437 [00:03<00:05, 44.76it/s][A
 41%|      | 178/437 [00:03<00:05, 44.74it/s][A
 42%|     | 183/437 [00:04<00:05, 44.87it/s][A
 43%|     | 188/437 [00:04<00:05, 44.99it/s][A
 44%|     | 193/437 [00:04<00:05, 45.13it/s][A
 45%|     | 198/437 [00:04<00:06, 37.94it/s][A
 46%|     | 203/437 [00:04<00:05, 39.99it/s][A
 48%|     | 208/437 [00:04<00:05, 41.54it/s][A
 49%|     | 213/437 [00:04<00:05, 42.78it/s][A
 50%|     | 218/437 [00:04<00:05, 43.42it/s][A
 51%|     | 223/437 [00:04<00:04, 44.15it/s][A
 52%|    | 228/437 [00:05<00:04, 44.58it/s][A
 53%|    | 233/437 [00:05<00:04, 44.78it/s][A
 54%|    | 238/437 [00:05<00:04, 44.57it/s][A
 56%|    | 243/437 [00:05<00:04, 44.30it/s][A
 57%|    | 248/437 [00:05<00:04, 44.33it/s][A
 58%|    | 253/437 [00:05<00:04, 44.57it/s][A
 59%|    | 258/437 [00:05<00:03, 44.86it/s][A
 60%|    | 263/437 [00:05<00:03, 45.14it/s][A
 61%|   | 268/437 [00:05<00:03, 45.26it/s][A
 62%|   | 273/437 [00:06<00:03, 45.35it/s][A
 64%|   | 278/437 [00:06<00:03, 45.32it/s][A
 65%|   | 283/437 [00:06<00:03, 45.06it/s][A
 66%|   | 288/437 [00:06<00:03, 44.79it/s][A
 67%|   | 293/437 [00:06<00:03, 44.64it/s][A
 68%|   | 298/437 [00:06<00:03, 44.72it/s][A
 69%|   | 303/437 [00:06<00:02, 44.80it/s][A
 70%|   | 308/437 [00:06<00:02, 45.08it/s][A
 72%|  | 313/437 [00:06<00:02, 45.31it/s][A
 73%|  | 318/437 [00:07<00:02, 45.45it/s][A
 74%|  | 323/437 [00:07<00:02, 45.34it/s][A
 75%|  | 328/437 [00:07<00:02, 45.07it/s][A
 76%|  | 333/437 [00:07<00:02, 41.07it/s][A
 77%|  | 338/437 [00:07<00:02, 42.24it/s][A
 78%|  | 343/437 [00:07<00:02, 43.20it/s][A
 80%|  | 348/437 [00:07<00:02, 43.75it/s][A
 81%|  | 353/437 [00:07<00:01, 44.19it/s][A
 82%| | 358/437 [00:08<00:01, 44.64it/s][A
 83%| | 363/437 [00:08<00:01, 44.90it/s][A
 84%| | 368/437 [00:08<00:01, 45.00it/s][A
 85%| | 373/437 [00:08<00:01, 44.71it/s][A
 86%| | 378/437 [00:08<00:01, 44.72it/s][A
 88%| | 383/437 [00:08<00:01, 44.83it/s][A
 89%| | 388/437 [00:08<00:01, 44.98it/s][A
 90%| | 393/437 [00:08<00:00, 45.09it/s][A
 91%| | 398/437 [00:08<00:00, 45.15it/s][A
 92%|| 403/437 [00:09<00:00, 45.27it/s][A
 93%|| 408/437 [00:09<00:00, 45.19it/s][A
 95%|| 413/437 [00:09<00:00, 45.18it/s][A
 96%|| 418/437 [00:09<00:00, 45.00it/s][A
 97%|| 423/437 [00:09<00:00, 45.01it/s][A
 98%|| 428/437 [00:09<00:00, 45.03it/s][A
 99%|| 433/437 [00:09<00:00, 45.05it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.05it/s][A100%|| 830/830 [05:05<00:00,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:30:05,066 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830
[INFO|configuration_utils.py:351] 2023-08-28 10:30:05,545 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:30:10,177 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:30:10,293 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:30:10,359 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:30:11,189 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:30:11,189 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166 (score: 1.0192089080810547).
                                                 100%|| 830/830 [05:23<00:00,  3.81it/s]100%|| 830/830 [05:23<00:00,  2.57it/s]
[INFO|trainer.py:1894] 2023-08-28 10:30:22,495 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 10:30:22,667 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:30:26,286 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:30:26,440 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:30:26,504 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:30:27,158 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   train_runtime            = 0:05:23.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   train_samples            =      10600
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   train_samples_per_second =    163.948
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:27,158 >>   train_steps_per_second   =      2.567
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.7889, 'eval_samples_per_second': 356.425, 'eval_steps_per_second': 44.643, 'epoch': 5.0}
{'train_runtime': 323.2725, 'train_samples_per_second': 163.948, 'train_steps_per_second': 2.567, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:30:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:30:27,394 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:30:27,394 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 10:30:27,394 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 56.31it/s]  3%|         | 12/437 [00:00<00:08, 49.58it/s]  4%|         | 18/437 [00:00<00:08, 47.88it/s]  5%|         | 23/437 [00:00<00:08, 47.12it/s]  6%|         | 28/437 [00:00<00:08, 46.72it/s]  8%|         | 33/437 [00:00<00:08, 46.48it/s]  9%|         | 38/437 [00:00<00:08, 46.22it/s] 10%|         | 43/437 [00:00<00:08, 45.87it/s] 11%|         | 48/437 [00:01<00:08, 45.29it/s] 12%|        | 53/437 [00:01<00:08, 45.19it/s] 13%|        | 58/437 [00:01<00:08, 45.28it/s] 14%|        | 63/437 [00:01<00:08, 45.31it/s] 16%|        | 68/437 [00:01<00:08, 45.48it/s] 17%|        | 73/437 [00:01<00:08, 45.48it/s] 18%|        | 78/437 [00:01<00:07, 45.58it/s] 19%|        | 83/437 [00:01<00:07, 45.54it/s] 20%|        | 88/437 [00:01<00:07, 45.42it/s] 21%|       | 93/437 [00:02<00:07, 45.35it/s] 22%|       | 98/437 [00:02<00:07, 45.32it/s] 24%|       | 103/437 [00:02<00:07, 45.34it/s] 25%|       | 108/437 [00:02<00:07, 45.37it/s] 26%|       | 113/437 [00:02<00:07, 42.94it/s] 27%|       | 118/437 [00:02<00:07, 43.91it/s] 28%|       | 123/437 [00:02<00:07, 44.56it/s] 29%|       | 128/437 [00:02<00:06, 44.98it/s] 30%|       | 133/437 [00:02<00:06, 45.13it/s] 32%|      | 138/437 [00:03<00:06, 45.17it/s] 33%|      | 143/437 [00:03<00:06, 45.22it/s] 34%|      | 148/437 [00:03<00:06, 45.29it/s] 35%|      | 153/437 [00:03<00:06, 45.06it/s] 36%|      | 158/437 [00:03<00:06, 45.28it/s] 37%|      | 163/437 [00:03<00:06, 45.40it/s] 38%|      | 168/437 [00:03<00:05, 45.47it/s] 40%|      | 173/437 [00:03<00:05, 45.65it/s] 41%|      | 178/437 [00:03<00:05, 45.72it/s] 42%|     | 183/437 [00:04<00:05, 45.67it/s] 43%|     | 188/437 [00:04<00:05, 45.62it/s] 44%|     | 193/437 [00:04<00:05, 45.50it/s] 45%|     | 198/437 [00:04<00:05, 45.26it/s] 46%|     | 203/437 [00:04<00:05, 45.25it/s] 48%|     | 208/437 [00:04<00:05, 45.41it/s] 49%|     | 213/437 [00:04<00:04, 45.48it/s] 50%|     | 218/437 [00:04<00:04, 45.61it/s] 51%|     | 223/437 [00:04<00:04, 45.58it/s] 52%|    | 228/437 [00:05<00:04, 45.62it/s] 53%|    | 233/437 [00:05<00:04, 45.71it/s] 54%|    | 238/437 [00:05<00:04, 45.60it/s] 56%|    | 243/437 [00:05<00:04, 45.44it/s] 57%|    | 248/437 [00:05<00:04, 45.27it/s] 58%|    | 253/437 [00:05<00:04, 44.02it/s] 59%|    | 258/437 [00:05<00:04, 44.60it/s] 60%|    | 263/437 [00:05<00:03, 44.96it/s] 61%|   | 268/437 [00:05<00:03, 45.20it/s] 62%|   | 273/437 [00:06<00:03, 45.42it/s] 64%|   | 278/437 [00:06<00:03, 45.43it/s] 65%|   | 283/437 [00:06<00:03, 45.52it/s] 66%|   | 288/437 [00:06<00:03, 45.49it/s] 67%|   | 293/437 [00:06<00:03, 45.23it/s] 68%|   | 298/437 [00:06<00:03, 45.19it/s] 69%|   | 303/437 [00:06<00:02, 45.36it/s] 70%|   | 308/437 [00:06<00:02, 45.42it/s] 72%|  | 313/437 [00:06<00:02, 45.57it/s] 73%|  | 318/437 [00:06<00:02, 45.53it/s] 74%|  | 323/437 [00:07<00:02, 45.62it/s] 75%|  | 328/437 [00:07<00:02, 45.73it/s] 76%|  | 333/437 [00:07<00:02, 45.60it/s] 77%|  | 338/437 [00:07<00:02, 45.43it/s] 78%|  | 343/437 [00:07<00:02, 45.28it/s] 80%|  | 348/437 [00:07<00:01, 45.33it/s] 81%|  | 353/437 [00:07<00:01, 45.36it/s] 82%| | 358/437 [00:07<00:01, 45.51it/s] 83%| | 363/437 [00:07<00:01, 45.50it/s] 84%| | 368/437 [00:08<00:01, 45.71it/s] 85%| | 373/437 [00:08<00:01, 45.67it/s] 86%| | 378/437 [00:08<00:01, 45.59it/s] 88%| | 383/437 [00:08<00:01, 45.46it/s] 89%| | 388/437 [00:08<00:01, 45.29it/s] 90%| | 393/437 [00:08<00:00, 45.13it/s] 91%| | 398/437 [00:08<00:00, 45.26it/s] 92%|| 403/437 [00:08<00:00, 45.29it/s] 93%|| 408/437 [00:08<00:00, 45.54it/s] 95%|| 413/437 [00:09<00:00, 45.57it/s] 96%|| 418/437 [00:09<00:00, 45.56it/s] 97%|| 423/437 [00:09<00:00, 45.53it/s] 98%|| 428/437 [00:09<00:00, 45.41it/s] 99%|| 433/437 [00:09<00:00, 45.28it/s]100%|| 437/437 [00:09<00:00, 45.46it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:30:37,025 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,025 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,025 >>   eval_loss               =     1.0192
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,025 >>   eval_runtime            = 0:00:09.63
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,026 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,026 >>   eval_samples_per_second =    362.246
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,026 >>   eval_steps_per_second   =     45.372
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:30:37,026 >>   perplexity              =      2.771
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:45,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:45,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:45,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:45,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:45,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:30:45,805 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:30:45,806 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:30:46,413 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:30:47,479 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:30:47,479 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:50,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:50,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:50,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:50,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:30:50,611 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:30:51,538 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:30:51,540 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:30:52,368 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:30:52,647 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:30:52,647 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.75it/s]Extractor Predicting: 12it [00:07,  1.75it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.74it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.69it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:19,  1.73it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:21,  1.67it/s]Extractor Predicting: 38it [00:22,  1.66it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:23,  1.65it/s]Extractor Predicting: 41it [00:24,  1.64it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.61it/s]Extractor Predicting: 45it [00:26,  1.62it/s]Extractor Predicting: 46it [00:27,  1.65it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:28,  1.63it/s]Extractor Predicting: 49it [00:29,  1.62it/s]Extractor Predicting: 50it [00:29,  1.61it/s]Extractor Predicting: 51it [00:30,  1.62it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.63it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:34,  1.64it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.62it/s]Extractor Predicting: 61it [00:36,  1.63it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:37,  1.69it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.71it/s]Extractor Predicting: 66it [00:39,  1.70it/s]Extractor Predicting: 67it [00:40,  1.73it/s]Extractor Predicting: 68it [00:40,  1.71it/s]Extractor Predicting: 69it [00:41,  1.74it/s]Extractor Predicting: 70it [00:41,  1.77it/s]Extractor Predicting: 71it [00:42,  1.77it/s]Extractor Predicting: 72it [00:42,  1.76it/s]Extractor Predicting: 73it [00:43,  1.77it/s]Extractor Predicting: 74it [00:44,  1.76it/s]Extractor Predicting: 75it [00:44,  1.72it/s]Extractor Predicting: 76it [00:45,  1.70it/s]Extractor Predicting: 77it [00:45,  1.74it/s]Extractor Predicting: 78it [00:46,  1.77it/s]Extractor Predicting: 79it [00:46,  1.77it/s]Extractor Predicting: 80it [00:47,  1.76it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:48,  1.78it/s]Extractor Predicting: 83it [00:49,  1.78it/s]Extractor Predicting: 84it [00:49,  1.74it/s]Extractor Predicting: 85it [00:50,  1.78it/s]Extractor Predicting: 86it [00:50,  1.76it/s]Extractor Predicting: 87it [00:51,  1.77it/s]Extractor Predicting: 88it [00:52,  1.78it/s]Extractor Predicting: 89it [00:52,  1.76it/s]Extractor Predicting: 90it [00:53,  1.77it/s]Extractor Predicting: 91it [00:53,  1.76it/s]Extractor Predicting: 92it [00:54,  1.66it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.70it/s]Extractor Predicting: 95it [00:56,  1.67it/s]Extractor Predicting: 96it [00:56,  1.68it/s]Extractor Predicting: 97it [00:57,  1.65it/s]Extractor Predicting: 98it [00:57,  1.70it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:59,  1.68it/s]Extractor Predicting: 101it [00:59,  1.71it/s]Extractor Predicting: 102it [01:00,  1.56it/s]Extractor Predicting: 103it [01:01,  1.54it/s]Extractor Predicting: 104it [01:01,  1.58it/s]Extractor Predicting: 105it [01:02,  1.61it/s]Extractor Predicting: 106it [01:02,  1.63it/s]Extractor Predicting: 107it [01:03,  1.67it/s]Extractor Predicting: 108it [01:04,  1.69it/s]Extractor Predicting: 109it [01:04,  1.61it/s]Extractor Predicting: 110it [01:05,  1.62it/s]Extractor Predicting: 111it [01:06,  1.63it/s]Extractor Predicting: 112it [01:06,  1.64it/s]Extractor Predicting: 113it [01:07,  1.65it/s]Extractor Predicting: 114it [01:07,  1.63it/s]Extractor Predicting: 115it [01:08,  1.63it/s]Extractor Predicting: 116it [01:09,  1.67it/s]Extractor Predicting: 117it [01:09,  1.67it/s]Extractor Predicting: 118it [01:10,  1.69it/s]Extractor Predicting: 119it [01:10,  1.73it/s]Extractor Predicting: 120it [01:11,  1.72it/s]Extractor Predicting: 121it [01:11,  1.73it/s]Extractor Predicting: 122it [01:12,  1.75it/s]Extractor Predicting: 123it [01:13,  1.72it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:14,  1.74it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:15,  1.70it/s]Extractor Predicting: 128it [01:16,  1.68it/s]Extractor Predicting: 129it [01:16,  1.72it/s]Extractor Predicting: 130it [01:17,  1.68it/s]Extractor Predicting: 131it [01:17,  1.66it/s]Extractor Predicting: 132it [01:18,  1.68it/s]Extractor Predicting: 133it [01:19,  1.67it/s]Extractor Predicting: 134it [01:19,  1.66it/s]Extractor Predicting: 135it [01:20,  1.69it/s]Extractor Predicting: 136it [01:20,  1.72it/s]Extractor Predicting: 137it [01:21,  1.66it/s]Extractor Predicting: 138it [01:22,  1.65it/s]Extractor Predicting: 139it [01:22,  1.69it/s]Extractor Predicting: 140it [01:23,  1.69it/s]Extractor Predicting: 141it [01:23,  1.70it/s]Extractor Predicting: 142it [01:24,  1.67it/s]Extractor Predicting: 143it [01:24,  1.70it/s]Extractor Predicting: 144it [01:25,  1.70it/s]Extractor Predicting: 145it [01:25,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:33,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:33,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:33,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:33,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:33,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:32:33,931 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:32:33,932 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:32:34,502 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:32:35,547 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:32:35,547 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:38,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:38,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:38,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:38,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:38,542 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:32:39,199 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:32:39,200 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:32:39,796 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:32:39,959 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:32:39,959 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:13,  1.76it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.63it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:18,  1.62it/s]Extractor Predicting: 32it [00:19,  1.62it/s]Extractor Predicting: 33it [00:19,  1.64it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.73it/s]Extractor Predicting: 38it [00:22,  1.73it/s]Extractor Predicting: 39it [00:23,  1.73it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:24,  1.73it/s]Extractor Predicting: 42it [00:25,  1.73it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:26,  1.72it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:27,  1.66it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:28,  1.70it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.69it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.73it/s]Extractor Predicting: 55it [00:32,  1.69it/s]Extractor Predicting: 56it [00:33,  1.70it/s]Extractor Predicting: 57it [00:33,  1.72it/s]Extractor Predicting: 58it [00:34,  1.73it/s]Extractor Predicting: 59it [00:35,  1.74it/s]Extractor Predicting: 60it [00:35,  1.83it/s]Extractor Predicting: 61it [00:36,  1.86it/s]Extractor Predicting: 62it [00:36,  1.89it/s]Extractor Predicting: 63it [00:37,  1.89it/s]Extractor Predicting: 64it [00:37,  1.90it/s]Extractor Predicting: 65it [00:38,  1.92it/s]Extractor Predicting: 66it [00:38,  1.92it/s]Extractor Predicting: 67it [00:39,  1.94it/s]Extractor Predicting: 68it [00:39,  1.99it/s]Extractor Predicting: 69it [00:40,  2.03it/s]Extractor Predicting: 70it [00:40,  2.00it/s]Extractor Predicting: 71it [00:41,  1.96it/s]Extractor Predicting: 72it [00:41,  1.98it/s]Extractor Predicting: 73it [00:42,  1.99it/s]Extractor Predicting: 74it [00:42,  1.97it/s]Extractor Predicting: 75it [00:43,  1.95it/s]Extractor Predicting: 76it [00:43,  2.00it/s]Extractor Predicting: 77it [00:44,  1.99it/s]Extractor Predicting: 78it [00:44,  1.98it/s]Extractor Predicting: 79it [00:45,  2.00it/s]Extractor Predicting: 80it [00:45,  2.01it/s]Extractor Predicting: 81it [00:46,  1.99it/s]Extractor Predicting: 82it [00:46,  1.97it/s]Extractor Predicting: 83it [00:47,  1.90it/s]Extractor Predicting: 84it [00:47,  1.91it/s]Extractor Predicting: 85it [00:48,  1.93it/s]Extractor Predicting: 86it [00:48,  1.88it/s]Extractor Predicting: 87it [00:49,  1.83it/s]Extractor Predicting: 88it [00:50,  1.80it/s]Extractor Predicting: 89it [00:50,  1.76it/s]Extractor Predicting: 90it [00:51,  1.68it/s]Extractor Predicting: 91it [00:51,  1.69it/s]Extractor Predicting: 92it [00:52,  1.67it/s]Extractor Predicting: 93it [00:53,  1.72it/s]Extractor Predicting: 94it [00:53,  1.70it/s]Extractor Predicting: 95it [00:54,  1.68it/s]Extractor Predicting: 96it [00:54,  1.67it/s]Extractor Predicting: 97it [00:55,  1.66it/s]Extractor Predicting: 98it [00:56,  1.66it/s]Extractor Predicting: 99it [00:56,  1.69it/s]Extractor Predicting: 100it [00:57,  1.68it/s]Extractor Predicting: 101it [00:57,  1.66it/s]Extractor Predicting: 102it [00:58,  1.65it/s]Extractor Predicting: 103it [00:59,  1.70it/s]Extractor Predicting: 104it [00:59,  1.66it/s]Extractor Predicting: 105it [01:00,  1.67it/s]Extractor Predicting: 106it [01:00,  1.62it/s]Extractor Predicting: 107it [01:01,  1.61it/s]Extractor Predicting: 108it [01:02,  1.62it/s]Extractor Predicting: 109it [01:02,  1.66it/s]Extractor Predicting: 110it [01:03,  1.66it/s]Extractor Predicting: 111it [01:03,  1.63it/s]Extractor Predicting: 112it [01:04,  1.60it/s]Extractor Predicting: 113it [01:05,  1.61it/s]Extractor Predicting: 114it [01:05,  1.67it/s]Extractor Predicting: 115it [01:06,  1.46it/s]Extractor Predicting: 116it [01:07,  1.51it/s]Extractor Predicting: 117it [01:07,  1.57it/s]Extractor Predicting: 118it [01:08,  1.56it/s]Extractor Predicting: 119it [01:09,  1.55it/s]Extractor Predicting: 120it [01:09,  1.57it/s]Extractor Predicting: 121it [01:10,  1.55it/s]Extractor Predicting: 122it [01:11,  1.59it/s]Extractor Predicting: 123it [01:11,  1.62it/s]Extractor Predicting: 124it [01:12,  1.62it/s]Extractor Predicting: 125it [01:12,  1.61it/s]Extractor Predicting: 126it [01:13,  1.57it/s]Extractor Predicting: 127it [01:14,  1.58it/s]Extractor Predicting: 128it [01:14,  1.56it/s]Extractor Predicting: 129it [01:15,  1.57it/s]Extractor Predicting: 130it [01:16,  1.57it/s]Extractor Predicting: 131it [01:16,  1.58it/s]Extractor Predicting: 132it [01:17,  1.59it/s]Extractor Predicting: 133it [01:17,  1.60it/s]Extractor Predicting: 134it [01:18,  1.59it/s]Extractor Predicting: 135it [01:19,  1.59it/s]Extractor Predicting: 136it [01:19,  1.56it/s]Extractor Predicting: 137it [01:20,  1.58it/s]Extractor Predicting: 138it [01:21,  1.58it/s]Extractor Predicting: 139it [01:21,  1.55it/s]Extractor Predicting: 140it [01:22,  1.58it/s]Extractor Predicting: 141it [01:23,  1.59it/s]Extractor Predicting: 142it [01:23,  1.61it/s]Extractor Predicting: 143it [01:24,  1.57it/s]Extractor Predicting: 144it [01:24,  1.57it/s]Extractor Predicting: 145it [01:25,  1.59it/s]Extractor Predicting: 146it [01:26,  1.60it/s]Extractor Predicting: 147it [01:26,  1.61it/s]Extractor Predicting: 148it [01:27,  1.64it/s]Extractor Predicting: 149it [01:28,  1.61it/s]Extractor Predicting: 150it [01:28,  1.62it/s]Extractor Predicting: 151it [01:29,  1.65it/s]Extractor Predicting: 152it [01:29,  1.68it/s]Extractor Predicting: 153it [01:30,  1.70it/s]Extractor Predicting: 154it [01:30,  1.73it/s]Extractor Predicting: 155it [01:31,  1.74it/s]Extractor Predicting: 156it [01:32,  1.74it/s]Extractor Predicting: 157it [01:32,  1.74it/s]Extractor Predicting: 158it [01:33,  1.76it/s]Extractor Predicting: 159it [01:33,  1.82it/s]Extractor Predicting: 160it [01:34,  1.76it/s]Extractor Predicting: 161it [01:34,  1.74it/s]Extractor Predicting: 162it [01:35,  1.70it/s]Extractor Predicting: 163it [01:36,  1.71it/s]Extractor Predicting: 164it [01:36,  1.69it/s]Extractor Predicting: 165it [01:37,  1.67it/s]Extractor Predicting: 166it [01:37,  1.68it/s]Extractor Predicting: 167it [01:38,  1.68it/s]Extractor Predicting: 168it [01:39,  1.68it/s]Extractor Predicting: 169it [01:39,  1.67it/s]Extractor Predicting: 170it [01:40,  1.70it/s]Extractor Predicting: 171it [01:40,  1.69it/s]Extractor Predicting: 172it [01:41,  1.67it/s]Extractor Predicting: 173it [01:42,  1.67it/s]Extractor Predicting: 174it [01:42,  1.67it/s]Extractor Predicting: 175it [01:43,  1.67it/s]Extractor Predicting: 176it [01:43,  1.67it/s]Extractor Predicting: 177it [01:44,  1.73it/s]Extractor Predicting: 178it [01:44,  1.71it/s]Extractor Predicting: 179it [01:45,  1.75it/s]Extractor Predicting: 180it [01:46,  1.76it/s]Extractor Predicting: 181it [01:46,  1.74it/s]Extractor Predicting: 182it [01:47,  1.76it/s]Extractor Predicting: 183it [01:47,  1.77it/s]Extractor Predicting: 184it [01:48,  1.75it/s]Extractor Predicting: 185it [01:48,  1.79it/s]Extractor Predicting: 186it [01:49,  1.79it/s]Extractor Predicting: 187it [01:50,  1.75it/s]Extractor Predicting: 188it [01:50,  1.80it/s]Extractor Predicting: 189it [01:51,  1.81it/s]Extractor Predicting: 190it [01:51,  1.86it/s]Extractor Predicting: 191it [01:52,  1.77it/s]Extractor Predicting: 192it [01:52,  1.78it/s]Extractor Predicting: 193it [01:53,  1.80it/s]Extractor Predicting: 194it [01:53,  1.84it/s]Extractor Predicting: 195it [01:54,  1.81it/s]Extractor Predicting: 196it [01:54,  1.81it/s]Extractor Predicting: 197it [01:55,  1.83it/s]Extractor Predicting: 198it [01:56,  1.79it/s]Extractor Predicting: 199it [01:56,  1.75it/s]Extractor Predicting: 200it [01:57,  1.77it/s]Extractor Predicting: 201it [01:57,  1.75it/s]Extractor Predicting: 202it [01:58,  1.79it/s]Extractor Predicting: 203it [01:58,  1.83it/s]Extractor Predicting: 204it [01:59,  1.81it/s]Extractor Predicting: 205it [01:59,  1.82it/s]Extractor Predicting: 206it [02:00,  1.89it/s]Extractor Predicting: 207it [02:01,  1.85it/s]Extractor Predicting: 208it [02:01,  1.91it/s]Extractor Predicting: 209it [02:02,  1.88it/s]Extractor Predicting: 210it [02:02,  1.86it/s]Extractor Predicting: 211it [02:03,  1.86it/s]Extractor Predicting: 212it [02:03,  1.91it/s]Extractor Predicting: 213it [02:04,  1.93it/s]Extractor Predicting: 214it [02:04,  1.95it/s]Extractor Predicting: 215it [02:05,  2.00it/s]Extractor Predicting: 216it [02:05,  1.99it/s]Extractor Predicting: 217it [02:06,  1.93it/s]Extractor Predicting: 218it [02:06,  1.91it/s]Extractor Predicting: 219it [02:07,  1.93it/s]Extractor Predicting: 220it [02:07,  1.91it/s]Extractor Predicting: 221it [02:08,  1.93it/s]Extractor Predicting: 222it [02:08,  2.00it/s]Extractor Predicting: 223it [02:09,  1.93it/s]Extractor Predicting: 224it [02:09,  1.97it/s]Extractor Predicting: 225it [02:10,  1.95it/s]Extractor Predicting: 226it [02:10,  1.96it/s]Extractor Predicting: 227it [02:11,  1.99it/s]Extractor Predicting: 228it [02:11,  1.97it/s]Extractor Predicting: 229it [02:12,  1.86it/s]Extractor Predicting: 230it [02:13,  1.76it/s]Extractor Predicting: 231it [02:13,  1.69it/s]Extractor Predicting: 232it [02:14,  1.66it/s]Extractor Predicting: 233it [02:14,  1.65it/s]Extractor Predicting: 234it [02:15,  1.64it/s]Extractor Predicting: 235it [02:16,  1.65it/s]Extractor Predicting: 236it [02:16,  1.65it/s]Extractor Predicting: 237it [02:17,  1.61it/s]Extractor Predicting: 238it [02:18,  1.54it/s]Extractor Predicting: 239it [02:18,  1.57it/s]Extractor Predicting: 240it [02:19,  1.58it/s]Extractor Predicting: 241it [02:20,  1.34it/s]Extractor Predicting: 242it [02:20,  1.43it/s]Extractor Predicting: 243it [02:21,  1.44it/s]Extractor Predicting: 244it [02:22,  1.50it/s]Extractor Predicting: 245it [02:22,  1.53it/s]Extractor Predicting: 246it [02:23,  1.52it/s]Extractor Predicting: 247it [02:24,  1.50it/s]Extractor Predicting: 248it [02:24,  1.52it/s]Extractor Predicting: 249it [02:25,  1.49it/s]Extractor Predicting: 250it [02:26,  1.52it/s]Extractor Predicting: 251it [02:26,  1.53it/s]Extractor Predicting: 252it [02:27,  1.56it/s]Extractor Predicting: 253it [02:28,  1.58it/s]Extractor Predicting: 254it [02:28,  1.57it/s]Extractor Predicting: 255it [02:29,  1.58it/s]Extractor Predicting: 256it [02:29,  1.58it/s]Extractor Predicting: 257it [02:30,  1.65it/s]Extractor Predicting: 258it [02:31,  1.67it/s]Extractor Predicting: 259it [02:31,  1.69it/s]Extractor Predicting: 260it [02:32,  1.72it/s]Extractor Predicting: 261it [02:32,  1.72it/s]Extractor Predicting: 262it [02:33,  1.72it/s]Extractor Predicting: 263it [02:33,  1.73it/s]Extractor Predicting: 264it [02:34,  1.74it/s]Extractor Predicting: 265it [02:35,  1.70it/s]Extractor Predicting: 266it [02:35,  1.73it/s]Extractor Predicting: 267it [02:36,  1.73it/s]Extractor Predicting: 268it [02:36,  1.70it/s]Extractor Predicting: 269it [02:37,  1.72it/s]Extractor Predicting: 270it [02:38,  1.71it/s]Extractor Predicting: 271it [02:38,  1.70it/s]Extractor Predicting: 272it [02:39,  1.70it/s]Extractor Predicting: 273it [02:39,  1.69it/s]Extractor Predicting: 274it [02:40,  1.68it/s]Extractor Predicting: 275it [02:40,  1.72it/s]Extractor Predicting: 276it [02:41,  1.72it/s]Extractor Predicting: 277it [02:42,  1.70it/s]Extractor Predicting: 278it [02:42,  1.66it/s]Extractor Predicting: 279it [02:43,  1.70it/s]Extractor Predicting: 280it [02:43,  1.69it/s]Extractor Predicting: 281it [02:44,  1.73it/s]Extractor Predicting: 282it [02:45,  1.71it/s]Extractor Predicting: 283it [02:45,  1.72it/s]Extractor Predicting: 284it [02:46,  1.74it/s]Extractor Predicting: 285it [02:46,  1.71it/s]Extractor Predicting: 286it [02:47,  1.69it/s]Extractor Predicting: 287it [02:48,  1.72it/s]Extractor Predicting: 288it [02:48,  1.69it/s]Extractor Predicting: 289it [02:49,  1.69it/s]Extractor Predicting: 290it [02:49,  1.70it/s]Extractor Predicting: 291it [02:50,  1.70it/s]Extractor Predicting: 292it [02:50,  1.71it/s]Extractor Predicting: 293it [02:51,  1.70it/s]Extractor Predicting: 294it [02:52,  1.71it/s]Extractor Predicting: 295it [02:52,  1.71it/s]Extractor Predicting: 296it [02:53,  1.71it/s]Extractor Predicting: 297it [02:53,  1.73it/s]Extractor Predicting: 298it [02:54,  1.69it/s]Extractor Predicting: 299it [02:55,  1.74it/s]Extractor Predicting: 300it [02:55,  1.72it/s]Extractor Predicting: 301it [02:56,  1.74it/s]Extractor Predicting: 302it [02:56,  1.73it/s]Extractor Predicting: 303it [02:57,  1.78it/s]Extractor Predicting: 304it [02:57,  1.76it/s]Extractor Predicting: 305it [02:58,  1.81it/s]Extractor Predicting: 306it [02:58,  1.78it/s]Extractor Predicting: 307it [02:59,  1.75it/s]Extractor Predicting: 308it [03:00,  1.74it/s]Extractor Predicting: 309it [03:00,  1.76it/s]Extractor Predicting: 310it [03:01,  1.71it/s]Extractor Predicting: 311it [03:01,  1.71it/s]Extractor Predicting: 312it [03:02,  1.71it/s]Extractor Predicting: 313it [03:03,  1.74it/s]Extractor Predicting: 314it [03:03,  1.73it/s]Extractor Predicting: 315it [03:04,  1.76it/s]Extractor Predicting: 316it [03:04,  1.75it/s]Extractor Predicting: 317it [03:05,  1.74it/s]Extractor Predicting: 318it [03:05,  1.80it/s]Extractor Predicting: 319it [03:06,  1.79it/s]Extractor Predicting: 320it [03:06,  1.83it/s]Extractor Predicting: 321it [03:07,  1.79it/s]Extractor Predicting: 322it [03:08,  1.77it/s]Extractor Predicting: 323it [03:08,  1.76it/s]Extractor Predicting: 324it [03:09,  1.74it/s]Extractor Predicting: 325it [03:09,  1.73it/s]Extractor Predicting: 326it [03:10,  1.73it/s]Extractor Predicting: 327it [03:11,  1.71it/s]Extractor Predicting: 328it [03:11,  1.70it/s]Extractor Predicting: 329it [03:12,  1.65it/s]Extractor Predicting: 330it [03:12,  1.67it/s]Extractor Predicting: 331it [03:13,  1.68it/s]Extractor Predicting: 332it [03:13,  1.73it/s]Extractor Predicting: 333it [03:14,  1.73it/s]Extractor Predicting: 334it [03:15,  1.71it/s]Extractor Predicting: 335it [03:15,  1.71it/s]Extractor Predicting: 336it [03:16,  1.71it/s]Extractor Predicting: 337it [03:16,  1.71it/s]Extractor Predicting: 338it [03:17,  1.71it/s]Extractor Predicting: 339it [03:18,  1.48it/s]Extractor Predicting: 340it [03:18,  1.54it/s]Extractor Predicting: 341it [03:19,  1.57it/s]Extractor Predicting: 342it [03:20,  1.64it/s]Extractor Predicting: 343it [03:20,  1.61it/s]Extractor Predicting: 344it [03:21,  1.65it/s]Extractor Predicting: 345it [03:21,  1.63it/s]Extractor Predicting: 346it [03:22,  1.67it/s]Extractor Predicting: 347it [03:23,  1.68it/s]Extractor Predicting: 348it [03:23,  1.71it/s]Extractor Predicting: 349it [03:24,  1.63it/s]Extractor Predicting: 350it [03:25,  1.58it/s]Extractor Predicting: 351it [03:25,  1.58it/s]Extractor Predicting: 352it [03:26,  1.61it/s]Extractor Predicting: 353it [03:26,  1.61it/s]Extractor Predicting: 354it [03:27,  1.63it/s]Extractor Predicting: 355it [03:28,  1.61it/s]Extractor Predicting: 356it [03:28,  1.66it/s]Extractor Predicting: 357it [03:29,  1.64it/s]Extractor Predicting: 358it [03:29,  1.63it/s]Extractor Predicting: 359it [03:30,  1.62it/s]Extractor Predicting: 360it [03:31,  1.57it/s]Extractor Predicting: 361it [03:31,  1.63it/s]Extractor Predicting: 362it [03:32,  1.67it/s]Extractor Predicting: 363it [03:32,  1.66it/s]Extractor Predicting: 364it [03:33,  1.69it/s]Extractor Predicting: 365it [03:34,  1.70it/s]Extractor Predicting: 366it [03:34,  1.65it/s]Extractor Predicting: 367it [03:35,  1.65it/s]Extractor Predicting: 368it [03:36,  1.63it/s]Extractor Predicting: 369it [03:36,  1.66it/s]Extractor Predicting: 370it [03:37,  1.65it/s]Extractor Predicting: 371it [03:37,  1.66it/s]Extractor Predicting: 372it [03:38,  1.68it/s]Extractor Predicting: 373it [03:38,  1.71it/s]Extractor Predicting: 374it [03:39,  1.72it/s]Extractor Predicting: 375it [03:40,  1.70it/s]Extractor Predicting: 376it [03:40,  1.73it/s]Extractor Predicting: 377it [03:41,  1.72it/s]Extractor Predicting: 378it [03:41,  1.74it/s]Extractor Predicting: 379it [03:42,  1.76it/s]Extractor Predicting: 380it [03:42,  1.76it/s]Extractor Predicting: 381it [03:43,  1.75it/s]Extractor Predicting: 382it [03:44,  1.72it/s]Extractor Predicting: 383it [03:44,  1.73it/s]Extractor Predicting: 384it [03:45,  1.75it/s]Extractor Predicting: 385it [03:45,  1.76it/s]Extractor Predicting: 386it [03:46,  1.80it/s]Extractor Predicting: 387it [03:46,  1.81it/s]Extractor Predicting: 388it [03:47,  1.77it/s]Extractor Predicting: 389it [03:48,  1.75it/s]Extractor Predicting: 390it [03:48,  1.75it/s]Extractor Predicting: 391it [03:49,  1.75it/s]Extractor Predicting: 392it [03:49,  1.76it/s]Extractor Predicting: 393it [03:50,  1.77it/s]Extractor Predicting: 394it [03:50,  1.79it/s]Extractor Predicting: 395it [03:51,  1.71it/s]Extractor Predicting: 396it [03:52,  1.74it/s]Extractor Predicting: 397it [03:52,  1.77it/s]Extractor Predicting: 398it [03:53,  1.73it/s]Extractor Predicting: 399it [03:53,  1.73it/s]Extractor Predicting: 400it [03:54,  1.73it/s]Extractor Predicting: 401it [03:54,  1.75it/s]Extractor Predicting: 402it [03:55,  1.78it/s]Extractor Predicting: 403it [03:56,  1.76it/s]Extractor Predicting: 404it [03:56,  1.79it/s]Extractor Predicting: 405it [03:57,  1.79it/s]Extractor Predicting: 406it [03:57,  1.79it/s]Extractor Predicting: 407it [03:58,  1.77it/s]Extractor Predicting: 408it [03:58,  1.78it/s]Extractor Predicting: 409it [03:59,  1.81it/s]Extractor Predicting: 410it [03:59,  1.80it/s]Extractor Predicting: 411it [04:00,  1.76it/s]Extractor Predicting: 412it [04:01,  1.79it/s]Extractor Predicting: 413it [04:01,  1.80it/s]Extractor Predicting: 414it [04:02,  1.78it/s]Extractor Predicting: 415it [04:02,  1.82it/s]Extractor Predicting: 416it [04:03,  1.82it/s]Extractor Predicting: 417it [04:03,  1.80it/s]Extractor Predicting: 418it [04:04,  1.82it/s]Extractor Predicting: 419it [04:04,  1.80it/s]Extractor Predicting: 420it [04:05,  1.77it/s]Extractor Predicting: 421it [04:06,  1.77it/s]Extractor Predicting: 422it [04:06,  1.76it/s]Extractor Predicting: 423it [04:07,  1.77it/s]Extractor Predicting: 424it [04:08,  1.54it/s]Extractor Predicting: 425it [04:08,  1.79it/s]Extractor Predicting: 425it [04:08,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:00,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:00,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:00,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:00,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:00,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:37:01,099 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:37:01,100 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:37:01,768 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:37:02,930 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:37:02,930 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:06,236 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:06,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:06,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:06,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:37:06,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:37:07,178 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:37:07,180 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:37:07,810 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:37:08,067 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:37:08,067 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.87it/s]Extractor Predicting: 7it [00:04,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:37:14,220 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:37:14,222 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:37:14,241 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:37:14,243 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:37:14,253 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:37:22,822 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:37:22,841 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:37:22,922 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:37:22,922 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:37:22,961 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,993 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,993 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,993 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,993 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,993 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:37:22,994 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:37:23,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:23,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:24,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:25,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:26,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:27,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:28,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:28,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:29,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:30,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:31,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:31,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:32,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:33,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:33,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:34,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:35,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:35,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:36,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:37,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:37,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:38,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:16, 16.68s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:40,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:40,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:41,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:42,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:43,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:43,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:44,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:45,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:46,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:46,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:47,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:48,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:49,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:49,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:50,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:51,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:52,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:52,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:53,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:53,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:54,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:55,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:56,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:57,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:58,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:35<05:23, 17.98s/it][WARNING|generation_utils.py:914] 2023-08-28 10:37:58,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:37:59,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:00,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:00,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:01,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:02,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:03,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:04,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:05,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:06,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:06,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:07,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:08,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:09,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:09,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:10,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:10,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:11,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:12,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:13,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:13,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:14,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:15,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:52<04:57, 17.52s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:15,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:16,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:17,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:18,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:19,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:19,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:20,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:21,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:22,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:22,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:23,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:24,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:25,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:25,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:27,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:27,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:28,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:28,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:29,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:30,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:31,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:31,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:32,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:09<04:36, 17.31s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:32,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:33,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:34,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:34,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:35,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:36,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:36,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:37,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:37,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:38,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:39,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:39,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:40,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:41,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:41,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:42,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:43,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:44,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:44,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:45,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:45,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:46,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:47,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:47,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:48,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:49,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:50,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:50,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:51,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:29<04:32, 18.15s/it][WARNING|generation_utils.py:914] 2023-08-28 10:38:52,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:53,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:53,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:54,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:54,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:56,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:56,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:57,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:58,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:58,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:38:59,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:00,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:00,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:01,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:02,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:02,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:04,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:04,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:05,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:06,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:06,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<03:59, 17.10s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:07,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:08,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:08,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:09,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:10,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:10,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:11,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:12,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:12,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:13,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:14,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:15,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:15,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:16,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:17,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:17,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:18,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:19,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:20,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:21,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:21,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:22,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:23,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:23,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:24,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:25,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:25,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:26,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:26,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:04<03:55, 18.08s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:27,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:28,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:28,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:29,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:30,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:31,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:31,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:32,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:33,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:33,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:34,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:35,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:35,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:36,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:37,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:37,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:38,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:39,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:39,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:40,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:40,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:41,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:42,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:43,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:43,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:44,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:44,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:22<03:36, 18.02s/it][WARNING|generation_utils.py:914] 2023-08-28 10:39:45,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:46,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:46,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:47,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:48,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:49,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:49,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:50,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:51,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:52,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:52,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:53,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:54,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:55,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:55,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:56,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:56,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:57,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:58,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:39:59,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:00,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:01,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:01,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:02,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:03,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:41<03:21, 18.35s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:04,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:05,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:05,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:06,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:07,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:07,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:08,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:09,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:09,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:10,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:11,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:11,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:12,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:13,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:13,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:14,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:15,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:16,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:17,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:18,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:19,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:20,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:21,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:58<02:59, 17.94s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:21,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:22,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:23,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:23,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:24,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:25,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:26,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:26,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:28,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:29,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:30,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:30,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:31,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:32,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:32,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:33,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:34,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:34,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:36,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:37,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:37,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:38,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:15<02:39, 17.77s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:39,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:39,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:40,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:41,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:42,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:42,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:43,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:44,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:44,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:45,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:46,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:46,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:47,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:48,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:49,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:49,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:50,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:51,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:52,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:52,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:53,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:54,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:54,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:55,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:56,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:33<02:23, 17.89s/it][WARNING|generation_utils.py:914] 2023-08-28 10:40:57,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:57,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:58,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:58,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:40:59,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:00,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:01,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:01,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:02,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:03,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:03,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:04,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:04,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:05,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:06,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:06,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:07,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:08,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:08,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:09,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:09,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:10,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:11,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:12,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:12,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:50<02:01, 17.36s/it][WARNING|generation_utils.py:914] 2023-08-28 10:41:13,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:13,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:14,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:15,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:16,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:16,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:17,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:18,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:19,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:20,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:21,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:21,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:22,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:23,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:24,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:24,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:25,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:26,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:27,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:27,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:28,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:29,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:30,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:30,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:31,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:32,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:09<01:48, 18.09s/it][WARNING|generation_utils.py:914] 2023-08-28 10:41:33,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:33,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:34,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:35,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:36,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:36,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:37,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:38,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:38,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:39,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:40,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:40,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:41,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:42,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:42,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:43,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:44,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:44,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:45,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:46,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:46,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:47,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:48,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:48,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:49,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:26<01:28, 17.78s/it][WARNING|generation_utils.py:914] 2023-08-28 10:41:50,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:50,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:51,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:52,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:53,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:54,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:54,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:55,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:56,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:57,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:58,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:41:59,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:00,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:00,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:01,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:02,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:03,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:03,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:04,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:05,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:05,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:06,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:07,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:08,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:08,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:09,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:10,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:47<01:14, 18.65s/it][WARNING|generation_utils.py:914] 2023-08-28 10:42:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:11,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:12,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:12,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:13,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:14,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:14,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:15,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:16,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:16,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:17,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:18,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:18,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:19,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:20,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:20,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:21,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:22,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:22,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:23,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:24,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:25,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:25,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:26,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:03<00:53, 17.91s/it][WARNING|generation_utils.py:914] 2023-08-28 10:42:26,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:27,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:28,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:29,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:29,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:30,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:31,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:32,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:32,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:33,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:34,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:34,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:35,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:36,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:36,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:37,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:38,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:38,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:39,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:40,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:40,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:41,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:42,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:19<00:34, 17.34s/it][WARNING|generation_utils.py:914] 2023-08-28 10:42:43,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:43,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:44,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:44,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:45,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:46,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:46,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:47,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:48,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:48,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:49,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:50,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:50,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:51,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:51,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:53,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:53,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:54,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:55,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:55,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:56,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:56,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:57,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:58,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:58,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:42:59,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:00,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:01,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:01,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:02,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:03,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:03,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:04,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:05,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:42<00:18, 18.98s/it][WARNING|generation_utils.py:914] 2023-08-28 10:43:05,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:06,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:07,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:07,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:08,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:09,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:09,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:10,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:11,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:11,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:12,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:13,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:13,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:14,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:15,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:16,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:16,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:17,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:18,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:19,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:19,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:20,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:21,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:43:21,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:58<00:00, 18.23s/it]Generating: 100%|| 20/20 [05:58<00:00, 17.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:31,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:31,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:31,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:31,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:31,550 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:43:32,182 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:43:32,183 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:43:32,758 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:43:33,844 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:43:33,844 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:36,848 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:36,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:36,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:36,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:43:36,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:43:37,533 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:43:37,534 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:43:38,167 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:43:38,339 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:43:38,340 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.49it/s]Extractor Estimating: 6it [00:04,  1.52it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.46it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:12,  1.50it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:14,  1.48it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:16,  1.44it/s]Extractor Estimating: 25it [00:16,  1.49it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:18,  1.56it/s]Extractor Estimating: 28it [00:18,  1.55it/s]Extractor Estimating: 29it [00:19,  1.55it/s]Extractor Estimating: 30it [00:19,  1.61it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:21,  1.61it/s]Extractor Estimating: 33it [00:21,  1.54it/s]Extractor Estimating: 34it [00:22,  1.55it/s]Extractor Estimating: 35it [00:23,  1.58it/s]Extractor Estimating: 36it [00:23,  1.54it/s]Extractor Estimating: 37it [00:24,  1.54it/s]Extractor Estimating: 38it [00:25,  1.51it/s]Extractor Estimating: 39it [00:25,  1.54it/s]Extractor Estimating: 40it [00:26,  1.55it/s]Extractor Estimating: 41it [00:27,  1.52it/s]Extractor Estimating: 42it [00:27,  1.58it/s]Extractor Estimating: 43it [00:28,  1.57it/s]Extractor Estimating: 44it [00:29,  1.52it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:30,  1.37it/s]Extractor Estimating: 47it [00:31,  1.40it/s]Extractor Estimating: 48it [00:31,  1.41it/s]Extractor Estimating: 49it [00:32,  1.40it/s]Extractor Estimating: 50it [00:33,  1.40it/s]Extractor Estimating: 51it [00:34,  1.44it/s]Extractor Estimating: 52it [00:34,  1.50it/s]Extractor Estimating: 53it [00:35,  1.52it/s]Extractor Estimating: 54it [00:35,  1.52it/s]Extractor Estimating: 55it [00:36,  1.57it/s]Extractor Estimating: 56it [00:37,  1.59it/s]Extractor Estimating: 57it [00:37,  1.53it/s]Extractor Estimating: 58it [00:38,  1.59it/s]Extractor Estimating: 59it [00:39,  1.56it/s]Extractor Estimating: 60it [00:39,  1.53it/s]Extractor Estimating: 61it [00:40,  1.50it/s]Extractor Estimating: 62it [00:41,  1.55it/s]Extractor Estimating: 63it [00:41,  1.56it/s]Extractor Estimating: 64it [00:42,  1.54it/s]Extractor Estimating: 65it [00:43,  1.51it/s]Extractor Estimating: 66it [00:43,  1.57it/s]Extractor Estimating: 67it [00:44,  1.60it/s]Extractor Estimating: 68it [00:44,  1.61it/s]Extractor Estimating: 69it [00:45,  1.60it/s]Extractor Estimating: 70it [00:46,  1.55it/s]Extractor Estimating: 71it [00:46,  1.51it/s]Extractor Estimating: 72it [00:47,  1.52it/s]Extractor Estimating: 73it [00:48,  1.50it/s]Extractor Estimating: 74it [00:48,  1.56it/s]Extractor Estimating: 75it [00:49,  1.58it/s]Extractor Estimating: 76it [00:50,  1.59it/s]Extractor Estimating: 77it [00:50,  1.61it/s]Extractor Estimating: 78it [00:51,  1.60it/s]Extractor Estimating: 79it [00:51,  1.60it/s]Extractor Estimating: 80it [00:52,  1.58it/s]Extractor Estimating: 81it [00:53,  1.57it/s]Extractor Estimating: 82it [00:53,  1.57it/s]Extractor Estimating: 83it [00:54,  1.55it/s]Extractor Estimating: 84it [00:55,  1.53it/s]Extractor Estimating: 85it [00:55,  1.50it/s]Extractor Estimating: 86it [00:56,  1.37it/s]Extractor Estimating: 87it [00:57,  1.43it/s]Extractor Estimating: 88it [00:57,  1.48it/s]Extractor Estimating: 89it [00:58,  1.52it/s]Extractor Estimating: 90it [00:59,  1.46it/s]Extractor Estimating: 91it [01:00,  1.46it/s]Extractor Estimating: 92it [01:00,  1.52it/s]Extractor Estimating: 93it [01:01,  1.54it/s]Extractor Estimating: 94it [01:01,  1.58it/s]Extractor Estimating: 95it [01:02,  1.60it/s]Extractor Estimating: 96it [01:03,  1.51it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:04,  1.57it/s]Extractor Estimating: 99it [01:05,  1.54it/s]Extractor Estimating: 100it [01:05,  1.56it/s]Extractor Estimating: 101it [01:06,  1.53it/s]Extractor Estimating: 102it [01:07,  1.57it/s]Extractor Estimating: 103it [01:07,  1.55it/s]Extractor Estimating: 104it [01:08,  1.54it/s]Extractor Estimating: 105it [01:08,  1.58it/s]Extractor Estimating: 106it [01:09,  1.56it/s]Extractor Estimating: 107it [01:10,  1.57it/s]Extractor Estimating: 108it [01:10,  1.62it/s]Extractor Estimating: 109it [01:11,  1.63it/s]Extractor Estimating: 110it [01:12,  1.58it/s]Extractor Estimating: 111it [01:12,  1.60it/s]Extractor Estimating: 112it [01:13,  1.59it/s]Extractor Estimating: 113it [01:13,  1.62it/s]Extractor Estimating: 114it [01:14,  1.60it/s]Extractor Estimating: 115it [01:15,  1.60it/s]Extractor Estimating: 116it [01:15,  1.62it/s]Extractor Estimating: 117it [01:16,  1.64it/s]Extractor Estimating: 118it [01:16,  1.66it/s]Extractor Estimating: 119it [01:17,  1.62it/s]Extractor Estimating: 120it [01:18,  1.58it/s]Extractor Estimating: 121it [01:18,  1.58it/s]Extractor Estimating: 122it [01:19,  1.58it/s]Extractor Estimating: 123it [01:20,  1.60it/s]Extractor Estimating: 124it [01:20,  1.58it/s]Extractor Estimating: 125it [01:21,  1.45it/s]Extractor Estimating: 126it [01:22,  1.45it/s]Extractor Estimating: 127it [01:22,  1.47it/s]Extractor Estimating: 128it [01:23,  1.47it/s]Extractor Estimating: 129it [01:24,  1.47it/s]Extractor Estimating: 130it [01:25,  1.45it/s]Extractor Estimating: 131it [01:25,  1.47it/s]Extractor Estimating: 132it [01:26,  1.44it/s]Extractor Estimating: 133it [01:27,  1.46it/s]Extractor Estimating: 134it [01:27,  1.41it/s]Extractor Estimating: 135it [01:28,  1.42it/s]Extractor Estimating: 136it [01:29,  1.41it/s]Extractor Estimating: 137it [01:29,  1.43it/s]Extractor Estimating: 138it [01:30,  1.44it/s]Extractor Estimating: 139it [01:31,  1.37it/s]Extractor Estimating: 140it [01:32,  1.41it/s]Extractor Estimating: 141it [01:32,  1.44it/s]Extractor Estimating: 142it [01:33,  1.48it/s]Extractor Estimating: 143it [01:34,  1.53it/s]Extractor Estimating: 144it [01:34,  1.44it/s]Extractor Estimating: 145it [01:35,  1.42it/s]Extractor Estimating: 146it [01:36,  1.39it/s]Extractor Estimating: 147it [01:36,  1.43it/s]Extractor Estimating: 148it [01:37,  1.41it/s]Extractor Estimating: 149it [01:38,  1.38it/s]Extractor Estimating: 150it [01:39,  1.42it/s]Extractor Estimating: 151it [01:39,  1.47it/s]Extractor Estimating: 152it [01:40,  1.53it/s]Extractor Estimating: 153it [01:40,  1.60it/s]Extractor Estimating: 154it [01:41,  1.55it/s]Extractor Estimating: 155it [01:42,  1.57it/s]Extractor Estimating: 156it [01:42,  1.59it/s]Extractor Estimating: 157it [01:43,  1.63it/s]Extractor Estimating: 158it [01:43,  1.66it/s]Extractor Estimating: 159it [01:44,  1.71it/s]Extractor Estimating: 160it [01:45,  1.63it/s]Extractor Estimating: 161it [01:45,  1.61it/s]Extractor Estimating: 162it [01:46,  1.55it/s]Extractor Estimating: 163it [01:47,  1.52it/s]Extractor Estimating: 164it [01:47,  1.57it/s]Extractor Estimating: 165it [01:48,  1.55it/s]Extractor Estimating: 166it [01:49,  1.53it/s]Extractor Estimating: 167it [01:49,  1.54it/s]Extractor Estimating: 168it [01:50,  1.55it/s]Extractor Estimating: 169it [01:50,  1.57it/s]Extractor Estimating: 170it [01:51,  1.57it/s]Extractor Estimating: 171it [01:52,  1.57it/s]Extractor Estimating: 172it [01:52,  1.58it/s]Extractor Estimating: 173it [01:53,  1.63it/s]Extractor Estimating: 174it [01:54,  1.64it/s]Extractor Estimating: 175it [01:54,  1.62it/s]Extractor Estimating: 176it [01:55,  1.63it/s]Extractor Estimating: 177it [01:55,  1.61it/s]Extractor Estimating: 178it [01:56,  1.59it/s]Extractor Estimating: 179it [01:57,  1.50it/s]Extractor Estimating: 180it [01:57,  1.53it/s]Extractor Estimating: 181it [01:58,  1.58it/s]Extractor Estimating: 182it [01:59,  1.60it/s]Extractor Estimating: 183it [01:59,  1.58it/s]Extractor Estimating: 184it [02:00,  1.59it/s]Extractor Estimating: 185it [02:01,  1.59it/s]Extractor Estimating: 186it [02:01,  1.58it/s]Extractor Estimating: 187it [02:02,  1.62it/s]Extractor Estimating: 188it [02:02,  1.57it/s]Extractor Estimating: 189it [02:03,  1.58it/s]Extractor Estimating: 190it [02:04,  1.58it/s]Extractor Estimating: 191it [02:04,  1.62it/s]Extractor Estimating: 192it [02:05,  1.67it/s]Extractor Estimating: 193it [02:05,  1.71it/s]Extractor Estimating: 194it [02:06,  1.67it/s]Extractor Estimating: 195it [02:07,  1.65it/s]Extractor Estimating: 196it [02:07,  1.54it/s]Extractor Estimating: 197it [02:08,  1.63it/s]Extractor Estimating: 198it [02:09,  1.66it/s]Extractor Estimating: 199it [02:09,  1.67it/s]Extractor Estimating: 200it [02:10,  1.69it/s]Extractor Estimating: 201it [02:10,  1.67it/s]Extractor Estimating: 202it [02:11,  1.62it/s]Extractor Estimating: 203it [02:12,  1.62it/s]Extractor Estimating: 204it [02:12,  1.59it/s]Extractor Estimating: 205it [02:13,  1.59it/s]Extractor Estimating: 206it [02:14,  1.56it/s]Extractor Estimating: 207it [02:14,  1.47it/s]Extractor Estimating: 208it [02:15,  1.44it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:16,  1.52it/s]Extractor Estimating: 211it [02:17,  1.44it/s]Extractor Estimating: 212it [02:18,  1.51it/s]Extractor Estimating: 213it [02:19,  1.35it/s]Extractor Estimating: 214it [02:19,  1.39it/s]Extractor Estimating: 215it [02:20,  1.42it/s]Extractor Estimating: 216it [02:20,  1.50it/s]Extractor Estimating: 217it [02:21,  1.56it/s]Extractor Estimating: 218it [02:22,  1.49it/s]Extractor Estimating: 219it [02:22,  1.48it/s]Extractor Estimating: 220it [02:23,  1.45it/s]Extractor Estimating: 221it [02:24,  1.49it/s]Extractor Estimating: 222it [02:24,  1.50it/s]Extractor Estimating: 223it [02:25,  1.52it/s]Extractor Estimating: 224it [02:26,  1.53it/s]Extractor Estimating: 225it [02:27,  1.47it/s]Extractor Estimating: 226it [02:27,  1.47it/s]Extractor Estimating: 227it [02:28,  1.45it/s]Extractor Estimating: 228it [02:29,  1.47it/s]Extractor Estimating: 229it [02:29,  1.47it/s]Extractor Estimating: 230it [02:30,  1.53it/s]Extractor Estimating: 231it [02:30,  1.56it/s]Extractor Estimating: 232it [02:31,  1.54it/s]Extractor Estimating: 233it [02:32,  1.53it/s]Extractor Estimating: 234it [02:32,  1.49it/s]Extractor Estimating: 235it [02:33,  1.52it/s]Extractor Estimating: 236it [02:34,  1.54it/s]Extractor Estimating: 237it [02:34,  1.56it/s]Extractor Estimating: 238it [02:35,  1.54it/s]Extractor Estimating: 239it [02:36,  1.52it/s]Extractor Estimating: 240it [02:36,  1.48it/s]Extractor Estimating: 241it [02:37,  1.50it/s]Extractor Estimating: 242it [02:38,  1.50it/s]Extractor Estimating: 243it [02:38,  1.47it/s]Extractor Estimating: 244it [02:39,  1.44it/s]Extractor Estimating: 245it [02:40,  1.44it/s]Extractor Estimating: 246it [02:40,  1.50it/s]Extractor Estimating: 247it [02:41,  1.47it/s]Extractor Estimating: 248it [02:42,  1.44it/s]Extractor Estimating: 249it [02:43,  1.46it/s]Extractor Estimating: 250it [02:43,  1.47it/s]Extractor Estimating: 251it [02:44,  1.54it/s]Extractor Estimating: 252it [02:44,  1.58it/s]Extractor Estimating: 253it [02:45,  1.53it/s]Extractor Estimating: 254it [02:46,  1.49it/s]Extractor Estimating: 255it [02:46,  1.51it/s]Extractor Estimating: 256it [02:47,  1.55it/s]Extractor Estimating: 257it [02:48,  1.54it/s]Extractor Estimating: 258it [02:48,  1.56it/s]Extractor Estimating: 259it [02:49,  1.56it/s]Extractor Estimating: 260it [02:50,  1.59it/s]Extractor Estimating: 261it [02:50,  1.55it/s]Extractor Estimating: 262it [02:51,  1.58it/s]Extractor Estimating: 263it [02:52,  1.52it/s]Extractor Estimating: 264it [02:52,  1.50it/s]Extractor Estimating: 265it [02:53,  1.52it/s]Extractor Estimating: 266it [02:54,  1.53it/s]Extractor Estimating: 267it [02:54,  1.54it/s]Extractor Estimating: 268it [02:55,  1.53it/s]Extractor Estimating: 269it [02:56,  1.47it/s]Extractor Estimating: 270it [02:56,  1.44it/s]Extractor Estimating: 271it [02:57,  1.45it/s]Extractor Estimating: 272it [02:58,  1.49it/s]Extractor Estimating: 273it [02:58,  1.49it/s]Extractor Estimating: 274it [02:59,  1.44it/s]Extractor Estimating: 275it [03:00,  1.47it/s]Extractor Estimating: 276it [03:00,  1.52it/s]Extractor Estimating: 277it [03:01,  1.49it/s]Extractor Estimating: 278it [03:02,  1.52it/s]Extractor Estimating: 279it [03:02,  1.47it/s]Extractor Estimating: 280it [03:03,  1.43it/s]Extractor Estimating: 281it [03:04,  1.39it/s]Extractor Estimating: 282it [03:05,  1.44it/s]Extractor Estimating: 283it [03:05,  1.49it/s]Extractor Estimating: 284it [03:06,  1.49it/s]Extractor Estimating: 285it [03:07,  1.48it/s]Extractor Estimating: 286it [03:07,  1.51it/s]Extractor Estimating: 287it [03:08,  1.53it/s]Extractor Estimating: 288it [03:08,  1.55it/s]Extractor Estimating: 289it [03:09,  1.55it/s]Extractor Estimating: 290it [03:10,  1.48it/s]Extractor Estimating: 291it [03:10,  1.50it/s]Extractor Estimating: 292it [03:11,  1.48it/s]Extractor Estimating: 293it [03:12,  1.38it/s]Extractor Estimating: 294it [03:13,  1.37it/s]Extractor Estimating: 295it [03:13,  1.40it/s]Extractor Estimating: 296it [03:14,  1.40it/s]Extractor Estimating: 297it [03:15,  1.46it/s]Extractor Estimating: 298it [03:15,  1.47it/s]Extractor Estimating: 299it [03:16,  1.48it/s]Extractor Estimating: 300it [03:17,  1.48it/s]Extractor Estimating: 301it [03:17,  1.46it/s]Extractor Estimating: 302it [03:18,  1.48it/s]Extractor Estimating: 303it [03:19,  1.45it/s]Extractor Estimating: 304it [03:19,  1.45it/s]Extractor Estimating: 305it [03:20,  1.41it/s]Extractor Estimating: 306it [03:21,  1.46it/s]Extractor Estimating: 307it [03:22,  1.46it/s]Extractor Estimating: 308it [03:22,  1.43it/s]Extractor Estimating: 309it [03:23,  1.49it/s]Extractor Estimating: 310it [03:24,  1.44it/s]Extractor Estimating: 311it [03:24,  1.49it/s]Extractor Estimating: 312it [03:25,  1.50it/s]Extractor Estimating: 313it [03:26,  1.49it/s]Extractor Estimating: 314it [03:26,  1.52it/s]Extractor Estimating: 315it [03:27,  1.55it/s]Extractor Estimating: 316it [03:28,  1.52it/s]Extractor Estimating: 317it [03:28,  1.55it/s]Extractor Estimating: 318it [03:29,  1.52it/s]Extractor Estimating: 319it [03:29,  1.56it/s]Extractor Estimating: 320it [03:30,  1.59it/s]Extractor Estimating: 321it [03:31,  1.50it/s]Extractor Estimating: 322it [03:31,  1.53it/s]Extractor Estimating: 323it [03:32,  1.57it/s]Extractor Estimating: 324it [03:33,  1.57it/s]Extractor Estimating: 325it [03:33,  1.53it/s]Extractor Estimating: 326it [03:34,  1.58it/s]Extractor Estimating: 327it [03:35,  1.61it/s]Extractor Estimating: 328it [03:35,  1.63it/s]Extractor Estimating: 329it [03:36,  1.63it/s]Extractor Estimating: 330it [03:36,  1.57it/s]Extractor Estimating: 331it [03:37,  1.55it/s]Extractor Estimating: 332it [03:38,  1.52it/s]Extractor Estimating: 333it [03:38,  1.57it/s]Extractor Estimating: 334it [03:39,  1.56it/s]Extractor Estimating: 335it [03:40,  1.55it/s]Extractor Estimating: 336it [03:40,  1.63it/s]Extractor Estimating: 337it [03:41,  1.67it/s]Extractor Estimating: 338it [03:41,  1.66it/s]Extractor Estimating: 339it [03:42,  1.55it/s]Extractor Estimating: 340it [03:43,  1.57it/s]Extractor Estimating: 341it [03:43,  1.65it/s]Extractor Estimating: 342it [03:44,  1.65it/s]Extractor Estimating: 343it [03:44,  1.69it/s]Extractor Estimating: 344it [03:45,  1.66it/s]Extractor Estimating: 345it [03:46,  1.71it/s]Extractor Estimating: 346it [03:46,  1.62it/s]Extractor Estimating: 347it [03:47,  1.59it/s]Extractor Estimating: 348it [03:48,  1.55it/s]Extractor Estimating: 349it [03:48,  1.49it/s]Extractor Estimating: 350it [03:49,  1.51it/s]Extractor Estimating: 351it [03:50,  1.51it/s]Extractor Estimating: 352it [03:50,  1.56it/s]Extractor Estimating: 353it [03:51,  1.57it/s]Extractor Estimating: 354it [03:52,  1.52it/s]Extractor Estimating: 355it [03:52,  1.54it/s]Extractor Estimating: 356it [03:53,  1.60it/s]Extractor Estimating: 357it [03:53,  1.58it/s]Extractor Estimating: 358it [03:54,  1.55it/s]Extractor Estimating: 359it [03:55,  1.58it/s]Extractor Estimating: 360it [03:55,  1.55it/s]Extractor Estimating: 361it [03:56,  1.47it/s]Extractor Estimating: 362it [03:57,  1.51it/s]Extractor Estimating: 363it [03:58,  1.46it/s]Extractor Estimating: 364it [03:58,  1.47it/s]Extractor Estimating: 365it [03:59,  1.48it/s]Extractor Estimating: 366it [04:00,  1.47it/s]Extractor Estimating: 367it [04:00,  1.48it/s]Extractor Estimating: 368it [04:01,  1.45it/s]Extractor Estimating: 369it [04:02,  1.49it/s]Extractor Estimating: 370it [04:02,  1.46it/s]Extractor Estimating: 371it [04:03,  1.50it/s]Extractor Estimating: 372it [04:04,  1.55it/s]Extractor Estimating: 373it [04:04,  1.51it/s]Extractor Estimating: 374it [04:05,  1.58it/s]Extractor Estimating: 375it [04:05,  1.59it/s]Extractor Estimating: 376it [04:06,  1.53it/s]Extractor Estimating: 377it [04:07,  1.52it/s]Extractor Estimating: 378it [04:07,  1.48it/s]Extractor Estimating: 379it [04:08,  1.34it/s]Extractor Estimating: 380it [04:09,  1.43it/s]Extractor Estimating: 381it [04:10,  1.49it/s]Extractor Estimating: 382it [04:10,  1.46it/s]Extractor Estimating: 383it [04:11,  1.46it/s]Extractor Estimating: 384it [04:12,  1.53it/s]Extractor Estimating: 385it [04:12,  1.49it/s]Extractor Estimating: 386it [04:13,  1.50it/s]Extractor Estimating: 387it [04:14,  1.48it/s]Extractor Estimating: 388it [04:14,  1.50it/s]Extractor Estimating: 389it [04:15,  1.47it/s]Extractor Estimating: 390it [04:16,  1.55it/s]Extractor Estimating: 391it [04:16,  1.52it/s]Extractor Estimating: 392it [04:17,  1.52it/s]Extractor Estimating: 393it [04:18,  1.55it/s]Extractor Estimating: 394it [04:18,  1.51it/s]Extractor Estimating: 395it [04:19,  1.51it/s]Extractor Estimating: 396it [04:20,  1.50it/s]Extractor Estimating: 397it [04:20,  1.53it/s]Extractor Estimating: 398it [04:21,  1.53it/s]Extractor Estimating: 399it [04:22,  1.51it/s]Extractor Estimating: 400it [04:22,  1.53it/s]Extractor Estimating: 401it [04:23,  1.50it/s]Extractor Estimating: 402it [04:23,  1.55it/s]Extractor Estimating: 403it [04:24,  1.52it/s]Extractor Estimating: 404it [04:25,  1.52it/s]Extractor Estimating: 405it [04:25,  1.57it/s]Extractor Estimating: 406it [04:26,  1.56it/s]Extractor Estimating: 407it [04:27,  1.55it/s]Extractor Estimating: 408it [04:27,  1.59it/s]Extractor Estimating: 409it [04:28,  1.55it/s]Extractor Estimating: 410it [04:29,  1.55it/s]Extractor Estimating: 411it [04:29,  1.52it/s]Extractor Estimating: 412it [04:30,  1.44it/s]Extractor Estimating: 413it [04:31,  1.49it/s]Extractor Estimating: 414it [04:31,  1.54it/s]Extractor Estimating: 415it [04:32,  1.58it/s]Extractor Estimating: 416it [04:33,  1.56it/s]Extractor Estimating: 417it [04:33,  1.59it/s]Extractor Estimating: 418it [04:34,  1.56it/s]Extractor Estimating: 419it [04:35,  1.51it/s]Extractor Estimating: 420it [04:35,  1.57it/s]Extractor Estimating: 421it [04:36,  1.47it/s]Extractor Estimating: 422it [04:37,  1.46it/s]Extractor Estimating: 423it [04:37,  1.50it/s]Extractor Estimating: 424it [04:38,  1.50it/s]Extractor Estimating: 425it [04:38,  1.58it/s]Extractor Estimating: 426it [04:39,  1.54it/s]Extractor Estimating: 427it [04:40,  1.52it/s]Extractor Estimating: 428it [04:41,  1.48it/s]Extractor Estimating: 429it [04:41,  1.45it/s]Extractor Estimating: 430it [04:42,  1.43it/s]Extractor Estimating: 431it [04:43,  1.49it/s]Extractor Estimating: 432it [04:43,  1.42it/s]Extractor Estimating: 433it [04:44,  1.49it/s]Extractor Estimating: 434it [04:45,  1.48it/s]Extractor Estimating: 435it [04:45,  1.45it/s]Extractor Estimating: 436it [04:46,  1.41it/s]Extractor Estimating: 437it [04:47,  1.40it/s]Extractor Estimating: 438it [04:47,  1.46it/s]Extractor Estimating: 439it [04:48,  1.46it/s]Extractor Estimating: 440it [04:49,  1.50it/s]Extractor Estimating: 441it [04:49,  1.46it/s]Extractor Estimating: 442it [04:50,  1.49it/s]Extractor Estimating: 443it [04:51,  1.55it/s]Extractor Estimating: 444it [04:51,  1.53it/s]Extractor Estimating: 445it [04:52,  1.53it/s]Extractor Estimating: 446it [04:53,  1.53it/s]Extractor Estimating: 447it [04:53,  1.53it/s]Extractor Estimating: 448it [04:54,  1.54it/s]Extractor Estimating: 449it [04:55,  1.50it/s]Extractor Estimating: 450it [04:55,  1.49it/s]Extractor Estimating: 451it [04:56,  1.50it/s]Extractor Estimating: 452it [04:57,  1.52it/s]Extractor Estimating: 453it [04:57,  1.58it/s]Extractor Estimating: 454it [04:58,  1.57it/s]Extractor Estimating: 455it [04:58,  1.65it/s]Extractor Estimating: 456it [04:59,  1.57it/s]Extractor Estimating: 457it [05:00,  1.60it/s]Extractor Estimating: 458it [05:00,  1.64it/s]Extractor Estimating: 459it [05:01,  1.68it/s]Extractor Estimating: 460it [05:01,  1.65it/s]Extractor Estimating: 461it [05:02,  1.66it/s]Extractor Estimating: 462it [05:03,  1.67it/s]Extractor Estimating: 463it [05:03,  1.72it/s]Extractor Estimating: 464it [05:04,  1.74it/s]Extractor Estimating: 465it [05:04,  1.72it/s]Extractor Estimating: 466it [05:05,  1.72it/s]Extractor Estimating: 467it [05:06,  1.71it/s]Extractor Estimating: 468it [05:06,  1.72it/s]Extractor Estimating: 469it [05:07,  1.72it/s]Extractor Estimating: 470it [05:07,  1.71it/s]Extractor Estimating: 471it [05:08,  1.53it/s]Extractor Estimating: 472it [05:09,  1.51it/s]Extractor Estimating: 473it [05:09,  1.52it/s]Extractor Estimating: 474it [05:10,  1.57it/s]Extractor Estimating: 475it [05:11,  1.53it/s]Extractor Estimating: 476it [05:11,  1.58it/s]Extractor Estimating: 477it [05:12,  1.48it/s]Extractor Estimating: 478it [05:13,  1.52it/s]Extractor Estimating: 479it [05:13,  1.46it/s]Extractor Estimating: 480it [05:14,  1.49it/s]Extractor Estimating: 481it [05:15,  1.53it/s]Extractor Estimating: 482it [05:15,  1.51it/s]Extractor Estimating: 483it [05:16,  1.49it/s]Extractor Estimating: 484it [05:17,  1.51it/s]Extractor Estimating: 485it [05:17,  1.51it/s]Extractor Estimating: 486it [05:18,  1.54it/s]Extractor Estimating: 487it [05:19,  1.57it/s]Extractor Estimating: 488it [05:19,  1.58it/s]Extractor Estimating: 489it [05:20,  1.51it/s]Extractor Estimating: 490it [05:21,  1.42it/s]Extractor Estimating: 491it [05:21,  1.43it/s]Extractor Estimating: 492it [05:22,  1.44it/s]Extractor Estimating: 493it [05:23,  1.46it/s]Extractor Estimating: 494it [05:23,  1.46it/s]Extractor Estimating: 495it [05:24,  1.50it/s]Extractor Estimating: 496it [05:25,  1.46it/s]Extractor Estimating: 497it [05:25,  1.47it/s]Extractor Estimating: 498it [05:26,  1.46it/s]Extractor Estimating: 499it [05:27,  1.51it/s]Extractor Estimating: 500it [05:27,  1.83it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:26,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:26,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:49:27,001 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:49:27,002 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:27,663 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:28,785 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:28,785 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:31,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:31,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:31,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:31,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:31,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:49:32,612 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:49:32,613 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:33,213 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:33,429 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:33,430 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:00:56,633 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:00:56,670 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 10461 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 25273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25373, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.111, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.117, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.103, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.108, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.122, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.180, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 264, avg_time 1.118, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 364, avg_time 1.119, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.105, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.111, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 228, avg_time 2.208, loss:nan
g_step 1200, step 328, avg_time 1.118, loss:nan
g_step 1300, step 428, avg_time 1.089, loss:nan
g_step 1400, step 92, avg_time 1.098, loss:nan
g_step 1500, step 192, avg_time 1.115, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 292, avg_time 2.199, loss:nan
g_step 1700, step 392, avg_time 1.114, loss:nan
g_step 1800, step 56, avg_time 1.098, loss:nan
g_step 1900, step 156, avg_time 1.100, loss:nan
g_step 2000, step 256, avg_time 1.129, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 356, avg_time 2.167, loss:nan
g_step 2200, step 20, avg_time 1.111, loss:nan
g_step 2300, step 120, avg_time 1.112, loss:nan
g_step 2400, step 220, avg_time 1.092, loss:nan
g_step 2500, step 320, avg_time 1.117, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 420, avg_time 2.167, loss:nan
g_step 2700, step 84, avg_time 1.094, loss:nan
g_step 2800, step 184, avg_time 1.113, loss:nan
g_step 2900, step 284, avg_time 1.132, loss:nan
g_step 3000, step 384, avg_time 1.101, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.153, loss:nan
g_step 3200, step 148, avg_time 1.113, loss:nan
g_step 3300, step 248, avg_time 1.102, loss:nan
g_step 3400, step 348, avg_time 1.101, loss:nan
g_step 3500, step 12, avg_time 1.102, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.155, loss:nan
g_step 3700, step 212, avg_time 1.122, loss:nan
g_step 3800, step 312, avg_time 1.093, loss:nan
g_step 3900, step 412, avg_time 1.105, loss:nan
g_step 4000, step 76, avg_time 1.128, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.167, loss:nan
g_step 4200, step 276, avg_time 1.082, loss:nan
g_step 4300, step 376, avg_time 1.107, loss:nan
g_step 4400, step 40, avg_time 1.117, loss:nan
g_step 4500, step 140, avg_time 1.108, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 240, avg_time 2.154, loss:nan
g_step 4700, step 340, avg_time 1.107, loss:nan
g_step 4800, step 4, avg_time 1.107, loss:nan
g_step 4900, step 104, avg_time 1.108, loss:nan
g_step 5000, step 204, avg_time 1.106, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 304, avg_time 2.137, loss:nan
g_step 5200, step 404, avg_time 1.139, loss:nan
g_step 5300, step 68, avg_time 1.112, loss:nan
g_step 5400, step 168, avg_time 1.097, loss:nan
g_step 5500, step 268, avg_time 1.106, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 368, avg_time 2.199, loss:nan
g_step 5700, step 32, avg_time 1.103, loss:nan
g_step 5800, step 132, avg_time 1.131, loss:nan
g_step 5900, step 232, avg_time 1.106, loss:nan
g_step 6000, step 332, avg_time 1.098, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 432, avg_time 2.154, loss:nan
g_step 6200, step 96, avg_time 1.112, loss:nan
g_step 6300, step 196, avg_time 1.102, loss:nan
g_step 6400, step 296, avg_time 1.119, loss:nan
g_step 6500, step 396, avg_time 1.094, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 60, avg_time 2.157, loss:nan
g_step 6700, step 160, avg_time 1.105, loss:nan
g_step 6800, step 260, avg_time 1.114, loss:nan
g_step 6900, step 360, avg_time 1.116, loss:nan
g_step 7000, step 24, avg_time 1.131, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 124, avg_time 2.184, loss:nan
g_step 7200, step 224, avg_time 1.115, loss:nan
g_step 7300, step 324, avg_time 1.103, loss:nan
g_step 7400, step 424, avg_time 1.101, loss:nan
g_step 7500, step 88, avg_time 1.130, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 188, avg_time 2.163, loss:nan
g_step 7700, step 288, avg_time 1.106, loss:nan
g_step 7800, step 388, avg_time 1.119, loss:nan
g_step 7900, step 52, avg_time 1.103, loss:nan
g_step 8000, step 152, avg_time 1.140, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 252, avg_time 2.176, loss:nan
g_step 8200, step 352, avg_time 1.090, loss:nan
g_step 8300, step 16, avg_time 1.092, loss:nan
g_step 8400, step 116, avg_time 1.101, loss:nan
g_step 8500, step 216, avg_time 1.113, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 316, avg_time 2.185, loss:nan
g_step 8700, step 416, avg_time 1.085, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:00:56 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:00:56 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-00-56_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:00:57 - WARNING - datasets.builder -   Using custom data configuration default-42bd61c3677ceed0
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-42bd61c3677ceed0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:01:00,555 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:01:00,623 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:01:00,623 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:01:00,624 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:01:00,844 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,948 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,948 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,948 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,949 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:01:00,949 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:01:01,573 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:01:04,716 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:01:04,716 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-42bd61c3677ceed0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.67ba/s] 18%|        | 2/11 [00:00<00:02,  3.61ba/s] 27%|       | 3/11 [00:00<00:01,  4.03ba/s] 36%|      | 4/11 [00:01<00:01,  4.23ba/s] 45%|     | 5/11 [00:01<00:01,  4.37ba/s] 55%|    | 6/11 [00:01<00:01,  4.44ba/s] 64%|   | 7/11 [00:01<00:00,  4.47ba/s] 73%|  | 8/11 [00:01<00:00,  4.51ba/s] 82%| | 9/11 [00:02<00:00,  4.54ba/s] 91%| | 10/11 [00:02<00:00,  4.57ba/s]100%|| 11/11 [00:02<00:00,  5.43ba/s]100%|| 11/11 [00:02<00:00,  4.53ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.11ba/s] 50%|     | 2/4 [00:00<00:00,  3.51ba/s] 75%|  | 3/4 [00:00<00:00,  3.93ba/s]100%|| 4/4 [00:00<00:00,  5.05ba/s]100%|| 4/4 [00:00<00:00,  4.39ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  4.88ba/s] 27%|       | 3/11 [00:00<00:00,  8.11ba/s] 45%|     | 5/11 [00:00<00:00,  9.24ba/s] 64%|   | 7/11 [00:00<00:00,  9.81ba/s] 82%| | 9/11 [00:00<00:00, 10.07ba/s]100%|| 11/11 [00:01<00:00, 11.23ba/s]100%|| 11/11 [00:01<00:00, 10.01ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.50ba/s] 75%|  | 3/4 [00:00<00:00,  8.50ba/s]100%|| 4/4 [00:00<00:00,  9.55ba/s]
[INFO|trainer.py:414] 2023-08-28 14:01:10,846 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:01:10,937 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:01:10,937 >>   Num examples = 10479
[INFO|trainer.py:1149] 2023-08-28 14:01:10,937 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:01:10,937 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:01:10,937 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:01:10,937 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:01:10,937 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:53,  3.51it/s]  0%|          | 2/820 [00:00<03:49,  3.57it/s]  0%|          | 3/820 [00:00<03:47,  3.59it/s]  0%|          | 4/820 [00:01<03:46,  3.60it/s]  1%|          | 5/820 [00:01<03:47,  3.59it/s]  1%|          | 6/820 [00:01<03:46,  3.59it/s]  1%|          | 7/820 [00:01<03:45,  3.60it/s]  1%|          | 8/820 [00:02<03:55,  3.45it/s]  1%|          | 9/820 [00:02<03:51,  3.50it/s]  1%|          | 10/820 [00:02<03:49,  3.53it/s]  1%|         | 11/820 [00:03<03:47,  3.56it/s]  1%|         | 12/820 [00:03<03:45,  3.58it/s]  2%|         | 13/820 [00:03<03:45,  3.59it/s]  2%|         | 14/820 [00:03<03:44,  3.59it/s]  2%|         | 15/820 [00:04<03:43,  3.60it/s]  2%|         | 16/820 [00:04<03:43,  3.60it/s]  2%|         | 17/820 [00:04<03:43,  3.60it/s]  2%|         | 18/820 [00:05<03:42,  3.60it/s]  2%|         | 19/820 [00:05<03:42,  3.60it/s]  2%|         | 20/820 [00:05<03:41,  3.61it/s]  3%|         | 21/820 [00:05<03:41,  3.61it/s]  3%|         | 22/820 [00:06<03:40,  3.62it/s]  3%|         | 23/820 [00:06<03:40,  3.61it/s]  3%|         | 24/820 [00:06<03:40,  3.61it/s]  3%|         | 25/820 [00:06<03:40,  3.61it/s]  3%|         | 26/820 [00:07<03:48,  3.48it/s]  3%|         | 27/820 [00:07<03:45,  3.52it/s]  3%|         | 28/820 [00:07<03:43,  3.55it/s]  4%|         | 29/820 [00:08<03:41,  3.57it/s]  4%|         | 30/820 [00:08<03:40,  3.58it/s]  4%|         | 31/820 [00:08<03:39,  3.59it/s]  4%|         | 32/820 [00:08<03:39,  3.59it/s]  4%|         | 33/820 [00:09<03:38,  3.60it/s]  4%|         | 34/820 [00:09<03:37,  3.61it/s]  4%|         | 35/820 [00:09<03:37,  3.61it/s]  4%|         | 36/820 [00:10<03:36,  3.62it/s]  5%|         | 37/820 [00:10<03:47,  3.45it/s]  5%|         | 38/820 [00:10<03:43,  3.50it/s]  5%|         | 39/820 [00:10<03:40,  3.54it/s]  5%|         | 40/820 [00:11<03:38,  3.56it/s]  5%|         | 41/820 [00:11<03:37,  3.58it/s]  5%|         | 42/820 [00:11<03:36,  3.59it/s]  5%|         | 43/820 [00:12<03:35,  3.60it/s]  5%|         | 44/820 [00:12<03:34,  3.61it/s]  5%|         | 45/820 [00:12<03:34,  3.62it/s]  6%|         | 46/820 [00:12<03:34,  3.62it/s]  6%|         | 47/820 [00:13<03:33,  3.62it/s]  6%|         | 48/820 [00:13<03:41,  3.48it/s]  6%|         | 49/820 [00:13<03:39,  3.52it/s]  6%|         | 50/820 [00:13<03:36,  3.55it/s]  6%|         | 51/820 [00:14<03:35,  3.56it/s]  6%|         | 52/820 [00:14<03:34,  3.58it/s]  6%|         | 53/820 [00:14<03:33,  3.59it/s]  7%|         | 54/820 [00:15<03:32,  3.61it/s]  7%|         | 55/820 [00:15<03:31,  3.61it/s]  7%|         | 56/820 [00:15<03:31,  3.61it/s]  7%|         | 57/820 [00:15<03:31,  3.62it/s]  7%|         | 58/820 [00:16<03:30,  3.62it/s]  7%|         | 59/820 [00:16<03:32,  3.59it/s]  7%|         | 60/820 [00:16<03:31,  3.60it/s]  7%|         | 61/820 [00:17<03:30,  3.61it/s]  8%|         | 62/820 [00:17<03:29,  3.61it/s]  8%|         | 63/820 [00:17<03:29,  3.61it/s]  8%|         | 64/820 [00:17<03:29,  3.62it/s]  8%|         | 65/820 [00:18<03:28,  3.62it/s]  8%|         | 66/820 [00:18<03:28,  3.62it/s]  8%|         | 67/820 [00:18<03:27,  3.62it/s]  8%|         | 68/820 [00:18<03:27,  3.62it/s]  8%|         | 69/820 [00:19<03:27,  3.62it/s]  9%|         | 70/820 [00:19<03:33,  3.51it/s]  9%|         | 71/820 [00:19<03:31,  3.54it/s]  9%|         | 72/820 [00:20<03:29,  3.56it/s]  9%|         | 73/820 [00:20<03:28,  3.58it/s]  9%|         | 74/820 [00:20<03:27,  3.59it/s]  9%|         | 75/820 [00:20<03:26,  3.60it/s]  9%|         | 76/820 [00:21<03:26,  3.61it/s]  9%|         | 77/820 [00:21<03:25,  3.61it/s] 10%|         | 78/820 [00:21<03:25,  3.61it/s] 10%|         | 79/820 [00:22<03:25,  3.61it/s] 10%|         | 80/820 [00:22<03:24,  3.62it/s] 10%|         | 81/820 [00:22<03:38,  3.39it/s] 10%|         | 82/820 [00:22<03:33,  3.46it/s] 10%|         | 83/820 [00:23<03:30,  3.50it/s] 10%|         | 84/820 [00:23<03:27,  3.54it/s] 10%|         | 85/820 [00:23<03:26,  3.56it/s] 10%|         | 86/820 [00:24<03:24,  3.58it/s] 11%|         | 87/820 [00:24<03:24,  3.59it/s] 11%|         | 88/820 [00:24<03:23,  3.59it/s] 11%|         | 89/820 [00:24<03:22,  3.60it/s] 11%|         | 90/820 [00:25<03:22,  3.61it/s] 11%|         | 91/820 [00:25<03:21,  3.61it/s] 11%|         | 92/820 [00:25<03:28,  3.49it/s] 11%|        | 93/820 [00:26<03:26,  3.52it/s] 11%|        | 94/820 [00:26<03:24,  3.55it/s] 12%|        | 95/820 [00:26<03:23,  3.57it/s] 12%|        | 96/820 [00:26<03:21,  3.58it/s] 12%|        | 97/820 [00:27<03:20,  3.60it/s] 12%|        | 98/820 [00:27<03:20,  3.60it/s] 12%|        | 99/820 [00:27<03:19,  3.61it/s] 12%|        | 100/820 [00:27<03:19,  3.62it/s] 12%|        | 101/820 [00:28<03:18,  3.62it/s] 12%|        | 102/820 [00:28<03:18,  3.61it/s] 13%|        | 103/820 [00:28<03:25,  3.49it/s] 13%|        | 104/820 [00:29<03:22,  3.53it/s] 13%|        | 105/820 [00:29<03:20,  3.56it/s] 13%|        | 106/820 [00:29<03:19,  3.58it/s] 13%|        | 107/820 [00:29<03:18,  3.59it/s] 13%|        | 108/820 [00:30<03:17,  3.60it/s] 13%|        | 109/820 [00:30<03:17,  3.61it/s] 13%|        | 110/820 [00:30<03:16,  3.61it/s] 14%|        | 111/820 [00:31<03:16,  3.62it/s] 14%|        | 112/820 [00:31<03:15,  3.61it/s] 14%|        | 113/820 [00:31<03:15,  3.62it/s] 14%|        | 114/820 [00:31<03:24,  3.46it/s] 14%|        | 115/820 [00:32<03:20,  3.51it/s] 14%|        | 116/820 [00:32<03:18,  3.54it/s] 14%|        | 117/820 [00:32<03:17,  3.57it/s] 14%|        | 118/820 [00:32<03:15,  3.58it/s] 15%|        | 119/820 [00:33<03:15,  3.59it/s] 15%|        | 120/820 [00:33<03:14,  3.61it/s] 15%|        | 121/820 [00:33<03:13,  3.61it/s] 15%|        | 122/820 [00:34<03:13,  3.61it/s] 15%|        | 123/820 [00:34<03:12,  3.61it/s] 15%|        | 124/820 [00:34<03:12,  3.62it/s] 15%|        | 125/820 [00:34<03:28,  3.34it/s] 15%|        | 126/820 [00:35<03:23,  3.42it/s] 15%|        | 127/820 [00:35<03:19,  3.47it/s] 16%|        | 128/820 [00:35<03:16,  3.52it/s] 16%|        | 129/820 [00:36<03:14,  3.55it/s] 16%|        | 130/820 [00:36<03:13,  3.57it/s] 16%|        | 131/820 [00:36<03:11,  3.59it/s] 16%|        | 132/820 [00:36<03:11,  3.60it/s] 16%|        | 133/820 [00:37<03:10,  3.60it/s] 16%|        | 134/820 [00:37<03:10,  3.61it/s] 16%|        | 135/820 [00:37<03:09,  3.62it/s] 17%|        | 136/820 [00:38<03:18,  3.45it/s] 17%|        | 137/820 [00:38<03:15,  3.50it/s] 17%|        | 138/820 [00:38<03:12,  3.53it/s] 17%|        | 139/820 [00:38<03:11,  3.56it/s] 17%|        | 140/820 [00:39<03:10,  3.58it/s] 17%|        | 141/820 [00:39<03:09,  3.59it/s] 17%|        | 142/820 [00:39<03:08,  3.60it/s] 17%|        | 143/820 [00:40<03:07,  3.61it/s] 18%|        | 144/820 [00:40<03:07,  3.61it/s] 18%|        | 145/820 [00:40<03:07,  3.61it/s] 18%|        | 146/820 [00:40<03:06,  3.61it/s] 18%|        | 147/820 [00:41<03:14,  3.47it/s] 18%|        | 148/820 [00:41<03:11,  3.51it/s] 18%|        | 149/820 [00:41<03:09,  3.54it/s] 18%|        | 150/820 [00:41<03:07,  3.57it/s] 18%|        | 151/820 [00:42<03:06,  3.58it/s] 19%|        | 152/820 [00:42<03:05,  3.59it/s] 19%|        | 153/820 [00:42<03:05,  3.60it/s] 19%|        | 154/820 [00:43<03:04,  3.61it/s] 19%|        | 155/820 [00:43<03:04,  3.61it/s] 19%|        | 156/820 [00:43<03:03,  3.61it/s] 19%|        | 157/820 [00:43<03:03,  3.61it/s] 19%|        | 158/820 [00:44<03:14,  3.41it/s] 19%|        | 159/820 [00:44<03:10,  3.47it/s] 20%|        | 160/820 [00:44<03:07,  3.51it/s] 20%|        | 161/820 [00:45<03:06,  3.54it/s] 20%|        | 162/820 [00:45<03:04,  3.57it/s] 20%|        | 163/820 [00:45<03:03,  3.59it/s] 20%|        | 164/820 [00:45<02:49,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 14:01:56,784 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:01:56,784 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:01:56,784 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.84it/s][A
  3%|         | 12/437 [00:00<00:08, 49.70it/s][A
  4%|         | 18/437 [00:00<00:08, 47.56it/s][A
  5%|         | 23/437 [00:00<00:08, 46.70it/s][A
  6%|         | 28/437 [00:00<00:08, 46.22it/s][A
  8%|         | 33/437 [00:00<00:08, 45.76it/s][A
  9%|         | 38/437 [00:00<00:08, 45.55it/s][A
 10%|         | 43/437 [00:00<00:08, 45.19it/s][A
 11%|         | 48/437 [00:01<00:08, 45.23it/s][A
 12%|        | 53/437 [00:01<00:08, 45.34it/s][A
 13%|        | 58/437 [00:01<00:08, 43.64it/s][A
 14%|        | 63/437 [00:01<00:08, 44.34it/s][A
 16%|        | 68/437 [00:01<00:08, 44.73it/s][A
 17%|        | 73/437 [00:01<00:08, 44.92it/s][A
 18%|        | 78/437 [00:01<00:07, 44.97it/s][A
 19%|        | 83/437 [00:01<00:07, 44.92it/s][A
 20%|        | 88/437 [00:01<00:07, 44.83it/s][A
 21%|       | 93/437 [00:02<00:07, 43.38it/s][A
 22%|       | 98/437 [00:02<00:07, 43.99it/s][A
 24%|       | 103/437 [00:02<00:07, 44.49it/s][A
 25%|       | 108/437 [00:02<00:07, 44.80it/s][A
 26%|       | 113/437 [00:02<00:07, 45.02it/s][A
 27%|       | 118/437 [00:02<00:07, 45.20it/s][A
 28%|       | 123/437 [00:02<00:06, 45.33it/s][A
 29%|       | 128/437 [00:02<00:06, 45.31it/s][A
 30%|       | 133/437 [00:02<00:06, 45.05it/s][A
 32%|      | 138/437 [00:03<00:06, 44.99it/s][A
 33%|      | 143/437 [00:03<00:06, 45.09it/s][A
 34%|      | 148/437 [00:03<00:06, 45.13it/s][A
 35%|      | 153/437 [00:03<00:06, 45.26it/s][A
 36%|      | 158/437 [00:03<00:06, 45.32it/s][A
 37%|      | 163/437 [00:03<00:06, 45.38it/s][A
 38%|      | 168/437 [00:03<00:05, 45.45it/s][A
 40%|      | 173/437 [00:03<00:05, 45.38it/s][A
 41%|      | 178/437 [00:03<00:05, 45.19it/s][A
 42%|     | 183/437 [00:04<00:05, 44.93it/s][A
 43%|     | 188/437 [00:04<00:05, 44.91it/s][A
 44%|     | 193/437 [00:04<00:06, 38.41it/s][A
 45%|     | 198/437 [00:04<00:05, 40.28it/s][A
 46%|     | 203/437 [00:04<00:05, 41.78it/s][A
 48%|     | 208/437 [00:04<00:05, 42.94it/s][A
 49%|     | 213/437 [00:04<00:05, 43.66it/s][A
 50%|     | 218/437 [00:04<00:04, 44.28it/s][A
 51%|     | 223/437 [00:04<00:04, 44.66it/s][A
 52%|    | 228/437 [00:05<00:04, 44.80it/s][A
 53%|    | 233/437 [00:05<00:04, 44.59it/s][A
 54%|    | 238/437 [00:05<00:04, 44.44it/s][A
 56%|    | 243/437 [00:05<00:04, 44.58it/s][A
 57%|    | 248/437 [00:05<00:04, 44.79it/s][A
 58%|    | 253/437 [00:05<00:04, 44.91it/s][A
 59%|    | 258/437 [00:05<00:03, 44.97it/s][A
 60%|    | 263/437 [00:05<00:03, 45.26it/s][A
 61%|   | 268/437 [00:05<00:03, 45.24it/s][A
 62%|   | 273/437 [00:06<00:03, 45.36it/s][A
 64%|   | 278/437 [00:06<00:03, 45.33it/s][A
 65%|   | 283/437 [00:06<00:03, 45.20it/s][A
 66%|   | 288/437 [00:06<00:03, 45.15it/s][A
 67%|   | 293/437 [00:06<00:03, 45.16it/s][A
 68%|   | 298/437 [00:06<00:03, 45.20it/s][A
 69%|   | 303/437 [00:06<00:02, 45.29it/s][A
 70%|   | 308/437 [00:06<00:02, 45.28it/s][A
 72%|  | 313/437 [00:06<00:02, 45.36it/s][A
 73%|  | 318/437 [00:07<00:02, 42.42it/s][A
 74%|  | 323/437 [00:07<00:02, 43.86it/s][A
 75%|  | 328/437 [00:07<00:02, 42.21it/s][A
 76%|  | 333/437 [00:07<00:02, 43.08it/s][A
 77%|  | 338/437 [00:07<00:02, 43.79it/s][A
 78%|  | 343/437 [00:07<00:02, 44.27it/s][A
 80%|  | 348/437 [00:07<00:01, 44.67it/s][A
 81%|  | 353/437 [00:07<00:01, 44.90it/s][A
 82%| | 358/437 [00:08<00:01, 45.01it/s][A
 83%| | 363/437 [00:08<00:01, 44.95it/s][A
 84%| | 368/437 [00:08<00:01, 44.87it/s][A
 85%| | 373/437 [00:08<00:01, 45.00it/s][A
 86%| | 378/437 [00:08<00:01, 45.08it/s][A
 88%| | 383/437 [00:08<00:01, 45.17it/s][A
 89%| | 388/437 [00:08<00:01, 45.35it/s][A
 90%| | 393/437 [00:08<00:00, 45.34it/s][A
 91%| | 398/437 [00:08<00:00, 45.45it/s][A
 92%|| 403/437 [00:09<00:00, 45.34it/s][A
 93%|| 408/437 [00:09<00:00, 45.17it/s][A
 95%|| 413/437 [00:09<00:00, 45.02it/s][A
 96%|| 418/437 [00:09<00:00, 23.03it/s][A
 97%|| 423/437 [00:09<00:00, 27.42it/s][A
 98%|| 427/437 [00:09<00:00, 27.59it/s][A
 99%|| 432/437 [00:10<00:00, 31.89it/s][A
100%|| 437/437 [00:10<00:00, 35.22it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 35.22it/s][A 20%|        | 164/820 [00:56<02:49,  3.87it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:02:07,231 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 14:02:07,512 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:02:11,057 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:02:11,225 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:02:11,327 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/special_tokens_map.json
 20%|        | 165/820 [01:02<55:01,  5.04s/it] 20%|        | 166/820 [01:02<39:22,  3.61s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|        | 167/820 [01:02<28:26,  2.61s/it] 20%|        | 168/820 [01:02<20:47,  1.91s/it] 21%|        | 169/820 [01:03<15:26,  1.42s/it] 21%|        | 170/820 [01:03<11:48,  1.09s/it] 21%|        | 171/820 [01:03<09:09,  1.18it/s] 21%|        | 172/820 [01:04<07:18,  1.48it/s] 21%|        | 173/820 [01:04<06:01,  1.79it/s] 21%|        | 174/820 [01:04<05:06,  2.10it/s] 21%|       | 175/820 [01:04<04:29,  2.40it/s] 21%|       | 176/820 [01:05<04:02,  2.66it/s] 22%|       | 177/820 [01:05<03:43,  2.88it/s] 22%|       | 178/820 [01:05<03:30,  3.05it/s] 22%|       | 179/820 [01:06<03:20,  3.19it/s] 22%|       | 180/820 [01:06<03:14,  3.29it/s] 22%|       | 181/820 [01:06<03:12,  3.32it/s] 22%|       | 182/820 [01:06<03:08,  3.39it/s] 22%|       | 183/820 [01:07<03:05,  3.44it/s] 22%|       | 184/820 [01:07<03:03,  3.47it/s] 23%|       | 185/820 [01:07<03:01,  3.50it/s] 23%|       | 186/820 [01:07<03:00,  3.52it/s] 23%|       | 187/820 [01:08<02:59,  3.53it/s] 23%|       | 188/820 [01:08<02:58,  3.54it/s] 23%|       | 189/820 [01:08<02:57,  3.55it/s] 23%|       | 190/820 [01:09<02:56,  3.57it/s] 23%|       | 191/820 [01:09<02:55,  3.58it/s] 23%|       | 192/820 [01:09<03:00,  3.49it/s] 24%|       | 193/820 [01:09<02:57,  3.52it/s] 24%|       | 194/820 [01:10<02:56,  3.55it/s] 24%|       | 195/820 [01:10<02:55,  3.57it/s] 24%|       | 196/820 [01:10<02:54,  3.58it/s] 24%|       | 197/820 [01:11<02:53,  3.60it/s] 24%|       | 198/820 [01:11<02:52,  3.60it/s] 24%|       | 199/820 [01:11<02:52,  3.61it/s] 24%|       | 200/820 [01:11<02:51,  3.61it/s] 25%|       | 201/820 [01:12<02:51,  3.62it/s] 25%|       | 202/820 [01:12<02:50,  3.62it/s] 25%|       | 203/820 [01:12<02:57,  3.47it/s] 25%|       | 204/820 [01:13<02:55,  3.51it/s] 25%|       | 205/820 [01:13<02:53,  3.54it/s] 25%|       | 206/820 [01:13<02:52,  3.56it/s] 25%|       | 207/820 [01:13<02:51,  3.57it/s] 25%|       | 208/820 [01:14<02:50,  3.59it/s] 25%|       | 209/820 [01:14<02:49,  3.60it/s] 26%|       | 210/820 [01:14<02:49,  3.61it/s] 26%|       | 211/820 [01:14<02:48,  3.62it/s] 26%|       | 212/820 [01:15<02:48,  3.62it/s] 26%|       | 213/820 [01:15<02:47,  3.62it/s] 26%|       | 214/820 [01:15<02:51,  3.53it/s] 26%|       | 215/820 [01:16<02:49,  3.56it/s] 26%|       | 216/820 [01:16<02:48,  3.58it/s] 26%|       | 217/820 [01:16<02:48,  3.59it/s] 27%|       | 218/820 [01:16<02:47,  3.60it/s] 27%|       | 219/820 [01:17<02:46,  3.61it/s] 27%|       | 220/820 [01:17<02:46,  3.61it/s] 27%|       | 221/820 [01:17<02:45,  3.61it/s] 27%|       | 222/820 [01:18<02:45,  3.62it/s] 27%|       | 223/820 [01:18<02:45,  3.62it/s] 27%|       | 224/820 [01:18<02:44,  3.62it/s] 27%|       | 225/820 [01:18<02:54,  3.42it/s] 28%|       | 226/820 [01:19<02:51,  3.47it/s] 28%|       | 227/820 [01:19<02:49,  3.51it/s] 28%|       | 228/820 [01:19<02:47,  3.54it/s] 28%|       | 229/820 [01:20<02:45,  3.57it/s] 28%|       | 230/820 [01:20<02:44,  3.58it/s] 28%|       | 231/820 [01:20<02:44,  3.59it/s] 28%|       | 232/820 [01:20<02:43,  3.60it/s] 28%|       | 233/820 [01:21<02:42,  3.61it/s] 29%|       | 234/820 [01:21<02:42,  3.61it/s] 29%|       | 235/820 [01:21<02:41,  3.62it/s] 29%|       | 236/820 [01:21<02:48,  3.47it/s] 29%|       | 237/820 [01:22<02:45,  3.51it/s] 29%|       | 238/820 [01:22<02:44,  3.54it/s] 29%|       | 239/820 [01:22<02:42,  3.57it/s] 29%|       | 240/820 [01:23<02:41,  3.58it/s] 29%|       | 241/820 [01:23<02:41,  3.59it/s] 30%|       | 242/820 [01:23<02:40,  3.60it/s] 30%|       | 243/820 [01:23<02:39,  3.61it/s] 30%|       | 244/820 [01:24<02:39,  3.61it/s] 30%|       | 245/820 [01:24<02:38,  3.62it/s] 30%|       | 246/820 [01:24<02:38,  3.62it/s] 30%|       | 247/820 [01:25<02:45,  3.46it/s] 30%|       | 248/820 [01:25<02:43,  3.50it/s] 30%|       | 249/820 [01:25<02:41,  3.54it/s] 30%|       | 250/820 [01:25<02:39,  3.57it/s] 31%|       | 251/820 [01:26<02:40,  3.54it/s] 31%|       | 252/820 [01:26<02:39,  3.56it/s] 31%|       | 253/820 [01:26<02:38,  3.58it/s] 31%|       | 254/820 [01:27<02:37,  3.59it/s] 31%|       | 255/820 [01:27<02:36,  3.60it/s] 31%|       | 256/820 [01:27<02:36,  3.60it/s] 31%|      | 257/820 [01:27<02:36,  3.61it/s] 31%|      | 258/820 [01:28<02:35,  3.61it/s] 32%|      | 259/820 [01:28<02:35,  3.62it/s] 32%|      | 260/820 [01:28<02:34,  3.62it/s] 32%|      | 261/820 [01:28<02:34,  3.62it/s] 32%|      | 262/820 [01:29<02:37,  3.53it/s] 32%|      | 263/820 [01:29<02:36,  3.56it/s] 32%|      | 264/820 [01:29<02:35,  3.58it/s] 32%|      | 265/820 [01:30<02:34,  3.59it/s] 32%|      | 266/820 [01:30<02:33,  3.60it/s] 33%|      | 267/820 [01:30<02:33,  3.61it/s] 33%|      | 268/820 [01:30<02:33,  3.60it/s] 33%|      | 269/820 [01:31<02:33,  3.60it/s] 33%|      | 270/820 [01:31<02:32,  3.61it/s] 33%|      | 271/820 [01:31<02:32,  3.61it/s] 33%|      | 272/820 [01:32<02:32,  3.61it/s] 33%|      | 273/820 [01:32<02:37,  3.48it/s] 33%|      | 274/820 [01:32<02:35,  3.52it/s] 34%|      | 275/820 [01:32<02:33,  3.54it/s] 34%|      | 276/820 [01:33<02:32,  3.57it/s] 34%|      | 277/820 [01:33<02:31,  3.58it/s] 34%|      | 278/820 [01:33<02:30,  3.60it/s] 34%|      | 279/820 [01:33<02:30,  3.60it/s] 34%|      | 280/820 [01:34<02:29,  3.61it/s] 34%|      | 281/820 [01:34<02:29,  3.61it/s] 34%|      | 282/820 [01:34<02:28,  3.62it/s] 35%|      | 283/820 [01:35<02:28,  3.62it/s] 35%|      | 284/820 [01:35<02:34,  3.46it/s] 35%|      | 285/820 [01:35<02:32,  3.51it/s] 35%|      | 286/820 [01:35<02:30,  3.54it/s] 35%|      | 287/820 [01:36<02:29,  3.57it/s] 35%|      | 288/820 [01:36<02:28,  3.58it/s] 35%|      | 289/820 [01:36<02:27,  3.59it/s] 35%|      | 290/820 [01:37<02:27,  3.60it/s] 35%|      | 291/820 [01:37<02:26,  3.60it/s] 36%|      | 292/820 [01:37<02:26,  3.61it/s] 36%|      | 293/820 [01:37<02:25,  3.61it/s] 36%|      | 294/820 [01:38<02:25,  3.61it/s] 36%|      | 295/820 [01:38<02:35,  3.39it/s] 36%|      | 296/820 [01:38<02:31,  3.45it/s] 36%|      | 297/820 [01:39<02:29,  3.50it/s] 36%|      | 298/820 [01:39<02:27,  3.53it/s] 36%|      | 299/820 [01:39<02:26,  3.55it/s] 37%|      | 300/820 [01:39<02:25,  3.57it/s] 37%|      | 301/820 [01:40<02:24,  3.59it/s] 37%|      | 302/820 [01:40<02:24,  3.59it/s] 37%|      | 303/820 [01:40<02:23,  3.60it/s] 37%|      | 304/820 [01:41<02:23,  3.60it/s] 37%|      | 305/820 [01:41<02:23,  3.60it/s] 37%|      | 306/820 [01:41<02:30,  3.41it/s] 37%|      | 307/820 [01:41<02:28,  3.46it/s] 38%|      | 308/820 [01:42<02:25,  3.51it/s] 38%|      | 309/820 [01:42<02:24,  3.53it/s] 38%|      | 310/820 [01:42<02:23,  3.56it/s] 38%|      | 311/820 [01:42<02:22,  3.57it/s] 38%|      | 312/820 [01:43<02:21,  3.59it/s] 38%|      | 313/820 [01:43<02:21,  3.60it/s] 38%|      | 314/820 [01:43<02:20,  3.60it/s] 38%|      | 315/820 [01:44<02:20,  3.60it/s] 39%|      | 316/820 [01:44<02:20,  3.60it/s] 39%|      | 317/820 [01:44<02:29,  3.36it/s] 39%|      | 318/820 [01:45<02:26,  3.43it/s] 39%|      | 319/820 [01:45<02:23,  3.48it/s] 39%|      | 320/820 [01:45<02:22,  3.52it/s] 39%|      | 321/820 [01:45<02:20,  3.55it/s] 39%|      | 322/820 [01:46<02:19,  3.57it/s] 39%|      | 323/820 [01:46<02:18,  3.58it/s] 40%|      | 324/820 [01:46<02:18,  3.59it/s] 40%|      | 325/820 [01:46<02:17,  3.59it/s] 40%|      | 326/820 [01:47<02:17,  3.60it/s] 40%|      | 327/820 [01:47<02:16,  3.61it/s] 40%|      | 328/820 [01:47<02:11,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 14:02:58,679 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:02:58,679 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:02:58,680 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.1625, 'eval_samples_per_second': 343.32, 'eval_steps_per_second': 43.001, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:09, 44.08it/s][A
  3%|         | 12/437 [00:00<00:09, 47.20it/s][A
  4%|         | 17/437 [00:00<00:09, 46.53it/s][A
  5%|         | 22/437 [00:00<00:09, 46.04it/s][A
  6%|         | 27/437 [00:00<00:08, 45.92it/s][A
  7%|         | 32/437 [00:00<00:08, 45.56it/s][A
  8%|         | 37/437 [00:00<00:08, 45.24it/s][A
 10%|         | 42/437 [00:00<00:08, 44.94it/s][A
 11%|         | 47/437 [00:01<00:08, 44.96it/s][A
 12%|        | 52/437 [00:01<00:08, 45.01it/s][A
 13%|        | 57/437 [00:01<00:08, 45.13it/s][A
 14%|        | 62/437 [00:01<00:08, 45.20it/s][A
 15%|        | 67/437 [00:01<00:08, 45.24it/s][A
 16%|        | 72/437 [00:01<00:08, 45.32it/s][A
 18%|        | 77/437 [00:01<00:07, 45.15it/s][A
 19%|        | 82/437 [00:01<00:07, 45.00it/s][A
 20%|        | 87/437 [00:01<00:07, 44.80it/s][A
 21%|        | 92/437 [00:02<00:07, 44.81it/s][A
 22%|       | 97/437 [00:02<00:07, 44.93it/s][A
 23%|       | 102/437 [00:02<00:07, 44.96it/s][A
 24%|       | 107/437 [00:02<00:07, 45.19it/s][A
 26%|       | 112/437 [00:02<00:07, 45.16it/s][A
 27%|       | 117/437 [00:02<00:07, 45.23it/s][A
 28%|       | 122/437 [00:02<00:06, 45.16it/s][A
 29%|       | 127/437 [00:02<00:06, 44.91it/s][A
 30%|       | 132/437 [00:02<00:07, 40.05it/s][A
 31%|      | 137/437 [00:03<00:07, 41.57it/s][A
 32%|      | 142/437 [00:03<00:06, 42.67it/s][A
 34%|      | 147/437 [00:03<00:06, 43.44it/s][A
 35%|      | 152/437 [00:03<00:06, 44.07it/s][A
 36%|      | 157/437 [00:03<00:06, 44.47it/s][A
 37%|      | 162/437 [00:03<00:06, 44.80it/s][A
 38%|      | 167/437 [00:03<00:06, 44.86it/s][A
 39%|      | 172/437 [00:03<00:05, 44.61it/s][A
 41%|      | 177/437 [00:03<00:05, 44.61it/s][A
 42%|     | 182/437 [00:04<00:05, 44.69it/s][A
 43%|     | 187/437 [00:04<00:05, 44.95it/s][A
 44%|     | 192/437 [00:04<00:05, 44.94it/s][A
 45%|     | 197/437 [00:04<00:05, 45.09it/s][A
 46%|     | 202/437 [00:04<00:05, 45.17it/s][A
 47%|     | 207/437 [00:04<00:05, 45.28it/s][A
 49%|     | 212/437 [00:04<00:04, 45.17it/s][A
 50%|     | 217/437 [00:04<00:04, 44.89it/s][A
 51%|     | 222/437 [00:04<00:04, 44.74it/s][A
 52%|    | 227/437 [00:05<00:04, 44.78it/s][A
 53%|    | 232/437 [00:05<00:04, 41.11it/s][A
 54%|    | 238/437 [00:05<00:04, 43.76it/s][A
 56%|    | 243/437 [00:05<00:04, 44.23it/s][A
 57%|    | 248/437 [00:05<00:04, 44.56it/s][A
 58%|    | 253/437 [00:05<00:04, 44.78it/s][A
 59%|    | 258/437 [00:05<00:03, 45.02it/s][A
 60%|    | 263/437 [00:05<00:03, 45.10it/s][A
 61%|   | 268/437 [00:06<00:03, 43.01it/s][A
 62%|   | 273/437 [00:06<00:03, 43.61it/s][A
 64%|   | 278/437 [00:06<00:03, 43.90it/s][A
 65%|   | 283/437 [00:06<00:03, 44.22it/s][A
 66%|   | 288/437 [00:06<00:03, 44.33it/s][A
 67%|   | 293/437 [00:06<00:03, 44.83it/s][A
 68%|   | 298/437 [00:06<00:03, 45.02it/s][A
 69%|   | 303/437 [00:06<00:02, 45.09it/s][A
 70%|   | 308/437 [00:06<00:02, 44.93it/s][A
 72%|  | 313/437 [00:07<00:02, 45.02it/s][A
 73%|  | 318/437 [00:07<00:02, 45.03it/s][A
 74%|  | 323/437 [00:07<00:02, 45.08it/s][A
 75%|  | 328/437 [00:07<00:02, 44.96it/s][A
 76%|  | 333/437 [00:07<00:02, 41.89it/s][A
 77%|  | 338/437 [00:07<00:04, 22.86it/s][A
 78%|  | 342/437 [00:08<00:03, 25.02it/s][A
 79%|  | 347/437 [00:08<00:03, 29.21it/s][A
 81%|  | 352/437 [00:08<00:02, 32.84it/s][A
 82%| | 357/437 [00:08<00:02, 35.95it/s][A
 83%| | 362/437 [00:08<00:01, 38.37it/s][A
 84%| | 367/437 [00:08<00:01, 40.29it/s][A
 85%| | 372/437 [00:08<00:01, 41.73it/s][A
 86%| | 377/437 [00:08<00:01, 42.57it/s][A
 87%| | 382/437 [00:08<00:01, 43.03it/s][A
 89%| | 387/437 [00:09<00:01, 43.56it/s][A
 90%| | 392/437 [00:09<00:01, 44.02it/s][A
 91%| | 397/437 [00:09<00:00, 44.44it/s][A
 92%|| 402/437 [00:09<00:00, 44.75it/s][A
 93%|| 407/437 [00:09<00:00, 44.89it/s][A
 94%|| 412/437 [00:09<00:00, 45.06it/s][A
 95%|| 417/437 [00:09<00:00, 45.10it/s][A
 97%|| 422/437 [00:09<00:00, 44.91it/s][A
 98%|| 427/437 [00:09<00:00, 44.79it/s][A
 99%|| 432/437 [00:10<00:00, 44.73it/s][A
100%|| 437/437 [00:10<00:00, 44.95it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 44.95it/s][A 40%|      | 328/820 [01:57<02:11,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:03:09,131 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 14:03:09,627 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:03:13,811 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:03:13,984 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:03:14,066 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/special_tokens_map.json
 40%|      | 329/820 [02:04<42:26,  5.19s/it] 40%|      | 330/820 [02:04<30:19,  3.71s/it] 40%|      | 331/820 [02:04<21:51,  2.68s/it] 40%|      | 332/820 [02:05<15:57,  1.96s/it] 41%|      | 333/820 [02:05<11:49,  1.46s/it] 41%|      | 334/820 [02:05<08:55,  1.10s/it] 41%|      | 335/820 [02:06<06:54,  1.17it/s] 41%|      | 336/820 [02:06<05:33,  1.45it/s] 41%|      | 337/820 [02:06<04:33,  1.77it/s] 41%|      | 338/820 [02:06<03:50,  2.09it/s] 41%|     | 339/820 [02:07<03:21,  2.39it/s] 41%|     | 340/820 [02:07<03:00,  2.66it/s] 42%|     | 341/820 [02:07<02:46,  2.88it/s] 42%|     | 342/820 [02:08<02:35,  3.07it/s] 42%|     | 343/820 [02:08<02:28,  3.21it/s] 42%|     | 344/820 [02:08<02:23,  3.32it/s] 42%|     | 345/820 [02:08<02:19,  3.41it/s] 42%|     | 346/820 [02:09<02:16,  3.46it/s] 42%|     | 347/820 [02:09<02:21,  3.35it/s] 42%|     | 348/820 [02:09<02:18,  3.42it/s] 43%|     | 349/820 [02:10<02:15,  3.48it/s] 43%|     | 350/820 [02:10<02:13,  3.51it/s] 43%|     | 351/820 [02:10<02:12,  3.55it/s] 43%|     | 352/820 [02:10<02:11,  3.57it/s] 43%|     | 353/820 [02:11<02:10,  3.59it/s] 43%|     | 354/820 [02:11<02:09,  3.59it/s] 43%|     | 355/820 [02:11<02:09,  3.59it/s] 43%|     | 356/820 [02:11<02:08,  3.60it/s] 44%|     | 357/820 [02:12<02:08,  3.61it/s] 44%|     | 358/820 [02:12<02:19,  3.30it/s] 44%|     | 359/820 [02:12<02:16,  3.39it/s] 44%|     | 360/820 [02:13<02:13,  3.45it/s] 44%|     | 361/820 [02:13<02:10,  3.50it/s] 44%|     | 362/820 [02:13<02:09,  3.54it/s] 44%|     | 363/820 [02:13<02:08,  3.56it/s] 44%|     | 364/820 [02:14<02:07,  3.58it/s] 45%|     | 365/820 [02:14<02:06,  3.59it/s] 45%|     | 366/820 [02:14<02:06,  3.60it/s] 45%|     | 367/820 [02:15<02:05,  3.61it/s] 45%|     | 368/820 [02:15<02:05,  3.61it/s] 45%|     | 369/820 [02:15<02:08,  3.51it/s] 45%|     | 370/820 [02:15<02:07,  3.54it/s] 45%|     | 371/820 [02:16<02:05,  3.56it/s] 45%|     | 372/820 [02:16<02:05,  3.58it/s] 45%|     | 373/820 [02:16<02:04,  3.59it/s] 46%|     | 374/820 [02:17<02:03,  3.60it/s] 46%|     | 375/820 [02:17<02:03,  3.61it/s] 46%|     | 376/820 [02:17<02:03,  3.61it/s] 46%|     | 377/820 [02:17<02:02,  3.61it/s] 46%|     | 378/820 [02:18<02:02,  3.62it/s] 46%|     | 379/820 [02:18<02:01,  3.62it/s] 46%|     | 380/820 [02:18<02:07,  3.44it/s] 46%|     | 381/820 [02:19<02:05,  3.49it/s] 47%|     | 382/820 [02:19<02:04,  3.53it/s] 47%|     | 383/820 [02:19<02:03,  3.55it/s] 47%|     | 384/820 [02:19<02:02,  3.57it/s] 47%|     | 385/820 [02:20<02:01,  3.58it/s] 47%|     | 386/820 [02:20<02:00,  3.60it/s] 47%|     | 387/820 [02:20<02:00,  3.61it/s] 47%|     | 388/820 [02:20<01:59,  3.61it/s] 47%|     | 389/820 [02:21<01:59,  3.61it/s] 48%|     | 390/820 [02:21<01:58,  3.61it/s] 48%|     | 391/820 [02:21<02:06,  3.40it/s] 48%|     | 392/820 [02:22<02:03,  3.47it/s] 48%|     | 393/820 [02:22<02:01,  3.51it/s] 48%|     | 394/820 [02:22<02:00,  3.54it/s] 48%|     | 395/820 [02:22<01:59,  3.56it/s] 48%|     | 396/820 [02:23<01:58,  3.58it/s] 48%|     | 397/820 [02:23<01:57,  3.59it/s] 49%|     | 398/820 [02:23<01:57,  3.60it/s] 49%|     | 399/820 [02:24<01:56,  3.61it/s] 49%|     | 400/820 [02:24<01:56,  3.61it/s] 49%|     | 401/820 [02:24<01:55,  3.61it/s] 49%|     | 402/820 [02:24<02:01,  3.45it/s] 49%|     | 403/820 [02:25<01:59,  3.50it/s] 49%|     | 404/820 [02:25<01:57,  3.54it/s] 49%|     | 405/820 [02:25<01:56,  3.56it/s] 50%|     | 406/820 [02:26<01:55,  3.57it/s] 50%|     | 407/820 [02:26<01:55,  3.58it/s] 50%|     | 408/820 [02:26<01:54,  3.59it/s] 50%|     | 409/820 [02:26<01:54,  3.60it/s] 50%|     | 410/820 [02:27<01:53,  3.61it/s] 50%|     | 411/820 [02:27<01:53,  3.60it/s] 50%|     | 412/820 [02:27<01:53,  3.60it/s] 50%|     | 413/820 [02:27<01:52,  3.60it/s] 50%|     | 414/820 [02:28<01:52,  3.60it/s] 51%|     | 415/820 [02:28<01:52,  3.61it/s] 51%|     | 416/820 [02:28<01:51,  3.61it/s] 51%|     | 417/820 [02:29<01:51,  3.61it/s] 51%|     | 418/820 [02:29<01:51,  3.61it/s] 51%|     | 419/820 [02:29<01:51,  3.61it/s] 51%|     | 420/820 [02:29<01:50,  3.61it/s] 51%|    | 421/820 [02:30<01:50,  3.61it/s] 51%|    | 422/820 [02:30<01:50,  3.61it/s] 52%|    | 423/820 [02:30<01:56,  3.40it/s] 52%|    | 424/820 [02:31<01:54,  3.47it/s] 52%|    | 425/820 [02:31<01:52,  3.51it/s] 52%|    | 426/820 [02:31<01:51,  3.54it/s] 52%|    | 427/820 [02:31<01:50,  3.56it/s] 52%|    | 428/820 [02:32<01:49,  3.58it/s] 52%|    | 429/820 [02:32<01:48,  3.59it/s] 52%|    | 430/820 [02:32<01:48,  3.60it/s] 53%|    | 431/820 [02:33<01:47,  3.60it/s] 53%|    | 432/820 [02:33<01:47,  3.60it/s] 53%|    | 433/820 [02:33<01:47,  3.61it/s] 53%|    | 434/820 [02:33<01:51,  3.48it/s] 53%|    | 435/820 [02:34<01:49,  3.51it/s] 53%|    | 436/820 [02:34<01:48,  3.54it/s] 53%|    | 437/820 [02:34<01:47,  3.56it/s] 53%|    | 438/820 [02:34<01:46,  3.57it/s] 54%|    | 439/820 [02:35<01:46,  3.58it/s] 54%|    | 440/820 [02:35<01:45,  3.59it/s] 54%|    | 441/820 [02:35<01:45,  3.60it/s] 54%|    | 442/820 [02:36<01:44,  3.60it/s] 54%|    | 443/820 [02:36<01:44,  3.60it/s] 54%|    | 444/820 [02:36<01:44,  3.60it/s] 54%|    | 445/820 [02:36<01:47,  3.49it/s] 54%|    | 446/820 [02:37<01:46,  3.53it/s] 55%|    | 447/820 [02:37<01:45,  3.55it/s] 55%|    | 448/820 [02:37<01:44,  3.57it/s] 55%|    | 449/820 [02:38<01:43,  3.58it/s] 55%|    | 450/820 [02:38<01:42,  3.59it/s] 55%|    | 451/820 [02:38<01:42,  3.60it/s] 55%|    | 452/820 [02:38<01:42,  3.60it/s] 55%|    | 453/820 [02:39<01:41,  3.60it/s] 55%|    | 454/820 [02:39<01:41,  3.60it/s] 55%|    | 455/820 [02:39<01:41,  3.60it/s] 56%|    | 456/820 [02:40<01:47,  3.37it/s] 56%|    | 457/820 [02:40<01:45,  3.43it/s] 56%|    | 458/820 [02:40<01:43,  3.49it/s] 56%|    | 459/820 [02:40<01:42,  3.53it/s] 56%|    | 460/820 [02:41<01:41,  3.55it/s] 56%|    | 461/820 [02:41<01:40,  3.57it/s] 56%|    | 462/820 [02:41<01:39,  3.58it/s] 56%|    | 463/820 [02:42<01:39,  3.59it/s] 57%|    | 464/820 [02:42<01:38,  3.60it/s] 57%|    | 465/820 [02:42<01:38,  3.61it/s] 57%|    | 466/820 [02:42<01:38,  3.60it/s] 57%|    | 467/820 [02:43<01:46,  3.32it/s] 57%|    | 468/820 [02:43<01:43,  3.40it/s] 57%|    | 469/820 [02:43<01:41,  3.46it/s] 57%|    | 470/820 [02:44<01:40,  3.49it/s] 57%|    | 471/820 [02:44<01:38,  3.53it/s] 58%|    | 472/820 [02:44<01:37,  3.56it/s] 58%|    | 473/820 [02:44<01:37,  3.57it/s] 58%|    | 474/820 [02:45<01:36,  3.59it/s] 58%|    | 475/820 [02:45<01:35,  3.59it/s] 58%|    | 476/820 [02:45<01:35,  3.60it/s] 58%|    | 477/820 [02:45<01:35,  3.59it/s] 58%|    | 478/820 [02:46<01:40,  3.40it/s] 58%|    | 479/820 [02:46<01:38,  3.46it/s] 59%|    | 480/820 [02:46<01:37,  3.50it/s] 59%|    | 481/820 [02:47<01:36,  3.52it/s] 59%|    | 482/820 [02:47<01:35,  3.55it/s] 59%|    | 483/820 [02:47<01:34,  3.57it/s] 59%|    | 484/820 [02:47<01:33,  3.58it/s] 59%|    | 485/820 [02:48<01:33,  3.60it/s] 59%|    | 486/820 [02:48<01:32,  3.60it/s] 59%|    | 487/820 [02:48<01:32,  3.60it/s] 60%|    | 488/820 [02:49<01:32,  3.61it/s] 60%|    | 489/820 [02:49<01:38,  3.37it/s] 60%|    | 490/820 [02:49<01:36,  3.43it/s] 60%|    | 491/820 [02:49<01:34,  3.49it/s] 60%|    | 492/820 [02:50<01:26,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 14:04:01,113 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:04:01,114 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:04:01,114 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.1698, 'eval_samples_per_second': 343.076, 'eval_steps_per_second': 42.971, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.78it/s][A
  3%|         | 12/437 [00:00<00:08, 49.42it/s][A
  4%|         | 18/437 [00:00<00:08, 47.60it/s][A
  5%|         | 23/437 [00:00<00:08, 46.67it/s][A
  6%|         | 28/437 [00:00<00:08, 46.16it/s][A
  8%|         | 33/437 [00:00<00:08, 45.61it/s][A
  9%|         | 38/437 [00:00<00:08, 45.22it/s][A
 10%|         | 43/437 [00:00<00:08, 44.96it/s][A
 11%|         | 48/437 [00:01<00:08, 44.99it/s][A
 12%|        | 53/437 [00:01<00:08, 45.26it/s][A
 13%|        | 58/437 [00:01<00:08, 45.34it/s][A
 14%|        | 63/437 [00:01<00:08, 45.24it/s][A
 16%|        | 68/437 [00:01<00:08, 45.25it/s][A
 17%|        | 73/437 [00:01<00:08, 45.25it/s][A
 18%|        | 78/437 [00:01<00:07, 45.02it/s][A
 19%|        | 83/437 [00:01<00:07, 44.88it/s][A
 20%|        | 88/437 [00:01<00:07, 44.76it/s][A
 21%|       | 93/437 [00:02<00:08, 42.71it/s][A
 22%|       | 98/437 [00:02<00:07, 43.50it/s][A
 24%|       | 103/437 [00:02<00:07, 43.98it/s][A
 25%|       | 108/437 [00:02<00:07, 44.49it/s][A
 26%|       | 113/437 [00:02<00:07, 44.74it/s][A
 27%|       | 118/437 [00:02<00:07, 44.87it/s][A
 28%|       | 123/437 [00:02<00:06, 44.94it/s][A
 29%|       | 128/437 [00:02<00:07, 41.82it/s][A
 30%|       | 133/437 [00:02<00:07, 42.94it/s][A
 32%|      | 138/437 [00:03<00:06, 43.63it/s][A
 33%|      | 143/437 [00:03<00:06, 44.12it/s][A
 34%|      | 148/437 [00:03<00:06, 44.36it/s][A
 35%|      | 153/437 [00:03<00:06, 44.71it/s][A
 36%|      | 158/437 [00:03<00:06, 44.81it/s][A
 37%|      | 163/437 [00:03<00:06, 45.00it/s][A
 38%|      | 168/437 [00:03<00:06, 44.83it/s][A
 40%|      | 173/437 [00:03<00:05, 44.67it/s][A
 41%|      | 178/437 [00:03<00:05, 44.80it/s][A
 42%|     | 183/437 [00:04<00:05, 44.85it/s][A
 43%|     | 188/437 [00:04<00:05, 45.06it/s][A
 44%|     | 193/437 [00:04<00:05, 45.13it/s][A
 45%|     | 198/437 [00:04<00:05, 45.22it/s][A
 46%|     | 203/437 [00:04<00:05, 45.14it/s][A
 48%|     | 208/437 [00:04<00:05, 45.17it/s][A
 49%|     | 213/437 [00:04<00:04, 45.07it/s][A
 50%|     | 218/437 [00:04<00:04, 45.06it/s][A
 51%|     | 223/437 [00:04<00:04, 44.98it/s][A
 52%|    | 228/437 [00:05<00:05, 38.27it/s][A
 53%|    | 233/437 [00:05<00:05, 40.72it/s][A
 54%|    | 238/437 [00:05<00:07, 25.90it/s][A
 56%|    | 243/437 [00:05<00:06, 30.06it/s][A
 57%|    | 248/437 [00:05<00:05, 33.49it/s][A
 58%|    | 253/437 [00:05<00:05, 36.39it/s][A
 59%|    | 258/437 [00:06<00:04, 38.70it/s][A
 60%|    | 263/437 [00:06<00:04, 40.50it/s][A
 61%|   | 268/437 [00:06<00:04, 41.92it/s][A
 62%|   | 273/437 [00:06<00:03, 42.90it/s][A
 64%|   | 278/437 [00:06<00:03, 43.24it/s][A
 65%|   | 283/437 [00:06<00:03, 43.53it/s][A
 66%|   | 288/437 [00:06<00:03, 43.82it/s][A
 67%|   | 293/437 [00:06<00:03, 44.28it/s][A
 68%|   | 298/437 [00:06<00:03, 44.58it/s][A
 69%|   | 303/437 [00:07<00:02, 44.79it/s][A
 70%|   | 308/437 [00:07<00:02, 44.95it/s][A
 72%|  | 313/437 [00:07<00:02, 45.12it/s][A
 73%|  | 318/437 [00:07<00:02, 45.22it/s][A
 74%|  | 323/437 [00:07<00:02, 45.02it/s][A
 75%|  | 328/437 [00:07<00:02, 44.83it/s][A
 76%|  | 333/437 [00:07<00:02, 44.79it/s][A
 77%|  | 338/437 [00:07<00:02, 45.00it/s][A
 78%|  | 343/437 [00:07<00:02, 45.13it/s][A
 80%|  | 348/437 [00:08<00:01, 45.20it/s][A
 81%|  | 353/437 [00:08<00:01, 45.17it/s][A
 82%| | 358/437 [00:08<00:01, 45.20it/s][A
 83%| | 363/437 [00:08<00:01, 45.20it/s][A
 84%| | 368/437 [00:08<00:01, 45.13it/s][A
 85%| | 373/437 [00:08<00:01, 44.95it/s][A
 86%| | 378/437 [00:08<00:01, 44.88it/s][A
 88%| | 383/437 [00:08<00:01, 45.02it/s][A
 89%| | 388/437 [00:08<00:01, 45.02it/s][A
 90%| | 393/437 [00:09<00:00, 45.19it/s][A
 91%| | 398/437 [00:09<00:00, 45.27it/s][A
 92%|| 403/437 [00:09<00:00, 45.27it/s][A
 93%|| 408/437 [00:09<00:00, 45.28it/s][A
 95%|| 413/437 [00:09<00:00, 45.24it/s][A
 96%|| 418/437 [00:09<00:00, 44.98it/s][A
 97%|| 423/437 [00:09<00:00, 44.96it/s][A
 98%|| 428/437 [00:09<00:00, 45.02it/s][A
 99%|| 433/437 [00:09<00:00, 45.13it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 45.13it/s][A 60%|    | 492/820 [03:00<01:26,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:04:11,340 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 14:04:11,568 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:04:15,610 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:04:15,876 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:04:15,983 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/special_tokens_map.json
 60%|    | 493/820 [03:06<28:30,  5.23s/it] 60%|    | 494/820 [03:07<20:21,  3.75s/it] 60%|    | 495/820 [03:07<14:39,  2.71s/it] 60%|    | 496/820 [03:07<10:41,  1.98s/it] 61%|    | 497/820 [03:08<07:59,  1.48s/it] 61%|    | 498/820 [03:08<06:01,  1.12s/it] 61%|    | 499/820 [03:08<04:39,  1.15it/s] 61%|    | 500/820 [03:09<03:41,  1.44it/s]                                                  61%|    | 500/820 [03:09<03:41,  1.44it/s] 61%|    | 501/820 [03:09<03:01,  1.76it/s] 61%|    | 502/820 [03:09<02:33,  2.07it/s] 61%|   | 503/820 [03:09<02:13,  2.37it/s] 61%|   | 504/820 [03:10<01:59,  2.64it/s] 62%|   | 505/820 [03:10<01:49,  2.87it/s] 62%|   | 506/820 [03:10<01:42,  3.06it/s] 62%|   | 507/820 [03:10<01:37,  3.21it/s] 62%|   | 508/820 [03:11<01:37,  3.18it/s] 62%|   | 509/820 [03:11<01:34,  3.30it/s] 62%|   | 510/820 [03:11<01:31,  3.39it/s] 62%|   | 511/820 [03:12<01:29,  3.46it/s] 62%|   | 512/820 [03:12<01:27,  3.51it/s] 63%|   | 513/820 [03:12<01:26,  3.54it/s] 63%|   | 514/820 [03:12<01:25,  3.56it/s] 63%|   | 515/820 [03:13<01:25,  3.58it/s] 63%|   | 516/820 [03:13<01:24,  3.59it/s] 63%|   | 517/820 [03:13<01:24,  3.60it/s] 63%|   | 518/820 [03:14<01:23,  3.60it/s] 63%|   | 519/820 [03:14<01:28,  3.42it/s] 63%|   | 520/820 [03:14<01:26,  3.47it/s] 64%|   | 521/820 [03:14<01:25,  3.52it/s] 64%|   | 522/820 [03:15<01:24,  3.54it/s] 64%|   | 523/820 [03:15<01:23,  3.56it/s] 64%|   | 524/820 [03:15<01:22,  3.57it/s] 64%|   | 525/820 [03:16<01:22,  3.58it/s] 64%|   | 526/820 [03:16<01:21,  3.59it/s] 64%|   | 527/820 [03:16<01:21,  3.60it/s] 64%|   | 528/820 [03:16<01:21,  3.60it/s] 65%|   | 529/820 [03:17<01:20,  3.60it/s] 65%|   | 530/820 [03:17<01:23,  3.46it/s] 65%|   | 531/820 [03:17<01:22,  3.50it/s] 65%|   | 532/820 [03:18<01:21,  3.53it/s] 65%|   | 533/820 [03:18<01:20,  3.55it/s] 65%|   | 534/820 [03:18<01:20,  3.57it/s] 65%|   | 535/820 [03:18<01:19,  3.58it/s] 65%|   | 536/820 [03:19<01:19,  3.59it/s] 65%|   | 537/820 [03:19<01:18,  3.60it/s] 66%|   | 538/820 [03:19<01:18,  3.60it/s] 66%|   | 539/820 [03:19<01:18,  3.59it/s] 66%|   | 540/820 [03:20<01:17,  3.60it/s] 66%|   | 541/820 [03:20<01:21,  3.44it/s] 66%|   | 542/820 [03:20<01:19,  3.49it/s] 66%|   | 543/820 [03:21<01:18,  3.53it/s] 66%|   | 544/820 [03:21<01:17,  3.55it/s] 66%|   | 545/820 [03:21<01:17,  3.56it/s] 67%|   | 546/820 [03:21<01:16,  3.57it/s] 67%|   | 547/820 [03:22<01:16,  3.58it/s] 67%|   | 548/820 [03:22<01:15,  3.59it/s] 67%|   | 549/820 [03:22<01:15,  3.59it/s] 67%|   | 550/820 [03:23<01:15,  3.59it/s] 67%|   | 551/820 [03:23<01:14,  3.59it/s] 67%|   | 552/820 [03:23<01:16,  3.50it/s] 67%|   | 553/820 [03:23<01:15,  3.53it/s] 68%|   | 554/820 [03:24<01:14,  3.55it/s] 68%|   | 555/820 [03:24<01:14,  3.57it/s] 68%|   | 556/820 [03:24<01:13,  3.58it/s] 68%|   | 557/820 [03:25<01:13,  3.59it/s] 68%|   | 558/820 [03:25<01:12,  3.59it/s] 68%|   | 559/820 [03:25<01:12,  3.59it/s] 68%|   | 560/820 [03:25<01:12,  3.60it/s] 68%|   | 561/820 [03:26<01:11,  3.60it/s] 69%|   | 562/820 [03:26<01:11,  3.60it/s] 69%|   | 563/820 [03:26<01:14,  3.46it/s] 69%|   | 564/820 [03:26<01:13,  3.50it/s] 69%|   | 565/820 [03:27<01:12,  3.53it/s] 69%|   | 566/820 [03:27<01:11,  3.55it/s] 69%|   | 567/820 [03:27<01:10,  3.57it/s] 69%|   | 568/820 [03:28<01:10,  3.58it/s] 69%|   | 569/820 [03:28<01:09,  3.59it/s] 70%|   | 570/820 [03:28<01:09,  3.59it/s] 70%|   | 571/820 [03:28<01:09,  3.60it/s] 70%|   | 572/820 [03:29<01:08,  3.60it/s] 70%|   | 573/820 [03:29<01:08,  3.60it/s] 70%|   | 574/820 [03:29<01:08,  3.59it/s] 70%|   | 575/820 [03:30<01:08,  3.60it/s] 70%|   | 576/820 [03:30<01:07,  3.60it/s] 70%|   | 577/820 [03:30<01:07,  3.60it/s] 70%|   | 578/820 [03:30<01:07,  3.61it/s] 71%|   | 579/820 [03:31<01:06,  3.61it/s] 71%|   | 580/820 [03:31<01:06,  3.61it/s] 71%|   | 581/820 [03:31<01:06,  3.60it/s] 71%|   | 582/820 [03:31<01:05,  3.61it/s] 71%|   | 583/820 [03:32<01:08,  3.44it/s] 71%|   | 584/820 [03:32<01:07,  3.49it/s] 71%|  | 585/820 [03:32<01:06,  3.52it/s] 71%|  | 586/820 [03:33<01:06,  3.55it/s] 72%|  | 587/820 [03:33<01:05,  3.56it/s] 72%|  | 588/820 [03:33<01:04,  3.57it/s] 72%|  | 589/820 [03:33<01:04,  3.58it/s] 72%|  | 590/820 [03:34<01:04,  3.59it/s] 72%|  | 591/820 [03:34<01:03,  3.60it/s] 72%|  | 592/820 [03:34<01:03,  3.59it/s] 72%|  | 593/820 [03:35<01:03,  3.59it/s] 72%|  | 594/820 [03:35<01:04,  3.48it/s] 73%|  | 595/820 [03:35<01:03,  3.52it/s] 73%|  | 596/820 [03:35<01:03,  3.54it/s] 73%|  | 597/820 [03:36<01:02,  3.56it/s] 73%|  | 598/820 [03:36<01:02,  3.58it/s] 73%|  | 599/820 [03:36<01:01,  3.59it/s] 73%|  | 600/820 [03:37<01:01,  3.60it/s] 73%|  | 601/820 [03:37<01:00,  3.60it/s] 73%|  | 602/820 [03:37<01:00,  3.60it/s] 74%|  | 603/820 [03:37<01:00,  3.59it/s] 74%|  | 604/820 [03:38<01:00,  3.59it/s] 74%|  | 605/820 [03:38<01:01,  3.47it/s] 74%|  | 606/820 [03:38<01:01,  3.51it/s] 74%|  | 607/820 [03:39<01:00,  3.54it/s] 74%|  | 608/820 [03:39<00:59,  3.56it/s] 74%|  | 609/820 [03:39<00:59,  3.57it/s] 74%|  | 610/820 [03:39<00:58,  3.58it/s] 75%|  | 611/820 [03:40<00:58,  3.59it/s] 75%|  | 612/820 [03:40<00:57,  3.59it/s] 75%|  | 613/820 [03:40<00:57,  3.60it/s] 75%|  | 614/820 [03:40<00:57,  3.60it/s] 75%|  | 615/820 [03:41<00:56,  3.60it/s] 75%|  | 616/820 [03:41<01:00,  3.38it/s] 75%|  | 617/820 [03:41<00:59,  3.44it/s] 75%|  | 618/820 [03:42<00:57,  3.48it/s] 75%|  | 619/820 [03:42<00:57,  3.52it/s] 76%|  | 620/820 [03:42<00:56,  3.55it/s] 76%|  | 621/820 [03:42<00:55,  3.57it/s] 76%|  | 622/820 [03:43<00:55,  3.58it/s] 76%|  | 623/820 [03:43<00:54,  3.59it/s] 76%|  | 624/820 [03:43<00:54,  3.60it/s] 76%|  | 625/820 [03:44<00:54,  3.60it/s] 76%|  | 626/820 [03:44<00:53,  3.60it/s] 76%|  | 627/820 [03:44<00:57,  3.34it/s] 77%|  | 628/820 [03:44<00:56,  3.41it/s] 77%|  | 629/820 [03:45<00:55,  3.47it/s] 77%|  | 630/820 [03:45<00:54,  3.51it/s] 77%|  | 631/820 [03:45<00:53,  3.53it/s] 77%|  | 632/820 [03:46<00:52,  3.56it/s] 77%|  | 633/820 [03:46<00:52,  3.57it/s] 77%|  | 634/820 [03:46<00:51,  3.58it/s] 77%|  | 635/820 [03:46<00:51,  3.60it/s] 78%|  | 636/820 [03:47<00:51,  3.60it/s] 78%|  | 637/820 [03:47<00:50,  3.61it/s] 78%|  | 638/820 [03:47<00:56,  3.23it/s] 78%|  | 639/820 [03:48<00:54,  3.34it/s] 78%|  | 640/820 [03:48<00:52,  3.41it/s] 78%|  | 641/820 [03:48<00:51,  3.47it/s] 78%|  | 642/820 [03:48<00:50,  3.51it/s] 78%|  | 643/820 [03:49<00:49,  3.54it/s] 79%|  | 644/820 [03:49<00:49,  3.56it/s] 79%|  | 645/820 [03:49<00:49,  3.57it/s] 79%|  | 646/820 [03:50<00:48,  3.58it/s] 79%|  | 647/820 [03:50<00:48,  3.59it/s] 79%|  | 648/820 [03:50<00:47,  3.59it/s] 79%|  | 649/820 [03:51<00:51,  3.31it/s] 79%|  | 650/820 [03:51<00:50,  3.39it/s] 79%|  | 651/820 [03:51<00:48,  3.45it/s] 80%|  | 652/820 [03:51<00:48,  3.50it/s] 80%|  | 653/820 [03:52<00:47,  3.53it/s] 80%|  | 654/820 [03:52<00:46,  3.55it/s] 80%|  | 655/820 [03:52<00:46,  3.57it/s] 80%|  | 656/820 [03:52<00:42,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 14:05:03,823 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:05:03,823 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:05:03,823 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.0163, 'eval_samples_per_second': 348.332, 'eval_steps_per_second': 43.629, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:11, 38.09it/s][A
  3%|         | 12/437 [00:00<00:09, 44.07it/s][A
  4%|         | 17/437 [00:00<00:09, 44.72it/s][A
  5%|         | 22/437 [00:00<00:09, 45.10it/s][A
  6%|         | 27/437 [00:00<00:09, 45.22it/s][A
  7%|         | 32/437 [00:00<00:08, 45.31it/s][A
  8%|         | 37/437 [00:00<00:08, 45.34it/s][A
 10%|         | 42/437 [00:00<00:09, 43.71it/s][A
 11%|         | 47/437 [00:01<00:08, 44.22it/s][A
 12%|        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|        | 57/437 [00:01<00:08, 44.56it/s][A
 14%|        | 62/437 [00:01<00:08, 44.89it/s][A
 15%|        | 67/437 [00:01<00:08, 45.06it/s][A
 16%|        | 72/437 [00:01<00:08, 45.23it/s][A
 18%|        | 77/437 [00:01<00:07, 45.28it/s][A
 19%|        | 82/437 [00:01<00:07, 45.11it/s][A
 20%|        | 87/437 [00:01<00:07, 45.16it/s][A
 21%|        | 92/437 [00:02<00:07, 45.12it/s][A
 22%|       | 97/437 [00:02<00:07, 44.89it/s][A
 23%|       | 102/437 [00:02<00:07, 45.06it/s][A
 24%|       | 107/437 [00:02<00:07, 44.53it/s][A
 26%|       | 112/437 [00:02<00:07, 41.04it/s][A
 27%|       | 117/437 [00:02<00:10, 29.12it/s][A
 28%|       | 122/437 [00:02<00:09, 31.96it/s][A
 29%|       | 127/437 [00:03<00:08, 35.20it/s][A
 30%|       | 132/437 [00:03<00:08, 37.83it/s][A
 31%|      | 137/437 [00:03<00:07, 39.84it/s][A
 32%|      | 142/437 [00:03<00:07, 41.41it/s][A
 34%|      | 147/437 [00:03<00:06, 42.58it/s][A
 35%|      | 152/437 [00:03<00:06, 43.48it/s][A
 36%|      | 157/437 [00:03<00:06, 43.72it/s][A
 37%|      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|      | 167/437 [00:03<00:06, 41.89it/s][A
 39%|      | 172/437 [00:04<00:06, 42.98it/s][A
 41%|      | 177/437 [00:04<00:05, 43.78it/s][A
 42%|     | 182/437 [00:04<00:05, 44.23it/s][A
 43%|     | 187/437 [00:04<00:05, 44.67it/s][A
 44%|     | 192/437 [00:04<00:05, 44.99it/s][A
 45%|     | 197/437 [00:04<00:05, 45.07it/s][A
 46%|     | 202/437 [00:04<00:05, 45.07it/s][A
 47%|     | 207/437 [00:04<00:05, 44.82it/s][A
 49%|     | 212/437 [00:04<00:05, 44.82it/s][A
 50%|     | 217/437 [00:05<00:04, 45.01it/s][A
 51%|     | 222/437 [00:05<00:04, 45.22it/s][A
 52%|    | 227/437 [00:05<00:04, 45.31it/s][A
 53%|    | 232/437 [00:05<00:04, 45.29it/s][A
 54%|    | 237/437 [00:05<00:04, 45.47it/s][A
 55%|    | 242/437 [00:05<00:04, 45.35it/s][A
 57%|    | 247/437 [00:05<00:04, 45.31it/s][A
 58%|    | 252/437 [00:05<00:04, 45.15it/s][A
 59%|    | 257/437 [00:05<00:03, 45.07it/s][A
 60%|    | 262/437 [00:06<00:03, 45.10it/s][A
 61%|    | 267/437 [00:06<00:03, 45.04it/s][A
 62%|   | 272/437 [00:06<00:03, 45.17it/s][A
 63%|   | 277/437 [00:06<00:03, 45.33it/s][A
 65%|   | 282/437 [00:06<00:03, 45.34it/s][A
 66%|   | 287/437 [00:06<00:03, 45.42it/s][A
 67%|   | 292/437 [00:06<00:03, 44.15it/s][A
 68%|   | 297/437 [00:06<00:03, 44.49it/s][A
 69%|   | 302/437 [00:06<00:03, 44.71it/s][A
 70%|   | 307/437 [00:07<00:02, 44.88it/s][A
 71%|  | 312/437 [00:07<00:02, 44.92it/s][A
 73%|  | 317/437 [00:07<00:02, 45.08it/s][A
 74%|  | 322/437 [00:07<00:02, 45.19it/s][A
 75%|  | 327/437 [00:07<00:02, 45.30it/s][A
 76%|  | 332/437 [00:07<00:02, 45.18it/s][A
 77%|  | 337/437 [00:07<00:02, 45.07it/s][A
 78%|  | 342/437 [00:07<00:02, 45.06it/s][A
 79%|  | 347/437 [00:07<00:01, 45.02it/s][A
 81%|  | 352/437 [00:08<00:01, 45.17it/s][A
 82%| | 357/437 [00:08<00:01, 45.24it/s][A
 83%| | 362/437 [00:08<00:01, 45.31it/s][A
 84%| | 367/437 [00:08<00:01, 45.27it/s][A
 85%| | 372/437 [00:08<00:01, 45.32it/s][A
 86%| | 377/437 [00:08<00:01, 45.30it/s][A
 87%| | 382/437 [00:08<00:01, 45.25it/s][A
 89%| | 387/437 [00:08<00:01, 45.23it/s][A
 90%| | 392/437 [00:08<00:00, 45.23it/s][A
 91%| | 397/437 [00:09<00:00, 45.18it/s][A
 92%|| 402/437 [00:09<00:00, 45.14it/s][A
 93%|| 407/437 [00:09<00:00, 45.26it/s][A
 94%|| 412/437 [00:09<00:00, 45.31it/s][A
 95%|| 417/437 [00:09<00:00, 45.40it/s][A
 97%|| 422/437 [00:09<00:00, 45.38it/s][A
 98%|| 427/437 [00:09<00:00, 43.28it/s][A
 99%|| 432/437 [00:09<00:00, 44.01it/s][A
100%|| 437/437 [00:09<00:00, 44.50it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.50it/s][A 80%|  | 656/820 [04:02<00:42,  3.85it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:05:14,072 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 14:05:14,254 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:05:17,653 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:05:17,797 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:05:17,905 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/special_tokens_map.json
 80%|  | 657/820 [04:08<13:08,  4.84s/it] 80%|  | 658/820 [04:08<09:22,  3.47s/it] 80%|  | 659/820 [04:08<06:44,  2.51s/it] 80%|  | 660/820 [04:09<04:55,  1.84s/it] 81%|  | 661/820 [04:09<03:38,  1.38s/it] 81%|  | 662/820 [04:09<02:45,  1.05s/it] 81%|  | 663/820 [04:10<02:09,  1.21it/s] 81%|  | 664/820 [04:10<01:43,  1.51it/s] 81%|  | 665/820 [04:10<01:24,  1.82it/s] 81%|  | 666/820 [04:10<01:11,  2.14it/s] 81%| | 667/820 [04:11<01:02,  2.44it/s] 81%| | 668/820 [04:11<00:56,  2.70it/s] 82%| | 669/820 [04:11<00:51,  2.93it/s] 82%| | 670/820 [04:12<00:48,  3.11it/s] 82%| | 671/820 [04:12<00:45,  3.24it/s] 82%| | 672/820 [04:12<00:44,  3.35it/s] 82%| | 673/820 [04:12<00:42,  3.42it/s] 82%| | 674/820 [04:13<00:42,  3.41it/s] 82%| | 675/820 [04:13<00:41,  3.46it/s] 82%| | 676/820 [04:13<00:41,  3.51it/s] 83%| | 677/820 [04:14<00:40,  3.54it/s] 83%| | 678/820 [04:14<00:39,  3.56it/s] 83%| | 679/820 [04:14<00:39,  3.57it/s] 83%| | 680/820 [04:14<00:39,  3.58it/s] 83%| | 681/820 [04:15<00:38,  3.59it/s] 83%| | 682/820 [04:15<00:38,  3.60it/s] 83%| | 683/820 [04:15<00:37,  3.61it/s] 83%| | 684/820 [04:15<00:37,  3.61it/s] 84%| | 685/820 [04:16<00:38,  3.54it/s] 84%| | 686/820 [04:16<00:37,  3.56it/s] 84%| | 687/820 [04:16<00:37,  3.57it/s] 84%| | 688/820 [04:17<00:36,  3.59it/s] 84%| | 689/820 [04:17<00:36,  3.59it/s] 84%| | 690/820 [04:17<00:36,  3.60it/s] 84%| | 691/820 [04:17<00:35,  3.61it/s] 84%| | 692/820 [04:18<00:35,  3.61it/s] 85%| | 693/820 [04:18<00:35,  3.61it/s] 85%| | 694/820 [04:18<00:34,  3.60it/s] 85%| | 695/820 [04:19<00:34,  3.61it/s] 85%| | 696/820 [04:19<00:35,  3.49it/s] 85%| | 697/820 [04:19<00:34,  3.52it/s] 85%| | 698/820 [04:19<00:34,  3.54it/s] 85%| | 699/820 [04:20<00:33,  3.56it/s] 85%| | 700/820 [04:20<00:33,  3.58it/s] 85%| | 701/820 [04:20<00:33,  3.59it/s] 86%| | 702/820 [04:20<00:32,  3.60it/s] 86%| | 703/820 [04:21<00:32,  3.60it/s] 86%| | 704/820 [04:21<00:32,  3.60it/s] 86%| | 705/820 [04:21<00:31,  3.61it/s] 86%| | 706/820 [04:22<00:31,  3.61it/s] 86%| | 707/820 [04:22<00:32,  3.48it/s] 86%| | 708/820 [04:22<00:31,  3.52it/s] 86%| | 709/820 [04:22<00:31,  3.55it/s] 87%| | 710/820 [04:23<00:30,  3.57it/s] 87%| | 711/820 [04:23<00:30,  3.58it/s] 87%| | 712/820 [04:23<00:30,  3.59it/s] 87%| | 713/820 [04:24<00:29,  3.60it/s] 87%| | 714/820 [04:24<00:29,  3.60it/s] 87%| | 715/820 [04:24<00:29,  3.60it/s] 87%| | 716/820 [04:24<00:28,  3.60it/s] 87%| | 717/820 [04:25<00:28,  3.61it/s] 88%| | 718/820 [04:25<00:29,  3.44it/s] 88%| | 719/820 [04:25<00:28,  3.49it/s] 88%| | 720/820 [04:26<00:28,  3.53it/s] 88%| | 721/820 [04:26<00:27,  3.55it/s] 88%| | 722/820 [04:26<00:27,  3.56it/s] 88%| | 723/820 [04:26<00:27,  3.57it/s] 88%| | 724/820 [04:27<00:26,  3.58it/s] 88%| | 725/820 [04:27<00:26,  3.58it/s] 89%| | 726/820 [04:27<00:26,  3.59it/s] 89%| | 727/820 [04:28<00:25,  3.60it/s] 89%| | 728/820 [04:28<00:25,  3.60it/s] 89%| | 729/820 [04:28<00:26,  3.49it/s] 89%| | 730/820 [04:28<00:25,  3.53it/s] 89%| | 731/820 [04:29<00:25,  3.56it/s] 89%| | 732/820 [04:29<00:24,  3.57it/s] 89%| | 733/820 [04:29<00:24,  3.58it/s] 90%| | 734/820 [04:29<00:23,  3.58it/s] 90%| | 735/820 [04:30<00:23,  3.59it/s] 90%| | 736/820 [04:30<00:23,  3.59it/s] 90%| | 737/820 [04:30<00:23,  3.60it/s] 90%| | 738/820 [04:31<00:22,  3.60it/s] 90%| | 739/820 [04:31<00:22,  3.61it/s] 90%| | 740/820 [04:31<00:22,  3.61it/s] 90%| | 741/820 [04:31<00:21,  3.61it/s] 90%| | 742/820 [04:32<00:21,  3.61it/s] 91%| | 743/820 [04:32<00:21,  3.61it/s] 91%| | 744/820 [04:32<00:21,  3.61it/s] 91%| | 745/820 [04:33<00:20,  3.62it/s] 91%| | 746/820 [04:33<00:20,  3.61it/s] 91%| | 747/820 [04:33<00:21,  3.47it/s] 91%| | 748/820 [04:33<00:20,  3.52it/s] 91%|| 749/820 [04:34<00:20,  3.54it/s] 91%|| 750/820 [04:34<00:19,  3.56it/s] 92%|| 751/820 [04:34<00:19,  3.58it/s] 92%|| 752/820 [04:34<00:18,  3.59it/s] 92%|| 753/820 [04:35<00:18,  3.59it/s] 92%|| 754/820 [04:35<00:18,  3.60it/s] 92%|| 755/820 [04:35<00:18,  3.60it/s] 92%|| 756/820 [04:36<00:17,  3.60it/s] 92%|| 757/820 [04:36<00:17,  3.60it/s] 92%|| 758/820 [04:36<00:17,  3.47it/s] 93%|| 759/820 [04:36<00:17,  3.51it/s] 93%|| 760/820 [04:37<00:16,  3.54it/s] 93%|| 761/820 [04:37<00:16,  3.56it/s] 93%|| 762/820 [04:37<00:16,  3.58it/s] 93%|| 763/820 [04:38<00:15,  3.59it/s] 93%|| 764/820 [04:38<00:15,  3.60it/s] 93%|| 765/820 [04:38<00:15,  3.60it/s] 93%|| 766/820 [04:38<00:15,  3.60it/s] 94%|| 767/820 [04:39<00:14,  3.60it/s] 94%|| 768/820 [04:39<00:14,  3.60it/s] 94%|| 769/820 [04:39<00:14,  3.46it/s] 94%|| 770/820 [04:40<00:14,  3.51it/s] 94%|| 771/820 [04:40<00:13,  3.54it/s] 94%|| 772/820 [04:40<00:13,  3.55it/s] 94%|| 773/820 [04:40<00:13,  3.56it/s] 94%|| 774/820 [04:41<00:12,  3.57it/s] 95%|| 775/820 [04:41<00:12,  3.58it/s] 95%|| 776/820 [04:41<00:12,  3.59it/s] 95%|| 777/820 [04:41<00:11,  3.60it/s] 95%|| 778/820 [04:42<00:11,  3.61it/s] 95%|| 779/820 [04:42<00:11,  3.61it/s] 95%|| 780/820 [04:42<00:11,  3.44it/s] 95%|| 781/820 [04:43<00:11,  3.48it/s] 95%|| 782/820 [04:43<00:10,  3.52it/s] 95%|| 783/820 [04:43<00:10,  3.54it/s] 96%|| 784/820 [04:43<00:10,  3.56it/s] 96%|| 785/820 [04:44<00:09,  3.58it/s] 96%|| 786/820 [04:44<00:09,  3.59it/s] 96%|| 787/820 [04:44<00:09,  3.60it/s] 96%|| 788/820 [04:45<00:08,  3.60it/s] 96%|| 789/820 [04:45<00:08,  3.61it/s] 96%|| 790/820 [04:45<00:08,  3.60it/s] 96%|| 791/820 [04:45<00:08,  3.40it/s] 97%|| 792/820 [04:46<00:08,  3.46it/s] 97%|| 793/820 [04:46<00:07,  3.51it/s] 97%|| 794/820 [04:46<00:07,  3.54it/s] 97%|| 795/820 [04:47<00:07,  3.56it/s] 97%|| 796/820 [04:47<00:06,  3.58it/s] 97%|| 797/820 [04:47<00:06,  3.59it/s] 97%|| 798/820 [04:47<00:06,  3.60it/s] 97%|| 799/820 [04:48<00:05,  3.60it/s] 98%|| 800/820 [04:48<00:05,  3.60it/s] 98%|| 801/820 [04:48<00:05,  3.60it/s] 98%|| 802/820 [04:49<00:05,  3.41it/s] 98%|| 803/820 [04:49<00:04,  3.46it/s] 98%|| 804/820 [04:49<00:04,  3.50it/s] 98%|| 805/820 [04:49<00:04,  3.53it/s] 98%|| 806/820 [04:50<00:03,  3.55it/s] 98%|| 807/820 [04:50<00:03,  3.56it/s] 99%|| 808/820 [04:50<00:03,  3.58it/s] 99%|| 809/820 [04:51<00:03,  3.59it/s] 99%|| 810/820 [04:51<00:02,  3.60it/s] 99%|| 811/820 [04:51<00:02,  3.61it/s] 99%|| 812/820 [04:51<00:02,  3.61it/s] 99%|| 813/820 [04:52<00:02,  3.45it/s] 99%|| 814/820 [04:52<00:01,  3.50it/s] 99%|| 815/820 [04:52<00:01,  3.53it/s]100%|| 816/820 [04:52<00:01,  3.56it/s]100%|| 817/820 [04:53<00:00,  3.47it/s]100%|| 818/820 [04:53<00:00,  3.50it/s]100%|| 819/820 [04:53<00:00,  3.53it/s]100%|| 820/820 [04:54<00:00,  3.82it/s][INFO|trainer.py:2140] 2023-08-28 14:06:05,011 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:06:05,011 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:06:05,011 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.962, 'eval_samples_per_second': 350.231, 'eval_steps_per_second': 43.867, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.61it/s][A
  3%|         | 12/437 [00:00<00:08, 49.69it/s][A
  4%|         | 18/437 [00:00<00:08, 47.67it/s][A
  5%|         | 23/437 [00:00<00:08, 46.81it/s][A
  6%|         | 28/437 [00:00<00:08, 46.20it/s][A
  8%|         | 33/437 [00:00<00:08, 45.76it/s][A
  9%|         | 38/437 [00:00<00:08, 45.48it/s][A
 10%|         | 43/437 [00:00<00:09, 43.43it/s][A
 11%|         | 48/437 [00:01<00:08, 44.11it/s][A
 12%|        | 53/437 [00:01<00:09, 41.21it/s][A
 13%|        | 58/437 [00:01<00:08, 42.66it/s][A
 14%|        | 63/437 [00:01<00:13, 28.53it/s][A
 15%|        | 67/437 [00:01<00:13, 26.60it/s][A
 16%|        | 72/437 [00:01<00:11, 30.70it/s][A
 18%|        | 77/437 [00:02<00:10, 34.06it/s][A
 19%|        | 82/437 [00:02<00:09, 36.90it/s][A
 20%|        | 87/437 [00:02<00:08, 39.22it/s][A
 21%|        | 92/437 [00:02<00:08, 40.98it/s][A
 22%|       | 97/437 [00:02<00:08, 42.34it/s][A
 23%|       | 102/437 [00:02<00:07, 43.28it/s][A
 24%|       | 107/437 [00:02<00:07, 43.64it/s][A
 26%|       | 112/437 [00:02<00:07, 43.78it/s][A
 27%|       | 117/437 [00:02<00:07, 43.84it/s][A
 28%|       | 122/437 [00:03<00:07, 44.21it/s][A
 29%|       | 127/437 [00:03<00:06, 44.63it/s][A
 30%|       | 132/437 [00:03<00:06, 44.87it/s][A
 31%|      | 137/437 [00:03<00:06, 45.13it/s][A
 32%|      | 142/437 [00:03<00:06, 45.29it/s][A
 34%|      | 147/437 [00:03<00:06, 45.47it/s][A
 35%|      | 152/437 [00:03<00:06, 45.33it/s][A
 36%|      | 157/437 [00:03<00:06, 45.04it/s][A
 37%|      | 162/437 [00:03<00:06, 44.83it/s][A
 38%|      | 167/437 [00:04<00:06, 39.76it/s][A
 39%|      | 172/437 [00:04<00:06, 41.39it/s][A
 41%|      | 177/437 [00:04<00:06, 42.68it/s][A
 42%|     | 182/437 [00:04<00:05, 43.48it/s][A
 43%|     | 187/437 [00:04<00:05, 44.00it/s][A
 44%|     | 192/437 [00:04<00:05, 44.45it/s][A
 45%|     | 197/437 [00:04<00:05, 44.86it/s][A
 46%|     | 202/437 [00:04<00:05, 45.17it/s][A
 47%|     | 207/437 [00:04<00:05, 44.88it/s][A
 49%|     | 212/437 [00:05<00:05, 44.71it/s][A
 50%|     | 217/437 [00:05<00:04, 44.72it/s][A
 51%|     | 222/437 [00:05<00:04, 44.86it/s][A
 52%|    | 227/437 [00:05<00:04, 45.11it/s][A
 53%|    | 232/437 [00:05<00:04, 45.30it/s][A
 54%|    | 237/437 [00:05<00:04, 45.44it/s][A
 55%|    | 242/437 [00:05<00:04, 45.58it/s][A
 57%|    | 247/437 [00:05<00:04, 45.46it/s][A
 58%|    | 252/437 [00:05<00:04, 45.16it/s][A
 59%|    | 257/437 [00:06<00:04, 44.96it/s][A
 60%|    | 262/437 [00:06<00:03, 44.88it/s][A
 61%|    | 267/437 [00:06<00:03, 45.07it/s][A
 62%|   | 272/437 [00:06<00:03, 45.13it/s][A
 63%|   | 277/437 [00:06<00:03, 45.32it/s][A
 65%|   | 282/437 [00:06<00:03, 45.33it/s][A
 66%|   | 287/437 [00:06<00:03, 45.46it/s][A
 67%|   | 292/437 [00:06<00:03, 45.48it/s][A
 68%|   | 297/437 [00:06<00:03, 45.25it/s][A
 69%|   | 302/437 [00:07<00:03, 44.99it/s][A
 70%|   | 307/437 [00:07<00:02, 44.94it/s][A
 71%|  | 312/437 [00:07<00:02, 45.01it/s][A
 73%|  | 317/437 [00:07<00:02, 45.18it/s][A
 74%|  | 322/437 [00:07<00:02, 45.28it/s][A
 75%|  | 327/437 [00:07<00:02, 45.34it/s][A
 76%|  | 332/437 [00:07<00:02, 45.46it/s][A
 77%|  | 337/437 [00:07<00:02, 45.38it/s][A
 78%|  | 342/437 [00:07<00:02, 45.22it/s][A
 79%|  | 347/437 [00:08<00:01, 45.19it/s][A
 81%|  | 352/437 [00:08<00:01, 44.98it/s][A
 82%| | 357/437 [00:08<00:01, 44.96it/s][A
 83%| | 362/437 [00:08<00:01, 45.17it/s][A
 84%| | 367/437 [00:08<00:01, 45.21it/s][A
 85%| | 372/437 [00:08<00:01, 45.40it/s][A
 86%| | 377/437 [00:08<00:01, 45.48it/s][A
 87%| | 382/437 [00:08<00:01, 45.45it/s][A
 89%| | 387/437 [00:08<00:01, 45.35it/s][A
 90%| | 392/437 [00:09<00:00, 45.22it/s][A
 91%| | 397/437 [00:09<00:00, 45.15it/s][A
 92%|| 402/437 [00:09<00:00, 45.06it/s][A
 93%|| 407/437 [00:09<00:00, 45.07it/s][A
 94%|| 412/437 [00:09<00:00, 45.23it/s][A
 95%|| 417/437 [00:09<00:00, 45.34it/s][A
 97%|| 422/437 [00:09<00:00, 45.47it/s][A
 98%|| 427/437 [00:09<00:00, 45.48it/s][A
 99%|| 432/437 [00:09<00:00, 45.49it/s][A
100%|| 437/437 [00:10<00:00, 43.38it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 43.38it/s][A100%|| 820/820 [05:04<00:00,  3.82it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:06:15,235 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 14:06:15,443 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:06:19,592 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:06:19,771 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:06:19,862 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:06:21,846 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:06:21,847 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164 (score: 1.0192089080810547).
                                                 100%|| 820/820 [05:21<00:00,  3.82it/s]100%|| 820/820 [05:21<00:00,  2.55it/s]
[INFO|trainer.py:1894] 2023-08-28 14:06:32,817 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 14:06:33,172 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:06:36,497 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:06:36,779 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:06:36,909 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:06:37,709 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,709 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,709 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,710 >>   train_runtime            = 0:05:21.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,710 >>   train_samples            =      10479
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,710 >>   train_samples_per_second =    162.816
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:37,710 >>   train_steps_per_second   =      2.548
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.0375, 'eval_samples_per_second': 347.598, 'eval_steps_per_second': 43.537, 'epoch': 5.0}
{'train_runtime': 321.8058, 'train_samples_per_second': 162.816, 'train_steps_per_second': 2.548, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 14:06:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:06:38,023 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:06:38,023 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 14:06:38,023 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 56.07it/s]  3%|         | 12/437 [00:00<00:08, 49.71it/s]  4%|         | 18/437 [00:00<00:08, 47.92it/s]  5%|         | 23/437 [00:00<00:08, 47.17it/s]  6%|         | 28/437 [00:00<00:08, 46.66it/s]  8%|         | 33/437 [00:00<00:08, 46.51it/s]  9%|         | 38/437 [00:00<00:08, 46.30it/s] 10%|         | 43/437 [00:00<00:08, 45.90it/s] 11%|         | 48/437 [00:01<00:08, 45.35it/s] 12%|        | 53/437 [00:01<00:08, 45.08it/s] 13%|        | 58/437 [00:01<00:08, 45.35it/s] 14%|        | 63/437 [00:01<00:08, 45.28it/s] 16%|        | 68/437 [00:01<00:08, 45.62it/s] 17%|        | 73/437 [00:01<00:08, 41.03it/s] 18%|        | 78/437 [00:01<00:08, 42.40it/s] 19%|        | 83/437 [00:01<00:08, 43.40it/s] 20%|        | 88/437 [00:01<00:07, 44.19it/s] 21%|       | 93/437 [00:02<00:07, 44.47it/s] 22%|       | 98/437 [00:02<00:07, 44.80it/s] 24%|       | 103/437 [00:02<00:07, 44.94it/s] 25%|       | 108/437 [00:02<00:07, 45.12it/s] 26%|       | 113/437 [00:02<00:07, 44.88it/s] 27%|       | 118/437 [00:02<00:07, 44.82it/s] 28%|       | 123/437 [00:02<00:06, 44.98it/s] 29%|       | 128/437 [00:02<00:06, 45.17it/s] 30%|       | 133/437 [00:02<00:06, 45.36it/s] 32%|      | 138/437 [00:03<00:06, 45.48it/s] 33%|      | 143/437 [00:03<00:06, 45.50it/s] 34%|      | 148/437 [00:03<00:06, 45.54it/s] 35%|      | 153/437 [00:03<00:06, 45.41it/s] 36%|      | 158/437 [00:03<00:06, 45.23it/s] 37%|      | 163/437 [00:03<00:06, 44.99it/s] 38%|      | 168/437 [00:03<00:05, 45.01it/s] 40%|      | 173/437 [00:03<00:05, 45.20it/s] 41%|      | 178/437 [00:03<00:05, 45.35it/s] 42%|     | 183/437 [00:04<00:05, 45.54it/s] 43%|     | 188/437 [00:04<00:05, 45.65it/s] 44%|     | 193/437 [00:04<00:05, 45.58it/s] 45%|     | 198/437 [00:04<00:05, 45.44it/s] 46%|     | 203/437 [00:04<00:05, 45.25it/s] 48%|     | 208/437 [00:04<00:06, 33.70it/s] 49%|     | 213/437 [00:04<00:06, 36.62it/s] 50%|     | 218/437 [00:04<00:05, 38.98it/s] 51%|     | 223/437 [00:05<00:05, 40.85it/s] 52%|    | 228/437 [00:05<00:04, 42.17it/s] 53%|    | 233/437 [00:05<00:04, 43.18it/s] 54%|    | 238/437 [00:05<00:04, 43.95it/s] 56%|    | 243/437 [00:05<00:04, 44.38it/s] 57%|    | 248/437 [00:05<00:04, 44.32it/s] 58%|    | 253/437 [00:05<00:04, 44.44it/s] 59%|    | 258/437 [00:05<00:04, 44.63it/s] 60%|    | 263/437 [00:05<00:03, 45.03it/s] 61%|   | 268/437 [00:06<00:03, 45.21it/s] 62%|   | 273/437 [00:06<00:03, 45.38it/s] 64%|   | 278/437 [00:06<00:03, 45.47it/s] 65%|   | 283/437 [00:06<00:03, 45.44it/s] 66%|   | 288/437 [00:06<00:03, 45.40it/s] 67%|   | 293/437 [00:06<00:03, 45.16it/s] 68%|   | 298/437 [00:06<00:03, 44.95it/s] 69%|   | 303/437 [00:06<00:02, 45.04it/s] 70%|   | 308/437 [00:06<00:02, 45.13it/s] 72%|  | 313/437 [00:07<00:02, 45.37it/s] 73%|  | 318/437 [00:07<00:02, 45.54it/s] 74%|  | 323/437 [00:07<00:02, 45.54it/s] 75%|  | 328/437 [00:07<00:02, 45.58it/s] 76%|  | 333/437 [00:07<00:02, 45.43it/s] 77%|  | 338/437 [00:07<00:02, 45.17it/s] 78%|  | 343/437 [00:07<00:02, 40.37it/s] 80%|  | 348/437 [00:07<00:02, 41.95it/s] 81%|  | 353/437 [00:07<00:01, 43.07it/s] 82%| | 358/437 [00:08<00:01, 43.87it/s] 83%| | 363/437 [00:08<00:01, 44.51it/s] 84%| | 368/437 [00:08<00:01, 44.92it/s] 85%| | 373/437 [00:08<00:01, 45.19it/s] 86%| | 378/437 [00:08<00:01, 45.26it/s] 88%| | 383/437 [00:08<00:01, 44.92it/s] 89%| | 388/437 [00:08<00:01, 44.67it/s] 90%| | 393/437 [00:08<00:00, 44.90it/s] 91%| | 398/437 [00:08<00:00, 45.14it/s] 92%|| 403/437 [00:09<00:00, 45.35it/s] 93%|| 408/437 [00:09<00:00, 45.34it/s] 95%|| 413/437 [00:09<00:00, 45.47it/s] 96%|| 418/437 [00:09<00:00, 45.65it/s] 97%|| 423/437 [00:09<00:00, 45.70it/s] 98%|| 428/437 [00:09<00:00, 45.59it/s] 99%|| 433/437 [00:09<00:00, 45.30it/s]100%|| 437/437 [00:09<00:00, 44.57it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:06:47,844 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   eval_loss               =     1.0192
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   eval_runtime            = 0:00:09.82
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   eval_samples_per_second =    355.268
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   eval_steps_per_second   =     44.498
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:06:47,844 >>   perplexity              =      2.771
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:59,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:59,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:59,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:59,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:06:59,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:07:00,569 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:07:00,571 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:07:01,192 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:07:02,239 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:07:02,239 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:07:05,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:07:05,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:07:05,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:07:05,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:07:05,719 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:07:06,501 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:07:06,502 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:07:07,736 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:07:07,916 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:07:07,916 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.74it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:09,  1.72it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:11,  1.68it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.70it/s]Extractor Predicting: 23it [00:13,  1.71it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:15,  1.72it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:17,  1.71it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:18,  1.71it/s]Extractor Predicting: 33it [00:19,  1.76it/s]Extractor Predicting: 34it [00:19,  1.76it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:21,  1.66it/s]Extractor Predicting: 38it [00:22,  1.66it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:23,  1.66it/s]Extractor Predicting: 41it [00:24,  1.62it/s]Extractor Predicting: 42it [00:24,  1.64it/s]Extractor Predicting: 43it [00:25,  1.64it/s]Extractor Predicting: 44it [00:26,  1.63it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:27,  1.63it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:29,  1.65it/s]Extractor Predicting: 50it [00:29,  1.63it/s]Extractor Predicting: 51it [00:30,  1.61it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:31,  1.65it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:32,  1.62it/s]Extractor Predicting: 56it [00:33,  1.64it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:34,  1.61it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:36,  1.61it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:37,  1.53it/s]Extractor Predicting: 64it [00:38,  1.57it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:39,  1.64it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:40,  1.66it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:41,  1.74it/s]Extractor Predicting: 71it [00:42,  1.74it/s]Extractor Predicting: 72it [00:43,  1.73it/s]Extractor Predicting: 73it [00:43,  1.73it/s]Extractor Predicting: 74it [00:44,  1.69it/s]Extractor Predicting: 75it [00:44,  1.66it/s]Extractor Predicting: 76it [00:45,  1.65it/s]Extractor Predicting: 77it [00:46,  1.69it/s]Extractor Predicting: 78it [00:46,  1.73it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:47,  1.71it/s]Extractor Predicting: 81it [00:48,  1.72it/s]Extractor Predicting: 82it [00:48,  1.75it/s]Extractor Predicting: 83it [00:49,  1.74it/s]Extractor Predicting: 84it [00:50,  1.71it/s]Extractor Predicting: 85it [00:50,  1.75it/s]Extractor Predicting: 86it [00:51,  1.72it/s]Extractor Predicting: 87it [00:51,  1.75it/s]Extractor Predicting: 88it [00:52,  1.75it/s]Extractor Predicting: 89it [00:53,  1.74it/s]Extractor Predicting: 90it [00:53,  1.75it/s]Extractor Predicting: 91it [00:54,  1.74it/s]Extractor Predicting: 92it [00:54,  1.67it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.70it/s]Extractor Predicting: 95it [00:56,  1.68it/s]Extractor Predicting: 96it [00:57,  1.67it/s]Extractor Predicting: 97it [00:57,  1.65it/s]Extractor Predicting: 98it [00:58,  1.70it/s]Extractor Predicting: 99it [00:58,  1.68it/s]Extractor Predicting: 100it [00:59,  1.66it/s]Extractor Predicting: 101it [01:00,  1.69it/s]Extractor Predicting: 102it [01:00,  1.68it/s]Extractor Predicting: 103it [01:01,  1.63it/s]Extractor Predicting: 104it [01:02,  1.62it/s]Extractor Predicting: 105it [01:02,  1.64it/s]Extractor Predicting: 106it [01:03,  1.64it/s]Extractor Predicting: 107it [01:03,  1.68it/s]Extractor Predicting: 108it [01:04,  1.70it/s]Extractor Predicting: 109it [01:05,  1.65it/s]Extractor Predicting: 110it [01:05,  1.63it/s]Extractor Predicting: 111it [01:06,  1.64it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.68it/s]Extractor Predicting: 116it [01:09,  1.65it/s]Extractor Predicting: 117it [01:09,  1.67it/s]Extractor Predicting: 118it [01:10,  1.69it/s]Extractor Predicting: 119it [01:10,  1.73it/s]Extractor Predicting: 120it [01:11,  1.79it/s]Extractor Predicting: 121it [01:12,  1.78it/s]Extractor Predicting: 122it [01:12,  1.75it/s]Extractor Predicting: 123it [01:13,  1.73it/s]Extractor Predicting: 124it [01:13,  1.74it/s]Extractor Predicting: 125it [01:14,  1.74it/s]Extractor Predicting: 126it [01:14,  1.75it/s]Extractor Predicting: 127it [01:15,  1.75it/s]Extractor Predicting: 128it [01:16,  1.68it/s]Extractor Predicting: 129it [01:16,  1.73it/s]Extractor Predicting: 130it [01:17,  1.69it/s]Extractor Predicting: 131it [01:18,  1.55it/s]Extractor Predicting: 132it [01:18,  1.61it/s]Extractor Predicting: 133it [01:19,  1.59it/s]Extractor Predicting: 134it [01:19,  1.62it/s]Extractor Predicting: 135it [01:20,  1.66it/s]Extractor Predicting: 136it [01:20,  1.70it/s]Extractor Predicting: 137it [01:21,  1.68it/s]Extractor Predicting: 138it [01:22,  1.68it/s]Extractor Predicting: 139it [01:22,  1.66it/s]Extractor Predicting: 140it [01:23,  1.68it/s]Extractor Predicting: 141it [01:23,  1.68it/s]Extractor Predicting: 142it [01:24,  1.69it/s]Extractor Predicting: 143it [01:25,  1.71it/s]Extractor Predicting: 144it [01:25,  1.72it/s]Extractor Predicting: 145it [01:25,  2.21it/s]Extractor Predicting: 145it [01:25,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:51,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:51,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:51,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:51,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:51,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:08:52,142 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:08:52,143 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:08:52,804 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:08:53,850 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:08:53,850 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:57,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:57,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:57,329 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:57,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:08:57,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:08:58,300 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:08:58,301 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:08:59,033 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:08:59,377 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:08:59,377 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.71it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:13,  1.76it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.67it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:17,  1.62it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:19,  1.64it/s]Extractor Predicting: 34it [00:20,  1.48it/s]Extractor Predicting: 35it [00:21,  1.55it/s]Extractor Predicting: 36it [00:21,  1.61it/s]Extractor Predicting: 37it [00:22,  1.70it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:23,  1.70it/s]Extractor Predicting: 41it [00:24,  1.72it/s]Extractor Predicting: 42it [00:25,  1.72it/s]Extractor Predicting: 43it [00:25,  1.71it/s]Extractor Predicting: 44it [00:26,  1.73it/s]Extractor Predicting: 45it [00:26,  1.67it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.71it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:29,  1.69it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.72it/s]Extractor Predicting: 55it [00:32,  1.67it/s]Extractor Predicting: 56it [00:33,  1.68it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.73it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:35,  1.83it/s]Extractor Predicting: 61it [00:36,  1.85it/s]Extractor Predicting: 62it [00:36,  1.88it/s]Extractor Predicting: 63it [00:37,  1.83it/s]Extractor Predicting: 64it [00:37,  1.88it/s]Extractor Predicting: 65it [00:38,  1.90it/s]Extractor Predicting: 66it [00:38,  1.90it/s]Extractor Predicting: 67it [00:39,  1.93it/s]Extractor Predicting: 68it [00:39,  1.98it/s]Extractor Predicting: 69it [00:40,  2.01it/s]Extractor Predicting: 70it [00:40,  1.94it/s]Extractor Predicting: 71it [00:41,  1.95it/s]Extractor Predicting: 72it [00:41,  1.97it/s]Extractor Predicting: 73it [00:42,  1.98it/s]Extractor Predicting: 74it [00:42,  1.96it/s]Extractor Predicting: 75it [00:43,  1.94it/s]Extractor Predicting: 76it [00:43,  1.93it/s]Extractor Predicting: 77it [00:44,  1.96it/s]Extractor Predicting: 78it [00:44,  1.95it/s]Extractor Predicting: 79it [00:45,  1.97it/s]Extractor Predicting: 80it [00:45,  1.99it/s]Extractor Predicting: 81it [00:46,  1.98it/s]Extractor Predicting: 82it [00:46,  1.93it/s]Extractor Predicting: 83it [00:47,  1.92it/s]Extractor Predicting: 84it [00:47,  1.91it/s]Extractor Predicting: 85it [00:48,  1.93it/s]Extractor Predicting: 86it [00:49,  1.88it/s]Extractor Predicting: 87it [00:49,  1.82it/s]Extractor Predicting: 88it [00:50,  1.73it/s]Extractor Predicting: 89it [00:50,  1.71it/s]Extractor Predicting: 90it [00:51,  1.66it/s]Extractor Predicting: 91it [00:52,  1.68it/s]Extractor Predicting: 92it [00:52,  1.65it/s]Extractor Predicting: 93it [00:53,  1.64it/s]Extractor Predicting: 94it [00:53,  1.65it/s]Extractor Predicting: 95it [00:54,  1.66it/s]Extractor Predicting: 96it [00:55,  1.66it/s]Extractor Predicting: 97it [00:55,  1.67it/s]Extractor Predicting: 98it [00:56,  1.63it/s]Extractor Predicting: 99it [00:56,  1.67it/s]Extractor Predicting: 100it [00:57,  1.66it/s]Extractor Predicting: 101it [00:58,  1.67it/s]Extractor Predicting: 102it [00:58,  1.66it/s]Extractor Predicting: 103it [00:59,  1.66it/s]Extractor Predicting: 104it [00:59,  1.63it/s]Extractor Predicting: 105it [01:00,  1.65it/s]Extractor Predicting: 106it [01:01,  1.62it/s]Extractor Predicting: 107it [01:01,  1.62it/s]Extractor Predicting: 108it [01:02,  1.60it/s]Extractor Predicting: 109it [01:03,  1.64it/s]Extractor Predicting: 110it [01:03,  1.64it/s]Extractor Predicting: 111it [01:04,  1.63it/s]Extractor Predicting: 112it [01:04,  1.60it/s]Extractor Predicting: 113it [01:05,  1.59it/s]Extractor Predicting: 114it [01:06,  1.65it/s]Extractor Predicting: 115it [01:06,  1.64it/s]Extractor Predicting: 116it [01:07,  1.68it/s]Extractor Predicting: 117it [01:07,  1.71it/s]Extractor Predicting: 118it [01:08,  1.66it/s]Extractor Predicting: 119it [01:09,  1.62it/s]Extractor Predicting: 120it [01:09,  1.64it/s]Extractor Predicting: 121it [01:10,  1.63it/s]Extractor Predicting: 122it [01:10,  1.66it/s]Extractor Predicting: 123it [01:11,  1.64it/s]Extractor Predicting: 124it [01:12,  1.64it/s]Extractor Predicting: 125it [01:12,  1.63it/s]Extractor Predicting: 126it [01:13,  1.61it/s]Extractor Predicting: 127it [01:14,  1.61it/s]Extractor Predicting: 128it [01:14,  1.55it/s]Extractor Predicting: 129it [01:15,  1.57it/s]Extractor Predicting: 130it [01:16,  1.57it/s]Extractor Predicting: 131it [01:16,  1.62it/s]Extractor Predicting: 132it [01:17,  1.63it/s]Extractor Predicting: 133it [01:17,  1.61it/s]Extractor Predicting: 134it [01:18,  1.64it/s]Extractor Predicting: 135it [01:19,  1.63it/s]Extractor Predicting: 136it [01:19,  1.59it/s]Extractor Predicting: 137it [01:20,  1.61it/s]Extractor Predicting: 138it [01:20,  1.58it/s]Extractor Predicting: 139it [01:21,  1.59it/s]Extractor Predicting: 140it [01:22,  1.61it/s]Extractor Predicting: 141it [01:22,  1.62it/s]Extractor Predicting: 142it [01:23,  1.63it/s]Extractor Predicting: 143it [01:24,  1.56it/s]Extractor Predicting: 144it [01:24,  1.57it/s]Extractor Predicting: 145it [01:25,  1.59it/s]Extractor Predicting: 146it [01:25,  1.61it/s]Extractor Predicting: 147it [01:26,  1.64it/s]Extractor Predicting: 148it [01:27,  1.65it/s]Extractor Predicting: 149it [01:27,  1.61it/s]Extractor Predicting: 150it [01:28,  1.62it/s]Extractor Predicting: 151it [01:28,  1.66it/s]Extractor Predicting: 152it [01:29,  1.69it/s]Extractor Predicting: 153it [01:30,  1.67it/s]Extractor Predicting: 154it [01:30,  1.71it/s]Extractor Predicting: 155it [01:31,  1.74it/s]Extractor Predicting: 156it [01:31,  1.76it/s]Extractor Predicting: 157it [01:32,  1.75it/s]Extractor Predicting: 158it [01:33,  1.55it/s]Extractor Predicting: 159it [01:33,  1.63it/s]Extractor Predicting: 160it [01:34,  1.63it/s]Extractor Predicting: 161it [01:34,  1.63it/s]Extractor Predicting: 162it [01:35,  1.63it/s]Extractor Predicting: 163it [01:36,  1.65it/s]Extractor Predicting: 164it [01:36,  1.65it/s]Extractor Predicting: 165it [01:37,  1.62it/s]Extractor Predicting: 166it [01:38,  1.64it/s]Extractor Predicting: 167it [01:38,  1.64it/s]Extractor Predicting: 168it [01:39,  1.65it/s]Extractor Predicting: 169it [01:39,  1.64it/s]Extractor Predicting: 170it [01:40,  1.68it/s]Extractor Predicting: 171it [01:41,  1.67it/s]Extractor Predicting: 172it [01:41,  1.64it/s]Extractor Predicting: 173it [01:42,  1.63it/s]Extractor Predicting: 174it [01:42,  1.64it/s]Extractor Predicting: 175it [01:43,  1.64it/s]Extractor Predicting: 176it [01:44,  1.65it/s]Extractor Predicting: 177it [01:44,  1.71it/s]Extractor Predicting: 178it [01:45,  1.69it/s]Extractor Predicting: 179it [01:45,  1.70it/s]Extractor Predicting: 180it [01:46,  1.72it/s]Extractor Predicting: 181it [01:46,  1.70it/s]Extractor Predicting: 182it [01:47,  1.76it/s]Extractor Predicting: 183it [01:48,  1.76it/s]Extractor Predicting: 184it [01:48,  1.74it/s]Extractor Predicting: 185it [01:49,  1.74it/s]Extractor Predicting: 186it [01:49,  1.76it/s]Extractor Predicting: 187it [01:50,  1.71it/s]Extractor Predicting: 188it [01:50,  1.76it/s]Extractor Predicting: 189it [01:51,  1.79it/s]Extractor Predicting: 190it [01:51,  1.83it/s]Extractor Predicting: 191it [01:52,  1.70it/s]Extractor Predicting: 192it [01:53,  1.75it/s]Extractor Predicting: 193it [01:53,  1.77it/s]Extractor Predicting: 194it [01:54,  1.82it/s]Extractor Predicting: 195it [01:54,  1.78it/s]Extractor Predicting: 196it [01:55,  1.78it/s]Extractor Predicting: 197it [01:56,  1.74it/s]Extractor Predicting: 198it [01:56,  1.73it/s]Extractor Predicting: 199it [01:57,  1.71it/s]Extractor Predicting: 200it [01:57,  1.73it/s]Extractor Predicting: 201it [01:58,  1.74it/s]Extractor Predicting: 202it [01:58,  1.78it/s]Extractor Predicting: 203it [01:59,  1.75it/s]Extractor Predicting: 204it [01:59,  1.79it/s]Extractor Predicting: 205it [02:00,  1.80it/s]Extractor Predicting: 206it [02:01,  1.87it/s]Extractor Predicting: 207it [02:01,  1.84it/s]Extractor Predicting: 208it [02:02,  1.90it/s]Extractor Predicting: 209it [02:02,  1.84it/s]Extractor Predicting: 210it [02:03,  1.84it/s]Extractor Predicting: 211it [02:03,  1.84it/s]Extractor Predicting: 212it [02:04,  1.89it/s]Extractor Predicting: 213it [02:04,  1.92it/s]Extractor Predicting: 214it [02:05,  1.93it/s]Extractor Predicting: 215it [02:05,  1.92it/s]Extractor Predicting: 216it [02:06,  1.93it/s]Extractor Predicting: 217it [02:06,  1.89it/s]Extractor Predicting: 218it [02:07,  1.87it/s]Extractor Predicting: 219it [02:07,  1.90it/s]Extractor Predicting: 220it [02:08,  1.91it/s]Extractor Predicting: 221it [02:08,  1.89it/s]Extractor Predicting: 222it [02:09,  1.96it/s]Extractor Predicting: 223it [02:09,  1.91it/s]Extractor Predicting: 224it [02:10,  1.94it/s]Extractor Predicting: 225it [02:11,  1.93it/s]Extractor Predicting: 226it [02:11,  1.94it/s]Extractor Predicting: 227it [02:11,  1.98it/s]Extractor Predicting: 228it [02:12,  1.96it/s]Extractor Predicting: 229it [02:13,  1.81it/s]Extractor Predicting: 230it [02:13,  1.77it/s]Extractor Predicting: 231it [02:14,  1.69it/s]Extractor Predicting: 232it [02:15,  1.66it/s]Extractor Predicting: 233it [02:15,  1.67it/s]Extractor Predicting: 234it [02:16,  1.61it/s]Extractor Predicting: 235it [02:16,  1.63it/s]Extractor Predicting: 236it [02:17,  1.64it/s]Extractor Predicting: 237it [02:18,  1.60it/s]Extractor Predicting: 238it [02:18,  1.59it/s]Extractor Predicting: 239it [02:19,  1.56it/s]Extractor Predicting: 240it [02:20,  1.57it/s]Extractor Predicting: 241it [02:20,  1.61it/s]Extractor Predicting: 242it [02:21,  1.64it/s]Extractor Predicting: 243it [02:21,  1.59it/s]Extractor Predicting: 244it [02:22,  1.60it/s]Extractor Predicting: 245it [02:23,  1.61it/s]Extractor Predicting: 246it [02:23,  1.64it/s]Extractor Predicting: 247it [02:24,  1.59it/s]Extractor Predicting: 248it [02:25,  1.60it/s]Extractor Predicting: 249it [02:25,  1.52it/s]Extractor Predicting: 250it [02:26,  1.55it/s]Extractor Predicting: 251it [02:27,  1.57it/s]Extractor Predicting: 252it [02:27,  1.59it/s]Extractor Predicting: 253it [02:28,  1.61it/s]Extractor Predicting: 254it [02:28,  1.55it/s]Extractor Predicting: 255it [02:29,  1.56it/s]Extractor Predicting: 256it [02:30,  1.59it/s]Extractor Predicting: 257it [02:30,  1.66it/s]Extractor Predicting: 258it [02:31,  1.69it/s]Extractor Predicting: 259it [02:31,  1.68it/s]Extractor Predicting: 260it [02:32,  1.72it/s]Extractor Predicting: 261it [02:32,  1.72it/s]Extractor Predicting: 262it [02:33,  1.73it/s]Extractor Predicting: 263it [02:34,  1.75it/s]Extractor Predicting: 264it [02:34,  1.77it/s]Extractor Predicting: 265it [02:35,  1.71it/s]Extractor Predicting: 266it [02:35,  1.75it/s]Extractor Predicting: 267it [02:36,  1.78it/s]Extractor Predicting: 268it [02:36,  1.75it/s]Extractor Predicting: 269it [02:37,  1.76it/s]Extractor Predicting: 270it [02:38,  1.75it/s]Extractor Predicting: 271it [02:38,  1.70it/s]Extractor Predicting: 272it [02:39,  1.71it/s]Extractor Predicting: 273it [02:39,  1.71it/s]Extractor Predicting: 274it [02:40,  1.70it/s]Extractor Predicting: 275it [02:41,  1.74it/s]Extractor Predicting: 276it [02:41,  1.75it/s]Extractor Predicting: 277it [02:42,  1.73it/s]Extractor Predicting: 278it [02:42,  1.69it/s]Extractor Predicting: 279it [02:43,  1.67it/s]Extractor Predicting: 280it [02:44,  1.68it/s]Extractor Predicting: 281it [02:44,  1.71it/s]Extractor Predicting: 282it [02:45,  1.71it/s]Extractor Predicting: 283it [02:45,  1.72it/s]Extractor Predicting: 284it [02:46,  1.74it/s]Extractor Predicting: 285it [02:46,  1.68it/s]Extractor Predicting: 286it [02:47,  1.69it/s]Extractor Predicting: 287it [02:48,  1.72it/s]Extractor Predicting: 288it [02:48,  1.73it/s]Extractor Predicting: 289it [02:49,  1.50it/s]Extractor Predicting: 290it [02:50,  1.54it/s]Extractor Predicting: 291it [02:50,  1.58it/s]Extractor Predicting: 292it [02:51,  1.62it/s]Extractor Predicting: 293it [02:51,  1.63it/s]Extractor Predicting: 294it [02:52,  1.66it/s]Extractor Predicting: 295it [02:53,  1.68it/s]Extractor Predicting: 296it [02:53,  1.62it/s]Extractor Predicting: 297it [02:54,  1.66it/s]Extractor Predicting: 298it [02:54,  1.64it/s]Extractor Predicting: 299it [02:55,  1.70it/s]Extractor Predicting: 300it [02:56,  1.70it/s]Extractor Predicting: 301it [02:56,  1.71it/s]Extractor Predicting: 302it [02:57,  1.67it/s]Extractor Predicting: 303it [02:57,  1.73it/s]Extractor Predicting: 304it [02:58,  1.73it/s]Extractor Predicting: 305it [02:58,  1.77it/s]Extractor Predicting: 306it [02:59,  1.74it/s]Extractor Predicting: 307it [03:00,  1.72it/s]Extractor Predicting: 308it [03:00,  1.65it/s]Extractor Predicting: 309it [03:01,  1.69it/s]Extractor Predicting: 310it [03:01,  1.65it/s]Extractor Predicting: 311it [03:02,  1.66it/s]Extractor Predicting: 312it [03:03,  1.67it/s]Extractor Predicting: 313it [03:03,  1.68it/s]Extractor Predicting: 314it [03:04,  1.68it/s]Extractor Predicting: 315it [03:04,  1.72it/s]Extractor Predicting: 316it [03:05,  1.72it/s]Extractor Predicting: 317it [03:06,  1.71it/s]Extractor Predicting: 318it [03:06,  1.77it/s]Extractor Predicting: 319it [03:07,  1.73it/s]Extractor Predicting: 320it [03:07,  1.78it/s]Extractor Predicting: 321it [03:08,  1.75it/s]Extractor Predicting: 322it [03:08,  1.74it/s]Extractor Predicting: 323it [03:09,  1.73it/s]Extractor Predicting: 324it [03:10,  1.71it/s]Extractor Predicting: 325it [03:10,  1.65it/s]Extractor Predicting: 326it [03:11,  1.67it/s]Extractor Predicting: 327it [03:11,  1.66it/s]Extractor Predicting: 328it [03:12,  1.67it/s]Extractor Predicting: 329it [03:13,  1.62it/s]Extractor Predicting: 330it [03:13,  1.64it/s]Extractor Predicting: 331it [03:14,  1.62it/s]Extractor Predicting: 332it [03:14,  1.68it/s]Extractor Predicting: 333it [03:15,  1.69it/s]Extractor Predicting: 334it [03:16,  1.70it/s]Extractor Predicting: 335it [03:16,  1.69it/s]Extractor Predicting: 336it [03:17,  1.70it/s]Extractor Predicting: 337it [03:17,  1.66it/s]Extractor Predicting: 338it [03:18,  1.68it/s]Extractor Predicting: 339it [03:19,  1.66it/s]Extractor Predicting: 340it [03:19,  1.72it/s]Extractor Predicting: 341it [03:20,  1.69it/s]Extractor Predicting: 342it [03:20,  1.73it/s]Extractor Predicting: 343it [03:21,  1.63it/s]Extractor Predicting: 344it [03:22,  1.67it/s]Extractor Predicting: 345it [03:22,  1.65it/s]Extractor Predicting: 346it [03:23,  1.70it/s]Extractor Predicting: 347it [03:23,  1.70it/s]Extractor Predicting: 348it [03:24,  1.69it/s]Extractor Predicting: 349it [03:25,  1.63it/s]Extractor Predicting: 350it [03:25,  1.63it/s]Extractor Predicting: 351it [03:26,  1.62it/s]Extractor Predicting: 352it [03:26,  1.65it/s]Extractor Predicting: 353it [03:27,  1.61it/s]Extractor Predicting: 354it [03:28,  1.63it/s]Extractor Predicting: 355it [03:28,  1.62it/s]Extractor Predicting: 356it [03:29,  1.67it/s]Extractor Predicting: 357it [03:29,  1.65it/s]Extractor Predicting: 358it [03:30,  1.62it/s]Extractor Predicting: 359it [03:31,  1.61it/s]Extractor Predicting: 360it [03:31,  1.59it/s]Extractor Predicting: 361it [03:32,  1.64it/s]Extractor Predicting: 362it [03:33,  1.68it/s]Extractor Predicting: 363it [03:33,  1.65it/s]Extractor Predicting: 364it [03:34,  1.69it/s]Extractor Predicting: 365it [03:34,  1.71it/s]Extractor Predicting: 366it [03:35,  1.68it/s]Extractor Predicting: 367it [03:35,  1.68it/s]Extractor Predicting: 368it [03:36,  1.67it/s]Extractor Predicting: 369it [03:37,  1.65it/s]Extractor Predicting: 370it [03:37,  1.65it/s]Extractor Predicting: 371it [03:38,  1.68it/s]Extractor Predicting: 372it [03:38,  1.70it/s]Extractor Predicting: 373it [03:39,  1.73it/s]Extractor Predicting: 374it [03:40,  1.73it/s]Extractor Predicting: 375it [03:40,  1.69it/s]Extractor Predicting: 376it [03:41,  1.72it/s]Extractor Predicting: 377it [03:41,  1.73it/s]Extractor Predicting: 378it [03:42,  1.75it/s]Extractor Predicting: 379it [03:42,  1.76it/s]Extractor Predicting: 380it [03:43,  1.76it/s]Extractor Predicting: 381it [03:44,  1.76it/s]Extractor Predicting: 382it [03:44,  1.73it/s]Extractor Predicting: 383it [03:45,  1.72it/s]Extractor Predicting: 384it [03:45,  1.74it/s]Extractor Predicting: 385it [03:46,  1.75it/s]Extractor Predicting: 386it [03:46,  1.79it/s]Extractor Predicting: 387it [03:47,  1.81it/s]Extractor Predicting: 388it [03:48,  1.77it/s]Extractor Predicting: 389it [03:48,  1.73it/s]Extractor Predicting: 390it [03:49,  1.51it/s]Extractor Predicting: 391it [03:50,  1.58it/s]Extractor Predicting: 392it [03:50,  1.61it/s]Extractor Predicting: 393it [03:51,  1.66it/s]Extractor Predicting: 394it [03:51,  1.67it/s]Extractor Predicting: 395it [03:52,  1.64it/s]Extractor Predicting: 396it [03:53,  1.69it/s]Extractor Predicting: 397it [03:53,  1.73it/s]Extractor Predicting: 398it [03:54,  1.70it/s]Extractor Predicting: 399it [03:54,  1.71it/s]Extractor Predicting: 400it [03:55,  1.64it/s]Extractor Predicting: 401it [03:55,  1.68it/s]Extractor Predicting: 402it [03:56,  1.74it/s]Extractor Predicting: 403it [03:57,  1.73it/s]Extractor Predicting: 404it [03:57,  1.76it/s]Extractor Predicting: 405it [03:58,  1.76it/s]Extractor Predicting: 406it [03:58,  1.70it/s]Extractor Predicting: 407it [03:59,  1.70it/s]Extractor Predicting: 408it [03:59,  1.75it/s]Extractor Predicting: 409it [04:00,  1.78it/s]Extractor Predicting: 410it [04:01,  1.78it/s]Extractor Predicting: 411it [04:01,  1.75it/s]Extractor Predicting: 412it [04:02,  1.68it/s]Extractor Predicting: 413it [04:02,  1.71it/s]Extractor Predicting: 414it [04:03,  1.73it/s]Extractor Predicting: 415it [04:03,  1.78it/s]Extractor Predicting: 416it [04:04,  1.79it/s]Extractor Predicting: 417it [04:05,  1.77it/s]Extractor Predicting: 418it [04:05,  1.73it/s]Extractor Predicting: 419it [04:06,  1.74it/s]Extractor Predicting: 420it [04:06,  1.74it/s]Extractor Predicting: 421it [04:07,  1.75it/s]Extractor Predicting: 422it [04:07,  1.74it/s]Extractor Predicting: 423it [04:08,  1.74it/s]Extractor Predicting: 424it [04:09,  1.71it/s]Extractor Predicting: 425it [04:09,  1.95it/s]Extractor Predicting: 425it [04:09,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:29,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:29,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:29,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:29,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:29,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:13:30,556 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:13:30,557 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:13:31,198 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:13:32,267 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:13:32,267 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:35,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:35,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:35,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:35,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:35,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:13:36,421 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:13:36,422 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:13:37,026 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:13:37,198 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:13:37,198 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.86it/s]Extractor Predicting: 7it [00:04,  1.65it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:13:43,819 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:13:43,821 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:13:43,888 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:13:43,889 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:13:43,913 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:13:55,498 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:13:55,498 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:13:55,702 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:13:55,735 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:13:55,852 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:13:55,946 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:13:56,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:57,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:57,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:58,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:59,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:13:59,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:00,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:01,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:01,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:02,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:03,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:04,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:04,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:05,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:06,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:06,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:07,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:08,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:08,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:09,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:10,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:10,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:11,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:12,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:16<05:15, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:13,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:13,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:14,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:15,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:15,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:16,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:16,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:17,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:18,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:19,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:19,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:20,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:20,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:21,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:22,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:22,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:23,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:24,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:25,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:25,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:26,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:27,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:27,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:28,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:29,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:29,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:30,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:31,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:35<05:23, 18.00s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:31,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:32,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:33,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:33,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:35,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:35,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:36,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:36,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:37,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:38,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:39,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:39,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:40,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:41,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:42,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:42,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:43,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:44,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:44,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:45,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:46,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:46,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:47,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:48,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:52<04:56, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 14:14:48,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:49,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:50,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:51,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:52,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:53,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:53,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:54,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:55,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:56,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:57,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:57,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:58,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:59,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:14:59,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:00,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:01,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:01,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:02,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:03,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:03,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:04,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:05,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:09<04:35, 17.23s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:05,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:06,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:06,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:07,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:08,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:08,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:09,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:10,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:10,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:11,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:12,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:12,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:13,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:14,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:14,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:15,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:15,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:16,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:17,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:17,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:18,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:18,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:19,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:20,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:20,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:21,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:22,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:22,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:24,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:24,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:29<04:33, 18.20s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:25,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:26,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:26,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:27,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:28,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:28,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:29,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:29,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:30,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:31,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:31,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:32,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:33,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:33,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:34,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:35,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:35,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:36,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:37,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:37,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:38,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:39,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:39,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:44<03:59, 17.12s/it][WARNING|generation_utils.py:914] 2023-08-28 14:15:40,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:41,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:41,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:42,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:43,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:43,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:44,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:45,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:46,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:47,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:48,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:48,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:49,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:50,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:50,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:51,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:52,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:53,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:54,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:54,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:55,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:56,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:56,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:57,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:58,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:58,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:59,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:15:59,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:04<03:54, 18.07s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:00,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:01,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:01,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:02,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:03,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:04,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:04,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:05,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:06,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:06,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:07,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:08,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:08,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:09,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:10,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:10,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:11,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:12,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:12,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:13,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:13,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:14,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:15,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:16,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:16,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:17,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:22<03:35, 17.99s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:18,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:19,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:19,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:20,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:21,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:21,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:22,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:23,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:24,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:25,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:25,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:26,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:27,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:28,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:28,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:29,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:30,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:30,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:31,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:32,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:33,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:34,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:34,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:35,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:36,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:41<03:21, 18.34s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:37,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:38,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:38,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:39,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:40,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:40,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:41,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:42,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:42,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:43,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:44,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:44,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:45,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:45,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:46,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:47,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:48,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:48,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:49,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:50,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:51,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:52,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:53,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:53,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:57<02:58, 17.87s/it][WARNING|generation_utils.py:914] 2023-08-28 14:16:54,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:55,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:55,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:56,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:57,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:58,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:16:59,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:00,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:00,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:01,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:02,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:02,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:03,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:04,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:04,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:05,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:06,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:06,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:07,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:08,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:08,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:09,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:10,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:10,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:15<02:38, 17.66s/it][WARNING|generation_utils.py:914] 2023-08-28 14:17:11,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:12,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:13,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:13,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:14,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:15,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:16,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:16,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:17,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:18,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:18,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:19,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:20,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:21,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:22,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:22,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:23,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:24,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:25,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:25,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:26,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:27,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:27,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:28,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:33<02:22, 17.78s/it][WARNING|generation_utils.py:914] 2023-08-28 14:17:29,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:30,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:30,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:31,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:32,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:32,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:33,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:34,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:34,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:35,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:36,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:36,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:37,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:38,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:38,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:39,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:39,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:40,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:40,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:41,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:42,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:43,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:43,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:44,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:44,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:49<02:00, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 14:17:45,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:46,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:46,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:47,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:48,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:49,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:49,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:50,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:51,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:52,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:53,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:53,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:54,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:55,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:55,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:56,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:57,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:58,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:58,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:17:59,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:00,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:01,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:01,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:02,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:03,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:04,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:04,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:09<01:48, 18.06s/it][WARNING|generation_utils.py:914] 2023-08-28 14:18:05,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:06,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:06,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:07,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:08,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:09,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:09,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:10,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:11,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:11,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:12,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:13,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:13,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:14,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:15,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:16,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:16,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:17,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:18,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:18,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:20,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:20,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:21,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:22,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:26<01:29, 17.93s/it][WARNING|generation_utils.py:914] 2023-08-28 14:18:23,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:23,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:24,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:25,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:26,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:27,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:27,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:28,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:29,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:30,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:31,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:32,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:33,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:33,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:34,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:35,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:36,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:37,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:37,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:38,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:38,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:39,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:40,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:41,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:41,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:42,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:43,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:47<01:15, 18.77s/it][WARNING|generation_utils.py:914] 2023-08-28 14:18:43,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:44,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:45,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:45,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:46,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:47,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:47,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:48,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:49,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:49,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:50,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:50,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:51,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:52,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:53,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:53,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:54,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:55,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:55,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:56,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:57,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:58,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:58,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:59,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:03<00:53, 17.94s/it][WARNING|generation_utils.py:914] 2023-08-28 14:18:59,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:00,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:01,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:02,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:02,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:03,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:04,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:05,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:05,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:06,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:06,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:07,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:08,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:08,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:09,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:10,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:11,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:11,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:12,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:13,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:13,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:14,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:15,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:19<00:34, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-28 14:19:15,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:16,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:17,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:18,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:19,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:19,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:20,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:21,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:21,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:22,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:23,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:23,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:24,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:25,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:25,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:26,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:27,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:27,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:28,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:29,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:29,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:30,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:31,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:32,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:32,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:33,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:34,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:35,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:35,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:36,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:36,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:37,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:38,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:42<00:19, 19.00s/it][WARNING|generation_utils.py:914] 2023-08-28 14:19:38,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:39,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:40,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:40,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:41,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:42,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:42,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:43,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:44,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:44,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:45,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:46,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:46,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:47,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:47,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:49,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:49,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:50,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:51,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:52,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:52,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:53,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:54,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:54,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [05:58<00:00, 18.24s/it]Generating: 100%|| 20/20 [05:58<00:00, 17.94s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:04,714 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:04,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:04,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:04,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:04,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:20:05,400 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:20:05,401 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:20:05,995 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:20:07,072 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:20:07,073 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:10,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:10,250 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:10,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:10,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:20:10,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:20:10,949 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:20:10,951 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:20:11,550 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:20:11,725 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:20:11,725 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.58it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.39it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.43it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.45it/s]Extractor Estimating: 10it [00:06,  1.47it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:08,  1.51it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:10,  1.45it/s]Extractor Estimating: 17it [00:11,  1.45it/s]Extractor Estimating: 18it [00:12,  1.45it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.51it/s]Extractor Estimating: 21it [00:14,  1.47it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.48it/s]Extractor Estimating: 24it [00:16,  1.43it/s]Extractor Estimating: 25it [00:17,  1.49it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:18,  1.56it/s]Extractor Estimating: 28it [00:18,  1.54it/s]Extractor Estimating: 29it [00:19,  1.57it/s]Extractor Estimating: 30it [00:20,  1.63it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:21,  1.59it/s]Extractor Estimating: 33it [00:22,  1.52it/s]Extractor Estimating: 34it [00:22,  1.55it/s]Extractor Estimating: 35it [00:23,  1.58it/s]Extractor Estimating: 36it [00:24,  1.54it/s]Extractor Estimating: 37it [00:24,  1.54it/s]Extractor Estimating: 38it [00:25,  1.49it/s]Extractor Estimating: 39it [00:25,  1.54it/s]Extractor Estimating: 40it [00:26,  1.51it/s]Extractor Estimating: 41it [00:27,  1.49it/s]Extractor Estimating: 42it [00:27,  1.56it/s]Extractor Estimating: 43it [00:28,  1.56it/s]Extractor Estimating: 44it [00:29,  1.55it/s]Extractor Estimating: 45it [00:29,  1.59it/s]Extractor Estimating: 46it [00:30,  1.50it/s]Extractor Estimating: 47it [00:31,  1.50it/s]Extractor Estimating: 48it [00:31,  1.46it/s]Extractor Estimating: 49it [00:32,  1.48it/s]Extractor Estimating: 50it [00:33,  1.46it/s]Extractor Estimating: 51it [00:33,  1.49it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.55it/s]Extractor Estimating: 54it [00:35,  1.60it/s]Extractor Estimating: 55it [00:36,  1.59it/s]Extractor Estimating: 56it [00:37,  1.61it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:38,  1.62it/s]Extractor Estimating: 59it [00:38,  1.60it/s]Extractor Estimating: 60it [00:39,  1.57it/s]Extractor Estimating: 61it [00:40,  1.53it/s]Extractor Estimating: 62it [00:40,  1.58it/s]Extractor Estimating: 63it [00:41,  1.55it/s]Extractor Estimating: 64it [00:42,  1.54it/s]Extractor Estimating: 65it [00:42,  1.52it/s]Extractor Estimating: 66it [00:43,  1.58it/s]Extractor Estimating: 67it [00:44,  1.61it/s]Extractor Estimating: 68it [00:44,  1.62it/s]Extractor Estimating: 69it [00:45,  1.61it/s]Extractor Estimating: 70it [00:45,  1.56it/s]Extractor Estimating: 71it [00:46,  1.52it/s]Extractor Estimating: 72it [00:47,  1.53it/s]Extractor Estimating: 73it [00:47,  1.51it/s]Extractor Estimating: 74it [00:48,  1.56it/s]Extractor Estimating: 75it [00:49,  1.58it/s]Extractor Estimating: 76it [00:49,  1.61it/s]Extractor Estimating: 77it [00:50,  1.62it/s]Extractor Estimating: 78it [00:51,  1.61it/s]Extractor Estimating: 79it [00:51,  1.61it/s]Extractor Estimating: 80it [00:52,  1.44it/s]Extractor Estimating: 81it [00:53,  1.48it/s]Extractor Estimating: 82it [00:53,  1.50it/s]Extractor Estimating: 83it [00:54,  1.49it/s]Extractor Estimating: 84it [00:55,  1.49it/s]Extractor Estimating: 85it [00:55,  1.47it/s]Extractor Estimating: 86it [00:56,  1.37it/s]Extractor Estimating: 87it [00:57,  1.37it/s]Extractor Estimating: 88it [00:58,  1.43it/s]Extractor Estimating: 89it [00:58,  1.48it/s]Extractor Estimating: 90it [00:59,  1.42it/s]Extractor Estimating: 91it [01:00,  1.45it/s]Extractor Estimating: 92it [01:00,  1.50it/s]Extractor Estimating: 93it [01:01,  1.52it/s]Extractor Estimating: 94it [01:02,  1.51it/s]Extractor Estimating: 95it [01:02,  1.55it/s]Extractor Estimating: 96it [01:03,  1.50it/s]Extractor Estimating: 97it [01:04,  1.49it/s]Extractor Estimating: 98it [01:04,  1.55it/s]Extractor Estimating: 99it [01:05,  1.52it/s]Extractor Estimating: 100it [01:05,  1.53it/s]Extractor Estimating: 101it [01:06,  1.53it/s]Extractor Estimating: 102it [01:07,  1.49it/s]Extractor Estimating: 103it [01:07,  1.50it/s]Extractor Estimating: 104it [01:08,  1.49it/s]Extractor Estimating: 105it [01:09,  1.47it/s]Extractor Estimating: 106it [01:09,  1.50it/s]Extractor Estimating: 107it [01:10,  1.52it/s]Extractor Estimating: 108it [01:11,  1.57it/s]Extractor Estimating: 109it [01:11,  1.59it/s]Extractor Estimating: 110it [01:12,  1.53it/s]Extractor Estimating: 111it [01:13,  1.56it/s]Extractor Estimating: 112it [01:13,  1.56it/s]Extractor Estimating: 113it [01:14,  1.59it/s]Extractor Estimating: 114it [01:15,  1.59it/s]Extractor Estimating: 115it [01:15,  1.59it/s]Extractor Estimating: 116it [01:16,  1.61it/s]Extractor Estimating: 117it [01:16,  1.63it/s]Extractor Estimating: 118it [01:17,  1.62it/s]Extractor Estimating: 119it [01:18,  1.62it/s]Extractor Estimating: 120it [01:18,  1.57it/s]Extractor Estimating: 121it [01:19,  1.57it/s]Extractor Estimating: 122it [01:20,  1.58it/s]Extractor Estimating: 123it [01:20,  1.57it/s]Extractor Estimating: 124it [01:21,  1.57it/s]Extractor Estimating: 125it [01:21,  1.60it/s]Extractor Estimating: 126it [01:22,  1.55it/s]Extractor Estimating: 127it [01:23,  1.55it/s]Extractor Estimating: 128it [01:23,  1.49it/s]Extractor Estimating: 129it [01:24,  1.51it/s]Extractor Estimating: 130it [01:25,  1.48it/s]Extractor Estimating: 131it [01:25,  1.50it/s]Extractor Estimating: 132it [01:26,  1.47it/s]Extractor Estimating: 133it [01:27,  1.48it/s]Extractor Estimating: 134it [01:28,  1.46it/s]Extractor Estimating: 135it [01:28,  1.46it/s]Extractor Estimating: 136it [01:29,  1.45it/s]Extractor Estimating: 137it [01:30,  1.47it/s]Extractor Estimating: 138it [01:30,  1.44it/s]Extractor Estimating: 139it [01:31,  1.42it/s]Extractor Estimating: 140it [01:32,  1.45it/s]Extractor Estimating: 141it [01:32,  1.48it/s]Extractor Estimating: 142it [01:33,  1.51it/s]Extractor Estimating: 143it [01:34,  1.53it/s]Extractor Estimating: 144it [01:34,  1.49it/s]Extractor Estimating: 145it [01:35,  1.46it/s]Extractor Estimating: 146it [01:36,  1.42it/s]Extractor Estimating: 147it [01:36,  1.45it/s]Extractor Estimating: 148it [01:37,  1.42it/s]Extractor Estimating: 149it [01:38,  1.47it/s]Extractor Estimating: 150it [01:38,  1.49it/s]Extractor Estimating: 151it [01:39,  1.54it/s]Extractor Estimating: 152it [01:40,  1.58it/s]Extractor Estimating: 153it [01:40,  1.61it/s]Extractor Estimating: 154it [01:41,  1.61it/s]Extractor Estimating: 155it [01:41,  1.61it/s]Extractor Estimating: 156it [01:42,  1.64it/s]Extractor Estimating: 157it [01:43,  1.67it/s]Extractor Estimating: 158it [01:43,  1.65it/s]Extractor Estimating: 159it [01:44,  1.71it/s]Extractor Estimating: 160it [01:44,  1.69it/s]Extractor Estimating: 161it [01:45,  1.65it/s]Extractor Estimating: 162it [01:46,  1.58it/s]Extractor Estimating: 163it [01:46,  1.54it/s]Extractor Estimating: 164it [01:47,  1.59it/s]Extractor Estimating: 165it [01:48,  1.59it/s]Extractor Estimating: 166it [01:49,  1.41it/s]Extractor Estimating: 167it [01:49,  1.44it/s]Extractor Estimating: 168it [01:50,  1.48it/s]Extractor Estimating: 169it [01:50,  1.51it/s]Extractor Estimating: 170it [01:51,  1.50it/s]Extractor Estimating: 171it [01:52,  1.51it/s]Extractor Estimating: 172it [01:52,  1.54it/s]Extractor Estimating: 173it [01:53,  1.60it/s]Extractor Estimating: 174it [01:54,  1.61it/s]Extractor Estimating: 175it [01:54,  1.54it/s]Extractor Estimating: 176it [01:55,  1.56it/s]Extractor Estimating: 177it [01:56,  1.56it/s]Extractor Estimating: 178it [01:56,  1.55it/s]Extractor Estimating: 179it [01:57,  1.47it/s]Extractor Estimating: 180it [01:58,  1.48it/s]Extractor Estimating: 181it [01:58,  1.54it/s]Extractor Estimating: 182it [01:59,  1.57it/s]Extractor Estimating: 183it [02:00,  1.55it/s]Extractor Estimating: 184it [02:00,  1.53it/s]Extractor Estimating: 185it [02:01,  1.57it/s]Extractor Estimating: 186it [02:01,  1.57it/s]Extractor Estimating: 187it [02:02,  1.61it/s]Extractor Estimating: 188it [02:03,  1.55it/s]Extractor Estimating: 189it [02:03,  1.48it/s]Extractor Estimating: 190it [02:04,  1.51it/s]Extractor Estimating: 191it [02:05,  1.56it/s]Extractor Estimating: 192it [02:05,  1.54it/s]Extractor Estimating: 193it [02:06,  1.61it/s]Extractor Estimating: 194it [02:07,  1.60it/s]Extractor Estimating: 195it [02:07,  1.59it/s]Extractor Estimating: 196it [02:08,  1.51it/s]Extractor Estimating: 197it [02:08,  1.57it/s]Extractor Estimating: 198it [02:09,  1.61it/s]Extractor Estimating: 199it [02:10,  1.63it/s]Extractor Estimating: 200it [02:10,  1.65it/s]Extractor Estimating: 201it [02:11,  1.66it/s]Extractor Estimating: 202it [02:12,  1.61it/s]Extractor Estimating: 203it [02:12,  1.60it/s]Extractor Estimating: 204it [02:13,  1.58it/s]Extractor Estimating: 205it [02:13,  1.54it/s]Extractor Estimating: 206it [02:14,  1.52it/s]Extractor Estimating: 207it [02:15,  1.43it/s]Extractor Estimating: 208it [02:16,  1.42it/s]Extractor Estimating: 209it [02:16,  1.48it/s]Extractor Estimating: 210it [02:17,  1.50it/s]Extractor Estimating: 211it [02:18,  1.42it/s]Extractor Estimating: 212it [02:18,  1.50it/s]Extractor Estimating: 213it [02:19,  1.45it/s]Extractor Estimating: 214it [02:20,  1.45it/s]Extractor Estimating: 215it [02:20,  1.48it/s]Extractor Estimating: 216it [02:21,  1.55it/s]Extractor Estimating: 217it [02:22,  1.60it/s]Extractor Estimating: 218it [02:22,  1.54it/s]Extractor Estimating: 219it [02:23,  1.52it/s]Extractor Estimating: 220it [02:24,  1.48it/s]Extractor Estimating: 221it [02:24,  1.52it/s]Extractor Estimating: 222it [02:25,  1.52it/s]Extractor Estimating: 223it [02:26,  1.55it/s]Extractor Estimating: 224it [02:26,  1.56it/s]Extractor Estimating: 225it [02:27,  1.50it/s]Extractor Estimating: 226it [02:28,  1.51it/s]Extractor Estimating: 227it [02:28,  1.50it/s]Extractor Estimating: 228it [02:29,  1.51it/s]Extractor Estimating: 229it [02:30,  1.51it/s]Extractor Estimating: 230it [02:30,  1.56it/s]Extractor Estimating: 231it [02:31,  1.59it/s]Extractor Estimating: 232it [02:31,  1.57it/s]Extractor Estimating: 233it [02:32,  1.56it/s]Extractor Estimating: 234it [02:33,  1.51it/s]Extractor Estimating: 235it [02:33,  1.55it/s]Extractor Estimating: 236it [02:34,  1.56it/s]Extractor Estimating: 237it [02:35,  1.58it/s]Extractor Estimating: 238it [02:35,  1.58it/s]Extractor Estimating: 239it [02:36,  1.55it/s]Extractor Estimating: 240it [02:37,  1.50it/s]Extractor Estimating: 241it [02:37,  1.51it/s]Extractor Estimating: 242it [02:38,  1.52it/s]Extractor Estimating: 243it [02:39,  1.45it/s]Extractor Estimating: 244it [02:39,  1.43it/s]Extractor Estimating: 245it [02:40,  1.43it/s]Extractor Estimating: 246it [02:41,  1.50it/s]Extractor Estimating: 247it [02:41,  1.46it/s]Extractor Estimating: 248it [02:42,  1.43it/s]Extractor Estimating: 249it [02:43,  1.32it/s]Extractor Estimating: 250it [02:44,  1.36it/s]Extractor Estimating: 251it [02:44,  1.46it/s]Extractor Estimating: 252it [02:45,  1.52it/s]Extractor Estimating: 253it [02:46,  1.44it/s]Extractor Estimating: 254it [02:46,  1.43it/s]Extractor Estimating: 255it [02:47,  1.46it/s]Extractor Estimating: 256it [02:48,  1.50it/s]Extractor Estimating: 257it [02:48,  1.50it/s]Extractor Estimating: 258it [02:49,  1.53it/s]Extractor Estimating: 259it [02:50,  1.53it/s]Extractor Estimating: 260it [02:50,  1.57it/s]Extractor Estimating: 261it [02:51,  1.52it/s]Extractor Estimating: 262it [02:51,  1.56it/s]Extractor Estimating: 263it [02:52,  1.50it/s]Extractor Estimating: 264it [02:53,  1.51it/s]Extractor Estimating: 265it [02:54,  1.52it/s]Extractor Estimating: 266it [02:54,  1.53it/s]Extractor Estimating: 267it [02:55,  1.50it/s]Extractor Estimating: 268it [02:56,  1.50it/s]Extractor Estimating: 269it [02:56,  1.47it/s]Extractor Estimating: 270it [02:57,  1.43it/s]Extractor Estimating: 271it [02:58,  1.44it/s]Extractor Estimating: 272it [02:58,  1.48it/s]Extractor Estimating: 273it [02:59,  1.47it/s]Extractor Estimating: 274it [03:00,  1.40it/s]Extractor Estimating: 275it [03:00,  1.45it/s]Extractor Estimating: 276it [03:01,  1.49it/s]Extractor Estimating: 277it [03:02,  1.45it/s]Extractor Estimating: 278it [03:02,  1.48it/s]Extractor Estimating: 279it [03:03,  1.46it/s]Extractor Estimating: 280it [03:04,  1.42it/s]Extractor Estimating: 281it [03:05,  1.38it/s]Extractor Estimating: 282it [03:05,  1.39it/s]Extractor Estimating: 283it [03:06,  1.45it/s]Extractor Estimating: 284it [03:07,  1.48it/s]Extractor Estimating: 285it [03:07,  1.47it/s]Extractor Estimating: 286it [03:08,  1.50it/s]Extractor Estimating: 287it [03:09,  1.52it/s]Extractor Estimating: 288it [03:09,  1.54it/s]Extractor Estimating: 289it [03:10,  1.55it/s]Extractor Estimating: 290it [03:11,  1.48it/s]Extractor Estimating: 291it [03:11,  1.49it/s]Extractor Estimating: 292it [03:12,  1.48it/s]Extractor Estimating: 293it [03:13,  1.52it/s]Extractor Estimating: 294it [03:13,  1.48it/s]Extractor Estimating: 295it [03:14,  1.52it/s]Extractor Estimating: 296it [03:15,  1.49it/s]Extractor Estimating: 297it [03:15,  1.51it/s]Extractor Estimating: 298it [03:16,  1.50it/s]Extractor Estimating: 299it [03:17,  1.51it/s]Extractor Estimating: 300it [03:17,  1.51it/s]Extractor Estimating: 301it [03:18,  1.49it/s]Extractor Estimating: 302it [03:19,  1.51it/s]Extractor Estimating: 303it [03:19,  1.47it/s]Extractor Estimating: 304it [03:20,  1.48it/s]Extractor Estimating: 305it [03:21,  1.44it/s]Extractor Estimating: 306it [03:21,  1.49it/s]Extractor Estimating: 307it [03:22,  1.49it/s]Extractor Estimating: 308it [03:23,  1.46it/s]Extractor Estimating: 309it [03:23,  1.51it/s]Extractor Estimating: 310it [03:24,  1.49it/s]Extractor Estimating: 311it [03:25,  1.53it/s]Extractor Estimating: 312it [03:25,  1.52it/s]Extractor Estimating: 313it [03:26,  1.50it/s]Extractor Estimating: 314it [03:27,  1.54it/s]Extractor Estimating: 315it [03:27,  1.57it/s]Extractor Estimating: 316it [03:28,  1.54it/s]Extractor Estimating: 317it [03:28,  1.57it/s]Extractor Estimating: 318it [03:29,  1.54it/s]Extractor Estimating: 319it [03:30,  1.58it/s]Extractor Estimating: 320it [03:30,  1.58it/s]Extractor Estimating: 321it [03:31,  1.49it/s]Extractor Estimating: 322it [03:32,  1.53it/s]Extractor Estimating: 323it [03:32,  1.58it/s]Extractor Estimating: 324it [03:33,  1.42it/s]Extractor Estimating: 325it [03:34,  1.46it/s]Extractor Estimating: 326it [03:34,  1.53it/s]Extractor Estimating: 327it [03:35,  1.56it/s]Extractor Estimating: 328it [03:36,  1.57it/s]Extractor Estimating: 329it [03:36,  1.59it/s]Extractor Estimating: 330it [03:37,  1.58it/s]Extractor Estimating: 331it [03:38,  1.55it/s]Extractor Estimating: 332it [03:38,  1.51it/s]Extractor Estimating: 333it [03:39,  1.56it/s]Extractor Estimating: 334it [03:40,  1.55it/s]Extractor Estimating: 335it [03:40,  1.58it/s]Extractor Estimating: 336it [03:41,  1.60it/s]Extractor Estimating: 337it [03:41,  1.65it/s]Extractor Estimating: 338it [03:42,  1.64it/s]Extractor Estimating: 339it [03:43,  1.51it/s]Extractor Estimating: 340it [03:43,  1.56it/s]Extractor Estimating: 341it [03:44,  1.64it/s]Extractor Estimating: 342it [03:44,  1.63it/s]Extractor Estimating: 343it [03:45,  1.66it/s]Extractor Estimating: 344it [03:46,  1.62it/s]Extractor Estimating: 345it [03:46,  1.66it/s]Extractor Estimating: 346it [03:47,  1.59it/s]Extractor Estimating: 347it [03:48,  1.56it/s]Extractor Estimating: 348it [03:48,  1.57it/s]Extractor Estimating: 349it [03:49,  1.49it/s]Extractor Estimating: 350it [03:50,  1.50it/s]Extractor Estimating: 351it [03:50,  1.50it/s]Extractor Estimating: 352it [03:51,  1.52it/s]Extractor Estimating: 353it [03:52,  1.56it/s]Extractor Estimating: 354it [03:52,  1.51it/s]Extractor Estimating: 355it [03:53,  1.53it/s]Extractor Estimating: 356it [03:53,  1.59it/s]Extractor Estimating: 357it [03:54,  1.57it/s]Extractor Estimating: 358it [03:55,  1.55it/s]Extractor Estimating: 359it [03:55,  1.57it/s]Extractor Estimating: 360it [03:56,  1.51it/s]Extractor Estimating: 361it [03:57,  1.44it/s]Extractor Estimating: 362it [03:58,  1.49it/s]Extractor Estimating: 363it [03:58,  1.46it/s]Extractor Estimating: 364it [03:59,  1.48it/s]Extractor Estimating: 365it [04:00,  1.47it/s]Extractor Estimating: 366it [04:00,  1.47it/s]Extractor Estimating: 367it [04:01,  1.47it/s]Extractor Estimating: 368it [04:02,  1.43it/s]Extractor Estimating: 369it [04:02,  1.49it/s]Extractor Estimating: 370it [04:03,  1.46it/s]Extractor Estimating: 371it [04:04,  1.51it/s]Extractor Estimating: 372it [04:04,  1.55it/s]Extractor Estimating: 373it [04:05,  1.54it/s]Extractor Estimating: 374it [04:05,  1.60it/s]Extractor Estimating: 375it [04:06,  1.58it/s]Extractor Estimating: 376it [04:07,  1.52it/s]Extractor Estimating: 377it [04:07,  1.51it/s]Extractor Estimating: 378it [04:08,  1.50it/s]Extractor Estimating: 379it [04:09,  1.49it/s]Extractor Estimating: 380it [04:09,  1.56it/s]Extractor Estimating: 381it [04:10,  1.58it/s]Extractor Estimating: 382it [04:11,  1.53it/s]Extractor Estimating: 383it [04:11,  1.51it/s]Extractor Estimating: 384it [04:12,  1.58it/s]Extractor Estimating: 385it [04:13,  1.54it/s]Extractor Estimating: 386it [04:13,  1.48it/s]Extractor Estimating: 387it [04:14,  1.46it/s]Extractor Estimating: 388it [04:15,  1.51it/s]Extractor Estimating: 389it [04:15,  1.49it/s]Extractor Estimating: 390it [04:16,  1.56it/s]Extractor Estimating: 391it [04:17,  1.52it/s]Extractor Estimating: 392it [04:17,  1.52it/s]Extractor Estimating: 393it [04:18,  1.56it/s]Extractor Estimating: 394it [04:19,  1.55it/s]Extractor Estimating: 395it [04:19,  1.54it/s]Extractor Estimating: 396it [04:20,  1.53it/s]Extractor Estimating: 397it [04:21,  1.56it/s]Extractor Estimating: 398it [04:21,  1.56it/s]Extractor Estimating: 399it [04:22,  1.54it/s]Extractor Estimating: 400it [04:22,  1.55it/s]Extractor Estimating: 401it [04:23,  1.52it/s]Extractor Estimating: 402it [04:24,  1.42it/s]Extractor Estimating: 403it [04:25,  1.43it/s]Extractor Estimating: 404it [04:25,  1.47it/s]Extractor Estimating: 405it [04:26,  1.53it/s]Extractor Estimating: 406it [04:27,  1.50it/s]Extractor Estimating: 407it [04:27,  1.51it/s]Extractor Estimating: 408it [04:28,  1.56it/s]Extractor Estimating: 409it [04:28,  1.57it/s]Extractor Estimating: 410it [04:29,  1.56it/s]Extractor Estimating: 411it [04:30,  1.53it/s]Extractor Estimating: 412it [04:31,  1.44it/s]Extractor Estimating: 413it [04:31,  1.49it/s]Extractor Estimating: 414it [04:32,  1.52it/s]Extractor Estimating: 415it [04:32,  1.56it/s]Extractor Estimating: 416it [04:33,  1.54it/s]Extractor Estimating: 417it [04:34,  1.57it/s]Extractor Estimating: 418it [04:34,  1.53it/s]Extractor Estimating: 419it [04:35,  1.52it/s]Extractor Estimating: 420it [04:36,  1.57it/s]Extractor Estimating: 421it [04:36,  1.47it/s]Extractor Estimating: 422it [04:37,  1.44it/s]Extractor Estimating: 423it [04:38,  1.47it/s]Extractor Estimating: 424it [04:38,  1.50it/s]Extractor Estimating: 425it [04:39,  1.58it/s]Extractor Estimating: 426it [04:40,  1.53it/s]Extractor Estimating: 427it [04:40,  1.51it/s]Extractor Estimating: 428it [04:41,  1.47it/s]Extractor Estimating: 429it [04:42,  1.46it/s]Extractor Estimating: 430it [04:43,  1.41it/s]Extractor Estimating: 431it [04:43,  1.47it/s]Extractor Estimating: 432it [04:44,  1.40it/s]Extractor Estimating: 433it [04:45,  1.47it/s]Extractor Estimating: 434it [04:45,  1.48it/s]Extractor Estimating: 435it [04:46,  1.44it/s]Extractor Estimating: 436it [04:47,  1.40it/s]Extractor Estimating: 437it [04:48,  1.36it/s]Extractor Estimating: 438it [04:48,  1.43it/s]Extractor Estimating: 439it [04:49,  1.43it/s]Extractor Estimating: 440it [04:49,  1.49it/s]Extractor Estimating: 441it [04:50,  1.45it/s]Extractor Estimating: 442it [04:51,  1.46it/s]Extractor Estimating: 443it [04:51,  1.52it/s]Extractor Estimating: 444it [04:52,  1.52it/s]Extractor Estimating: 445it [04:53,  1.54it/s]Extractor Estimating: 446it [04:53,  1.54it/s]Extractor Estimating: 447it [04:54,  1.51it/s]Extractor Estimating: 448it [04:55,  1.52it/s]Extractor Estimating: 449it [04:55,  1.48it/s]Extractor Estimating: 450it [04:56,  1.50it/s]Extractor Estimating: 451it [04:57,  1.51it/s]Extractor Estimating: 452it [04:57,  1.50it/s]Extractor Estimating: 453it [04:58,  1.58it/s]Extractor Estimating: 454it [04:59,  1.58it/s]Extractor Estimating: 455it [04:59,  1.67it/s]Extractor Estimating: 456it [05:00,  1.59it/s]Extractor Estimating: 457it [05:00,  1.57it/s]Extractor Estimating: 458it [05:01,  1.61it/s]Extractor Estimating: 459it [05:02,  1.66it/s]Extractor Estimating: 460it [05:02,  1.63it/s]Extractor Estimating: 461it [05:03,  1.67it/s]Extractor Estimating: 462it [05:03,  1.62it/s]Extractor Estimating: 463it [05:04,  1.68it/s]Extractor Estimating: 464it [05:05,  1.71it/s]Extractor Estimating: 465it [05:05,  1.70it/s]Extractor Estimating: 466it [05:06,  1.71it/s]Extractor Estimating: 467it [05:06,  1.72it/s]Extractor Estimating: 468it [05:07,  1.66it/s]Extractor Estimating: 469it [05:08,  1.68it/s]Extractor Estimating: 470it [05:08,  1.68it/s]Extractor Estimating: 471it [05:09,  1.69it/s]Extractor Estimating: 472it [05:09,  1.64it/s]Extractor Estimating: 473it [05:10,  1.62it/s]Extractor Estimating: 474it [05:11,  1.65it/s]Extractor Estimating: 475it [05:11,  1.59it/s]Extractor Estimating: 476it [05:12,  1.63it/s]Extractor Estimating: 477it [05:13,  1.53it/s]Extractor Estimating: 478it [05:13,  1.54it/s]Extractor Estimating: 479it [05:14,  1.49it/s]Extractor Estimating: 480it [05:15,  1.52it/s]Extractor Estimating: 481it [05:15,  1.55it/s]Extractor Estimating: 482it [05:16,  1.55it/s]Extractor Estimating: 483it [05:17,  1.53it/s]Extractor Estimating: 484it [05:17,  1.54it/s]Extractor Estimating: 485it [05:18,  1.51it/s]Extractor Estimating: 486it [05:18,  1.55it/s]Extractor Estimating: 487it [05:19,  1.58it/s]Extractor Estimating: 488it [05:20,  1.60it/s]Extractor Estimating: 489it [05:20,  1.54it/s]Extractor Estimating: 490it [05:21,  1.45it/s]Extractor Estimating: 491it [05:22,  1.46it/s]Extractor Estimating: 492it [05:23,  1.47it/s]Extractor Estimating: 493it [05:23,  1.47it/s]Extractor Estimating: 494it [05:24,  1.48it/s]Extractor Estimating: 495it [05:24,  1.52it/s]Extractor Estimating: 496it [05:25,  1.48it/s]Extractor Estimating: 497it [05:26,  1.50it/s]Extractor Estimating: 498it [05:27,  1.48it/s]Extractor Estimating: 499it [05:27,  1.56it/s]Extractor Estimating: 500it [05:27,  1.89it/s]Extractor Estimating: 500it [05:27,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:01,131 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:01,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:01,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:01,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:01,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:26:01,513 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:26:01,514 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:26:01,799 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:26:02,888 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:26:02,888 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:04,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:04,551 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:04,552 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:04,552 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:26:04,552 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:26:04,886 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:26:04,887 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:26:05,588 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:26:05,757 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:26:05,757 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:38:03,976 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:38:04,065 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 10284 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 22495
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22595, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22595, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.132, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.146, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.136, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.135, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 71, avg_time 1.112, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 171, avg_time 2.196, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 271, avg_time 1.128, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 371, avg_time 1.153, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 1.113, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 142, avg_time 1.151, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 242, avg_time 2.188, loss:nan
g_step 1200, step 342, avg_time 1.142, loss:nan
g_step 1300, step 13, avg_time 1.123, loss:nan
g_step 1400, step 113, avg_time 1.126, loss:nan
g_step 1500, step 213, avg_time 1.138, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 313, avg_time 2.199, loss:nan
g_step 1700, step 413, avg_time 1.116, loss:nan
g_step 1800, step 84, avg_time 1.117, loss:nan
g_step 1900, step 184, avg_time 1.131, loss:nan
g_step 2000, step 284, avg_time 1.119, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 384, avg_time 2.212, loss:nan
g_step 2200, step 55, avg_time 1.101, loss:nan
g_step 2300, step 155, avg_time 1.147, loss:nan
g_step 2400, step 255, avg_time 1.142, loss:nan
g_step 2500, step 355, avg_time 1.132, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 26, avg_time 2.171, loss:nan
g_step 2700, step 126, avg_time 1.123, loss:nan
g_step 2800, step 226, avg_time 1.141, loss:nan
g_step 2900, step 326, avg_time 1.143, loss:nan
g_step 3000, step 426, avg_time 1.139, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 97, avg_time 2.193, loss:nan
g_step 3200, step 197, avg_time 1.136, loss:nan
g_step 3300, step 297, avg_time 1.124, loss:nan
g_step 3400, step 397, avg_time 1.153, loss:nan
g_step 3500, step 68, avg_time 1.129, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 168, avg_time 2.196, loss:nan
g_step 3700, step 268, avg_time 1.131, loss:nan
g_step 3800, step 368, avg_time 1.140, loss:nan
g_step 3900, step 39, avg_time 1.122, loss:nan
g_step 4000, step 139, avg_time 1.116, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 239, avg_time 2.177, loss:nan
g_step 4200, step 339, avg_time 1.173, loss:nan
g_step 4300, step 10, avg_time 1.126, loss:nan
g_step 4400, step 110, avg_time 1.105, loss:nan
g_step 4500, step 210, avg_time 1.138, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.223, loss:nan
g_step 4700, step 410, avg_time 1.120, loss:nan
g_step 4800, step 81, avg_time 1.130, loss:nan
g_step 4900, step 181, avg_time 1.145, loss:nan
g_step 5000, step 281, avg_time 1.133, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 381, avg_time 2.189, loss:nan
g_step 5200, step 52, avg_time 1.117, loss:nan
g_step 5300, step 152, avg_time 1.092, loss:nan
g_step 5400, step 252, avg_time 1.148, loss:nan
g_step 5500, step 352, avg_time 1.158, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 23, avg_time 2.181, loss:nan
g_step 5700, step 123, avg_time 1.112, loss:nan
g_step 5800, step 223, avg_time 1.152, loss:nan
g_step 5900, step 323, avg_time 1.105, loss:nan
g_step 6000, step 423, avg_time 1.152, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 94, avg_time 2.203, loss:nan
g_step 6200, step 194, avg_time 1.121, loss:nan
g_step 6300, step 294, avg_time 1.125, loss:nan
g_step 6400, step 394, avg_time 1.129, loss:nan
g_step 6500, step 65, avg_time 1.139, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 165, avg_time 2.204, loss:nan
g_step 6700, step 265, avg_time 1.126, loss:nan
g_step 6800, step 365, avg_time 1.124, loss:nan
g_step 6900, step 36, avg_time 1.122, loss:nan
g_step 7000, step 136, avg_time 1.124, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 236, avg_time 2.192, loss:nan
g_step 7200, step 336, avg_time 1.131, loss:nan
g_step 7300, step 7, avg_time 1.125, loss:nan
g_step 7400, step 107, avg_time 1.135, loss:nan
g_step 7500, step 207, avg_time 1.130, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 307, avg_time 2.209, loss:nan
g_step 7700, step 407, avg_time 1.108, loss:nan
g_step 7800, step 78, avg_time 1.120, loss:nan
g_step 7900, step 178, avg_time 1.119, loss:nan
g_step 8000, step 278, avg_time 1.133, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 378, avg_time 2.197, loss:nan
g_step 8200, step 49, avg_time 1.133, loss:nan
g_step 8300, step 149, avg_time 1.123, loss:nan
g_step 8400, step 249, avg_time 1.163, loss:nan
g_step 8500, step 349, avg_time 1.105, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:38:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:38:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-38-03_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:38:08 - WARNING - datasets.builder -   Using custom data configuration default-815d7edc6913b782
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-815d7edc6913b782/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:38:16,733 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:38:16,786 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:38:16,787 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:38:16,788 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:38:17,247 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:38:17,432 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:38:18,179 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:38:21,786 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:38:21,825 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-815d7edc6913b782/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:04,  2.07ba/s] 18%|        | 2/11 [00:00<00:02,  3.10ba/s] 27%|       | 3/11 [00:00<00:02,  3.69ba/s] 36%|      | 4/11 [00:01<00:01,  4.03ba/s] 45%|     | 5/11 [00:01<00:01,  4.22ba/s] 55%|    | 6/11 [00:01<00:01,  4.37ba/s] 64%|   | 7/11 [00:01<00:00,  4.47ba/s] 73%|  | 8/11 [00:01<00:00,  4.51ba/s] 82%| | 9/11 [00:02<00:00,  4.54ba/s] 91%| | 10/11 [00:02<00:00,  4.54ba/s]100%|| 11/11 [00:02<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.89ba/s] 50%|     | 2/4 [00:00<00:00,  3.65ba/s] 75%|  | 3/4 [00:00<00:00,  4.00ba/s]100%|| 4/4 [00:00<00:00,  5.13ba/s]100%|| 4/4 [00:00<00:00,  4.44ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:03,  2.79ba/s] 27%|       | 3/11 [00:00<00:01,  6.19ba/s] 45%|     | 5/11 [00:00<00:00,  7.91ba/s] 64%|   | 7/11 [00:01<00:00,  7.06ba/s] 82%| | 9/11 [00:01<00:00,  8.07ba/s]100%|| 11/11 [00:01<00:00,  9.79ba/s]100%|| 11/11 [00:01<00:00,  8.04ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  5.04ba/s] 75%|  | 3/4 [00:00<00:00,  8.22ba/s]100%|| 4/4 [00:00<00:00,  9.22ba/s]
[INFO|trainer.py:414] 2023-08-28 17:38:30,062 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:38:30,177 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:38:30,177 >>   Num examples = 10300
[INFO|trainer.py:1149] 2023-08-28 17:38:30,177 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:38:30,177 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:38:30,177 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:38:30,177 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:38:30,177 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:01<19:34,  1.46s/it]  0%|          | 2/805 [00:02<12:44,  1.05it/s]  0%|          | 3/805 [00:02<09:56,  1.35it/s]  0%|          | 4/805 [00:03<08:47,  1.52it/s]  1%|          | 5/805 [00:03<08:12,  1.63it/s]  1%|          | 6/805 [00:04<07:15,  1.83it/s]  1%|          | 7/805 [00:04<07:22,  1.80it/s]  1%|          | 8/805 [00:05<07:18,  1.82it/s]  1%|          | 9/805 [00:05<06:59,  1.90it/s]  1%|          | 10/805 [00:05<06:06,  2.17it/s]  1%|         | 11/805 [00:06<05:21,  2.47it/s]  1%|         | 12/805 [00:06<05:11,  2.55it/s]  2%|         | 13/805 [00:06<04:43,  2.79it/s]  2%|         | 14/805 [00:07<04:24,  2.99it/s]  2%|         | 15/805 [00:07<04:10,  3.15it/s]  2%|         | 16/805 [00:07<04:33,  2.89it/s]  2%|         | 17/805 [00:08<04:16,  3.07it/s]  2%|         | 18/805 [00:08<04:04,  3.21it/s]  2%|         | 19/805 [00:08<03:56,  3.32it/s]  2%|         | 20/805 [00:08<03:51,  3.40it/s]  3%|         | 21/805 [00:09<03:47,  3.45it/s]  3%|         | 22/805 [00:09<03:44,  3.49it/s]  3%|         | 23/805 [00:09<03:53,  3.35it/s]  3%|         | 24/805 [00:10<03:48,  3.42it/s]  3%|         | 25/805 [00:10<03:44,  3.47it/s]  3%|         | 26/805 [00:10<04:15,  3.05it/s]  3%|         | 27/805 [00:11<04:03,  3.19it/s]  3%|         | 28/805 [00:11<03:55,  3.30it/s]  4%|         | 29/805 [00:11<04:11,  3.09it/s]  4%|         | 30/805 [00:12<04:00,  3.23it/s]  4%|         | 31/805 [00:12<03:52,  3.33it/s]  4%|         | 32/805 [00:12<03:47,  3.40it/s]  4%|         | 33/805 [00:12<03:43,  3.46it/s]  4%|         | 34/805 [00:13<03:40,  3.50it/s]  4%|         | 35/805 [00:13<03:38,  3.53it/s]  4%|         | 36/805 [00:13<03:37,  3.54it/s]  5%|         | 37/805 [00:13<03:35,  3.56it/s]  5%|         | 38/805 [00:14<03:35,  3.57it/s]  5%|         | 39/805 [00:14<03:34,  3.57it/s]  5%|         | 40/805 [00:14<04:09,  3.06it/s]  5%|         | 41/805 [00:15<03:58,  3.20it/s]  5%|         | 42/805 [00:15<03:50,  3.31it/s]  5%|         | 43/805 [00:15<03:45,  3.39it/s]  5%|         | 44/805 [00:16<03:40,  3.44it/s]  6%|         | 45/805 [00:16<03:37,  3.49it/s]  6%|         | 46/805 [00:16<03:35,  3.52it/s]  6%|         | 47/805 [00:16<03:34,  3.54it/s]  6%|         | 48/805 [00:17<03:33,  3.55it/s]  6%|         | 49/805 [00:17<03:32,  3.56it/s]  6%|         | 50/805 [00:17<03:31,  3.57it/s]  6%|         | 51/805 [00:18<04:15,  2.95it/s]  6%|         | 52/805 [00:18<04:01,  3.11it/s]  7%|         | 53/805 [00:18<03:52,  3.24it/s]  7%|         | 54/805 [00:19<03:45,  3.34it/s]  7%|         | 55/805 [00:19<03:40,  3.40it/s]  7%|         | 56/805 [00:19<03:36,  3.46it/s]  7%|         | 57/805 [00:19<03:34,  3.49it/s]  7%|         | 58/805 [00:20<03:32,  3.52it/s]  7%|         | 59/805 [00:20<03:31,  3.53it/s]  7%|         | 60/805 [00:20<03:30,  3.55it/s]  8%|         | 61/805 [00:21<03:28,  3.57it/s]  8%|         | 62/805 [00:21<04:42,  2.63it/s]  8%|         | 63/805 [00:21<04:18,  2.87it/s]  8%|         | 64/805 [00:22<04:01,  3.07it/s]  8%|         | 65/805 [00:22<03:49,  3.22it/s]  8%|         | 66/805 [00:22<03:41,  3.33it/s]  8%|         | 67/805 [00:22<03:35,  3.42it/s]  8%|         | 68/805 [00:23<03:31,  3.49it/s]  9%|         | 69/805 [00:23<03:28,  3.53it/s]  9%|         | 70/805 [00:23<03:26,  3.57it/s]  9%|         | 71/805 [00:24<03:24,  3.59it/s]  9%|         | 72/805 [00:24<05:01,  2.43it/s]  9%|         | 73/805 [00:25<04:32,  2.69it/s]  9%|         | 74/805 [00:25<04:10,  2.92it/s]  9%|         | 75/805 [00:25<03:55,  3.10it/s]  9%|         | 76/805 [00:25<03:44,  3.25it/s] 10%|         | 77/805 [00:26<03:37,  3.35it/s] 10%|         | 78/805 [00:26<04:05,  2.96it/s] 10%|         | 79/805 [00:26<03:52,  3.13it/s] 10%|         | 80/805 [00:27<03:42,  3.27it/s] 10%|         | 81/805 [00:28<06:49,  1.77it/s] 10%|         | 82/805 [00:28<05:46,  2.09it/s] 10%|         | 83/805 [00:28<05:01,  2.40it/s] 10%|         | 84/805 [00:29<04:30,  2.67it/s] 11%|         | 85/805 [00:29<04:08,  2.90it/s] 11%|         | 86/805 [00:29<03:52,  3.09it/s] 11%|         | 87/805 [00:29<03:41,  3.24it/s] 11%|         | 88/805 [00:30<03:34,  3.35it/s] 11%|         | 89/805 [00:30<03:52,  3.08it/s] 11%|         | 90/805 [00:30<03:41,  3.23it/s] 11%|        | 91/805 [00:31<03:33,  3.34it/s] 11%|        | 92/805 [00:31<03:28,  3.43it/s] 12%|        | 93/805 [00:31<03:24,  3.49it/s] 12%|        | 94/805 [00:32<03:21,  3.53it/s] 12%|        | 95/805 [00:32<03:19,  3.56it/s] 12%|        | 96/805 [00:32<03:17,  3.59it/s] 12%|        | 97/805 [00:32<03:16,  3.60it/s] 12%|        | 98/805 [00:33<03:15,  3.61it/s] 12%|        | 99/805 [00:33<03:15,  3.62it/s] 12%|        | 100/805 [00:33<03:43,  3.16it/s] 13%|        | 101/805 [00:34<03:33,  3.29it/s] 13%|        | 102/805 [00:34<03:27,  3.39it/s] 13%|        | 103/805 [00:34<03:23,  3.46it/s] 13%|        | 104/805 [00:34<03:19,  3.51it/s] 13%|        | 105/805 [00:35<03:17,  3.54it/s] 13%|        | 106/805 [00:35<03:15,  3.57it/s] 13%|        | 107/805 [00:35<03:14,  3.59it/s] 13%|        | 108/805 [00:36<03:13,  3.61it/s] 14%|        | 109/805 [00:36<03:12,  3.62it/s] 14%|        | 110/805 [00:36<03:11,  3.62it/s] 14%|        | 111/805 [00:37<04:06,  2.82it/s] 14%|        | 112/805 [00:37<03:49,  3.02it/s] 14%|        | 113/805 [00:37<03:37,  3.18it/s] 14%|        | 114/805 [00:37<03:28,  3.31it/s] 14%|        | 115/805 [00:38<03:22,  3.40it/s] 14%|        | 116/805 [00:38<03:18,  3.47it/s] 15%|        | 117/805 [00:38<03:15,  3.52it/s] 15%|        | 118/805 [00:39<03:13,  3.55it/s] 15%|        | 119/805 [00:39<05:16,  2.17it/s] 15%|        | 120/805 [00:40<04:37,  2.47it/s] 15%|        | 121/805 [00:40<04:10,  2.73it/s] 15%|        | 122/805 [00:40<03:51,  2.95it/s] 15%|        | 123/805 [00:41<03:38,  3.12it/s] 15%|        | 124/805 [00:41<03:28,  3.26it/s] 16%|        | 125/805 [00:41<03:22,  3.36it/s] 16%|        | 126/805 [00:41<03:46,  2.99it/s] 16%|        | 127/805 [00:42<03:48,  2.96it/s] 16%|        | 128/805 [00:42<03:35,  3.14it/s] 16%|        | 129/805 [00:42<03:26,  3.27it/s] 16%|        | 130/805 [00:43<04:48,  2.34it/s] 16%|        | 131/805 [00:43<04:17,  2.61it/s] 16%|        | 132/805 [00:44<03:55,  2.86it/s] 17%|        | 133/805 [00:44<03:40,  3.05it/s] 17%|        | 134/805 [00:44<03:29,  3.21it/s] 17%|        | 135/805 [00:44<03:21,  3.32it/s] 17%|        | 136/805 [00:45<03:15,  3.41it/s] 17%|        | 137/805 [00:45<03:55,  2.83it/s] 17%|        | 138/805 [00:46<03:39,  3.04it/s] 17%|        | 139/805 [00:46<03:28,  3.19it/s] 17%|        | 140/805 [00:46<04:00,  2.76it/s] 18%|        | 141/805 [00:47<03:43,  2.97it/s] 18%|        | 142/805 [00:47<03:31,  3.14it/s] 18%|        | 143/805 [00:47<03:22,  3.27it/s] 18%|        | 144/805 [00:47<03:15,  3.37it/s] 18%|        | 145/805 [00:48<03:11,  3.45it/s] 18%|        | 146/805 [00:48<03:08,  3.50it/s] 18%|        | 147/805 [00:49<04:08,  2.64it/s] 18%|        | 148/805 [00:49<03:48,  2.88it/s] 19%|        | 149/805 [00:49<03:33,  3.07it/s] 19%|        | 150/805 [00:49<03:22,  3.23it/s] 19%|        | 151/805 [00:50<03:15,  3.34it/s] 19%|        | 152/805 [00:50<03:10,  3.42it/s] 19%|        | 153/805 [00:50<03:07,  3.48it/s] 19%|        | 154/805 [00:50<03:04,  3.52it/s] 19%|        | 155/805 [00:51<03:02,  3.55it/s] 19%|        | 156/805 [00:51<03:01,  3.58it/s] 20%|        | 157/805 [00:52<04:34,  2.36it/s] 20%|        | 158/805 [00:52<04:05,  2.63it/s] 20%|        | 159/805 [00:52<03:44,  2.87it/s] 20%|        | 160/805 [00:53<03:30,  3.06it/s] 20%|        | 161/805 [00:53<03:17,  3.26it/s][INFO|trainer.py:2140] 2023-08-28 17:39:23,510 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:39:23,510 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:39:23,510 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.70it/s][A
  3%|         | 12/437 [00:00<00:08, 49.62it/s][A
  4%|         | 18/437 [00:00<00:08, 47.72it/s][A
  5%|         | 23/437 [00:00<00:08, 46.78it/s][A
  6%|         | 28/437 [00:00<00:08, 46.38it/s][A
  8%|         | 33/437 [00:00<00:08, 45.61it/s][A
  9%|         | 38/437 [00:00<00:08, 45.43it/s][A
 10%|         | 43/437 [00:00<00:08, 45.31it/s][A
 11%|         | 48/437 [00:01<00:08, 45.23it/s][A
 12%|        | 53/437 [00:01<00:08, 45.43it/s][A
 13%|        | 58/437 [00:01<00:14, 25.47it/s][A
 14%|        | 63/437 [00:01<00:12, 29.45it/s][A
 16%|        | 68/437 [00:01<00:11, 32.98it/s][A
 17%|        | 73/437 [00:01<00:10, 35.97it/s][A
 18%|        | 78/437 [00:01<00:09, 38.49it/s][A
 19%|        | 83/437 [00:02<00:08, 40.40it/s][A
 20%|        | 88/437 [00:02<00:08, 41.82it/s][A
 21%|       | 93/437 [00:02<00:08, 42.85it/s][A
 22%|       | 98/437 [00:02<00:07, 43.18it/s][A
 24%|       | 103/437 [00:02<00:07, 43.61it/s][A
 25%|       | 108/437 [00:02<00:07, 44.15it/s][A
 26%|       | 113/437 [00:02<00:07, 44.61it/s][A
 27%|       | 118/437 [00:02<00:07, 44.94it/s][A
 28%|       | 123/437 [00:02<00:06, 45.18it/s][A
 29%|       | 128/437 [00:03<00:06, 45.21it/s][A
 30%|       | 133/437 [00:03<00:06, 45.32it/s][A
 32%|      | 138/437 [00:03<00:06, 45.19it/s][A
 33%|      | 143/437 [00:03<00:06, 45.03it/s][A
 34%|      | 148/437 [00:03<00:06, 45.00it/s][A
 35%|      | 153/437 [00:03<00:06, 44.96it/s][A
 36%|      | 158/437 [00:03<00:06, 45.18it/s][A
 37%|      | 163/437 [00:03<00:06, 45.38it/s][A
 38%|      | 168/437 [00:03<00:05, 45.46it/s][A
 40%|      | 173/437 [00:04<00:05, 45.54it/s][A
 41%|      | 178/437 [00:05<00:14, 18.35it/s][A
 42%|     | 182/437 [00:05<00:16, 15.76it/s][A
 43%|     | 187/437 [00:05<00:12, 19.83it/s][A
 44%|     | 192/437 [00:05<00:10, 24.04it/s][A
 45%|     | 197/437 [00:05<00:08, 28.11it/s][A
 46%|     | 202/437 [00:05<00:07, 31.86it/s][A
 47%|     | 207/437 [00:05<00:06, 35.03it/s][A
 49%|     | 212/437 [00:05<00:05, 37.72it/s][A
 50%|     | 217/437 [00:05<00:05, 39.83it/s][A
 51%|     | 222/437 [00:05<00:05, 41.04it/s][A
 52%|    | 227/437 [00:06<00:05, 41.88it/s][A
 53%|    | 232/437 [00:06<00:04, 42.83it/s][A
 54%|    | 237/437 [00:06<00:04, 43.58it/s][A
 55%|    | 242/437 [00:06<00:04, 44.22it/s][A
 57%|    | 247/437 [00:06<00:04, 44.63it/s][A
 58%|    | 252/437 [00:06<00:04, 44.93it/s][A
 59%|    | 257/437 [00:06<00:03, 45.21it/s][A
 60%|    | 262/437 [00:06<00:03, 45.30it/s][A
 61%|    | 267/437 [00:06<00:03, 45.13it/s][A
 62%|   | 272/437 [00:07<00:03, 45.13it/s][A
 63%|   | 277/437 [00:07<00:03, 45.01it/s][A
 65%|   | 282/437 [00:07<00:03, 45.08it/s][A
 66%|   | 287/437 [00:07<00:03, 45.13it/s][A
 67%|   | 292/437 [00:07<00:03, 45.33it/s][A
 68%|   | 297/437 [00:07<00:03, 45.43it/s][A
 69%|   | 302/437 [00:07<00:02, 45.51it/s][A
 70%|   | 307/437 [00:07<00:04, 32.29it/s][A
 71%|   | 311/437 [00:09<00:11, 11.15it/s][A
 72%|  | 316/437 [00:09<00:08, 14.63it/s][A
 73%|  | 321/437 [00:09<00:06, 18.53it/s][A
 75%|  | 326/437 [00:09<00:07, 15.82it/s][A
 76%|  | 332/437 [00:09<00:05, 20.68it/s][A
 77%|  | 337/437 [00:09<00:04, 24.61it/s][A
 78%|  | 342/437 [00:10<00:03, 28.49it/s][A
 79%|  | 347/437 [00:10<00:02, 32.03it/s][A
 81%|  | 352/437 [00:10<00:02, 35.17it/s][A
 82%| | 357/437 [00:10<00:02, 37.78it/s][A
 83%| | 362/437 [00:10<00:01, 39.87it/s][A
 84%| | 367/437 [00:10<00:01, 41.35it/s][A
 85%| | 372/437 [00:10<00:01, 42.22it/s][A
 86%| | 377/437 [00:11<00:01, 42.63it/s][A
 87%| | 382/437 [00:11<00:04, 12.19it/s][A
 88%| | 386/437 [00:12<00:04, 10.32it/s][A
 89%| | 391/437 [00:12<00:03, 13.70it/s][A
 91%| | 396/437 [00:12<00:02, 17.49it/s][A
 92%|| 401/437 [00:12<00:01, 21.57it/s][A
 93%|| 406/437 [00:12<00:01, 25.71it/s][A
 94%|| 411/437 [00:12<00:00, 29.62it/s][A
 95%|| 416/437 [00:13<00:00, 33.11it/s][A
 96%|| 421/437 [00:13<00:00, 36.14it/s][A
 97%|| 426/437 [00:13<00:00, 38.46it/s][A
 99%|| 431/437 [00:13<00:00, 39.94it/s][A
100%|| 436/437 [00:13<00:00, 41.30it/s][A
                                                 [A                                                 
100%|| 437/437 [00:13<00:00, 41.30it/s][A 20%|        | 161/805 [01:06<03:17,  3.26it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:39:39,245 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 17:39:39,837 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:39:48,706 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:39:49,105 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:39:49,294 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/special_tokens_map.json
 20%|        | 162/805 [01:24<1:41:09,  9.44s/it] 20%|        | 163/805 [01:24<1:13:27,  6.86s/it] 20%|        | 164/805 [01:25<52:15,  4.89s/it]   20%|        | 165/805 [01:25<37:24,  3.51s/it] 21%|        | 166/805 [01:25<27:02,  2.54s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|        | 167/805 [01:26<19:47,  1.86s/it] 21%|        | 168/805 [01:26<14:43,  1.39s/it] 21%|        | 169/805 [01:27<12:44,  1.20s/it] 21%|        | 170/805 [01:27<11:01,  1.04s/it] 21%|        | 171/805 [01:28<08:35,  1.23it/s] 21%|       | 172/805 [01:28<06:53,  1.53it/s] 21%|       | 173/805 [01:28<05:41,  1.85it/s] 22%|       | 174/805 [01:28<04:51,  2.16it/s] 22%|       | 175/805 [01:29<04:16,  2.45it/s] 22%|       | 176/805 [01:29<03:52,  2.71it/s] 22%|       | 177/805 [01:29<03:34,  2.92it/s] 22%|       | 178/805 [01:30<03:22,  3.09it/s] 22%|       | 179/805 [01:30<03:14,  3.22it/s] 22%|       | 180/805 [01:30<03:56,  2.64it/s] 22%|       | 181/805 [01:31<03:37,  2.86it/s] 23%|       | 182/805 [01:31<03:24,  3.04it/s] 23%|       | 183/805 [01:31<03:15,  3.19it/s] 23%|       | 184/805 [01:31<03:08,  3.29it/s] 23%|       | 185/805 [01:32<03:03,  3.37it/s] 23%|       | 186/805 [01:32<03:00,  3.43it/s] 23%|       | 187/805 [01:32<02:57,  3.47it/s] 23%|       | 188/805 [01:33<02:56,  3.50it/s] 23%|       | 189/805 [01:33<02:54,  3.52it/s] 24%|       | 190/805 [01:33<03:02,  3.37it/s] 24%|       | 191/805 [01:33<02:59,  3.42it/s] 24%|       | 192/805 [01:34<02:56,  3.47it/s] 24%|       | 193/805 [01:34<02:54,  3.51it/s] 24%|       | 194/805 [01:34<02:53,  3.53it/s] 24%|       | 195/805 [01:35<02:52,  3.55it/s] 24%|       | 196/805 [01:35<02:51,  3.56it/s] 24%|       | 197/805 [01:35<02:50,  3.56it/s] 25%|       | 198/805 [01:35<02:50,  3.57it/s] 25%|       | 199/805 [01:36<02:49,  3.57it/s] 25%|       | 200/805 [01:36<02:49,  3.57it/s] 25%|       | 201/805 [01:37<04:28,  2.25it/s] 25%|       | 202/805 [01:37<03:58,  2.53it/s] 25%|       | 203/805 [01:37<03:37,  2.77it/s] 25%|       | 204/805 [01:38<03:22,  2.97it/s] 25%|       | 205/805 [01:38<03:11,  3.13it/s] 26%|       | 206/805 [01:38<03:04,  3.25it/s] 26%|       | 207/805 [01:38<02:59,  3.34it/s] 26%|       | 208/805 [01:39<02:55,  3.40it/s] 26%|       | 209/805 [01:39<02:52,  3.45it/s] 26%|       | 210/805 [01:40<03:55,  2.53it/s] 26%|       | 211/805 [01:40<03:34,  2.77it/s] 26%|       | 212/805 [01:40<03:19,  2.97it/s] 26%|       | 213/805 [01:41<03:09,  3.13it/s] 27%|       | 214/805 [01:41<03:01,  3.25it/s] 27%|       | 215/805 [01:41<02:56,  3.34it/s] 27%|       | 216/805 [01:42<03:25,  2.87it/s] 27%|       | 217/805 [01:42<03:12,  3.05it/s] 27%|       | 218/805 [01:42<03:03,  3.21it/s] 27%|       | 219/805 [01:42<02:56,  3.32it/s] 27%|       | 220/805 [01:43<03:24,  2.87it/s] 27%|       | 221/805 [01:43<03:32,  2.75it/s] 28%|       | 222/805 [01:44<03:16,  2.97it/s] 28%|       | 223/805 [01:44<03:05,  3.14it/s] 28%|       | 224/805 [01:44<02:57,  3.27it/s] 28%|       | 225/805 [01:44<02:52,  3.37it/s] 28%|       | 226/805 [01:45<02:47,  3.45it/s] 28%|       | 227/805 [01:45<02:44,  3.50it/s] 28%|       | 228/805 [01:45<02:42,  3.54it/s] 28%|       | 229/805 [01:45<02:41,  3.58it/s] 29%|       | 230/805 [01:46<02:39,  3.60it/s] 29%|       | 231/805 [01:46<02:39,  3.61it/s] 29%|       | 232/805 [01:46<02:56,  3.25it/s] 29%|       | 233/805 [01:47<02:50,  3.35it/s] 29%|       | 234/805 [01:47<02:46,  3.43it/s] 29%|       | 235/805 [01:47<02:43,  3.49it/s] 29%|       | 236/805 [01:47<02:41,  3.53it/s] 29%|       | 237/805 [01:48<02:39,  3.56it/s] 30%|       | 238/805 [01:48<02:38,  3.58it/s] 30%|       | 239/805 [01:48<02:37,  3.60it/s] 30%|       | 240/805 [01:49<02:36,  3.61it/s] 30%|       | 241/805 [01:49<02:35,  3.62it/s] 30%|       | 242/805 [01:49<02:35,  3.62it/s] 30%|       | 243/805 [01:50<03:01,  3.10it/s] 30%|       | 244/805 [01:50<02:53,  3.24it/s] 30%|       | 245/805 [01:50<02:47,  3.35it/s] 31%|       | 246/805 [01:50<02:43,  3.42it/s] 31%|       | 247/805 [01:51<02:40,  3.48it/s] 31%|       | 248/805 [01:51<02:37,  3.53it/s] 31%|       | 249/805 [01:51<02:36,  3.56it/s] 31%|       | 250/805 [01:51<02:35,  3.58it/s] 31%|       | 251/805 [01:52<02:34,  3.59it/s] 31%|      | 252/805 [01:52<02:33,  3.61it/s] 31%|      | 253/805 [01:52<02:32,  3.62it/s] 32%|      | 254/805 [01:53<03:11,  2.88it/s] 32%|      | 255/805 [01:53<02:59,  3.07it/s] 32%|      | 256/805 [01:53<02:50,  3.22it/s] 32%|      | 257/805 [01:54<02:44,  3.33it/s] 32%|      | 258/805 [01:54<02:40,  3.42it/s] 32%|      | 259/805 [01:54<02:37,  3.48it/s] 32%|      | 260/805 [01:54<02:34,  3.52it/s] 32%|      | 261/805 [01:55<02:32,  3.56it/s] 33%|      | 262/805 [01:55<02:31,  3.58it/s] 33%|      | 263/805 [01:55<02:30,  3.59it/s] 33%|      | 264/805 [01:56<02:29,  3.61it/s] 33%|      | 265/805 [01:56<02:44,  3.28it/s] 33%|      | 266/805 [01:56<02:39,  3.38it/s] 33%|      | 267/805 [01:56<02:35,  3.46it/s] 33%|      | 268/805 [01:57<02:33,  3.51it/s] 33%|      | 269/805 [01:57<02:31,  3.54it/s] 34%|      | 270/805 [01:57<02:29,  3.57it/s] 34%|      | 271/805 [01:58<02:28,  3.59it/s] 34%|      | 272/805 [01:58<02:27,  3.60it/s] 34%|      | 273/805 [01:58<02:27,  3.61it/s] 34%|      | 274/805 [01:58<02:26,  3.61it/s] 34%|      | 275/805 [01:59<02:26,  3.62it/s] 34%|      | 276/805 [01:59<03:07,  2.82it/s] 34%|      | 277/805 [02:00<02:54,  3.02it/s] 35%|      | 278/805 [02:00<02:45,  3.18it/s] 35%|      | 279/805 [02:00<02:39,  3.30it/s] 35%|      | 280/805 [02:00<02:34,  3.40it/s] 35%|      | 281/805 [02:01<02:31,  3.47it/s] 35%|      | 282/805 [02:01<03:46,  2.31it/s] 35%|      | 283/805 [02:02<03:21,  2.59it/s] 35%|      | 284/805 [02:02<03:03,  2.84it/s] 35%|      | 285/805 [02:02<03:35,  2.41it/s] 36%|      | 286/805 [02:03<03:14,  2.67it/s] 36%|      | 287/805 [02:03<02:58,  2.90it/s] 36%|      | 288/805 [02:03<02:47,  3.09it/s] 36%|      | 289/805 [02:04<02:39,  3.23it/s] 36%|      | 290/805 [02:04<02:34,  3.34it/s] 36%|      | 291/805 [02:04<02:30,  3.42it/s] 36%|      | 292/805 [02:04<02:27,  3.48it/s] 36%|      | 293/805 [02:05<02:25,  3.53it/s] 37%|      | 294/805 [02:05<02:23,  3.56it/s] 37%|      | 295/805 [02:05<02:32,  3.35it/s] 37%|      | 296/805 [02:06<02:28,  3.43it/s] 37%|      | 297/805 [02:06<02:25,  3.49it/s] 37%|      | 298/805 [02:06<02:23,  3.53it/s] 37%|      | 299/805 [02:06<02:22,  3.56it/s] 37%|      | 300/805 [02:07<02:21,  3.58it/s] 37%|      | 301/805 [02:07<02:20,  3.60it/s] 38%|      | 302/805 [02:07<02:19,  3.61it/s] 38%|      | 303/805 [02:08<02:18,  3.62it/s] 38%|      | 304/805 [02:08<02:18,  3.62it/s] 38%|      | 305/805 [02:08<02:17,  3.63it/s] 38%|      | 306/805 [02:08<02:36,  3.19it/s] 38%|      | 307/805 [02:09<02:30,  3.31it/s] 38%|      | 308/805 [02:09<02:26,  3.40it/s] 38%|      | 309/805 [02:09<02:23,  3.47it/s] 39%|      | 310/805 [02:10<02:20,  3.52it/s] 39%|      | 311/805 [02:10<02:19,  3.55it/s] 39%|      | 312/805 [02:10<02:17,  3.58it/s] 39%|      | 313/805 [02:10<02:16,  3.59it/s] 39%|      | 314/805 [02:11<02:16,  3.61it/s] 39%|      | 315/805 [02:11<03:31,  2.32it/s] 39%|      | 316/805 [02:12<03:08,  2.60it/s] 39%|      | 317/805 [02:12<02:51,  2.85it/s] 40%|      | 318/805 [02:12<02:39,  3.04it/s] 40%|      | 319/805 [02:13<02:31,  3.20it/s] 40%|      | 320/805 [02:13<02:26,  3.32it/s] 40%|      | 321/805 [02:13<02:22,  3.41it/s] 40%|      | 322/805 [02:13<02:17,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 17:40:44,049 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:40:44,049 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:40:44,049 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 13.5654, 'eval_samples_per_second': 257.198, 'eval_steps_per_second': 32.214, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.15it/s][A
  3%|         | 12/437 [00:00<00:08, 49.73it/s][A
  4%|         | 18/437 [00:00<00:14, 29.57it/s][A
  5%|         | 23/437 [00:00<00:12, 33.78it/s][A
  6%|         | 28/437 [00:00<00:11, 36.99it/s][A
  8%|         | 33/437 [00:00<00:10, 39.34it/s][A
  9%|         | 38/437 [00:00<00:09, 41.14it/s][A
 10%|         | 43/437 [00:01<00:09, 42.45it/s][A
 11%|         | 48/437 [00:01<00:08, 43.32it/s][A
 12%|        | 53/437 [00:01<00:08, 43.73it/s][A
 13%|        | 58/437 [00:01<00:08, 43.89it/s][A
 14%|        | 63/437 [00:01<00:08, 44.18it/s][A
 16%|        | 68/437 [00:01<00:08, 44.55it/s][A
 17%|        | 73/437 [00:01<00:08, 44.84it/s][A
 18%|        | 78/437 [00:01<00:07, 44.99it/s][A
 19%|        | 83/437 [00:01<00:07, 45.20it/s][A
 20%|        | 88/437 [00:02<00:07, 45.33it/s][A
 21%|       | 93/437 [00:02<00:07, 45.33it/s][A
 22%|       | 98/437 [00:02<00:07, 45.28it/s][A
 24%|       | 103/437 [00:02<00:07, 45.13it/s][A
 25%|       | 108/437 [00:02<00:07, 45.10it/s][A
 26%|       | 113/437 [00:02<00:07, 45.10it/s][A
 27%|       | 118/437 [00:02<00:07, 45.21it/s][A
 28%|       | 123/437 [00:02<00:06, 45.33it/s][A
 29%|       | 128/437 [00:02<00:06, 45.42it/s][A
 30%|       | 133/437 [00:03<00:06, 45.26it/s][A
 32%|      | 138/437 [00:03<00:06, 45.40it/s][A
 33%|      | 143/437 [00:03<00:06, 45.39it/s][A
 34%|      | 148/437 [00:03<00:09, 28.92it/s][A
 35%|      | 153/437 [00:03<00:08, 32.51it/s][A
 36%|      | 158/437 [00:03<00:07, 35.58it/s][A
 37%|      | 163/437 [00:03<00:07, 38.01it/s][A
 38%|      | 168/437 [00:04<00:06, 40.04it/s][A
 40%|      | 173/437 [00:04<00:06, 41.62it/s][A
 41%|      | 178/437 [00:04<00:06, 42.59it/s][A
 42%|     | 183/437 [00:04<00:05, 43.30it/s][A
 43%|     | 188/437 [00:04<00:05, 43.54it/s][A
 44%|     | 193/437 [00:04<00:05, 43.81it/s][A
 45%|     | 198/437 [00:04<00:05, 44.20it/s][A
 46%|     | 203/437 [00:04<00:05, 44.32it/s][A
 48%|     | 208/437 [00:04<00:05, 44.54it/s][A
 49%|     | 213/437 [00:05<00:05, 44.68it/s][A
 50%|     | 218/437 [00:05<00:04, 44.86it/s][A
 51%|     | 223/437 [00:05<00:04, 44.97it/s][A
 52%|    | 228/437 [00:05<00:04, 45.10it/s][A
 53%|    | 233/437 [00:05<00:04, 44.99it/s][A
 54%|    | 238/437 [00:05<00:04, 45.04it/s][A
 56%|    | 243/437 [00:05<00:04, 44.96it/s][A
 57%|    | 248/437 [00:05<00:04, 45.06it/s][A
 58%|    | 253/437 [00:05<00:04, 45.22it/s][A
 59%|    | 258/437 [00:06<00:03, 45.35it/s][A
 60%|    | 263/437 [00:06<00:03, 45.34it/s][A
 61%|   | 268/437 [00:06<00:03, 45.41it/s][A
 62%|   | 273/437 [00:07<00:03, 45.43it/s][A
 64%|   | 278/437 [00:07<00:10, 14.52it/s][A
 65%|   | 283/437 [00:07<00:08, 18.26it/s][A
 66%|   | 288/437 [00:07<00:06, 22.25it/s][A
 67%|   | 293/437 [00:07<00:05, 26.32it/s][A
 68%|   | 298/437 [00:07<00:04, 30.14it/s][A
 69%|   | 303/437 [00:07<00:03, 33.55it/s][A
 70%|   | 308/437 [00:07<00:03, 36.40it/s][A
 72%|  | 313/437 [00:08<00:03, 38.75it/s][A
 73%|  | 318/437 [00:08<00:02, 40.23it/s][A
 74%|  | 323/437 [00:08<00:02, 41.44it/s][A
 75%|  | 328/437 [00:08<00:02, 42.48it/s][A
 76%|  | 333/437 [00:08<00:02, 43.39it/s][A
 77%|  | 338/437 [00:08<00:02, 43.98it/s][A
 78%|  | 343/437 [00:08<00:02, 44.54it/s][A
 80%|  | 348/437 [00:08<00:01, 44.83it/s][A
 81%|  | 353/437 [00:08<00:01, 44.99it/s][A
 82%| | 358/437 [00:09<00:01, 45.10it/s][A
 83%| | 363/437 [00:09<00:01, 45.08it/s][A
 84%| | 368/437 [00:09<00:01, 45.00it/s][A
 85%| | 373/437 [00:09<00:01, 45.01it/s][A
 86%| | 378/437 [00:09<00:01, 45.05it/s][A
 88%| | 383/437 [00:10<00:02, 20.94it/s][A
 89%| | 388/437 [00:10<00:01, 24.99it/s][A
 90%| | 393/437 [00:10<00:01, 28.95it/s][A
 91%| | 398/437 [00:10<00:01, 32.47it/s][A
 92%|| 403/437 [00:10<00:00, 35.50it/s][A
 93%|| 408/437 [00:10<00:00, 38.10it/s][A
 95%|| 413/437 [00:10<00:00, 40.11it/s][A
 96%|| 418/437 [00:11<00:01, 11.05it/s][A
 97%|| 423/437 [00:11<00:00, 14.41it/s][A
 98%|| 428/437 [00:12<00:00, 18.14it/s][A
 99%|| 433/437 [00:12<00:00, 22.13it/s][A
                                                 [A                                                 
100%|| 437/437 [00:12<00:00, 22.13it/s][A 40%|      | 322/805 [02:26<02:17,  3.51it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:40:59,142 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 17:41:02,283 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:41:24,281 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:41:25,395 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:41:26,673 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/special_tokens_map.json
 40%|      | 323/805 [03:00<1:53:23, 14.12s/it] 40%|      | 324/805 [03:00<1:20:36, 10.06s/it] 40%|      | 325/805 [03:01<56:58,  7.12s/it]   40%|      | 326/805 [03:01<41:08,  5.15s/it] 41%|      | 327/805 [03:01<29:24,  3.69s/it] 41%|      | 328/805 [03:02<21:12,  2.67s/it] 41%|      | 329/805 [03:02<15:28,  1.95s/it] 41%|      | 330/805 [03:02<11:28,  1.45s/it] 41%|      | 331/805 [03:03<08:40,  1.10s/it] 41%|      | 332/805 [03:03<06:43,  1.17it/s] 41%|     | 333/805 [03:04<06:31,  1.21it/s] 41%|     | 334/805 [03:04<05:12,  1.51it/s] 42%|     | 335/805 [03:04<04:17,  1.82it/s] 42%|     | 336/805 [03:04<03:39,  2.14it/s] 42%|     | 337/805 [03:05<03:12,  2.43it/s] 42%|     | 338/805 [03:05<02:53,  2.69it/s] 42%|     | 339/805 [03:05<02:40,  2.91it/s] 42%|     | 340/805 [03:06<02:30,  3.08it/s] 42%|     | 341/805 [03:06<02:24,  3.22it/s] 42%|     | 342/805 [03:06<02:27,  3.14it/s] 43%|     | 343/805 [03:06<02:21,  3.26it/s] 43%|     | 344/805 [03:07<02:17,  3.35it/s] 43%|     | 345/805 [03:07<02:14,  3.42it/s] 43%|     | 346/805 [03:07<02:12,  3.47it/s] 43%|     | 347/805 [03:08<02:10,  3.51it/s] 43%|     | 348/805 [03:08<02:09,  3.53it/s] 43%|     | 349/805 [03:08<02:08,  3.55it/s] 43%|     | 350/805 [03:08<02:07,  3.56it/s] 44%|     | 351/805 [03:09<02:07,  3.56it/s] 44%|     | 352/805 [03:09<02:06,  3.57it/s] 44%|     | 353/805 [03:09<02:12,  3.41it/s] 44%|     | 354/805 [03:10<02:10,  3.46it/s] 44%|     | 355/805 [03:10<02:08,  3.50it/s] 44%|     | 356/805 [03:10<02:07,  3.52it/s] 44%|     | 357/805 [03:10<02:06,  3.54it/s] 44%|     | 358/805 [03:11<02:05,  3.55it/s] 45%|     | 359/805 [03:11<02:05,  3.56it/s] 45%|     | 360/805 [03:11<02:04,  3.57it/s] 45%|     | 361/805 [03:12<02:04,  3.57it/s] 45%|     | 362/805 [03:12<02:04,  3.57it/s] 45%|     | 363/805 [03:12<02:03,  3.57it/s] 45%|     | 364/805 [03:12<02:03,  3.57it/s] 45%|     | 365/805 [03:13<02:03,  3.58it/s] 45%|     | 366/805 [03:13<02:02,  3.58it/s] 46%|     | 367/805 [03:13<02:02,  3.57it/s] 46%|     | 368/805 [03:13<02:02,  3.58it/s] 46%|     | 369/805 [03:14<02:01,  3.57it/s] 46%|     | 370/805 [03:14<02:01,  3.57it/s] 46%|     | 371/805 [03:14<02:01,  3.58it/s] 46%|     | 372/805 [03:15<02:01,  3.57it/s] 46%|     | 373/805 [03:15<02:00,  3.57it/s] 46%|     | 374/805 [03:15<02:00,  3.57it/s] 47%|     | 375/805 [03:16<02:06,  3.40it/s] 47%|     | 376/805 [03:16<02:04,  3.45it/s] 47%|     | 377/805 [03:16<02:02,  3.49it/s] 47%|     | 378/805 [03:16<02:01,  3.52it/s] 47%|     | 379/805 [03:17<02:00,  3.54it/s] 47%|     | 380/805 [03:17<01:59,  3.55it/s] 47%|     | 381/805 [03:17<01:59,  3.56it/s] 47%|     | 382/805 [03:17<01:58,  3.56it/s] 48%|     | 383/805 [03:18<01:58,  3.57it/s] 48%|     | 384/805 [03:18<01:58,  3.57it/s] 48%|     | 385/805 [03:18<01:57,  3.57it/s] 48%|     | 386/805 [03:19<02:05,  3.34it/s] 48%|     | 387/805 [03:19<02:02,  3.41it/s] 48%|     | 388/805 [03:19<02:00,  3.45it/s] 48%|     | 389/805 [03:19<01:59,  3.49it/s] 48%|     | 390/805 [03:20<01:58,  3.52it/s] 49%|     | 391/805 [03:20<01:56,  3.54it/s] 49%|     | 392/805 [03:20<01:56,  3.55it/s] 49%|     | 393/805 [03:21<01:55,  3.56it/s] 49%|     | 394/805 [03:21<01:55,  3.57it/s] 49%|     | 395/805 [03:21<01:54,  3.57it/s] 49%|     | 396/805 [03:21<01:54,  3.57it/s] 49%|     | 397/805 [03:22<02:16,  2.99it/s] 49%|     | 398/805 [03:22<02:09,  3.15it/s] 50%|     | 399/805 [03:22<02:04,  3.26it/s] 50%|     | 400/805 [03:23<02:00,  3.35it/s] 50%|     | 401/805 [03:23<01:58,  3.41it/s] 50%|     | 402/805 [03:23<01:56,  3.46it/s] 50%|     | 403/805 [03:24<01:55,  3.49it/s] 50%|     | 404/805 [03:24<01:53,  3.52it/s] 50%|     | 405/805 [03:24<01:53,  3.53it/s] 50%|     | 406/805 [03:24<01:52,  3.54it/s] 51%|     | 407/805 [03:26<03:46,  1.75it/s] 51%|     | 408/805 [03:26<03:35,  1.84it/s] 51%|     | 409/805 [03:26<03:02,  2.17it/s] 51%|     | 410/805 [03:27<03:00,  2.19it/s] 51%|     | 411/805 [03:27<02:38,  2.48it/s] 51%|     | 412/805 [03:27<02:23,  2.74it/s] 51%|    | 413/805 [03:28<02:12,  2.96it/s] 51%|    | 414/805 [03:28<02:04,  3.14it/s] 52%|    | 415/805 [03:28<01:59,  3.27it/s] 52%|    | 416/805 [03:29<01:55,  3.37it/s] 52%|    | 417/805 [03:29<01:52,  3.45it/s] 52%|    | 418/805 [03:29<02:16,  2.84it/s] 52%|    | 419/805 [03:30<02:06,  3.04it/s] 52%|    | 420/805 [03:30<02:00,  3.20it/s] 52%|    | 421/805 [03:30<01:55,  3.32it/s] 52%|    | 422/805 [03:30<01:52,  3.41it/s] 53%|    | 423/805 [03:31<01:50,  3.47it/s] 53%|    | 424/805 [03:31<01:48,  3.52it/s] 53%|    | 425/805 [03:31<01:46,  3.55it/s] 53%|    | 426/805 [03:31<01:45,  3.58it/s] 53%|    | 427/805 [03:32<01:45,  3.59it/s] 53%|    | 428/805 [03:32<01:44,  3.60it/s] 53%|    | 429/805 [03:33<02:08,  2.93it/s] 53%|    | 430/805 [03:33<02:00,  3.11it/s] 54%|    | 431/805 [03:33<01:55,  3.25it/s] 54%|    | 432/805 [03:33<01:51,  3.36it/s] 54%|    | 433/805 [03:34<01:48,  3.44it/s] 54%|    | 434/805 [03:34<01:46,  3.49it/s] 54%|    | 435/805 [03:34<01:44,  3.53it/s] 54%|    | 436/805 [03:34<01:43,  3.56it/s] 54%|    | 437/805 [03:35<01:42,  3.59it/s] 54%|    | 438/805 [03:35<01:41,  3.60it/s] 55%|    | 439/805 [03:35<01:41,  3.62it/s] 55%|    | 440/805 [03:36<02:41,  2.26it/s] 55%|    | 441/805 [03:36<02:22,  2.55it/s] 55%|    | 442/805 [03:37<02:09,  2.80it/s] 55%|    | 443/805 [03:37<02:00,  3.00it/s] 55%|    | 444/805 [03:37<01:53,  3.17it/s] 55%|    | 445/805 [03:37<01:49,  3.30it/s] 55%|    | 446/805 [03:38<01:45,  3.39it/s] 56%|    | 447/805 [03:38<01:43,  3.46it/s] 56%|    | 448/805 [03:38<01:41,  3.51it/s] 56%|    | 449/805 [03:39<02:05,  2.84it/s] 56%|    | 450/805 [03:39<01:56,  3.04it/s] 56%|    | 451/805 [03:39<01:50,  3.20it/s] 56%|    | 452/805 [03:40<01:46,  3.32it/s] 56%|    | 453/805 [03:40<01:43,  3.40it/s] 56%|    | 454/805 [03:40<01:41,  3.47it/s] 57%|    | 455/805 [03:40<01:39,  3.52it/s] 57%|    | 456/805 [03:41<01:38,  3.55it/s] 57%|    | 457/805 [03:41<01:37,  3.57it/s] 57%|    | 458/805 [03:41<01:36,  3.59it/s] 57%|    | 459/805 [03:42<01:36,  3.60it/s] 57%|    | 460/805 [03:42<01:50,  3.12it/s] 57%|    | 461/805 [03:42<01:45,  3.26it/s] 57%|    | 462/805 [03:43<01:41,  3.36it/s] 58%|    | 463/805 [03:43<01:39,  3.44it/s] 58%|    | 464/805 [03:43<01:53,  3.01it/s] 58%|    | 465/805 [03:44<01:47,  3.17it/s] 58%|    | 466/805 [03:44<01:42,  3.30it/s] 58%|    | 467/805 [03:44<01:39,  3.39it/s] 58%|    | 468/805 [03:44<01:37,  3.46it/s] 58%|    | 469/805 [03:45<01:35,  3.52it/s] 58%|    | 470/805 [03:45<01:34,  3.55it/s] 59%|    | 471/805 [03:45<01:33,  3.57it/s] 59%|    | 472/805 [03:45<01:32,  3.59it/s] 59%|    | 473/805 [03:46<01:32,  3.60it/s] 59%|    | 474/805 [03:46<01:44,  3.18it/s] 59%|    | 475/805 [03:46<01:39,  3.30it/s] 59%|    | 476/805 [03:47<02:31,  2.17it/s] 59%|    | 477/805 [03:47<02:13,  2.46it/s] 59%|    | 478/805 [03:48<01:59,  2.73it/s] 60%|    | 479/805 [03:48<01:50,  2.95it/s] 60%|    | 480/805 [03:48<01:44,  3.12it/s] 60%|    | 481/805 [03:49<01:39,  3.26it/s] 60%|    | 482/805 [03:49<01:41,  3.19it/s] 60%|    | 483/805 [03:49<01:47,  2.99it/s][INFO|trainer.py:2140] 2023-08-28 17:42:19,990 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:42:19,990 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:42:19,990 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 12.3096, 'eval_samples_per_second': 283.438, 'eval_steps_per_second': 35.501, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.68it/s][A
  3%|         | 12/437 [00:00<00:08, 49.71it/s][A
  4%|         | 18/437 [00:00<00:08, 47.86it/s][A
  5%|         | 23/437 [00:00<00:08, 47.12it/s][A
  6%|         | 28/437 [00:00<00:08, 46.48it/s][A
  8%|         | 33/437 [00:00<00:08, 45.89it/s][A
  9%|         | 38/437 [00:00<00:08, 45.48it/s][A
 10%|         | 43/437 [00:00<00:08, 45.11it/s][A
 11%|         | 48/437 [00:01<00:08, 45.17it/s][A
 12%|        | 53/437 [00:01<00:08, 45.12it/s][A
 13%|        | 58/437 [00:01<00:08, 45.37it/s][A
 14%|        | 63/437 [00:01<00:08, 45.48it/s][A
 16%|        | 68/437 [00:01<00:08, 45.54it/s][A
 17%|        | 73/437 [00:01<00:07, 45.56it/s][A
 18%|        | 78/437 [00:01<00:07, 45.40it/s][A
 19%|        | 83/437 [00:01<00:07, 45.17it/s][A
 20%|        | 88/437 [00:01<00:07, 45.09it/s][A
 21%|       | 93/437 [00:02<00:07, 45.06it/s][A
 22%|       | 98/437 [00:02<00:07, 45.12it/s][A
 24%|       | 103/437 [00:02<00:07, 45.19it/s][A
 25%|       | 108/437 [00:02<00:07, 45.35it/s][A
 26%|       | 113/437 [00:02<00:07, 45.43it/s][A
 27%|       | 118/437 [00:02<00:07, 45.37it/s][A
 28%|       | 123/437 [00:02<00:06, 45.28it/s][A
 29%|       | 128/437 [00:02<00:08, 34.94it/s][A
 30%|       | 133/437 [00:03<00:08, 37.70it/s][A
 32%|      | 138/437 [00:03<00:08, 34.01it/s][A
 33%|      | 143/437 [00:03<00:07, 36.86it/s][A
 34%|      | 148/437 [00:03<00:07, 39.09it/s][A
 35%|      | 153/437 [00:03<00:06, 40.91it/s][A
 36%|      | 158/437 [00:03<00:06, 42.21it/s][A
 37%|      | 163/437 [00:03<00:06, 43.21it/s][A
 38%|      | 168/437 [00:03<00:06, 43.94it/s][A
 40%|      | 173/437 [00:03<00:05, 44.39it/s][A
 41%|      | 178/437 [00:04<00:05, 44.43it/s][A
 42%|     | 183/437 [00:04<00:05, 44.34it/s][A
 43%|     | 188/437 [00:04<00:05, 44.32it/s][A
 44%|     | 193/437 [00:04<00:05, 44.71it/s][A
 45%|     | 198/437 [00:04<00:05, 44.96it/s][A
 46%|     | 203/437 [00:04<00:05, 45.17it/s][A
 48%|     | 208/437 [00:04<00:05, 45.33it/s][A
 49%|     | 213/437 [00:04<00:04, 45.40it/s][A
 50%|     | 218/437 [00:04<00:04, 45.40it/s][A
 51%|     | 223/437 [00:05<00:04, 45.05it/s][A
 52%|    | 228/437 [00:05<00:04, 44.89it/s][A
 53%|    | 233/437 [00:05<00:04, 44.90it/s][A
 54%|    | 238/437 [00:05<00:04, 45.05it/s][A
 56%|    | 243/437 [00:05<00:04, 45.18it/s][A
 57%|    | 248/437 [00:05<00:04, 45.21it/s][A
 58%|    | 253/437 [00:05<00:04, 45.42it/s][A
 59%|    | 258/437 [00:05<00:04, 42.52it/s][A
 60%|    | 263/437 [00:05<00:04, 43.45it/s][A
 61%|   | 268/437 [00:06<00:03, 43.94it/s][A
 62%|   | 273/437 [00:06<00:03, 44.14it/s][A
 64%|   | 278/437 [00:06<00:03, 44.33it/s][A
 65%|   | 283/437 [00:06<00:03, 44.62it/s][A
 66%|   | 288/437 [00:06<00:03, 44.89it/s][A
 67%|   | 293/437 [00:06<00:03, 45.08it/s][A
 68%|   | 298/437 [00:06<00:03, 44.84it/s][A
 69%|   | 303/437 [00:06<00:02, 45.07it/s][A
 70%|   | 308/437 [00:06<00:02, 45.26it/s][A
 72%|  | 313/437 [00:07<00:02, 45.25it/s][A
 73%|  | 318/437 [00:07<00:02, 45.19it/s][A
 74%|  | 323/437 [00:07<00:02, 45.19it/s][A
 75%|  | 328/437 [00:07<00:02, 45.15it/s][A
 76%|  | 333/437 [00:07<00:02, 45.23it/s][A
 77%|  | 338/437 [00:07<00:02, 45.30it/s][A
 78%|  | 343/437 [00:07<00:02, 45.24it/s][A
 80%|  | 348/437 [00:07<00:01, 45.28it/s][A
 81%|  | 353/437 [00:07<00:01, 45.31it/s][A
 82%| | 358/437 [00:08<00:01, 45.19it/s][A
 83%| | 363/437 [00:08<00:01, 45.22it/s][A
 84%| | 368/437 [00:08<00:01, 42.72it/s][A
 85%| | 373/437 [00:08<00:01, 43.64it/s][A
 86%| | 378/437 [00:08<00:01, 44.26it/s][A
 88%| | 383/437 [00:08<00:01, 44.52it/s][A
 89%| | 388/437 [00:08<00:01, 44.79it/s][A
 90%| | 393/437 [00:08<00:01, 42.66it/s][A
 91%| | 398/437 [00:09<00:00, 43.55it/s][A
 92%|| 403/437 [00:09<00:00, 44.17it/s][A
 93%|| 408/437 [00:09<00:00, 44.23it/s][A
 95%|| 413/437 [00:09<00:00, 44.53it/s][A
 96%|| 418/437 [00:09<00:00, 44.90it/s][A
 97%|| 423/437 [00:09<00:00, 45.08it/s][A
 98%|| 428/437 [00:09<00:00, 45.12it/s][A
 99%|| 433/437 [00:09<00:00, 44.80it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.80it/s][A 60%|    | 483/805 [03:59<01:47,  2.99it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:42:30,523 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 17:42:31,138 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:42:37,055 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:42:37,734 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:42:38,350 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/special_tokens_map.json
 60%|    | 484/805 [04:11<36:40,  6.86s/it] 60%|    | 485/805 [04:12<26:11,  4.91s/it] 60%|    | 486/805 [04:12<18:43,  3.52s/it] 60%|    | 487/805 [04:12<13:30,  2.55s/it] 61%|    | 488/805 [04:13<09:52,  1.87s/it] 61%|    | 489/805 [04:13<07:19,  1.39s/it] 61%|    | 490/805 [04:13<05:33,  1.06s/it] 61%|    | 491/805 [04:13<04:19,  1.21it/s] 61%|    | 492/805 [04:14<03:41,  1.41it/s] 61%|    | 493/805 [04:14<03:00,  1.73it/s] 61%|   | 494/805 [04:14<02:32,  2.04it/s] 61%|   | 495/805 [04:15<02:12,  2.35it/s] 62%|   | 496/805 [04:15<01:58,  2.62it/s] 62%|   | 497/805 [04:15<01:48,  2.85it/s] 62%|   | 498/805 [04:16<01:41,  3.03it/s] 62%|   | 499/805 [04:16<01:36,  3.18it/s] 62%|   | 500/805 [04:16<01:32,  3.29it/s]                                                  62%|   | 500/805 [04:16<01:32,  3.29it/s] 62%|   | 501/805 [04:16<01:30,  3.37it/s] 62%|   | 502/805 [04:17<01:28,  3.43it/s] 62%|   | 503/805 [04:17<01:55,  2.60it/s] 63%|   | 504/805 [04:18<01:45,  2.85it/s] 63%|   | 505/805 [04:18<01:38,  3.04it/s] 63%|   | 506/805 [04:18<01:33,  3.20it/s] 63%|   | 507/805 [04:18<01:29,  3.32it/s] 63%|   | 508/805 [04:19<01:27,  3.41it/s] 63%|   | 509/805 [04:19<01:25,  3.47it/s] 63%|   | 510/805 [04:19<01:23,  3.52it/s] 63%|   | 511/805 [04:19<01:22,  3.55it/s] 64%|   | 512/805 [04:20<01:21,  3.58it/s] 64%|   | 513/805 [04:20<01:47,  2.71it/s] 64%|   | 514/805 [04:21<01:39,  2.93it/s] 64%|   | 515/805 [04:21<01:33,  3.12it/s] 64%|   | 516/805 [04:21<01:28,  3.26it/s] 64%|   | 517/805 [04:21<01:25,  3.36it/s] 64%|   | 518/805 [04:22<01:23,  3.44it/s] 64%|   | 519/805 [04:22<01:21,  3.49it/s] 65%|   | 520/805 [04:22<01:20,  3.53it/s] 65%|   | 521/805 [04:23<01:19,  3.56it/s] 65%|   | 522/805 [04:23<01:18,  3.59it/s] 65%|   | 523/805 [04:24<02:01,  2.32it/s] 65%|   | 524/805 [04:24<01:48,  2.60it/s] 65%|   | 525/805 [04:24<01:38,  2.84it/s] 65%|   | 526/805 [04:24<01:31,  3.04it/s] 65%|   | 527/805 [04:25<01:26,  3.20it/s] 66%|   | 528/805 [04:25<01:23,  3.32it/s] 66%|   | 529/805 [04:25<01:21,  3.40it/s] 66%|   | 530/805 [04:26<01:56,  2.37it/s] 66%|   | 531/805 [04:26<01:55,  2.37it/s] 66%|   | 532/805 [04:27<01:43,  2.64it/s] 66%|   | 533/805 [04:27<01:47,  2.53it/s] 66%|   | 534/805 [04:27<01:37,  2.77it/s] 66%|   | 535/805 [04:28<01:30,  2.99it/s] 67%|   | 536/805 [04:28<01:25,  3.16it/s] 67%|   | 537/805 [04:28<01:21,  3.29it/s] 67%|   | 538/805 [04:28<01:18,  3.38it/s] 67%|   | 539/805 [04:29<01:17,  3.45it/s] 67%|   | 540/805 [04:29<01:15,  3.51it/s] 67%|   | 541/805 [04:30<01:43,  2.55it/s] 67%|   | 542/805 [04:30<01:33,  2.80it/s] 67%|   | 543/805 [04:30<01:27,  3.01it/s] 68%|   | 544/805 [04:30<01:22,  3.17it/s] 68%|   | 545/805 [04:31<01:18,  3.30it/s] 68%|   | 546/805 [04:31<01:16,  3.39it/s] 68%|   | 547/805 [04:31<01:14,  3.46it/s] 68%|   | 548/805 [04:32<01:13,  3.51it/s] 68%|   | 549/805 [04:32<01:12,  3.55it/s] 68%|   | 550/805 [04:32<01:11,  3.57it/s] 68%|   | 551/805 [04:33<01:23,  3.05it/s] 69%|   | 552/805 [04:33<01:19,  3.20it/s] 69%|   | 553/805 [04:33<01:15,  3.32it/s] 69%|   | 554/805 [04:33<01:13,  3.41it/s] 69%|   | 555/805 [04:34<01:11,  3.47it/s] 69%|   | 556/805 [04:34<01:10,  3.52it/s] 69%|   | 557/805 [04:34<01:09,  3.55it/s] 69%|   | 558/805 [04:35<01:09,  3.58it/s] 69%|   | 559/805 [04:35<01:08,  3.59it/s] 70%|   | 560/805 [04:35<01:07,  3.61it/s] 70%|   | 561/805 [04:35<01:07,  3.62it/s] 70%|   | 562/805 [04:36<01:13,  3.29it/s] 70%|   | 563/805 [04:36<01:11,  3.38it/s] 70%|   | 564/805 [04:36<01:09,  3.45it/s] 70%|   | 565/805 [04:37<01:08,  3.51it/s] 70%|   | 566/805 [04:37<01:07,  3.54it/s] 70%|   | 567/805 [04:37<01:06,  3.57it/s] 71%|   | 568/805 [04:37<01:06,  3.59it/s] 71%|   | 569/805 [04:38<01:05,  3.60it/s] 71%|   | 570/805 [04:38<01:05,  3.61it/s] 71%|   | 571/805 [04:38<01:04,  3.62it/s] 71%|   | 572/805 [04:38<01:04,  3.62it/s] 71%|   | 573/805 [04:39<01:18,  2.96it/s] 71%|  | 574/805 [04:39<01:13,  3.13it/s] 71%|  | 575/805 [04:39<01:10,  3.27it/s] 72%|  | 576/805 [04:40<01:07,  3.37it/s] 72%|  | 577/805 [04:40<01:06,  3.45it/s] 72%|  | 578/805 [04:40<01:04,  3.50it/s] 72%|  | 579/805 [04:41<01:03,  3.54it/s] 72%|  | 580/805 [04:41<01:03,  3.57it/s] 72%|  | 581/805 [04:41<01:02,  3.59it/s] 72%|  | 582/805 [04:41<01:01,  3.60it/s] 72%|  | 583/805 [04:42<01:01,  3.61it/s] 73%|  | 584/805 [04:42<01:09,  3.19it/s] 73%|  | 585/805 [04:42<01:06,  3.32it/s] 73%|  | 586/805 [04:43<01:04,  3.41it/s] 73%|  | 587/805 [04:43<01:02,  3.47it/s] 73%|  | 588/805 [04:43<01:01,  3.52it/s] 73%|  | 589/805 [04:43<01:00,  3.55it/s] 73%|  | 590/805 [04:44<01:00,  3.57it/s] 73%|  | 591/805 [04:44<00:59,  3.59it/s] 74%|  | 592/805 [04:44<00:59,  3.60it/s] 74%|  | 593/805 [04:45<00:58,  3.61it/s] 74%|  | 594/805 [04:45<00:58,  3.62it/s] 74%|  | 595/805 [04:45<00:57,  3.62it/s] 74%|  | 596/805 [04:45<00:57,  3.62it/s] 74%|  | 597/805 [04:46<00:57,  3.62it/s] 74%|  | 598/805 [04:46<00:57,  3.62it/s] 74%|  | 599/805 [04:46<00:56,  3.62it/s] 75%|  | 600/805 [04:46<00:56,  3.63it/s] 75%|  | 601/805 [04:47<00:56,  3.62it/s] 75%|  | 602/805 [04:47<00:56,  3.62it/s] 75%|  | 603/805 [04:47<00:55,  3.62it/s] 75%|  | 604/805 [04:48<01:00,  3.31it/s] 75%|  | 605/805 [04:48<00:58,  3.40it/s] 75%|  | 606/805 [04:48<00:57,  3.47it/s] 75%|  | 607/805 [04:49<00:56,  3.51it/s] 76%|  | 608/805 [04:49<00:55,  3.54it/s] 76%|  | 609/805 [04:49<00:57,  3.43it/s] 76%|  | 610/805 [04:49<00:56,  3.47it/s] 76%|  | 611/805 [04:50<00:55,  3.52it/s] 76%|  | 612/805 [04:50<00:54,  3.55it/s] 76%|  | 613/805 [04:50<00:53,  3.57it/s] 76%|  | 614/805 [04:50<00:53,  3.59it/s] 76%|  | 615/805 [04:51<00:57,  3.30it/s] 77%|  | 616/805 [04:51<00:55,  3.39it/s] 77%|  | 617/805 [04:51<00:54,  3.46it/s] 77%|  | 618/805 [04:52<00:53,  3.51it/s] 77%|  | 619/805 [04:52<00:52,  3.54it/s] 77%|  | 620/805 [04:52<00:54,  3.38it/s] 77%|  | 621/805 [04:53<00:55,  3.30it/s] 77%|  | 622/805 [04:53<00:54,  3.38it/s] 77%|  | 623/805 [04:53<00:52,  3.45it/s] 78%|  | 624/805 [04:53<00:51,  3.50it/s] 78%|  | 625/805 [04:54<00:50,  3.53it/s] 78%|  | 626/805 [04:54<00:57,  3.12it/s] 78%|  | 627/805 [04:54<00:54,  3.25it/s] 78%|  | 628/805 [04:55<00:52,  3.36it/s] 78%|  | 629/805 [04:55<00:51,  3.43it/s] 78%|  | 630/805 [04:55<00:50,  3.49it/s] 78%|  | 631/805 [04:55<00:49,  3.53it/s] 79%|  | 632/805 [04:56<00:48,  3.56it/s] 79%|  | 633/805 [04:56<00:48,  3.57it/s] 79%|  | 634/805 [04:56<00:47,  3.59it/s] 79%|  | 635/805 [04:57<00:47,  3.60it/s] 79%|  | 636/805 [04:57<00:46,  3.61it/s] 79%|  | 637/805 [04:57<00:50,  3.33it/s] 79%|  | 638/805 [04:57<00:48,  3.41it/s] 79%|  | 639/805 [04:58<00:47,  3.47it/s] 80%|  | 640/805 [04:58<00:46,  3.52it/s] 80%|  | 641/805 [04:58<00:46,  3.55it/s] 80%|  | 642/805 [04:59<00:45,  3.57it/s] 80%|  | 643/805 [04:59<00:45,  3.58it/s] 80%|  | 644/805 [04:59<00:44,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 17:43:29,824 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:43:29,824 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:43:29,824 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 9.8796, 'eval_samples_per_second': 353.152, 'eval_steps_per_second': 44.233, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.98it/s][A
  3%|         | 12/437 [00:00<00:08, 49.58it/s][A
  4%|         | 18/437 [00:00<00:08, 47.61it/s][A
  5%|         | 23/437 [00:00<00:08, 46.82it/s][A
  6%|         | 28/437 [00:00<00:08, 46.11it/s][A
  8%|         | 33/437 [00:00<00:08, 45.64it/s][A
  9%|         | 38/437 [00:00<00:08, 45.26it/s][A
 10%|         | 43/437 [00:00<00:09, 41.61it/s][A
 11%|         | 48/437 [00:01<00:09, 42.83it/s][A
 12%|        | 53/437 [00:01<00:08, 43.65it/s][A
 13%|        | 58/437 [00:01<00:08, 44.21it/s][A
 14%|        | 63/437 [00:01<00:08, 44.67it/s][A
 16%|        | 68/437 [00:01<00:08, 44.88it/s][A
 17%|        | 73/437 [00:01<00:08, 45.03it/s][A
 18%|        | 78/437 [00:01<00:07, 44.91it/s][A
 19%|        | 83/437 [00:01<00:07, 44.74it/s][A
 20%|        | 88/437 [00:02<00:10, 32.43it/s][A
 21%|       | 93/437 [00:02<00:09, 35.54it/s][A
 22%|       | 98/437 [00:02<00:08, 38.09it/s][A
 24%|       | 103/437 [00:02<00:08, 40.10it/s][A
 25%|       | 108/437 [00:02<00:07, 41.53it/s][A
 26%|       | 113/437 [00:02<00:07, 42.72it/s][A
 27%|       | 118/437 [00:02<00:07, 43.49it/s][A
 28%|       | 123/437 [00:03<00:12, 24.79it/s][A
 29%|       | 128/437 [00:03<00:10, 28.75it/s][A
 30%|       | 133/437 [00:03<00:09, 32.38it/s][A
 32%|      | 138/437 [00:03<00:11, 25.56it/s][A
 33%|      | 143/437 [00:03<00:09, 29.45it/s][A
 34%|      | 148/437 [00:04<00:08, 33.00it/s][A
 35%|      | 152/437 [00:04<00:10, 28.30it/s][A
 36%|      | 157/437 [00:04<00:08, 32.11it/s][A
 37%|      | 162/437 [00:04<00:07, 35.35it/s][A
 38%|      | 167/437 [00:04<00:07, 37.96it/s][A
 39%|      | 172/437 [00:04<00:06, 39.95it/s][A
 41%|      | 177/437 [00:04<00:06, 41.51it/s][A
 42%|     | 182/437 [00:04<00:05, 42.71it/s][A
 43%|     | 187/437 [00:04<00:05, 43.47it/s][A
 44%|     | 192/437 [00:04<00:05, 43.58it/s][A
 45%|     | 197/437 [00:05<00:05, 43.85it/s][A
 46%|     | 202/437 [00:05<00:05, 44.19it/s][A
 47%|     | 207/437 [00:05<00:05, 44.60it/s][A
 49%|     | 212/437 [00:05<00:05, 44.80it/s][A
 50%|     | 217/437 [00:05<00:04, 44.94it/s][A
 51%|     | 222/437 [00:05<00:04, 45.11it/s][A
 52%|    | 227/437 [00:05<00:04, 45.22it/s][A
 53%|    | 232/437 [00:05<00:04, 45.18it/s][A
 54%|    | 237/437 [00:05<00:04, 44.87it/s][A
 55%|    | 242/437 [00:06<00:05, 33.24it/s][A
 57%|    | 247/437 [00:06<00:05, 36.33it/s][A
 58%|    | 252/437 [00:06<00:04, 38.76it/s][A
 59%|    | 257/437 [00:06<00:04, 40.62it/s][A
 60%|    | 262/437 [00:06<00:04, 42.02it/s][A
 61%|    | 267/437 [00:06<00:03, 42.96it/s][A
 62%|   | 272/437 [00:06<00:03, 43.69it/s][A
 63%|   | 277/437 [00:07<00:03, 44.29it/s][A
 65%|   | 282/437 [00:07<00:03, 42.13it/s][A
 66%|   | 287/437 [00:07<00:03, 42.77it/s][A
 67%|   | 292/437 [00:07<00:03, 43.48it/s][A
 68%|   | 297/437 [00:07<00:03, 43.99it/s][A
 69%|   | 302/437 [00:07<00:03, 44.51it/s][A
 70%|   | 307/437 [00:07<00:02, 44.87it/s][A
 71%|  | 312/437 [00:07<00:02, 45.09it/s][A
 73%|  | 317/437 [00:07<00:02, 45.21it/s][A
 74%|  | 322/437 [00:07<00:02, 44.99it/s][A
 75%|  | 327/437 [00:08<00:02, 44.85it/s][A
 76%|  | 332/437 [00:08<00:02, 44.82it/s][A
 77%|  | 337/437 [00:08<00:02, 44.90it/s][A
 78%|  | 342/437 [00:08<00:02, 45.07it/s][A
 79%|  | 347/437 [00:08<00:01, 45.19it/s][A
 81%|  | 352/437 [00:08<00:01, 45.19it/s][A
 82%| | 357/437 [00:08<00:01, 45.35it/s][A
 83%| | 362/437 [00:08<00:01, 45.35it/s][A
 84%| | 367/437 [00:08<00:01, 45.21it/s][A
 85%| | 372/437 [00:09<00:01, 44.92it/s][A
 86%| | 377/437 [00:09<00:01, 44.79it/s][A
 87%| | 382/437 [00:09<00:01, 44.88it/s][A
 89%| | 387/437 [00:09<00:01, 45.04it/s][A
 90%| | 392/437 [00:09<00:00, 45.26it/s][A
 91%| | 397/437 [00:09<00:00, 45.28it/s][A
 92%|| 402/437 [00:09<00:00, 45.31it/s][A
 93%|| 407/437 [00:09<00:00, 45.16it/s][A
 94%|| 412/437 [00:10<00:00, 45.03it/s][A
 95%|| 417/437 [00:10<00:00, 42.84it/s][A
 97%|| 422/437 [00:10<00:00, 43.49it/s][A
 98%|| 427/437 [00:10<00:00, 43.89it/s][A
 99%|| 432/437 [00:10<00:00, 44.34it/s][A
100%|| 437/437 [00:10<00:00, 44.76it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 44.76it/s][A 80%|  | 644/805 [05:10<00:44,  3.65it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:43:40,655 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 17:43:41,553 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:43:48,965 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:43:49,563 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:43:49,695 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/special_tokens_map.json
 80%|  | 645/805 [05:21<18:15,  6.84s/it] 80%|  | 646/805 [05:22<13:01,  4.91s/it] 80%|  | 647/805 [05:22<09:16,  3.52s/it] 80%|  | 648/805 [05:22<06:40,  2.55s/it] 81%|  | 649/805 [05:23<04:51,  1.87s/it] 81%|  | 650/805 [05:23<03:35,  1.39s/it] 81%|  | 651/805 [05:23<02:43,  1.06s/it] 81%|  | 652/805 [05:23<02:06,  1.21it/s] 81%|  | 653/805 [05:24<01:40,  1.51it/s] 81%|  | 654/805 [05:24<01:22,  1.83it/s] 81%| | 655/805 [05:24<01:09,  2.14it/s] 81%| | 656/805 [05:25<01:01,  2.44it/s] 82%| | 657/805 [05:25<01:01,  2.40it/s] 82%| | 658/805 [05:25<00:55,  2.66it/s] 82%| | 659/805 [05:26<00:50,  2.88it/s] 82%| | 660/805 [05:26<01:17,  1.88it/s] 82%| | 661/805 [05:27<01:05,  2.19it/s] 82%| | 662/805 [05:27<01:03,  2.25it/s] 82%| | 663/805 [05:27<00:56,  2.53it/s] 82%| | 664/805 [05:28<00:50,  2.77it/s] 83%| | 665/805 [05:28<00:49,  2.82it/s] 83%| | 666/805 [05:28<00:46,  3.01it/s] 83%| | 667/805 [05:29<00:43,  3.16it/s] 83%| | 668/805 [05:29<00:41,  3.27it/s] 83%| | 669/805 [05:29<00:40,  3.36it/s] 83%| | 670/805 [05:29<00:39,  3.42it/s] 83%| | 671/805 [05:30<00:38,  3.46it/s] 83%| | 672/805 [05:30<00:38,  3.50it/s] 84%| | 673/805 [05:30<00:37,  3.52it/s] 84%| | 674/805 [05:31<00:37,  3.53it/s] 84%| | 675/805 [05:31<00:36,  3.54it/s] 84%| | 676/805 [05:31<00:39,  3.26it/s] 84%| | 677/805 [05:32<00:38,  3.35it/s] 84%| | 678/805 [05:32<00:37,  3.42it/s] 84%| | 679/805 [05:32<00:36,  3.46it/s] 84%| | 680/805 [05:32<00:35,  3.50it/s] 85%| | 681/805 [05:33<00:35,  3.52it/s] 85%| | 682/805 [05:33<00:34,  3.54it/s] 85%| | 683/805 [05:33<00:34,  3.55it/s] 85%| | 684/805 [05:33<00:34,  3.56it/s] 85%| | 685/805 [05:34<00:33,  3.56it/s] 85%| | 686/805 [05:34<00:33,  3.56it/s] 85%| | 687/805 [05:34<00:37,  3.11it/s] 85%| | 688/805 [05:35<00:36,  3.24it/s] 86%| | 689/805 [05:35<00:34,  3.33it/s] 86%| | 690/805 [05:35<00:33,  3.40it/s] 86%| | 691/805 [05:36<00:33,  3.45it/s] 86%| | 692/805 [05:36<00:32,  3.48it/s] 86%| | 693/805 [05:36<00:31,  3.51it/s] 86%| | 694/805 [05:36<00:31,  3.53it/s] 86%| | 695/805 [05:37<00:31,  3.54it/s] 86%| | 696/805 [05:37<00:30,  3.55it/s] 87%| | 697/805 [05:37<00:30,  3.56it/s] 87%| | 698/805 [05:38<00:34,  3.07it/s] 87%| | 699/805 [05:38<00:33,  3.20it/s] 87%| | 700/805 [05:38<00:31,  3.31it/s] 87%| | 701/805 [05:39<00:30,  3.38it/s] 87%| | 702/805 [05:39<00:30,  3.43it/s] 87%| | 703/805 [05:39<00:29,  3.47it/s] 87%| | 704/805 [05:39<00:28,  3.50it/s] 88%| | 705/805 [05:40<00:28,  3.52it/s] 88%| | 706/805 [05:40<00:28,  3.53it/s] 88%| | 707/805 [05:40<00:27,  3.54it/s] 88%| | 708/805 [05:40<00:27,  3.55it/s] 88%| | 709/805 [05:41<00:43,  2.22it/s] 88%| | 710/805 [05:42<00:37,  2.51it/s] 88%| | 711/805 [05:42<00:34,  2.75it/s] 88%| | 712/805 [05:42<00:31,  2.96it/s] 89%| | 713/805 [05:42<00:29,  3.12it/s] 89%| | 714/805 [05:43<00:28,  3.24it/s] 89%| | 715/805 [05:43<00:26,  3.35it/s] 89%| | 716/805 [05:43<00:25,  3.43it/s] 89%| | 717/805 [05:44<00:25,  3.49it/s] 89%| | 718/805 [05:44<00:28,  3.09it/s] 89%| | 719/805 [05:44<00:26,  3.23it/s] 89%| | 720/805 [05:45<00:25,  3.34it/s] 90%| | 721/805 [05:45<00:24,  3.42it/s] 90%| | 722/805 [05:45<00:23,  3.48it/s] 90%| | 723/805 [05:45<00:23,  3.52it/s] 90%| | 724/805 [05:46<00:22,  3.55it/s] 90%| | 725/805 [05:46<00:22,  3.58it/s] 90%| | 726/805 [05:46<00:21,  3.60it/s] 90%| | 727/805 [05:46<00:21,  3.60it/s] 90%| | 728/805 [05:47<00:21,  3.61it/s] 91%| | 729/805 [05:48<00:34,  2.23it/s] 91%| | 730/805 [05:48<00:29,  2.51it/s] 91%| | 731/805 [05:48<00:26,  2.77it/s] 91%| | 732/805 [05:48<00:24,  2.98it/s] 91%| | 733/805 [05:49<00:22,  3.14it/s] 91%| | 734/805 [05:49<00:21,  3.27it/s] 91%|| 735/805 [05:49<00:25,  2.72it/s] 91%|| 736/805 [05:50<00:23,  2.93it/s] 92%|| 737/805 [05:50<00:25,  2.71it/s] 92%|| 738/805 [05:50<00:22,  2.93it/s] 92%|| 739/805 [05:51<00:21,  3.12it/s] 92%|| 740/805 [05:51<00:19,  3.25it/s] 92%|| 741/805 [05:51<00:19,  3.35it/s] 92%|| 742/805 [05:52<00:18,  3.43it/s] 92%|| 743/805 [05:52<00:17,  3.49it/s] 92%|| 744/805 [05:52<00:17,  3.53it/s] 93%|| 745/805 [05:53<00:25,  2.32it/s] 93%|| 746/805 [05:53<00:24,  2.40it/s] 93%|| 747/805 [05:54<00:21,  2.67it/s] 93%|| 748/805 [05:54<00:19,  2.90it/s] 93%|| 749/805 [05:54<00:18,  3.08it/s] 93%|| 750/805 [05:54<00:17,  3.23it/s] 93%|| 751/805 [05:55<00:16,  3.34it/s] 93%|| 752/805 [05:55<00:15,  3.42it/s] 94%|| 753/805 [05:55<00:14,  3.48it/s] 94%|| 754/805 [05:55<00:14,  3.52it/s] 94%|| 755/805 [05:56<00:14,  3.55it/s] 94%|| 756/805 [05:56<00:13,  3.58it/s] 94%|| 757/805 [05:57<00:17,  2.81it/s] 94%|| 758/805 [05:57<00:15,  3.01it/s] 94%|| 759/805 [05:57<00:14,  3.18it/s] 94%|| 760/805 [05:57<00:13,  3.30it/s] 95%|| 761/805 [05:58<00:16,  2.64it/s] 95%|| 762/805 [05:58<00:14,  2.87it/s] 95%|| 763/805 [05:59<00:13,  3.06it/s] 95%|| 764/805 [05:59<00:12,  3.21it/s] 95%|| 765/805 [05:59<00:12,  3.33it/s] 95%|| 766/805 [05:59<00:13,  2.92it/s] 95%|| 767/805 [06:00<00:12,  3.10it/s] 95%|| 768/805 [06:00<00:11,  3.24it/s] 96%|| 769/805 [06:00<00:10,  3.35it/s] 96%|| 770/805 [06:01<00:10,  3.43it/s] 96%|| 771/805 [06:01<00:09,  3.49it/s] 96%|| 772/805 [06:01<00:09,  3.53it/s] 96%|| 773/805 [06:02<00:09,  3.25it/s] 96%|| 774/805 [06:02<00:09,  3.34it/s] 96%|| 775/805 [06:02<00:08,  3.42it/s] 96%|| 776/805 [06:03<00:11,  2.59it/s] 97%|| 777/805 [06:03<00:10,  2.62it/s] 97%|| 778/805 [06:03<00:09,  2.84it/s] 97%|| 779/805 [06:04<00:08,  3.04it/s] 97%|| 780/805 [06:04<00:07,  3.19it/s] 97%|| 781/805 [06:04<00:07,  3.31it/s] 97%|| 782/805 [06:04<00:06,  3.40it/s] 97%|| 783/805 [06:05<00:06,  3.47it/s] 97%|| 784/805 [06:05<00:05,  3.51it/s] 98%|| 785/805 [06:05<00:05,  3.38it/s] 98%|| 786/805 [06:06<00:05,  3.31it/s] 98%|| 787/805 [06:06<00:05,  3.40it/s] 98%|| 788/805 [06:06<00:04,  3.47it/s] 98%|| 789/805 [06:06<00:04,  3.51it/s] 98%|| 790/805 [06:07<00:04,  3.55it/s] 98%|| 791/805 [06:07<00:03,  3.57it/s] 98%|| 792/805 [06:07<00:03,  3.59it/s] 99%|| 793/805 [06:08<00:03,  3.60it/s] 99%|| 794/805 [06:08<00:03,  3.61it/s] 99%|| 795/805 [06:08<00:02,  3.62it/s] 99%|| 796/805 [06:08<00:02,  3.62it/s] 99%|| 797/805 [06:09<00:02,  3.42it/s] 99%|| 798/805 [06:09<00:02,  3.48it/s] 99%|| 799/805 [06:09<00:01,  3.52it/s] 99%|| 800/805 [06:10<00:01,  3.55it/s]100%|| 801/805 [06:10<00:01,  3.58it/s]100%|| 802/805 [06:10<00:00,  3.59it/s]100%|| 803/805 [06:10<00:00,  3.61it/s]100%|| 804/805 [06:11<00:00,  3.62it/s]100%|| 805/805 [06:11<00:00,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 17:44:41,575 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:44:41,576 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:44:41,576 >>   Batch size = 8
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.5664, 'eval_samples_per_second': 330.198, 'eval_steps_per_second': 41.358, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.52it/s][A
  3%|         | 12/437 [00:00<00:08, 49.56it/s][A
  4%|         | 18/437 [00:00<00:08, 47.61it/s][A
  5%|         | 23/437 [00:00<00:08, 46.69it/s][A
  6%|         | 28/437 [00:00<00:12, 33.81it/s][A
  8%|         | 33/437 [00:01<00:10, 36.91it/s][A
  9%|         | 38/437 [00:01<00:18, 21.06it/s][A
 10%|         | 43/437 [00:01<00:15, 25.33it/s][A
 11%|         | 48/437 [00:01<00:13, 29.36it/s][A
 12%|        | 53/437 [00:01<00:11, 32.96it/s][A
 13%|        | 58/437 [00:01<00:10, 35.91it/s][A
 14%|        | 63/437 [00:01<00:09, 38.38it/s][A
 16%|        | 68/437 [00:01<00:09, 40.31it/s][A
 17%|        | 73/437 [00:02<00:08, 41.73it/s][A
 18%|        | 78/437 [00:02<00:08, 42.41it/s][A
 19%|        | 83/437 [00:02<00:08, 43.01it/s][A
 20%|        | 88/437 [00:02<00:08, 43.58it/s][A
 21%|       | 93/437 [00:02<00:07, 44.14it/s][A
 22%|       | 98/437 [00:02<00:07, 44.58it/s][A
 24%|       | 103/437 [00:02<00:07, 44.86it/s][A
 25%|       | 108/437 [00:02<00:07, 45.08it/s][A
 26%|       | 113/437 [00:02<00:07, 45.17it/s][A
 27%|       | 118/437 [00:03<00:07, 44.98it/s][A
 28%|       | 123/437 [00:03<00:07, 44.80it/s][A
 29%|       | 128/437 [00:03<00:06, 44.72it/s][A
 30%|       | 133/437 [00:03<00:06, 44.79it/s][A
 32%|      | 138/437 [00:03<00:06, 44.95it/s][A
 33%|      | 143/437 [00:03<00:06, 45.07it/s][A
 34%|      | 148/437 [00:03<00:06, 45.21it/s][A
 35%|      | 153/437 [00:03<00:06, 45.39it/s][A
 36%|      | 158/437 [00:04<00:08, 33.29it/s][A
 37%|      | 163/437 [00:04<00:07, 36.28it/s][A
 38%|      | 168/437 [00:04<00:06, 38.57it/s][A
 40%|      | 173/437 [00:04<00:06, 40.52it/s][A
 41%|      | 178/437 [00:04<00:06, 41.95it/s][A
 42%|     | 183/437 [00:04<00:05, 43.01it/s][A
 43%|     | 188/437 [00:04<00:05, 43.80it/s][A
 44%|     | 193/437 [00:04<00:05, 44.19it/s][A
 45%|     | 198/437 [00:04<00:05, 44.00it/s][A
 46%|     | 203/437 [00:05<00:05, 44.06it/s][A
 48%|     | 208/437 [00:05<00:05, 44.40it/s][A
 49%|     | 213/437 [00:05<00:05, 44.75it/s][A
 50%|     | 218/437 [00:05<00:04, 44.94it/s][A
 51%|     | 223/437 [00:05<00:04, 45.07it/s][A
 52%|    | 228/437 [00:05<00:04, 45.19it/s][A
 53%|    | 233/437 [00:05<00:04, 45.26it/s][A
 54%|    | 238/437 [00:05<00:04, 45.19it/s][A
 56%|    | 243/437 [00:05<00:04, 44.90it/s][A
 57%|    | 248/437 [00:06<00:04, 44.79it/s][A
 58%|    | 253/437 [00:06<00:04, 44.87it/s][A
 59%|    | 258/437 [00:06<00:03, 45.01it/s][A
 60%|    | 263/437 [00:06<00:03, 45.15it/s][A
 61%|   | 268/437 [00:06<00:03, 45.19it/s][A
 62%|   | 273/437 [00:06<00:03, 45.37it/s][A
 64%|   | 278/437 [00:06<00:03, 45.41it/s][A
 65%|   | 283/437 [00:07<00:03, 45.36it/s][A
 66%|   | 288/437 [00:07<00:04, 31.16it/s][A
 67%|   | 293/437 [00:07<00:04, 34.49it/s][A
 68%|   | 298/437 [00:07<00:03, 37.26it/s][A
 69%|   | 303/437 [00:07<00:03, 39.44it/s][A
 70%|   | 308/437 [00:07<00:03, 41.03it/s][A
 72%|  | 313/437 [00:07<00:02, 42.21it/s][A
 73%|  | 318/437 [00:07<00:02, 43.17it/s][A
 74%|  | 323/437 [00:07<00:02, 43.71it/s][A
 75%|  | 328/437 [00:07<00:02, 43.82it/s][A
 76%|  | 333/437 [00:08<00:02, 44.00it/s][A
 77%|  | 338/437 [00:08<00:02, 44.31it/s][A
 78%|  | 343/437 [00:08<00:02, 44.59it/s][A
 80%|  | 348/437 [00:08<00:01, 44.71it/s][A
 81%|  | 353/437 [00:08<00:01, 44.90it/s][A
 82%| | 358/437 [00:08<00:01, 45.18it/s][A
 83%| | 363/437 [00:08<00:01, 45.35it/s][A
 84%| | 368/437 [00:08<00:01, 45.17it/s][A
 85%| | 373/437 [00:08<00:01, 45.04it/s][A
 86%| | 378/437 [00:09<00:01, 44.86it/s][A
 88%| | 383/437 [00:09<00:01, 44.89it/s][A
 89%| | 388/437 [00:09<00:01, 45.01it/s][A
 90%| | 393/437 [00:09<00:00, 45.13it/s][A
 91%| | 398/437 [00:09<00:00, 45.21it/s][A
 92%|| 403/437 [00:09<00:00, 45.33it/s][A
 93%|| 408/437 [00:09<00:00, 45.37it/s][A
 95%|| 413/437 [00:10<00:00, 45.25it/s][A
 96%|| 418/437 [00:10<00:00, 27.60it/s][A
 97%|| 423/437 [00:10<00:00, 31.31it/s][A
 98%|| 428/437 [00:10<00:00, 34.58it/s][A
 99%|| 433/437 [00:10<00:00, 37.34it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 37.34it/s][A100%|| 805/805 [06:22<00:00,  3.67it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:44:53,974 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 17:44:54,443 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:45:05,742 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:45:06,161 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:45:06,381 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:45:11,073 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:45:11,131 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161 (score: 1.0192089080810547).
                                                 100%|| 805/805 [07:10<00:00,  3.67it/s]100%|| 805/805 [07:10<00:00,  1.87it/s]
[INFO|trainer.py:1894] 2023-08-28 17:45:40,801 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 17:45:40,982 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:45:48,874 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:45:49,454 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:45:50,150 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:45:51,618 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   train_runtime            = 0:07:10.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   train_samples            =      10300
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   train_samples_per_second =    119.615
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:45:51,618 >>   train_steps_per_second   =       1.87
{'eval_loss': 1.0192089080810547, 'eval_runtime': 10.6423, 'eval_samples_per_second': 327.844, 'eval_steps_per_second': 41.063, 'epoch': 5.0}
{'train_runtime': 430.5472, 'train_samples_per_second': 119.615, 'train_steps_per_second': 1.87, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 17:45:53 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:45:53,484 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:45:53,484 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 17:45:53,484 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 57.29it/s]  3%|         | 12/437 [00:00<00:08, 50.32it/s]  4%|         | 18/437 [00:00<00:08, 48.35it/s]  5%|         | 23/437 [00:00<00:08, 47.53it/s]  6%|         | 28/437 [00:00<00:08, 47.04it/s]  8%|         | 33/437 [00:00<00:08, 46.66it/s]  9%|         | 38/437 [00:00<00:08, 46.32it/s] 10%|         | 43/437 [00:00<00:08, 46.19it/s] 11%|         | 48/437 [00:01<00:08, 45.65it/s] 12%|        | 53/437 [00:01<00:08, 45.58it/s] 13%|        | 58/437 [00:01<00:08, 45.75it/s] 14%|        | 63/437 [00:01<00:08, 45.75it/s] 16%|        | 68/437 [00:01<00:08, 45.93it/s] 17%|        | 73/437 [00:01<00:07, 45.96it/s] 18%|        | 78/437 [00:01<00:07, 45.98it/s] 19%|        | 83/437 [00:01<00:07, 45.81it/s] 20%|        | 88/437 [00:01<00:07, 45.50it/s] 21%|       | 93/437 [00:02<00:07, 45.11it/s] 22%|       | 98/437 [00:02<00:07, 45.11it/s] 24%|       | 103/437 [00:02<00:07, 45.35it/s] 25%|       | 108/437 [00:02<00:07, 45.46it/s] 26%|       | 113/437 [00:02<00:07, 45.67it/s] 27%|       | 118/437 [00:02<00:06, 45.80it/s] 28%|       | 123/437 [00:02<00:06, 45.92it/s] 29%|       | 128/437 [00:02<00:06, 45.95it/s] 30%|       | 133/437 [00:02<00:07, 38.35it/s] 32%|      | 138/437 [00:03<00:07, 40.43it/s] 33%|      | 143/437 [00:03<00:07, 41.98it/s] 34%|      | 148/437 [00:03<00:06, 43.08it/s] 35%|      | 153/437 [00:03<00:06, 44.00it/s] 36%|      | 158/437 [00:03<00:06, 44.65it/s] 37%|      | 163/437 [00:03<00:06, 45.11it/s] 38%|      | 168/437 [00:03<00:05, 45.35it/s] 40%|      | 173/437 [00:03<00:05, 45.11it/s] 41%|      | 178/437 [00:03<00:05, 44.83it/s] 42%|     | 183/437 [00:04<00:05, 44.96it/s] 43%|     | 188/437 [00:04<00:05, 45.17it/s] 44%|     | 193/437 [00:04<00:05, 45.45it/s] 45%|     | 198/437 [00:04<00:05, 45.64it/s] 46%|     | 203/437 [00:04<00:05, 45.77it/s] 48%|     | 208/437 [00:04<00:04, 45.85it/s] 49%|     | 213/437 [00:04<00:04, 45.87it/s] 50%|     | 218/437 [00:04<00:04, 45.63it/s] 51%|     | 223/437 [00:04<00:04, 45.27it/s] 52%|    | 228/437 [00:05<00:04, 45.25it/s] 53%|    | 233/437 [00:05<00:04, 45.24it/s] 54%|    | 238/437 [00:05<00:04, 45.44it/s] 56%|    | 243/437 [00:05<00:04, 45.27it/s] 57%|    | 248/437 [00:05<00:04, 45.61it/s] 58%|    | 253/437 [00:05<00:04, 45.80it/s] 59%|    | 258/437 [00:05<00:03, 45.90it/s] 60%|    | 263/437 [00:05<00:03, 45.62it/s] 61%|   | 268/437 [00:06<00:06, 28.07it/s] 62%|   | 273/437 [00:06<00:05, 31.82it/s] 64%|   | 278/437 [00:06<00:04, 35.11it/s] 65%|   | 283/437 [00:06<00:04, 37.83it/s] 66%|   | 288/437 [00:06<00:03, 40.01it/s] 67%|   | 293/437 [00:06<00:03, 41.65it/s] 68%|   | 298/437 [00:06<00:03, 42.81it/s] 69%|   | 303/437 [00:06<00:03, 43.62it/s] 70%|   | 308/437 [00:07<00:02, 43.98it/s] 72%|  | 313/437 [00:07<00:02, 44.05it/s] 73%|  | 318/437 [00:07<00:02, 44.34it/s] 74%|  | 323/437 [00:07<00:02, 44.86it/s] 75%|  | 328/437 [00:07<00:02, 45.22it/s] 76%|  | 333/437 [00:07<00:02, 45.45it/s] 77%|  | 338/437 [00:07<00:02, 45.55it/s] 78%|  | 343/437 [00:07<00:02, 45.64it/s] 80%|  | 348/437 [00:07<00:01, 45.68it/s] 81%|  | 353/437 [00:07<00:01, 45.44it/s] 82%| | 358/437 [00:08<00:03, 20.18it/s] 83%| | 363/437 [00:08<00:03, 24.48it/s] 84%| | 368/437 [00:08<00:02, 28.47it/s] 85%| | 373/437 [00:08<00:01, 32.15it/s] 86%| | 378/437 [00:09<00:02, 20.46it/s] 88%| | 383/437 [00:09<00:02, 24.57it/s] 89%| | 388/437 [00:09<00:01, 28.53it/s] 90%| | 393/437 [00:09<00:01, 32.17it/s] 91%| | 398/437 [00:09<00:01, 35.36it/s] 92%|| 403/437 [00:09<00:00, 38.01it/s] 93%|| 408/437 [00:09<00:00, 40.13it/s] 95%|| 413/437 [00:10<00:00, 41.64it/s] 96%|| 418/437 [00:10<00:00, 42.41it/s] 97%|| 423/437 [00:10<00:00, 42.96it/s] 98%|| 428/437 [00:10<00:00, 43.62it/s] 99%|| 433/437 [00:10<00:00, 44.22it/s]100%|| 437/437 [00:10<00:00, 41.10it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:46:04,135 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   eval_loss               =     1.0192
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   eval_runtime            = 0:00:10.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   eval_samples_per_second =    327.572
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   eval_steps_per_second   =     41.029
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:46:04,135 >>   perplexity              =      2.771
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:26,010 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:26,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:26,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:26,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:26,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:46:26,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:46:26,747 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:46:27,336 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:46:28,389 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:46:28,390 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:31,930 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:31,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:31,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:31,934 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:31,934 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:46:32,710 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:46:32,711 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:46:33,344 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:46:33,570 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:46:33,571 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.72it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.77it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.55it/s]Extractor Predicting: 38it [00:22,  1.57it/s]Extractor Predicting: 39it [00:23,  1.59it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:24,  1.61it/s]Extractor Predicting: 42it [00:25,  1.50it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:26,  1.56it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:29,  1.65it/s]Extractor Predicting: 50it [00:30,  1.54it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.63it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:34,  1.63it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:35,  1.49it/s]Extractor Predicting: 59it [00:36,  1.51it/s]Extractor Predicting: 60it [00:36,  1.55it/s]Extractor Predicting: 61it [00:37,  1.57it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:38,  1.67it/s]Extractor Predicting: 64it [00:39,  1.67it/s]Extractor Predicting: 65it [00:39,  1.56it/s]Extractor Predicting: 66it [00:40,  1.56it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:41,  1.65it/s]Extractor Predicting: 69it [00:42,  1.70it/s]Extractor Predicting: 70it [00:42,  1.73it/s]Extractor Predicting: 71it [00:43,  1.74it/s]Extractor Predicting: 72it [00:43,  1.74it/s]Extractor Predicting: 73it [00:44,  1.73it/s]Extractor Predicting: 74it [00:45,  1.75it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.67it/s]Extractor Predicting: 78it [00:47,  1.71it/s]Extractor Predicting: 79it [00:48,  1.72it/s]Extractor Predicting: 80it [00:48,  1.73it/s]Extractor Predicting: 81it [00:49,  1.74it/s]Extractor Predicting: 82it [00:49,  1.74it/s]Extractor Predicting: 83it [00:50,  1.74it/s]Extractor Predicting: 84it [00:51,  1.63it/s]Extractor Predicting: 85it [00:51,  1.69it/s]Extractor Predicting: 86it [00:52,  1.71it/s]Extractor Predicting: 87it [00:52,  1.73it/s]Extractor Predicting: 88it [00:53,  1.74it/s]Extractor Predicting: 89it [00:53,  1.72it/s]Extractor Predicting: 90it [00:54,  1.73it/s]Extractor Predicting: 91it [00:55,  1.73it/s]Extractor Predicting: 92it [00:55,  1.67it/s]Extractor Predicting: 93it [00:56,  1.64it/s]Extractor Predicting: 94it [00:56,  1.66it/s]Extractor Predicting: 95it [00:57,  1.65it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:58,  1.67it/s]Extractor Predicting: 98it [00:59,  1.71it/s]Extractor Predicting: 99it [00:59,  1.68it/s]Extractor Predicting: 100it [01:00,  1.66it/s]Extractor Predicting: 101it [01:01,  1.51it/s]Extractor Predicting: 102it [01:01,  1.55it/s]Extractor Predicting: 103it [01:02,  1.56it/s]Extractor Predicting: 104it [01:03,  1.20it/s]Extractor Predicting: 105it [01:04,  1.31it/s]Extractor Predicting: 106it [01:05,  1.40it/s]Extractor Predicting: 107it [01:05,  1.50it/s]Extractor Predicting: 108it [01:06,  1.48it/s]Extractor Predicting: 109it [01:06,  1.49it/s]Extractor Predicting: 110it [01:07,  1.54it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:08,  1.60it/s]Extractor Predicting: 113it [01:09,  1.63it/s]Extractor Predicting: 114it [01:09,  1.65it/s]Extractor Predicting: 115it [01:10,  1.65it/s]Extractor Predicting: 116it [01:11,  1.69it/s]Extractor Predicting: 117it [01:11,  1.66it/s]Extractor Predicting: 118it [01:12,  1.68it/s]Extractor Predicting: 119it [01:12,  1.73it/s]Extractor Predicting: 120it [01:13,  1.78it/s]Extractor Predicting: 121it [01:13,  1.79it/s]Extractor Predicting: 122it [01:14,  1.79it/s]Extractor Predicting: 123it [01:15,  1.75it/s]Extractor Predicting: 124it [01:15,  1.76it/s]Extractor Predicting: 125it [01:16,  1.77it/s]Extractor Predicting: 126it [01:17,  1.53it/s]Extractor Predicting: 127it [01:17,  1.59it/s]Extractor Predicting: 128it [01:18,  1.60it/s]Extractor Predicting: 129it [01:19,  1.13it/s]Extractor Predicting: 130it [01:20,  1.24it/s]Extractor Predicting: 131it [01:20,  1.36it/s]Extractor Predicting: 132it [01:21,  1.47it/s]Extractor Predicting: 133it [01:22,  1.33it/s]Extractor Predicting: 134it [01:23,  1.42it/s]Extractor Predicting: 135it [01:23,  1.38it/s]Extractor Predicting: 136it [01:25,  1.11it/s]Extractor Predicting: 137it [01:25,  1.23it/s]Extractor Predicting: 138it [01:26,  1.33it/s]Extractor Predicting: 139it [01:26,  1.44it/s]Extractor Predicting: 140it [01:27,  1.46it/s]Extractor Predicting: 141it [01:28,  1.53it/s]Extractor Predicting: 142it [01:28,  1.58it/s]Extractor Predicting: 143it [01:29,  1.63it/s]Extractor Predicting: 144it [01:29,  1.66it/s]Extractor Predicting: 145it [01:30,  1.59it/s]Extractor Predicting: 145it [01:30,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:38,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:38,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:38,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:38,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:38,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:48:39,098 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:48:39,099 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:48:39,752 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:48:40,896 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:48:40,896 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:44,270 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:44,337 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:44,337 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:44,337 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:48:44,337 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:48:45,681 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:48:45,682 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:48:46,758 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:48:47,213 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:48:47,213 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.52it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.52it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.43it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.46it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.37it/s]Extractor Predicting: 31it [00:19,  1.44it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.45it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:24,  1.67it/s]Extractor Predicting: 39it [00:24,  1.69it/s]Extractor Predicting: 40it [00:25,  1.71it/s]Extractor Predicting: 41it [00:25,  1.68it/s]Extractor Predicting: 42it [00:26,  1.69it/s]Extractor Predicting: 43it [00:27,  1.70it/s]Extractor Predicting: 44it [00:27,  1.72it/s]Extractor Predicting: 45it [00:28,  1.70it/s]Extractor Predicting: 46it [00:28,  1.71it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.58it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:31,  1.67it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:33,  1.65it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:35,  1.70it/s]Extractor Predicting: 58it [00:36,  1.74it/s]Extractor Predicting: 59it [00:36,  1.74it/s]Extractor Predicting: 60it [00:37,  1.84it/s]Extractor Predicting: 61it [00:38,  1.45it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.65it/s]Extractor Predicting: 64it [00:40,  1.15it/s]Extractor Predicting: 65it [00:41,  1.31it/s]Extractor Predicting: 66it [00:41,  1.44it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:42,  1.62it/s]Extractor Predicting: 69it [00:43,  1.75it/s]Extractor Predicting: 70it [00:43,  1.80it/s]Extractor Predicting: 71it [00:44,  1.85it/s]Extractor Predicting: 72it [00:44,  1.90it/s]Extractor Predicting: 73it [00:45,  1.93it/s]Extractor Predicting: 74it [00:46,  1.59it/s]Extractor Predicting: 75it [00:46,  1.68it/s]Extractor Predicting: 76it [00:47,  1.79it/s]Extractor Predicting: 77it [00:47,  1.87it/s]Extractor Predicting: 78it [00:48,  1.90it/s]Extractor Predicting: 79it [00:48,  1.83it/s]Extractor Predicting: 80it [00:49,  1.89it/s]Extractor Predicting: 81it [00:49,  1.91it/s]Extractor Predicting: 82it [00:50,  1.91it/s]Extractor Predicting: 83it [00:50,  1.92it/s]Extractor Predicting: 84it [00:51,  1.72it/s]Extractor Predicting: 85it [00:52,  1.78it/s]Extractor Predicting: 86it [00:52,  1.77it/s]Extractor Predicting: 87it [00:53,  1.75it/s]Extractor Predicting: 88it [00:53,  1.74it/s]Extractor Predicting: 89it [00:54,  1.62it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:55,  1.63it/s]Extractor Predicting: 92it [00:56,  1.62it/s]Extractor Predicting: 93it [00:56,  1.68it/s]Extractor Predicting: 94it [00:57,  1.53it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:58,  1.59it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [01:00,  1.62it/s]Extractor Predicting: 99it [01:00,  1.61it/s]Extractor Predicting: 100it [01:01,  1.63it/s]Extractor Predicting: 101it [01:02,  1.27it/s]Extractor Predicting: 102it [01:03,  1.36it/s]Extractor Predicting: 103it [01:03,  1.47it/s]Extractor Predicting: 104it [01:04,  1.50it/s]Extractor Predicting: 105it [01:05,  1.08it/s]Extractor Predicting: 106it [01:06,  1.19it/s]Extractor Predicting: 107it [01:07,  1.29it/s]Extractor Predicting: 108it [01:07,  1.38it/s]Extractor Predicting: 109it [01:08,  1.47it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:09,  1.55it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:11,  1.30it/s]Extractor Predicting: 114it [01:11,  1.43it/s]Extractor Predicting: 115it [01:12,  1.25it/s]Extractor Predicting: 116it [01:13,  1.37it/s]Extractor Predicting: 117it [01:13,  1.48it/s]Extractor Predicting: 118it [01:14,  1.50it/s]Extractor Predicting: 119it [01:15,  1.51it/s]Extractor Predicting: 120it [01:16,  1.44it/s]Extractor Predicting: 121it [01:16,  1.49it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:18,  1.25it/s]Extractor Predicting: 124it [01:18,  1.35it/s]Extractor Predicting: 125it [01:19,  1.42it/s]Extractor Predicting: 126it [01:20,  1.46it/s]Extractor Predicting: 127it [01:21,  1.34it/s]Extractor Predicting: 128it [01:21,  1.40it/s]Extractor Predicting: 129it [01:22,  1.46it/s]Extractor Predicting: 130it [01:23,  1.49it/s]Extractor Predicting: 131it [01:23,  1.54it/s]Extractor Predicting: 132it [01:24,  1.57it/s]Extractor Predicting: 133it [01:24,  1.60it/s]Extractor Predicting: 134it [01:25,  1.64it/s]Extractor Predicting: 135it [01:26,  1.64it/s]Extractor Predicting: 136it [01:26,  1.47it/s]Extractor Predicting: 137it [01:27,  1.53it/s]Extractor Predicting: 138it [01:28,  1.55it/s]Extractor Predicting: 139it [01:28,  1.56it/s]Extractor Predicting: 140it [01:29,  1.60it/s]Extractor Predicting: 141it [01:29,  1.61it/s]Extractor Predicting: 142it [01:30,  1.63it/s]Extractor Predicting: 143it [01:31,  1.59it/s]Extractor Predicting: 144it [01:31,  1.55it/s]Extractor Predicting: 145it [01:32,  1.58it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:33,  1.57it/s]Extractor Predicting: 148it [01:34,  1.62it/s]Extractor Predicting: 149it [01:34,  1.60it/s]Extractor Predicting: 150it [01:35,  1.61it/s]Extractor Predicting: 151it [01:36,  1.65it/s]Extractor Predicting: 152it [01:36,  1.65it/s]Extractor Predicting: 153it [01:37,  1.68it/s]Extractor Predicting: 154it [01:37,  1.71it/s]Extractor Predicting: 155it [01:38,  1.74it/s]Extractor Predicting: 156it [01:38,  1.76it/s]Extractor Predicting: 157it [01:39,  1.75it/s]Extractor Predicting: 158it [01:40,  1.54it/s]Extractor Predicting: 159it [01:40,  1.65it/s]Extractor Predicting: 160it [01:41,  1.65it/s]Extractor Predicting: 161it [01:42,  1.59it/s]Extractor Predicting: 162it [01:42,  1.59it/s]Extractor Predicting: 163it [01:43,  1.63it/s]Extractor Predicting: 164it [01:44,  1.54it/s]Extractor Predicting: 165it [01:44,  1.58it/s]Extractor Predicting: 166it [01:45,  1.60it/s]Extractor Predicting: 167it [01:45,  1.62it/s]Extractor Predicting: 168it [01:46,  1.63it/s]Extractor Predicting: 169it [01:47,  1.60it/s]Extractor Predicting: 170it [01:47,  1.63it/s]Extractor Predicting: 171it [01:48,  1.64it/s]Extractor Predicting: 172it [01:48,  1.63it/s]Extractor Predicting: 173it [01:49,  1.65it/s]Extractor Predicting: 174it [01:50,  1.66it/s]Extractor Predicting: 175it [01:50,  1.66it/s]Extractor Predicting: 176it [01:51,  1.66it/s]Extractor Predicting: 177it [01:51,  1.71it/s]Extractor Predicting: 178it [01:52,  1.42it/s]Extractor Predicting: 179it [01:53,  1.51it/s]Extractor Predicting: 180it [01:54,  1.59it/s]Extractor Predicting: 181it [01:55,  1.05it/s]Extractor Predicting: 182it [01:56,  1.22it/s]Extractor Predicting: 183it [01:56,  1.34it/s]Extractor Predicting: 184it [01:57,  1.38it/s]Extractor Predicting: 185it [01:58,  1.49it/s]Extractor Predicting: 186it [01:58,  1.57it/s]Extractor Predicting: 187it [01:59,  1.59it/s]Extractor Predicting: 188it [01:59,  1.67it/s]Extractor Predicting: 189it [02:00,  1.71it/s]Extractor Predicting: 190it [02:00,  1.78it/s]Extractor Predicting: 191it [02:01,  1.72it/s]Extractor Predicting: 192it [02:01,  1.76it/s]Extractor Predicting: 193it [02:02,  1.78it/s]Extractor Predicting: 194it [02:03,  1.83it/s]Extractor Predicting: 195it [02:03,  1.59it/s]Extractor Predicting: 196it [02:04,  1.64it/s]Extractor Predicting: 197it [02:04,  1.71it/s]Extractor Predicting: 198it [02:05,  1.72it/s]Extractor Predicting: 199it [02:06,  1.48it/s]Extractor Predicting: 200it [02:06,  1.57it/s]Extractor Predicting: 201it [02:07,  1.62it/s]Extractor Predicting: 202it [02:08,  1.70it/s]Extractor Predicting: 203it [02:08,  1.76it/s]Extractor Predicting: 204it [02:09,  1.70it/s]Extractor Predicting: 205it [02:09,  1.74it/s]Extractor Predicting: 206it [02:10,  1.84it/s]Extractor Predicting: 207it [02:10,  1.83it/s]Extractor Predicting: 208it [02:11,  1.67it/s]Extractor Predicting: 209it [02:12,  1.71it/s]Extractor Predicting: 210it [02:12,  1.75it/s]Extractor Predicting: 211it [02:13,  1.78it/s]Extractor Predicting: 212it [02:13,  1.85it/s]Extractor Predicting: 213it [02:14,  1.90it/s]Extractor Predicting: 214it [02:15,  1.51it/s]Extractor Predicting: 215it [02:15,  1.65it/s]Extractor Predicting: 216it [02:16,  1.74it/s]Extractor Predicting: 217it [02:16,  1.76it/s]Extractor Predicting: 218it [02:17,  1.79it/s]Extractor Predicting: 219it [02:17,  1.85it/s]Extractor Predicting: 220it [02:18,  1.88it/s]Extractor Predicting: 221it [02:18,  1.91it/s]Extractor Predicting: 222it [02:19,  1.98it/s]Extractor Predicting: 223it [02:19,  1.67it/s]Extractor Predicting: 224it [02:20,  1.77it/s]Extractor Predicting: 225it [02:20,  1.82it/s]Extractor Predicting: 226it [02:21,  1.87it/s]Extractor Predicting: 227it [02:22,  1.59it/s]Extractor Predicting: 228it [02:22,  1.68it/s]Extractor Predicting: 229it [02:23,  1.68it/s]Extractor Predicting: 230it [02:23,  1.69it/s]Extractor Predicting: 231it [02:24,  1.64it/s]Extractor Predicting: 232it [02:25,  1.58it/s]Extractor Predicting: 233it [02:25,  1.62it/s]Extractor Predicting: 234it [02:26,  1.62it/s]Extractor Predicting: 235it [02:27,  1.64it/s]Extractor Predicting: 236it [02:27,  1.64it/s]Extractor Predicting: 237it [02:28,  1.61it/s]Extractor Predicting: 238it [02:28,  1.60it/s]Extractor Predicting: 239it [02:29,  1.62it/s]Extractor Predicting: 240it [02:30,  1.56it/s]Extractor Predicting: 241it [02:30,  1.61it/s]Extractor Predicting: 242it [02:31,  1.64it/s]Extractor Predicting: 243it [02:32,  1.60it/s]Extractor Predicting: 244it [02:32,  1.63it/s]Extractor Predicting: 245it [02:33,  1.64it/s]Extractor Predicting: 246it [02:33,  1.66it/s]Extractor Predicting: 247it [02:34,  1.61it/s]Extractor Predicting: 248it [02:35,  1.62it/s]Extractor Predicting: 249it [02:36,  1.31it/s]Extractor Predicting: 250it [02:36,  1.39it/s]Extractor Predicting: 251it [02:37,  1.45it/s]Extractor Predicting: 252it [02:38,  1.51it/s]Extractor Predicting: 253it [02:38,  1.56it/s]Extractor Predicting: 254it [02:39,  1.57it/s]Extractor Predicting: 255it [02:39,  1.58it/s]Extractor Predicting: 256it [02:40,  1.61it/s]Extractor Predicting: 257it [02:41,  1.66it/s]Extractor Predicting: 258it [02:41,  1.69it/s]Extractor Predicting: 259it [02:42,  1.71it/s]Extractor Predicting: 260it [02:43,  1.50it/s]Extractor Predicting: 261it [02:43,  1.56it/s]Extractor Predicting: 262it [02:44,  1.61it/s]Extractor Predicting: 263it [02:44,  1.67it/s]Extractor Predicting: 264it [02:45,  1.70it/s]Extractor Predicting: 265it [02:46,  1.62it/s]Extractor Predicting: 266it [02:46,  1.68it/s]Extractor Predicting: 267it [02:47,  1.73it/s]Extractor Predicting: 268it [02:47,  1.71it/s]Extractor Predicting: 269it [02:48,  1.74it/s]Extractor Predicting: 270it [02:48,  1.73it/s]Extractor Predicting: 271it [02:49,  1.72it/s]Extractor Predicting: 272it [02:50,  1.72it/s]Extractor Predicting: 273it [02:50,  1.72it/s]Extractor Predicting: 274it [02:51,  1.60it/s]Extractor Predicting: 275it [02:51,  1.66it/s]Extractor Predicting: 276it [02:52,  1.69it/s]Extractor Predicting: 277it [02:53,  1.69it/s]Extractor Predicting: 278it [02:53,  1.66it/s]Extractor Predicting: 279it [02:54,  1.70it/s]Extractor Predicting: 280it [02:54,  1.70it/s]Extractor Predicting: 281it [02:55,  1.73it/s]Extractor Predicting: 282it [02:55,  1.72it/s]Extractor Predicting: 283it [02:56,  1.68it/s]Extractor Predicting: 284it [02:57,  1.72it/s]Extractor Predicting: 285it [02:57,  1.69it/s]Extractor Predicting: 286it [02:58,  1.69it/s]Extractor Predicting: 287it [02:58,  1.72it/s]Extractor Predicting: 288it [02:59,  1.72it/s]Extractor Predicting: 289it [03:00,  1.50it/s]Extractor Predicting: 290it [03:00,  1.56it/s]Extractor Predicting: 291it [03:01,  1.57it/s]Extractor Predicting: 292it [03:02,  1.61it/s]Extractor Predicting: 293it [03:02,  1.63it/s]Extractor Predicting: 294it [03:03,  1.66it/s]Extractor Predicting: 295it [03:03,  1.67it/s]Extractor Predicting: 296it [03:04,  1.69it/s]Extractor Predicting: 297it [03:05,  1.71it/s]Extractor Predicting: 298it [03:05,  1.67it/s]Extractor Predicting: 299it [03:06,  1.72it/s]Extractor Predicting: 300it [03:06,  1.61it/s]Extractor Predicting: 301it [03:07,  1.65it/s]Extractor Predicting: 302it [03:08,  1.66it/s]Extractor Predicting: 303it [03:08,  1.47it/s]Extractor Predicting: 304it [03:09,  1.54it/s]Extractor Predicting: 305it [03:10,  1.62it/s]Extractor Predicting: 306it [03:10,  1.65it/s]Extractor Predicting: 307it [03:11,  1.66it/s]Extractor Predicting: 308it [03:11,  1.59it/s]Extractor Predicting: 309it [03:12,  1.65it/s]Extractor Predicting: 310it [03:13,  1.63it/s]Extractor Predicting: 311it [03:13,  1.64it/s]Extractor Predicting: 312it [03:14,  1.45it/s]Extractor Predicting: 313it [03:15,  1.53it/s]Extractor Predicting: 314it [03:15,  1.58it/s]Extractor Predicting: 315it [03:16,  1.65it/s]Extractor Predicting: 316it [03:16,  1.67it/s]Extractor Predicting: 317it [03:17,  1.48it/s]Extractor Predicting: 318it [03:18,  1.59it/s]Extractor Predicting: 319it [03:18,  1.63it/s]Extractor Predicting: 320it [03:19,  1.66it/s]Extractor Predicting: 321it [03:20,  1.67it/s]Extractor Predicting: 322it [03:20,  1.68it/s]Extractor Predicting: 323it [03:21,  1.69it/s]Extractor Predicting: 324it [03:21,  1.69it/s]Extractor Predicting: 325it [03:22,  1.69it/s]Extractor Predicting: 326it [03:23,  1.65it/s]Extractor Predicting: 327it [03:23,  1.64it/s]Extractor Predicting: 328it [03:24,  1.65it/s]Extractor Predicting: 329it [03:25,  1.38it/s]Extractor Predicting: 330it [03:25,  1.46it/s]Extractor Predicting: 331it [03:26,  1.52it/s]Extractor Predicting: 332it [03:26,  1.60it/s]Extractor Predicting: 333it [03:27,  1.56it/s]Extractor Predicting: 334it [03:28,  1.60it/s]Extractor Predicting: 335it [03:28,  1.63it/s]Extractor Predicting: 336it [03:29,  1.65it/s]Extractor Predicting: 337it [03:30,  1.31it/s]Extractor Predicting: 338it [03:31,  1.39it/s]Extractor Predicting: 339it [03:31,  1.46it/s]Extractor Predicting: 340it [03:32,  1.57it/s]Extractor Predicting: 341it [03:32,  1.59it/s]Extractor Predicting: 342it [03:33,  1.52it/s]Extractor Predicting: 343it [03:34,  1.53it/s]Extractor Predicting: 344it [03:34,  1.59it/s]Extractor Predicting: 345it [03:35,  1.60it/s]Extractor Predicting: 346it [03:36,  1.65it/s]Extractor Predicting: 347it [03:36,  1.67it/s]Extractor Predicting: 348it [03:37,  1.70it/s]Extractor Predicting: 349it [03:37,  1.63it/s]Extractor Predicting: 350it [03:38,  1.54it/s]Extractor Predicting: 351it [03:39,  1.56it/s]Extractor Predicting: 352it [03:39,  1.60it/s]Extractor Predicting: 353it [03:40,  1.62it/s]Extractor Predicting: 354it [03:40,  1.65it/s]Extractor Predicting: 355it [03:41,  1.65it/s]Extractor Predicting: 356it [03:42,  1.69it/s]Extractor Predicting: 357it [03:42,  1.67it/s]Extractor Predicting: 358it [03:43,  1.66it/s]Extractor Predicting: 359it [03:44,  1.41it/s]Extractor Predicting: 360it [03:44,  1.45it/s]Extractor Predicting: 361it [03:45,  1.54it/s]Extractor Predicting: 362it [03:46,  1.58it/s]Extractor Predicting: 363it [03:46,  1.60it/s]Extractor Predicting: 364it [03:47,  1.65it/s]Extractor Predicting: 365it [03:47,  1.68it/s]Extractor Predicting: 366it [03:48,  1.66it/s]Extractor Predicting: 367it [03:49,  1.64it/s]Extractor Predicting: 368it [03:49,  1.63it/s]Extractor Predicting: 369it [03:50,  1.67it/s]Extractor Predicting: 370it [03:50,  1.66it/s]Extractor Predicting: 371it [03:51,  1.53it/s]Extractor Predicting: 372it [03:52,  1.60it/s]Extractor Predicting: 373it [03:52,  1.65it/s]Extractor Predicting: 374it [03:53,  1.68it/s]Extractor Predicting: 375it [03:53,  1.68it/s]Extractor Predicting: 376it [03:54,  1.65it/s]Extractor Predicting: 377it [03:55,  1.68it/s]Extractor Predicting: 378it [03:55,  1.71it/s]Extractor Predicting: 379it [03:56,  1.74it/s]Extractor Predicting: 380it [03:56,  1.75it/s]Extractor Predicting: 381it [03:57,  1.75it/s]Extractor Predicting: 382it [03:57,  1.73it/s]Extractor Predicting: 383it [03:58,  1.76it/s]Extractor Predicting: 384it [03:59,  1.78it/s]Extractor Predicting: 385it [03:59,  1.60it/s]Extractor Predicting: 386it [04:00,  1.68it/s]Extractor Predicting: 387it [04:00,  1.73it/s]Extractor Predicting: 388it [04:01,  1.72it/s]Extractor Predicting: 389it [04:02,  1.74it/s]Extractor Predicting: 390it [04:02,  1.51it/s]Extractor Predicting: 391it [04:03,  1.58it/s]Extractor Predicting: 392it [04:04,  1.63it/s]Extractor Predicting: 393it [04:04,  1.58it/s]Extractor Predicting: 394it [04:05,  1.64it/s]Extractor Predicting: 395it [04:05,  1.62it/s]Extractor Predicting: 396it [04:06,  1.68it/s]Extractor Predicting: 397it [04:07,  1.18it/s]Extractor Predicting: 398it [04:08,  1.29it/s]Extractor Predicting: 399it [04:09,  1.40it/s]Extractor Predicting: 400it [04:09,  1.48it/s]Extractor Predicting: 401it [04:10,  1.18it/s]Extractor Predicting: 402it [04:11,  1.33it/s]Extractor Predicting: 403it [04:12,  1.43it/s]Extractor Predicting: 404it [04:13,  1.04it/s]Extractor Predicting: 405it [04:14,  1.19it/s]Extractor Predicting: 406it [04:14,  1.32it/s]Extractor Predicting: 407it [04:15,  1.42it/s]Extractor Predicting: 408it [04:15,  1.46it/s]Extractor Predicting: 409it [04:16,  1.56it/s]Extractor Predicting: 410it [04:17,  1.62it/s]Extractor Predicting: 411it [04:17,  1.64it/s]Extractor Predicting: 412it [04:18,  1.71it/s]Extractor Predicting: 413it [04:19,  1.45it/s]Extractor Predicting: 414it [04:19,  1.54it/s]Extractor Predicting: 415it [04:20,  1.64it/s]Extractor Predicting: 416it [04:20,  1.69it/s]Extractor Predicting: 417it [04:21,  1.34it/s]Extractor Predicting: 418it [04:22,  1.46it/s]Extractor Predicting: 419it [04:22,  1.54it/s]Extractor Predicting: 420it [04:23,  1.61it/s]Extractor Predicting: 421it [04:25,  1.09s/it]Extractor Predicting: 422it [04:26,  1.07it/s]Extractor Predicting: 423it [04:26,  1.21it/s]Extractor Predicting: 424it [04:27,  1.28it/s]Extractor Predicting: 425it [04:27,  1.55it/s]Extractor Predicting: 425it [04:27,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:41,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:53:42,915 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:53:42,916 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:53:43,715 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:53:44,959 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:53:45,509 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:50,058 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:50,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:50,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:50,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:53:50,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:53:51,659 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:53:51,660 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:53:52,830 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:53:53,735 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:53:53,912 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.48it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.84it/s]Extractor Predicting: 7it [00:04,  1.64it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_3', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:18<05:45, 18.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:38<05:46, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:55<05:11, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [01:13<04:49, 18.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:34<04:47, 19.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:51<04:20, 18.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [02:13<04:15, 19.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [02:31<03:49, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:50<03:29, 19.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [03:08<03:08, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [03:25<02:43, 18.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:44<02:26, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [04:01<02:05, 17.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [04:21<01:51, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [04:39<01:32, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [05:00<01:17, 19.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [05:18<00:56, 18.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [05:35<00:36, 18.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [05:59<00:19, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [06:16<00:00, 19.13s/it]Generating: 100%|| 20/20 [06:16<00:00, 18.83s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Bourgeois was working for the French newspaper La Runion in the suburbs of Montferrat . Head Entity : Le Runion , Tail Entity : Montferrat .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 211, 'raw': 320}
{'target': 600, 'success': 235, 'raw': 352}
{'target': 600, 'success': 256, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 328, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 392, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 434, 'raw': 640}
{'target': 600, 'success': 457, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 503, 'raw': 736}
{'target': 600, 'success': 526, 'raw': 768}
{'target': 600, 'success': 548, 'raw': 800}
{'target': 600, 'success': 568, 'raw': 832}
{'target': 600, 'success': 596, 'raw': 864}
{'target': 600, 'success': 621, 'raw': 896}
{'prompt': 'Relation : location .', 'success_rate': 0.6930803571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.76, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The CIB has the largest number of active user data storage and management devices in existence . Head Entity : CIB , Tail Entity : CIDE .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 37, 'raw': 64}
{'target': 600, 'success': 61, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 556, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.76125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 104, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 146, 'raw': 224}
{'target': 600, 'success': 165, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 204, 'raw': 320}
{'target': 600, 'success': 228, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 260, 'raw': 416}
{'target': 600, 'success': 282, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 336, 'raw': 544}
{'target': 600, 'success': 357, 'raw': 576}
{'target': 600, 'success': 374, 'raw': 608}
{'target': 600, 'success': 395, 'raw': 640}
{'target': 600, 'success': 414, 'raw': 672}
{'target': 600, 'success': 438, 'raw': 704}
{'target': 600, 'success': 458, 'raw': 736}
{'target': 600, 'success': 476, 'raw': 768}
{'target': 600, 'success': 497, 'raw': 800}
{'target': 600, 'success': 517, 'raw': 832}
{'target': 600, 'success': 535, 'raw': 864}
{'target': 600, 'success': 551, 'raw': 896}
{'target': 600, 'success': 569, 'raw': 928}
{'target': 600, 'success': 590, 'raw': 960}
{'target': 600, 'success': 610, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6149193548387096, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 166, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 274, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 314, 'raw': 480}
{'target': 600, 'success': 337, 'raw': 512}
{'target': 600, 'success': 355, 'raw': 544}
{'target': 600, 'success': 375, 'raw': 576}
{'target': 600, 'success': 397, 'raw': 608}
{'target': 600, 'success': 415, 'raw': 640}
{'target': 600, 'success': 434, 'raw': 672}
{'target': 600, 'success': 459, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 496, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 542, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 586, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : competition class .', 'success_rate': 0.6530172413793104, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 170, 'raw': 256}
{'target': 600, 'success': 195, 'raw': 288}
{'target': 600, 'success': 215, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 267, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 556, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 603, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.6979166666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : father . Context : Later in Life , he was taken under the tutelage of his fourth son , Alexander the Great , who was married to a Roman knight named Horace , daughter of Alexander , and crowned King of Poland in 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 601, 'raw': 800}
{'prompt': 'Relation : father .', 'success_rate': 0.75125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n']
['Relation : instrument . Context : The Cello Strade is a classical rock opera performed by French operental virtuoso William Barlow , in which Barlow plays the organ . Head Entity : Cello Strade , Tail Entity : violin .\n', 'Relation : instrument . Context : Eros and the Cephalopodal Equus are two - operatic groups of the cephalopodal tuskelet , a small but effective tuskelet (   ) . Head Entity :   , Tail Entity : tuskelet .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 316, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 564, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 418, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 580, 'raw': 800}
{'target': 600, 'success': 599, 'raw': 832}
{'target': 600, 'success': 622, 'raw': 864}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.7199074074074074, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 453, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 621, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.77625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : On 31 March 2014 , the Romanian government appointed him a Vice President of the National Party . Head Entity : Prusarec , Tail Entity : Romanian .\n']
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 121, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 164, 'raw': 256}
{'target': 600, 'success': 187, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 231, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 272, 'raw': 416}
{'target': 600, 'success': 294, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 366, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 413, 'raw': 608}
{'target': 600, 'success': 439, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 483, 'raw': 704}
{'target': 600, 'success': 506, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 577, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', "('Church of America in the City of Los Angeles', 'occupation', '', 'The Church of America in the City of Los Angeles was founded in 1866 and the Church of America in the City of South Los Angeles was founded in 1868 .')", 'not enough values to unpack (expected 2, got 1)', "('Governor of New York City', 'occupation', '', 'He served as the Governor of New York City in two terms from 1872 , 1884 and 1897 , before resigning from office in 1892 .')"}}
['Relation : original broadcaster . Context : Later in the year , the band formed the independent band The Three Kingdoms , which reached number five on the New York Times \' " Fast Times " . Head Entity : The Three Kingdoms , Tail Entity : The Times .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8138020833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.8342391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 51, 'raw': 96}
{'target': 600, 'success': 71, 'raw': 128}
{'target': 600, 'success': 81, 'raw': 160}
{'target': 600, 'success': 97, 'raw': 192}
{'target': 600, 'success': 116, 'raw': 224}
{'target': 600, 'success': 133, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 158, 'raw': 320}
{'target': 600, 'success': 177, 'raw': 352}
{'target': 600, 'success': 197, 'raw': 384}
{'target': 600, 'success': 215, 'raw': 416}
{'target': 600, 'success': 239, 'raw': 448}
{'target': 600, 'success': 254, 'raw': 480}
{'target': 600, 'success': 275, 'raw': 512}
{'target': 600, 'success': 293, 'raw': 544}
{'target': 600, 'success': 307, 'raw': 576}
{'target': 600, 'success': 319, 'raw': 608}
{'target': 600, 'success': 336, 'raw': 640}
{'target': 600, 'success': 357, 'raw': 672}
{'target': 600, 'success': 372, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 424, 'raw': 800}
{'target': 600, 'success': 446, 'raw': 832}
{'target': 600, 'success': 462, 'raw': 864}
{'target': 600, 'success': 480, 'raw': 896}
{'target': 600, 'success': 497, 'raw': 928}
{'target': 600, 'success': 515, 'raw': 960}
{'target': 600, 'success': 531, 'raw': 992}
{'target': 600, 'success': 548, 'raw': 1024}
{'target': 600, 'success': 570, 'raw': 1056}
{'target': 600, 'success': 587, 'raw': 1088}
{'target': 600, 'success': 603, 'raw': 1120}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5383928571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 16635
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16735, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:19, 19.14s/it]Extractor Estimating: 2it [00:21,  9.03s/it]Extractor Estimating: 3it [00:22,  5.45s/it]Extractor Estimating: 4it [00:23,  3.61s/it]Extractor Estimating: 5it [00:24,  2.76s/it]Extractor Estimating: 6it [00:24,  2.03s/it]Extractor Estimating: 7it [00:26,  1.76s/it]Extractor Estimating: 8it [00:26,  1.47s/it]Extractor Estimating: 9it [00:27,  1.21s/it]Extractor Estimating: 10it [00:28,  1.04s/it]Extractor Estimating: 11it [00:28,  1.12it/s]Extractor Estimating: 12it [00:29,  1.21it/s]Extractor Estimating: 13it [00:30,  1.25it/s]Extractor Estimating: 14it [00:30,  1.31it/s]Extractor Estimating: 15it [00:31,  1.35it/s]Extractor Estimating: 16it [00:32,  1.39it/s]Extractor Estimating: 17it [00:32,  1.41it/s]Extractor Estimating: 18it [00:34,  1.19it/s]Extractor Estimating: 19it [00:34,  1.32it/s]Extractor Estimating: 20it [00:35,  1.37it/s]Extractor Estimating: 21it [00:37,  1.04s/it]Extractor Estimating: 22it [00:38,  1.00it/s]Extractor Estimating: 23it [00:38,  1.11it/s]Extractor Estimating: 24it [00:39,  1.19it/s]Extractor Estimating: 25it [00:40,  1.30it/s]Extractor Estimating: 26it [00:40,  1.37it/s]Extractor Estimating: 27it [00:41,  1.43it/s]Extractor Estimating: 28it [00:41,  1.46it/s]Extractor Estimating: 29it [00:42,  1.53it/s]Extractor Estimating: 30it [00:43,  1.60it/s]Extractor Estimating: 31it [00:44,  1.11it/s]Extractor Estimating: 32it [00:45,  1.24it/s]Extractor Estimating: 33it [00:45,  1.29it/s]Extractor Estimating: 34it [00:46,  1.20it/s]Extractor Estimating: 35it [00:47,  1.33it/s]Extractor Estimating: 36it [00:48,  1.37it/s]Extractor Estimating: 37it [00:48,  1.42it/s]Extractor Estimating: 38it [00:49,  1.42it/s]Extractor Estimating: 39it [00:50,  1.44it/s]Extractor Estimating: 40it [00:50,  1.47it/s]Extractor Estimating: 41it [00:51,  1.47it/s]Extractor Estimating: 42it [00:52,  1.54it/s]Extractor Estimating: 43it [00:52,  1.54it/s]Extractor Estimating: 44it [00:53,  1.42it/s]Extractor Estimating: 45it [00:54,  1.49it/s]Extractor Estimating: 46it [00:55,  1.17it/s]Extractor Estimating: 47it [00:56,  1.25it/s]Extractor Estimating: 48it [00:56,  1.27it/s]Extractor Estimating: 49it [00:57,  1.34it/s]Extractor Estimating: 50it [00:58,  1.36it/s]Extractor Estimating: 51it [00:58,  1.42it/s]Extractor Estimating: 52it [00:59,  1.50it/s]Extractor Estimating: 53it [01:00,  1.47it/s]Extractor Estimating: 54it [01:00,  1.54it/s]Extractor Estimating: 55it [01:01,  1.59it/s]Extractor Estimating: 56it [01:01,  1.61it/s]Extractor Estimating: 57it [01:02,  1.56it/s]Extractor Estimating: 58it [01:03,  1.38it/s]Extractor Estimating: 59it [01:04,  1.43it/s]Extractor Estimating: 60it [01:04,  1.46it/s]Extractor Estimating: 61it [01:05,  1.46it/s]Extractor Estimating: 62it [01:06,  1.53it/s]Extractor Estimating: 63it [01:06,  1.40it/s]Extractor Estimating: 64it [01:07,  1.43it/s]Extractor Estimating: 65it [01:08,  1.45it/s]Extractor Estimating: 66it [01:08,  1.53it/s]Extractor Estimating: 67it [01:09,  1.58it/s]Extractor Estimating: 68it [01:10,  1.49it/s]Extractor Estimating: 69it [01:10,  1.52it/s]Extractor Estimating: 70it [01:11,  1.49it/s]Extractor Estimating: 71it [01:12,  1.50it/s]Extractor Estimating: 72it [01:12,  1.53it/s]Extractor Estimating: 73it [01:13,  1.40it/s]Extractor Estimating: 74it [01:14,  1.41it/s]Extractor Estimating: 75it [01:14,  1.47it/s]Extractor Estimating: 76it [01:15,  1.54it/s]Extractor Estimating: 77it [01:16,  1.58it/s]Extractor Estimating: 78it [01:16,  1.59it/s]Extractor Estimating: 79it [01:17,  1.49it/s]Extractor Estimating: 80it [01:18,  1.51it/s]Extractor Estimating: 81it [01:18,  1.44it/s]Extractor Estimating: 82it [01:19,  1.48it/s]Extractor Estimating: 83it [01:20,  1.49it/s]Extractor Estimating: 84it [01:21,  1.25it/s]Extractor Estimating: 85it [01:21,  1.30it/s]Extractor Estimating: 86it [01:24,  1.15s/it]Extractor Estimating: 87it [01:24,  1.04s/it]Extractor Estimating: 88it [01:25,  1.10it/s]Extractor Estimating: 89it [01:26,  1.22it/s]Extractor Estimating: 90it [01:26,  1.26it/s]Extractor Estimating: 91it [01:27,  1.33it/s]Extractor Estimating: 92it [01:28,  1.39it/s]Extractor Estimating: 93it [01:28,  1.44it/s]Extractor Estimating: 94it [01:29,  1.51it/s]Extractor Estimating: 95it [01:29,  1.55it/s]Extractor Estimating: 96it [01:30,  1.51it/s]Extractor Estimating: 97it [01:31,  1.45it/s]Extractor Estimating: 98it [01:31,  1.53it/s]Extractor Estimating: 99it [01:32,  1.51it/s]Extractor Estimating: 100it [01:33,  1.53it/s]Extractor Estimating: 101it [01:33,  1.53it/s]Extractor Estimating: 102it [01:34,  1.37it/s]Extractor Estimating: 103it [01:35,  1.42it/s]Extractor Estimating: 104it [01:36,  1.44it/s]Extractor Estimating: 105it [01:36,  1.51it/s]Extractor Estimating: 106it [01:37,  1.53it/s]Extractor Estimating: 107it [01:38,  1.29it/s]Extractor Estimating: 108it [01:38,  1.40it/s]Extractor Estimating: 109it [01:39,  1.47it/s]Extractor Estimating: 110it [01:40,  1.48it/s]Extractor Estimating: 111it [01:40,  1.53it/s]Extractor Estimating: 112it [01:41,  1.32it/s]Extractor Estimating: 113it [01:42,  1.42it/s]Extractor Estimating: 114it [01:43,  1.47it/s]Extractor Estimating: 115it [01:43,  1.51it/s]Extractor Estimating: 116it [01:44,  1.55it/s]Extractor Estimating: 117it [01:45,  1.18it/s]Extractor Estimating: 118it [01:46,  1.30it/s]Extractor Estimating: 119it [01:46,  1.38it/s]Extractor Estimating: 120it [01:47,  1.42it/s]Extractor Estimating: 121it [01:48,  1.30it/s]Extractor Estimating: 122it [01:48,  1.38it/s]Extractor Estimating: 123it [01:49,  1.46it/s]Extractor Estimating: 124it [01:50,  1.49it/s]Extractor Estimating: 125it [01:50,  1.55it/s]Extractor Estimating: 126it [01:51,  1.43it/s]Extractor Estimating: 127it [01:52,  1.46it/s]Extractor Estimating: 128it [01:52,  1.46it/s]Extractor Estimating: 129it [01:53,  1.50it/s]Extractor Estimating: 130it [01:54,  1.47it/s]Extractor Estimating: 131it [01:54,  1.44it/s]Extractor Estimating: 132it [01:55,  1.43it/s]Extractor Estimating: 133it [01:56,  1.46it/s]Extractor Estimating: 134it [01:57,  1.45it/s]Extractor Estimating: 135it [01:57,  1.46it/s]Extractor Estimating: 136it [01:58,  1.41it/s]Extractor Estimating: 137it [01:59,  1.44it/s]Extractor Estimating: 138it [01:59,  1.45it/s]Extractor Estimating: 139it [02:00,  1.44it/s]Extractor Estimating: 140it [02:01,  1.47it/s]Extractor Estimating: 141it [02:01,  1.42it/s]Extractor Estimating: 142it [02:02,  1.47it/s]Extractor Estimating: 143it [02:03,  1.53it/s]Extractor Estimating: 144it [02:03,  1.50it/s]Extractor Estimating: 145it [02:04,  1.46it/s]Extractor Estimating: 146it [02:05,  1.39it/s]Extractor Estimating: 147it [02:06,  1.43it/s]Extractor Estimating: 148it [02:06,  1.42it/s]Extractor Estimating: 149it [02:07,  1.35it/s]Extractor Estimating: 150it [02:08,  1.41it/s]Extractor Estimating: 151it [02:09,  1.26it/s]Extractor Estimating: 152it [02:09,  1.37it/s]Extractor Estimating: 153it [02:10,  1.49it/s]Extractor Estimating: 154it [02:10,  1.53it/s]Extractor Estimating: 155it [02:11,  1.56it/s]Extractor Estimating: 156it [02:12,  1.55it/s]Extractor Estimating: 157it [02:12,  1.60it/s]Extractor Estimating: 158it [02:13,  1.65it/s]Extractor Estimating: 159it [02:13,  1.71it/s]Extractor Estimating: 160it [02:14,  1.69it/s]Extractor Estimating: 161it [02:15,  1.66it/s]Extractor Estimating: 162it [02:15,  1.56it/s]Extractor Estimating: 163it [02:16,  1.53it/s]Extractor Estimating: 164it [02:17,  1.58it/s]Extractor Estimating: 165it [02:17,  1.61it/s]Extractor Estimating: 166it [02:18,  1.57it/s]Extractor Estimating: 167it [02:19,  1.33it/s]Extractor Estimating: 168it [02:20,  1.40it/s]Extractor Estimating: 169it [02:20,  1.46it/s]Extractor Estimating: 170it [02:21,  1.50it/s]Extractor Estimating: 171it [02:21,  1.52it/s]Extractor Estimating: 172it [02:23,  1.27it/s]Extractor Estimating: 173it [02:23,  1.39it/s]Extractor Estimating: 174it [02:24,  1.47it/s]Extractor Estimating: 175it [02:24,  1.52it/s]Extractor Estimating: 176it [02:25,  1.56it/s]Extractor Estimating: 177it [02:26,  1.40it/s]Extractor Estimating: 178it [02:26,  1.45it/s]Extractor Estimating: 179it [02:27,  1.42it/s]Extractor Estimating: 180it [02:28,  1.50it/s]Extractor Estimating: 181it [02:28,  1.56it/s]Extractor Estimating: 182it [02:29,  1.52it/s]Extractor Estimating: 183it [02:30,  1.52it/s]Extractor Estimating: 184it [02:30,  1.56it/s]Extractor Estimating: 185it [02:31,  1.60it/s]Extractor Estimating: 186it [02:31,  1.60it/s]Extractor Estimating: 187it [02:32,  1.50it/s]Extractor Estimating: 188it [02:33,  1.49it/s]Extractor Estimating: 189it [02:34,  1.53it/s]Extractor Estimating: 190it [02:34,  1.56it/s]Extractor Estimating: 191it [02:35,  1.61it/s]Extractor Estimating: 192it [02:35,  1.49it/s]Extractor Estimating: 193it [02:36,  1.58it/s]Extractor Estimating: 194it [02:37,  1.59it/s]Extractor Estimating: 195it [02:37,  1.59it/s]Extractor Estimating: 196it [02:38,  1.52it/s]Extractor Estimating: 197it [02:39,  1.39it/s]Extractor Estimating: 198it [02:39,  1.48it/s]Extractor Estimating: 199it [02:40,  1.54it/s]Extractor Estimating: 200it [02:41,  1.59it/s]Extractor Estimating: 201it [02:41,  1.62it/s]Extractor Estimating: 202it [02:42,  1.56it/s]Extractor Estimating: 203it [02:43,  1.57it/s]Extractor Estimating: 204it [02:43,  1.56it/s]Extractor Estimating: 205it [02:44,  1.57it/s]Extractor Estimating: 206it [02:44,  1.54it/s]Extractor Estimating: 207it [02:45,  1.41it/s]Extractor Estimating: 208it [02:46,  1.39it/s]Extractor Estimating: 209it [02:47,  1.46it/s]Extractor Estimating: 210it [02:47,  1.49it/s]Extractor Estimating: 211it [02:48,  1.41it/s]Extractor Estimating: 212it [02:49,  1.49it/s]Extractor Estimating: 213it [02:49,  1.41it/s]Extractor Estimating: 214it [02:50,  1.44it/s]Extractor Estimating: 215it [02:51,  1.47it/s]Extractor Estimating: 216it [02:51,  1.55it/s]Extractor Estimating: 217it [02:52,  1.59it/s]Extractor Estimating: 218it [02:53,  1.24it/s]Extractor Estimating: 219it [02:54,  1.30it/s]Extractor Estimating: 220it [02:55,  1.32it/s]Extractor Estimating: 221it [02:55,  1.39it/s]Extractor Estimating: 222it [02:56,  1.36it/s]Extractor Estimating: 223it [02:57,  1.44it/s]Extractor Estimating: 224it [02:57,  1.47it/s]Extractor Estimating: 225it [02:58,  1.45it/s]Extractor Estimating: 226it [02:59,  1.46it/s]Extractor Estimating: 227it [02:59,  1.42it/s]Extractor Estimating: 228it [03:00,  1.36it/s]Extractor Estimating: 229it [03:01,  1.40it/s]Extractor Estimating: 230it [03:01,  1.48it/s]Extractor Estimating: 231it [03:02,  1.53it/s]Extractor Estimating: 232it [03:03,  1.39it/s]Extractor Estimating: 233it [03:03,  1.46it/s]Extractor Estimating: 234it [03:04,  1.45it/s]Extractor Estimating: 235it [03:05,  1.49it/s]Extractor Estimating: 236it [03:05,  1.53it/s]Extractor Estimating: 237it [03:06,  1.48it/s]Extractor Estimating: 238it [03:07,  1.53it/s]Extractor Estimating: 239it [03:07,  1.52it/s]Extractor Estimating: 240it [03:08,  1.49it/s]Extractor Estimating: 241it [03:09,  1.51it/s]Extractor Estimating: 242it [03:10,  1.37it/s]Extractor Estimating: 243it [03:10,  1.40it/s]Extractor Estimating: 244it [03:11,  1.40it/s]Extractor Estimating: 245it [03:12,  1.42it/s]Extractor Estimating: 246it [03:12,  1.49it/s]Extractor Estimating: 247it [03:13,  1.32it/s]Extractor Estimating: 248it [03:14,  1.36it/s]Extractor Estimating: 249it [03:15,  1.41it/s]Extractor Estimating: 250it [03:15,  1.43it/s]Extractor Estimating: 251it [03:16,  1.52it/s]Extractor Estimating: 252it [03:17,  1.19it/s]Extractor Estimating: 253it [03:18,  1.25it/s]Extractor Estimating: 254it [03:19,  1.31it/s]Extractor Estimating: 255it [03:19,  1.38it/s]Extractor Estimating: 256it [03:20,  1.10it/s]Extractor Estimating: 257it [03:21,  1.20it/s]Extractor Estimating: 258it [03:22,  1.30it/s]Extractor Estimating: 259it [03:22,  1.39it/s]Extractor Estimating: 260it [03:23,  1.34it/s]Extractor Estimating: 261it [03:24,  1.37it/s]Extractor Estimating: 262it [03:24,  1.45it/s]Extractor Estimating: 263it [03:25,  1.44it/s]Extractor Estimating: 264it [03:26,  1.48it/s]Extractor Estimating: 265it [03:27,  1.41it/s]Extractor Estimating: 266it [03:27,  1.45it/s]Extractor Estimating: 267it [03:28,  1.49it/s]Extractor Estimating: 268it [03:29,  1.49it/s]Extractor Estimating: 269it [03:29,  1.47it/s]Extractor Estimating: 270it [03:30,  1.43it/s]Extractor Estimating: 271it [03:31,  1.45it/s]Extractor Estimating: 272it [03:31,  1.50it/s]Extractor Estimating: 273it [03:32,  1.50it/s]Extractor Estimating: 274it [03:33,  1.47it/s]Extractor Estimating: 275it [03:33,  1.48it/s]Extractor Estimating: 276it [03:34,  1.53it/s]Extractor Estimating: 277it [03:35,  1.50it/s]Extractor Estimating: 278it [03:35,  1.53it/s]Extractor Estimating: 279it [03:36,  1.51it/s]Extractor Estimating: 280it [03:37,  1.42it/s]Extractor Estimating: 281it [03:37,  1.39it/s]Extractor Estimating: 282it [03:38,  1.46it/s]Extractor Estimating: 283it [03:39,  1.50it/s]Extractor Estimating: 284it [03:39,  1.52it/s]Extractor Estimating: 285it [03:40,  1.47it/s]Extractor Estimating: 286it [03:41,  1.51it/s]Extractor Estimating: 287it [03:41,  1.53it/s]Extractor Estimating: 288it [03:42,  1.56it/s]Extractor Estimating: 289it [03:43,  1.59it/s]Extractor Estimating: 290it [03:44,  1.17it/s]Extractor Estimating: 291it [03:45,  1.27it/s]Extractor Estimating: 292it [03:45,  1.31it/s]Extractor Estimating: 293it [03:46,  1.40it/s]Extractor Estimating: 294it [03:47,  1.17it/s]Extractor Estimating: 295it [03:48,  1.28it/s]Extractor Estimating: 296it [03:48,  1.32it/s]Extractor Estimating: 297it [03:49,  1.41it/s]Extractor Estimating: 298it [03:50,  1.44it/s]Extractor Estimating: 299it [03:50,  1.46it/s]Extractor Estimating: 300it [03:51,  1.24it/s]Extractor Estimating: 301it [03:52,  1.29it/s]Extractor Estimating: 302it [03:53,  1.35it/s]Extractor Estimating: 303it [03:53,  1.37it/s]Extractor Estimating: 304it [03:55,  1.02it/s]Extractor Estimating: 305it [03:56,  1.12it/s]Extractor Estimating: 306it [03:56,  1.24it/s]Extractor Estimating: 307it [03:57,  1.30it/s]Extractor Estimating: 308it [03:58,  1.30it/s]Extractor Estimating: 309it [03:58,  1.39it/s]Extractor Estimating: 310it [03:59,  1.41it/s]Extractor Estimating: 311it [04:00,  1.47it/s]Extractor Estimating: 312it [04:00,  1.50it/s]Extractor Estimating: 313it [04:01,  1.41it/s]Extractor Estimating: 314it [04:02,  1.48it/s]Extractor Estimating: 315it [04:02,  1.54it/s]Extractor Estimating: 316it [04:03,  1.53it/s]Extractor Estimating: 317it [04:04,  1.56it/s]Extractor Estimating: 318it [04:04,  1.41it/s]Extractor Estimating: 319it [04:05,  1.48it/s]Extractor Estimating: 320it [04:06,  1.55it/s]Extractor Estimating: 321it [04:06,  1.48it/s]Extractor Estimating: 322it [04:07,  1.52it/s]Extractor Estimating: 323it [04:08,  1.29it/s]Extractor Estimating: 324it [04:09,  1.36it/s]Extractor Estimating: 325it [04:09,  1.43it/s]Extractor Estimating: 326it [04:10,  1.51it/s]Extractor Estimating: 327it [04:10,  1.56it/s]Extractor Estimating: 328it [04:11,  1.49it/s]Extractor Estimating: 329it [04:12,  1.55it/s]Extractor Estimating: 330it [04:12,  1.56it/s]Extractor Estimating: 331it [04:13,  1.56it/s]Extractor Estimating: 332it [04:14,  1.54it/s]Extractor Estimating: 333it [04:14,  1.48it/s]Extractor Estimating: 334it [04:15,  1.50it/s]Extractor Estimating: 335it [04:16,  1.55it/s]Extractor Estimating: 336it [04:16,  1.62it/s]Extractor Estimating: 337it [04:17,  1.67it/s]Extractor Estimating: 338it [04:17,  1.64it/s]Extractor Estimating: 339it [04:19,  1.23it/s]Extractor Estimating: 340it [04:19,  1.35it/s]Extractor Estimating: 341it [04:20,  1.48it/s]Extractor Estimating: 342it [04:20,  1.53it/s]Extractor Estimating: 343it [04:21,  1.60it/s]Extractor Estimating: 344it [04:22,  1.58it/s]Extractor Estimating: 345it [04:22,  1.64it/s]Extractor Estimating: 346it [04:23,  1.59it/s]Extractor Estimating: 347it [04:23,  1.56it/s]Extractor Estimating: 348it [04:24,  1.58it/s]Extractor Estimating: 349it [04:25,  1.24it/s]Extractor Estimating: 350it [04:26,  1.33it/s]Extractor Estimating: 351it [04:27,  1.39it/s]Extractor Estimating: 352it [04:27,  1.47it/s]Extractor Estimating: 353it [04:28,  1.49it/s]Extractor Estimating: 354it [04:29,  1.48it/s]Extractor Estimating: 355it [04:29,  1.52it/s]Extractor Estimating: 356it [04:30,  1.59it/s]Extractor Estimating: 357it [04:30,  1.58it/s]Extractor Estimating: 358it [04:31,  1.53it/s]Extractor Estimating: 359it [04:32,  1.57it/s]Extractor Estimating: 360it [04:32,  1.55it/s]Extractor Estimating: 361it [04:33,  1.48it/s]Extractor Estimating: 362it [04:34,  1.53it/s]Extractor Estimating: 363it [04:34,  1.48it/s]Extractor Estimating: 364it [04:35,  1.49it/s]Extractor Estimating: 365it [04:36,  1.51it/s]Extractor Estimating: 366it [04:36,  1.51it/s]Extractor Estimating: 367it [04:37,  1.50it/s]Extractor Estimating: 368it [04:38,  1.47it/s]Extractor Estimating: 369it [04:38,  1.52it/s]Extractor Estimating: 370it [04:39,  1.49it/s]Extractor Estimating: 371it [04:40,  1.54it/s]Extractor Estimating: 372it [04:40,  1.59it/s]Extractor Estimating: 373it [04:41,  1.48it/s]Extractor Estimating: 374it [04:42,  1.55it/s]Extractor Estimating: 375it [04:42,  1.58it/s]Extractor Estimating: 376it [04:43,  1.53it/s]Extractor Estimating: 377it [04:44,  1.52it/s]Extractor Estimating: 378it [04:44,  1.36it/s]Extractor Estimating: 379it [04:45,  1.41it/s]Extractor Estimating: 380it [04:46,  1.50it/s]Extractor Estimating: 381it [04:46,  1.55it/s]Extractor Estimating: 382it [04:47,  1.51it/s]Extractor Estimating: 383it [04:48,  1.44it/s]Extractor Estimating: 384it [04:48,  1.53it/s]Extractor Estimating: 385it [04:50,  1.10it/s]Extractor Estimating: 386it [04:50,  1.21it/s]Extractor Estimating: 387it [04:51,  1.18it/s]Extractor Estimating: 388it [04:52,  1.17it/s]Extractor Estimating: 389it [04:53,  1.24it/s]Extractor Estimating: 390it [04:53,  1.36it/s]Extractor Estimating: 391it [04:54,  1.40it/s]Extractor Estimating: 392it [04:55,  1.43it/s]Extractor Estimating: 393it [04:55,  1.45it/s]Extractor Estimating: 394it [04:56,  1.48it/s]Extractor Estimating: 395it [04:57,  1.50it/s]Extractor Estimating: 396it [04:57,  1.49it/s]Extractor Estimating: 397it [04:58,  1.54it/s]Extractor Estimating: 398it [04:59,  1.49it/s]Extractor Estimating: 399it [04:59,  1.51it/s]Extractor Estimating: 400it [05:00,  1.53it/s]Extractor Estimating: 401it [05:01,  1.51it/s]Extractor Estimating: 402it [05:01,  1.56it/s]Extractor Estimating: 403it [05:02,  1.39it/s]Extractor Estimating: 404it [05:03,  1.45it/s]Extractor Estimating: 405it [05:03,  1.51it/s]Extractor Estimating: 406it [05:04,  1.52it/s]Extractor Estimating: 407it [05:05,  1.53it/s]Extractor Estimating: 408it [05:05,  1.52it/s]Extractor Estimating: 409it [05:06,  1.55it/s]Extractor Estimating: 410it [05:07,  1.55it/s]Extractor Estimating: 411it [05:07,  1.52it/s]Extractor Estimating: 412it [05:08,  1.45it/s]Extractor Estimating: 413it [05:09,  1.46it/s]Extractor Estimating: 414it [05:09,  1.53it/s]Extractor Estimating: 415it [05:10,  1.58it/s]Extractor Estimating: 416it [05:11,  1.56it/s]Extractor Estimating: 417it [05:11,  1.58it/s]Extractor Estimating: 418it [05:12,  1.41it/s]Extractor Estimating: 419it [05:13,  1.44it/s]Extractor Estimating: 420it [05:13,  1.51it/s]Extractor Estimating: 421it [05:14,  1.44it/s]Extractor Estimating: 422it [05:15,  1.44it/s]Extractor Estimating: 423it [05:16,  1.38it/s]Extractor Estimating: 424it [05:16,  1.44it/s]Extractor Estimating: 425it [05:17,  1.53it/s]Extractor Estimating: 426it [05:17,  1.51it/s]Extractor Estimating: 427it [05:18,  1.50it/s]Extractor Estimating: 428it [05:19,  1.39it/s]Extractor Estimating: 429it [05:20,  1.35it/s]Extractor Estimating: 430it [05:20,  1.37it/s]Extractor Estimating: 431it [05:21,  1.45it/s]Extractor Estimating: 432it [05:22,  1.39it/s]Extractor Estimating: 433it [05:23,  1.33it/s]Extractor Estimating: 434it [05:23,  1.39it/s]Extractor Estimating: 435it [05:24,  1.39it/s]Extractor Estimating: 436it [05:25,  1.36it/s]Extractor Estimating: 437it [05:26,  1.37it/s]Extractor Estimating: 438it [05:26,  1.34it/s]Extractor Estimating: 439it [05:27,  1.37it/s]Extractor Estimating: 440it [05:28,  1.44it/s]Extractor Estimating: 441it [05:28,  1.42it/s]Extractor Estimating: 442it [05:29,  1.46it/s]Extractor Estimating: 443it [05:30,  1.37it/s]Extractor Estimating: 444it [05:31,  1.40it/s]Extractor Estimating: 445it [05:31,  1.46it/s]Extractor Estimating: 446it [05:32,  1.48it/s]Extractor Estimating: 447it [05:32,  1.50it/s]Extractor Estimating: 448it [05:33,  1.38it/s]Extractor Estimating: 449it [05:34,  1.39it/s]Extractor Estimating: 450it [05:35,  1.44it/s]Extractor Estimating: 451it [05:35,  1.46it/s]Extractor Estimating: 452it [05:36,  1.49it/s]Extractor Estimating: 453it [05:37,  1.41it/s]Extractor Estimating: 454it [05:37,  1.45it/s]Extractor Estimating: 455it [05:38,  1.57it/s]Extractor Estimating: 456it [05:39,  1.53it/s]Extractor Estimating: 457it [05:39,  1.57it/s]Extractor Estimating: 458it [05:40,  1.59it/s]Extractor Estimating: 459it [05:40,  1.64it/s]Extractor Estimating: 460it [05:41,  1.61it/s]Extractor Estimating: 461it [05:42,  1.65it/s]Extractor Estimating: 462it [05:42,  1.66it/s]Extractor Estimating: 463it [05:43,  1.71it/s]Extractor Estimating: 464it [05:43,  1.70it/s]Extractor Estimating: 465it [05:44,  1.70it/s]Extractor Estimating: 466it [05:44,  1.70it/s]Extractor Estimating: 467it [05:45,  1.72it/s]Extractor Estimating: 468it [05:46,  1.72it/s]Extractor Estimating: 469it [05:46,  1.72it/s]Extractor Estimating: 470it [05:47,  1.68it/s]Extractor Estimating: 471it [05:47,  1.68it/s]Extractor Estimating: 472it [05:48,  1.63it/s]Extractor Estimating: 473it [05:49,  1.62it/s]Extractor Estimating: 474it [05:49,  1.64it/s]Extractor Estimating: 475it [05:51,  1.07it/s]Extractor Estimating: 476it [05:52,  1.21it/s]Extractor Estimating: 477it [05:52,  1.24it/s]Extractor Estimating: 478it [05:53,  1.34it/s]Extractor Estimating: 479it [05:54,  1.22it/s]Extractor Estimating: 480it [05:55,  1.31it/s]Extractor Estimating: 481it [05:55,  1.27it/s]Extractor Estimating: 482it [05:56,  1.34it/s]Extractor Estimating: 483it [05:57,  1.34it/s]Extractor Estimating: 484it [05:57,  1.40it/s]Extractor Estimating: 485it [05:58,  1.44it/s]Extractor Estimating: 486it [05:59,  1.49it/s]Extractor Estimating: 487it [05:59,  1.54it/s]Extractor Estimating: 488it [06:00,  1.48it/s]Extractor Estimating: 489it [06:01,  1.46it/s]Extractor Estimating: 490it [06:02,  1.41it/s]Extractor Estimating: 491it [06:02,  1.42it/s]Extractor Estimating: 492it [06:03,  1.44it/s]Extractor Estimating: 493it [06:04,  1.44it/s]Extractor Estimating: 494it [06:04,  1.45it/s]Extractor Estimating: 495it [06:05,  1.50it/s]Extractor Estimating: 496it [06:06,  1.47it/s]Extractor Estimating: 497it [06:06,  1.49it/s]Extractor Estimating: 498it [06:07,  1.43it/s]Extractor Estimating: 499it [06:08,  1.52it/s]Extractor Estimating: 500it [06:08,  1.43it/s]Extractor Estimating: 500it [06:08,  1.36it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9966 mean pseudo reward: 0.9076712194962052
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 31052
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31152, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31152, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.251, loss:1125.8623
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.967, loss:1070.3380
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.988, loss:1031.1043
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.045, loss:1021.2639
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.978, loss:971.9564
>> valid entity prec:0.5037, rec:0.5516, f1:0.5266
>> valid relation prec:0.2955, rec:0.0433, f1:0.0755
>> valid relation with NER prec:0.2955, rec:0.0433, f1:0.0755
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.263, loss:989.5205
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.967, loss:1005.1584
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.972, loss:1056.2370
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.961, loss:953.5190
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.964, loss:982.6409
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5487, rec:0.4861, f1:0.5155
>> valid relation prec:0.3169, rec:0.0645, f1:0.1072
>> valid relation with NER prec:0.3169, rec:0.0645, f1:0.1072
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.238, loss:959.6456
g_step 1200, step 368, avg_time 0.973, loss:983.8302
g_step 1300, step 52, avg_time 0.968, loss:970.5347
g_step 1400, step 152, avg_time 0.978, loss:903.2621
g_step 1500, step 252, avg_time 0.967, loss:938.0479
>> valid entity prec:0.5378, rec:0.5146, f1:0.5259
>> valid relation prec:0.2756, rec:0.0648, f1:0.1049
>> valid relation with NER prec:0.2756, rec:0.0648, f1:0.1049
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.273, loss:981.6997
g_step 1700, step 36, avg_time 0.962, loss:865.5908
g_step 1800, step 136, avg_time 0.976, loss:861.3027
g_step 1900, step 236, avg_time 0.974, loss:897.4571
g_step 2000, step 336, avg_time 0.968, loss:889.7629
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5266, rec:0.5127, f1:0.5196
>> valid relation prec:0.3143, rec:0.0743, f1:0.1202
>> valid relation with NER prec:0.3143, rec:0.0743, f1:0.1202
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.263, loss:864.2030
g_step 2200, step 120, avg_time 0.964, loss:825.5779
g_step 2300, step 220, avg_time 0.979, loss:849.0298
g_step 2400, step 320, avg_time 0.976, loss:829.2573
g_step 2500, step 4, avg_time 0.963, loss:870.7252
>> valid entity prec:0.5558, rec:0.4947, f1:0.5235
>> valid relation prec:0.2727, rec:0.0654, f1:0.1055
>> valid relation with NER prec:0.2727, rec:0.0654, f1:0.1055
g_step 2600, step 104, avg_time 2.237, loss:790.8774
g_step 2700, step 204, avg_time 0.973, loss:795.7783
g_step 2800, step 304, avg_time 0.967, loss:811.5009
g_step 2900, step 404, avg_time 0.977, loss:814.5146
g_step 3000, step 88, avg_time 0.978, loss:756.9971
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5648, rec:0.5097, f1:0.5358
>> valid relation prec:0.2131, rec:0.0683, f1:0.1034
>> valid relation with NER prec:0.2131, rec:0.0683, f1:0.1034
new max entity f1 on valid!
g_step 3100, step 188, avg_time 2.223, loss:763.9675
g_step 3200, step 288, avg_time 0.962, loss:768.4713
g_step 3300, step 388, avg_time 0.953, loss:779.9540
g_step 3400, step 72, avg_time 0.964, loss:735.2353
g_step 3500, step 172, avg_time 0.951, loss:746.8019
>> valid entity prec:0.5463, rec:0.5045, f1:0.5246
>> valid relation prec:0.2593, rec:0.0780, f1:0.1199
>> valid relation with NER prec:0.2593, rec:0.0780, f1:0.1199
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 272, avg_time 2.259, loss:735.6311
g_step 3700, step 372, avg_time 0.967, loss:752.4178
g_step 3800, step 56, avg_time 0.976, loss:720.0825
g_step 3900, step 156, avg_time 0.967, loss:694.0457
g_step 4000, step 256, avg_time 0.960, loss:715.5357
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5917, rec:0.4710, f1:0.5245
>> valid relation prec:0.3497, rec:0.0880, f1:0.1407
>> valid relation with NER prec:0.3497, rec:0.0880, f1:0.1407
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 356, avg_time 2.256, loss:731.4206
g_step 4200, step 40, avg_time 0.955, loss:672.1521
g_step 4300, step 140, avg_time 0.958, loss:670.1548
g_step 4400, step 240, avg_time 0.977, loss:687.7841
g_step 4500, step 340, avg_time 0.980, loss:695.3811
>> valid entity prec:0.4950, rec:0.4742, f1:0.4843
>> valid relation prec:0.2560, rec:0.0794, f1:0.1213
>> valid relation with NER prec:0.2560, rec:0.0794, f1:0.1213
g_step 4600, step 24, avg_time 2.237, loss:686.2994
g_step 4700, step 124, avg_time 0.965, loss:633.0167
g_step 4800, step 224, avg_time 0.975, loss:660.4208
g_step 4900, step 324, avg_time 0.967, loss:647.7713
g_step 5000, step 8, avg_time 0.983, loss:673.0320
learning rate was adjusted to 0.0008
>> valid entity prec:0.5611, rec:0.4707, f1:0.5119
>> valid relation prec:0.2922, rec:0.1004, f1:0.1494
>> valid relation with NER prec:0.2922, rec:0.1004, f1:0.1494
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 108, avg_time 2.230, loss:612.9089
g_step 5200, step 208, avg_time 0.972, loss:611.2685
g_step 5300, step 308, avg_time 0.972, loss:637.1891
g_step 5400, step 408, avg_time 0.967, loss:649.0071
g_step 5500, step 92, avg_time 0.975, loss:598.8581
>> valid entity prec:0.5356, rec:0.5293, f1:0.5324
>> valid relation prec:0.2426, rec:0.1058, f1:0.1474
>> valid relation with NER prec:0.2426, rec:0.1058, f1:0.1474
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 192, avg_time 2.268, loss:582.4754
g_step 5700, step 292, avg_time 0.970, loss:612.3450
g_step 5800, step 392, avg_time 0.961, loss:630.2311
g_step 5900, step 76, avg_time 0.971, loss:591.2899
g_step 6000, step 176, avg_time 0.971, loss:576.2367
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5666, rec:0.4956, f1:0.5287
>> valid relation prec:0.2309, rec:0.0840, f1:0.1232
>> valid relation with NER prec:0.2309, rec:0.0840, f1:0.1232
g_step 6100, step 276, avg_time 2.234, loss:585.1440
g_step 6200, step 376, avg_time 0.965, loss:601.8289
g_step 6300, step 60, avg_time 0.968, loss:561.3745
g_step 6400, step 160, avg_time 0.978, loss:551.4976
g_step 6500, step 260, avg_time 0.961, loss:553.3802
>> valid entity prec:0.5272, rec:0.5263, f1:0.5267
>> valid relation prec:0.2127, rec:0.1001, f1:0.1361
>> valid relation with NER prec:0.2127, rec:0.1001, f1:0.1361
g_step 6600, step 360, avg_time 2.238, loss:589.0071
g_step 6700, step 44, avg_time 0.979, loss:548.4359
g_step 6800, step 144, avg_time 0.986, loss:544.4420
g_step 6900, step 244, avg_time 0.969, loss:546.6391
g_step 7000, step 344, avg_time 0.967, loss:577.5248
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5346, rec:0.5400, f1:0.5372
>> valid relation prec:0.2405, rec:0.1139, f1:0.1545
>> valid relation with NER prec:0.2405, rec:0.1139, f1:0.1545
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7100, step 28, avg_time 2.274, loss:523.8175
g_step 7200, step 128, avg_time 0.962, loss:503.8494
g_step 7300, step 228, avg_time 0.970, loss:537.6842
g_step 7400, step 328, avg_time 0.973, loss:534.0785
g_step 7500, step 12, avg_time 0.960, loss:536.8830
>> valid entity prec:0.5370, rec:0.4988, f1:0.5172
>> valid relation prec:0.2110, rec:0.1124, f1:0.1467
>> valid relation with NER prec:0.2110, rec:0.1124, f1:0.1467
g_step 7600, step 112, avg_time 2.224, loss:502.4355
g_step 7700, step 212, avg_time 0.978, loss:487.1752
g_step 7800, step 312, avg_time 0.976, loss:540.7551
g_step 7900, step 412, avg_time 0.963, loss:523.7016
g_step 8000, step 96, avg_time 0.962, loss:476.1172
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5526, rec:0.4952, f1:0.5223
>> valid relation prec:0.2361, rec:0.0837, f1:0.1236
>> valid relation with NER prec:0.2361, rec:0.0837, f1:0.1236
g_step 8100, step 196, avg_time 2.234, loss:480.7040
g_step 8200, step 296, avg_time 0.972, loss:506.4776
g_step 8300, step 396, avg_time 0.958, loss:506.4476
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:22:16 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:22:16 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-22-16_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:22:18 - WARNING - datasets.builder -   Using custom data configuration default-7fdf449d5eb77735
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7fdf449d5eb77735/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:22:19,688 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:22:19,689 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:22:19,690 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:22:19,691 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:22:19,758 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,812 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,812 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,812 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,813 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,813 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:22:19,813 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:22:20,005 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:22:23,160 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:22:23,181 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7fdf449d5eb77735/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 21:22:23 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14c2a5be3440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:01<00:15,  1.57s/ba] 18%|        | 2/11 [00:01<00:06,  1.29ba/s] 27%|       | 3/11 [00:02<00:04,  1.92ba/s] 36%|      | 4/11 [00:02<00:02,  2.48ba/s] 45%|     | 5/11 [00:02<00:02,  2.96ba/s] 55%|    | 6/11 [00:02<00:01,  3.35ba/s] 64%|   | 7/11 [00:02<00:01,  3.65ba/s] 73%|  | 8/11 [00:03<00:00,  3.88ba/s] 82%| | 9/11 [00:03<00:00,  3.74ba/s] 91%| | 10/11 [00:03<00:00,  3.95ba/s]100%|| 11/11 [00:03<00:00,  3.02ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.19ba/s] 50%|     | 2/4 [00:00<00:00,  3.81ba/s] 75%|  | 3/4 [00:00<00:00,  4.08ba/s]100%|| 4/4 [00:00<00:00,  5.18ba/s]100%|| 4/4 [00:00<00:00,  4.56ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|         | 1/11 [00:00<00:02,  3.36ba/s] 27%|       | 3/11 [00:00<00:01,  6.93ba/s] 45%|     | 5/11 [00:00<00:00,  8.55ba/s] 64%|   | 7/11 [00:00<00:00,  9.50ba/s] 82%| | 9/11 [00:01<00:00,  7.97ba/s]100%|| 11/11 [00:01<00:00,  8.77ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  8.06ba/s] 75%|  | 3/4 [00:00<00:00, 10.14ba/s]100%|| 4/4 [00:00<00:00, 11.45ba/s]
[INFO|trainer.py:414] 2023-08-28 21:22:31,557 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:22:31,647 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:22:31,647 >>   Num examples = 10011
[INFO|trainer.py:1149] 2023-08-28 21:22:31,647 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:22:31,647 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:22:31,647 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:22:31,647 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:22:31,647 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:58,  3.27it/s]  0%|          | 2/780 [00:00<03:49,  3.38it/s]  0%|          | 3/780 [00:00<03:47,  3.42it/s]  1%|          | 4/780 [00:01<03:46,  3.43it/s]  1%|          | 5/780 [00:01<03:45,  3.44it/s]  1%|          | 6/780 [00:01<03:44,  3.45it/s]  1%|          | 7/780 [00:02<03:43,  3.46it/s]  1%|          | 8/780 [00:02<03:43,  3.46it/s]  1%|          | 9/780 [00:02<03:43,  3.45it/s]  1%|         | 10/780 [00:02<03:44,  3.43it/s]  1%|         | 11/780 [00:03<03:44,  3.43it/s]  2%|         | 12/780 [00:03<03:44,  3.42it/s]  2%|         | 13/780 [00:03<03:44,  3.42it/s]  2%|         | 14/780 [00:04<03:43,  3.42it/s]  2%|         | 15/780 [00:04<03:51,  3.31it/s]  2%|         | 16/780 [00:04<03:48,  3.34it/s]  2%|         | 17/780 [00:04<03:46,  3.36it/s]  2%|         | 18/780 [00:05<03:45,  3.38it/s]  2%|         | 19/780 [00:05<03:44,  3.39it/s]  3%|         | 20/780 [00:05<03:43,  3.40it/s]  3%|         | 21/780 [00:06<03:42,  3.41it/s]  3%|         | 22/780 [00:06<03:42,  3.41it/s]  3%|         | 23/780 [00:06<03:41,  3.41it/s]  3%|         | 24/780 [00:07<03:41,  3.42it/s]  3%|         | 25/780 [00:07<03:40,  3.42it/s]  3%|         | 26/780 [00:07<03:46,  3.34it/s]  3%|         | 27/780 [00:07<03:44,  3.36it/s]  4%|         | 28/780 [00:08<03:42,  3.38it/s]  4%|         | 29/780 [00:08<03:41,  3.39it/s]  4%|         | 30/780 [00:08<03:40,  3.40it/s]  4%|         | 31/780 [00:09<03:40,  3.40it/s]  4%|         | 32/780 [00:09<03:39,  3.41it/s]  4%|         | 33/780 [00:09<03:39,  3.41it/s]  4%|         | 34/780 [00:09<03:38,  3.41it/s]  4%|         | 35/780 [00:10<03:38,  3.41it/s]  5%|         | 36/780 [00:10<03:37,  3.41it/s]  5%|         | 37/780 [00:10<03:41,  3.35it/s]  5%|         | 38/780 [00:11<03:40,  3.37it/s]  5%|         | 39/780 [00:11<03:38,  3.39it/s]  5%|         | 40/780 [00:11<03:38,  3.39it/s]  5%|         | 41/780 [00:12<03:37,  3.40it/s]  5%|         | 42/780 [00:12<03:36,  3.41it/s]  6%|         | 43/780 [00:12<03:36,  3.41it/s]  6%|         | 44/780 [00:12<03:35,  3.41it/s]  6%|         | 45/780 [00:13<03:35,  3.41it/s]  6%|         | 46/780 [00:13<04:12,  2.90it/s]  6%|         | 47/780 [00:14<04:51,  2.52it/s]  6%|         | 48/780 [00:15<07:29,  1.63it/s]  6%|         | 49/780 [00:15<06:18,  1.93it/s]  6%|         | 50/780 [00:16<06:21,  1.91it/s]  7%|         | 51/780 [00:16<07:04,  1.72it/s]  7%|         | 52/780 [00:17<06:00,  2.02it/s]  7%|         | 53/780 [00:17<05:15,  2.30it/s]  7%|         | 54/780 [00:17<04:44,  2.55it/s]  7%|         | 55/780 [00:18<04:22,  2.76it/s]  7%|         | 56/780 [00:18<04:07,  2.93it/s]  7%|         | 57/780 [00:18<03:56,  3.06it/s]  7%|         | 58/780 [00:18<03:48,  3.16it/s]  8%|         | 59/780 [00:19<03:42,  3.23it/s]  8%|         | 60/780 [00:19<04:01,  2.98it/s]  8%|         | 61/780 [00:19<03:52,  3.10it/s]  8%|         | 62/780 [00:20<03:45,  3.19it/s]  8%|         | 63/780 [00:20<03:40,  3.25it/s]  8%|         | 64/780 [00:20<03:36,  3.30it/s]  8%|         | 65/780 [00:21<03:34,  3.34it/s]  8%|         | 66/780 [00:21<03:32,  3.36it/s]  9%|         | 67/780 [00:21<03:31,  3.38it/s]  9%|         | 68/780 [00:21<03:30,  3.39it/s]  9%|         | 69/780 [00:22<03:29,  3.40it/s]  9%|         | 70/780 [00:22<04:51,  2.44it/s]  9%|         | 71/780 [00:23<04:25,  2.67it/s]  9%|         | 72/780 [00:23<04:08,  2.85it/s]  9%|         | 73/780 [00:23<03:55,  3.00it/s]  9%|         | 74/780 [00:24<03:46,  3.12it/s] 10%|         | 75/780 [00:24<03:40,  3.20it/s] 10%|         | 76/780 [00:24<03:35,  3.26it/s] 10%|         | 77/780 [00:24<03:32,  3.31it/s] 10%|         | 78/780 [00:25<03:30,  3.34it/s] 10%|         | 79/780 [00:25<03:49,  3.06it/s] 10%|         | 80/780 [00:25<03:41,  3.16it/s] 10%|         | 81/780 [00:26<03:36,  3.23it/s] 11%|         | 82/780 [00:26<03:32,  3.29it/s] 11%|         | 83/780 [00:27<04:21,  2.66it/s] 11%|         | 84/780 [00:27<04:04,  2.85it/s] 11%|         | 85/780 [00:27<03:51,  3.00it/s] 11%|         | 86/780 [00:27<03:42,  3.11it/s] 11%|         | 87/780 [00:28<03:36,  3.20it/s] 11%|        | 88/780 [00:28<03:32,  3.26it/s] 11%|        | 89/780 [00:29<04:01,  2.86it/s] 12%|        | 90/780 [00:29<03:49,  3.00it/s] 12%|        | 91/780 [00:29<03:41,  3.12it/s] 12%|        | 92/780 [00:29<03:35,  3.20it/s] 12%|        | 93/780 [00:30<03:30,  3.26it/s] 12%|        | 94/780 [00:30<03:27,  3.30it/s] 12%|        | 95/780 [00:30<03:25,  3.34it/s] 12%|        | 96/780 [00:31<03:23,  3.36it/s] 12%|        | 97/780 [00:31<03:22,  3.38it/s] 13%|        | 98/780 [00:31<03:21,  3.39it/s] 13%|        | 99/780 [00:32<04:11,  2.70it/s] 13%|        | 100/780 [00:32<03:56,  2.88it/s] 13%|        | 101/780 [00:32<03:44,  3.02it/s] 13%|        | 102/780 [00:33<03:36,  3.13it/s] 13%|        | 103/780 [00:33<03:30,  3.21it/s] 13%|        | 104/780 [00:33<03:26,  3.27it/s] 13%|        | 105/780 [00:33<03:23,  3.31it/s] 14%|        | 106/780 [00:34<03:21,  3.34it/s] 14%|        | 107/780 [00:34<03:20,  3.36it/s] 14%|        | 108/780 [00:34<03:18,  3.38it/s] 14%|        | 109/780 [00:35<03:54,  2.86it/s] 14%|        | 110/780 [00:35<03:42,  3.01it/s] 14%|        | 111/780 [00:35<03:34,  3.12it/s] 14%|        | 112/780 [00:36<03:28,  3.20it/s] 14%|        | 113/780 [00:36<03:24,  3.26it/s] 15%|        | 114/780 [00:36<03:21,  3.31it/s] 15%|        | 115/780 [00:37<03:19,  3.34it/s] 15%|        | 116/780 [00:37<03:17,  3.36it/s] 15%|        | 117/780 [00:37<03:16,  3.38it/s] 15%|        | 118/780 [00:37<03:14,  3.40it/s] 15%|        | 119/780 [00:38<04:15,  2.58it/s] 15%|        | 120/780 [00:38<03:55,  2.80it/s] 16%|        | 121/780 [00:39<03:41,  2.97it/s] 16%|        | 122/780 [00:39<03:32,  3.10it/s] 16%|        | 123/780 [00:39<03:25,  3.20it/s] 16%|        | 124/780 [00:39<03:20,  3.28it/s] 16%|        | 125/780 [00:40<03:16,  3.33it/s] 16%|        | 126/780 [00:40<03:14,  3.37it/s] 16%|        | 127/780 [00:40<03:12,  3.40it/s] 16%|        | 128/780 [00:41<03:10,  3.42it/s] 17%|        | 129/780 [00:41<03:40,  2.96it/s] 17%|        | 130/780 [00:41<03:30,  3.09it/s] 17%|        | 131/780 [00:42<03:23,  3.19it/s] 17%|        | 132/780 [00:42<03:18,  3.27it/s] 17%|        | 133/780 [00:42<03:14,  3.32it/s] 17%|        | 134/780 [00:43<03:12,  3.36it/s] 17%|        | 135/780 [00:43<03:10,  3.39it/s] 17%|        | 136/780 [00:43<03:08,  3.41it/s] 18%|        | 137/780 [00:43<03:07,  3.43it/s] 18%|        | 138/780 [00:44<03:06,  3.44it/s] 18%|        | 139/780 [00:44<03:15,  3.28it/s] 18%|        | 140/780 [00:44<03:12,  3.33it/s] 18%|        | 141/780 [00:45<03:09,  3.37it/s] 18%|        | 142/780 [00:45<03:07,  3.40it/s] 18%|        | 143/780 [00:45<03:06,  3.41it/s] 18%|        | 144/780 [00:45<03:05,  3.43it/s] 19%|        | 145/780 [00:46<03:05,  3.43it/s] 19%|        | 146/780 [00:46<03:04,  3.44it/s] 19%|        | 147/780 [00:46<03:03,  3.45it/s] 19%|        | 148/780 [00:47<03:03,  3.45it/s] 19%|        | 149/780 [00:47<04:51,  2.17it/s] 19%|        | 150/780 [00:48<04:18,  2.44it/s] 19%|        | 151/780 [00:48<03:55,  2.68it/s] 19%|        | 152/780 [00:48<03:38,  2.87it/s] 20%|        | 153/780 [00:49<03:27,  3.02it/s] 20%|        | 154/780 [00:49<03:19,  3.14it/s] 20%|        | 155/780 [00:49<03:13,  3.23it/s] 20%|        | 156/780 [00:50<03:09,  3.29it/s][INFO|trainer.py:2140] 2023-08-28 21:23:22,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:23:22,109 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:23:22,109 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.01it/s][A
  3%|         | 12/437 [00:00<00:08, 49.57it/s][A
  4%|         | 18/437 [00:00<00:08, 47.82it/s][A
  5%|         | 23/437 [00:00<00:08, 47.01it/s][A
  6%|         | 28/437 [00:00<00:08, 46.52it/s][A
  8%|         | 33/437 [00:00<00:08, 46.26it/s][A
  9%|         | 38/437 [00:00<00:08, 45.57it/s][A
 10%|         | 43/437 [00:00<00:08, 45.15it/s][A
 11%|         | 48/437 [00:01<00:08, 45.04it/s][A
 12%|        | 53/437 [00:01<00:08, 45.13it/s][A
 13%|        | 58/437 [00:01<00:08, 45.15it/s][A
 14%|        | 63/437 [00:01<00:08, 45.40it/s][A
 16%|        | 68/437 [00:01<00:08, 45.41it/s][A
 17%|        | 73/437 [00:01<00:07, 45.56it/s][A
 18%|        | 78/437 [00:01<00:07, 45.42it/s][A
 19%|        | 83/437 [00:01<00:07, 45.16it/s][A
 20%|        | 88/437 [00:01<00:07, 44.94it/s][A
 21%|       | 93/437 [00:02<00:07, 44.73it/s][A
 22%|       | 98/437 [00:02<00:07, 44.84it/s][A
 24%|       | 103/437 [00:02<00:07, 44.91it/s][A
 25%|       | 108/437 [00:02<00:07, 45.27it/s][A
 26%|       | 113/437 [00:02<00:07, 45.24it/s][A
 27%|       | 118/437 [00:02<00:07, 45.46it/s][A
 28%|       | 123/437 [00:02<00:07, 42.88it/s][A
 29%|       | 128/437 [00:02<00:07, 43.65it/s][A
 30%|       | 133/437 [00:02<00:06, 44.01it/s][A
 32%|      | 138/437 [00:03<00:06, 44.20it/s][A
 33%|      | 143/437 [00:03<00:06, 44.46it/s][A
 34%|      | 148/437 [00:03<00:06, 44.69it/s][A
 35%|      | 153/437 [00:03<00:06, 44.93it/s][A
 36%|      | 158/437 [00:03<00:06, 45.04it/s][A
 37%|      | 163/437 [00:03<00:06, 44.94it/s][A
 38%|      | 168/437 [00:03<00:05, 44.94it/s][A
 40%|      | 173/437 [00:03<00:05, 45.07it/s][A
 41%|      | 178/437 [00:03<00:05, 45.06it/s][A
 42%|     | 183/437 [00:04<00:05, 45.15it/s][A
 43%|     | 188/437 [00:04<00:05, 45.15it/s][A
 44%|     | 193/437 [00:04<00:05, 45.12it/s][A
 45%|     | 198/437 [00:04<00:05, 45.17it/s][A
 46%|     | 203/437 [00:04<00:05, 45.22it/s][A
 48%|     | 208/437 [00:04<00:05, 45.18it/s][A
 49%|     | 213/437 [00:04<00:04, 45.10it/s][A
 50%|     | 218/437 [00:04<00:04, 45.18it/s][A
 51%|     | 223/437 [00:04<00:04, 44.80it/s][A
 52%|    | 228/437 [00:05<00:04, 45.29it/s][A
 53%|    | 233/437 [00:05<00:04, 45.11it/s][A
 54%|    | 238/437 [00:05<00:04, 45.17it/s][A
 56%|    | 243/437 [00:05<00:04, 45.16it/s][A
 57%|    | 248/437 [00:05<00:04, 45.29it/s][A
 58%|    | 253/437 [00:05<00:04, 45.23it/s][A
 59%|    | 258/437 [00:05<00:04, 42.63it/s][A
 60%|    | 263/437 [00:05<00:03, 43.51it/s][A
 61%|   | 268/437 [00:05<00:03, 44.08it/s][A
 62%|   | 273/437 [00:06<00:03, 44.47it/s][A
 64%|   | 278/437 [00:06<00:03, 44.67it/s][A
 65%|   | 283/437 [00:06<00:03, 44.77it/s][A
 66%|   | 288/437 [00:06<00:03, 45.04it/s][A
 67%|   | 293/437 [00:06<00:03, 45.14it/s][A
 68%|   | 298/437 [00:06<00:03, 44.86it/s][A
 69%|   | 303/437 [00:06<00:02, 44.92it/s][A
 70%|   | 308/437 [00:06<00:02, 45.02it/s][A
 72%|  | 313/437 [00:06<00:02, 45.18it/s][A
 73%|  | 318/437 [00:07<00:02, 45.17it/s][A
 74%|  | 323/437 [00:07<00:02, 45.15it/s][A
 75%|  | 328/437 [00:07<00:02, 45.25it/s][A
 76%|  | 333/437 [00:07<00:02, 45.29it/s][A
 77%|  | 338/437 [00:07<00:02, 45.18it/s][A
 78%|  | 343/437 [00:07<00:02, 45.06it/s][A
 80%|  | 348/437 [00:07<00:01, 45.08it/s][A
 81%|  | 353/437 [00:07<00:01, 45.06it/s][A
 82%| | 358/437 [00:07<00:01, 45.10it/s][A
 83%| | 363/437 [00:08<00:01, 45.13it/s][A
 84%| | 368/437 [00:08<00:01, 45.16it/s][A
 85%| | 373/437 [00:08<00:01, 45.29it/s][A
 86%| | 378/437 [00:08<00:01, 45.32it/s][A
 88%| | 383/437 [00:08<00:01, 45.14it/s][A
 89%| | 388/437 [00:08<00:01, 45.15it/s][A
 90%| | 393/437 [00:08<00:01, 35.70it/s][A
 91%| | 398/437 [00:08<00:01, 38.27it/s][A
 92%|| 403/437 [00:09<00:00, 40.27it/s][A
 93%|| 408/437 [00:09<00:00, 41.79it/s][A
 95%|| 413/437 [00:09<00:00, 42.89it/s][A
 96%|| 418/437 [00:09<00:00, 43.69it/s][A
 97%|| 423/437 [00:09<00:00, 44.34it/s][A
 98%|| 428/437 [00:09<00:00, 44.70it/s][A
 99%|| 433/437 [00:09<00:00, 44.43it/s][A                                                 
                                                 [A 20%|        | 156/780 [01:00<03:09,  3.29it/s]
100%|| 437/437 [00:09<00:00, 44.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:23:32,981 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:23:33,565 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:23:42,718 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:23:42,971 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:23:43,396 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:52<3:17:26, 19.02s/it] 20%|        | 158/780 [01:53<2:19:42, 13.48s/it] 20%|        | 159/780 [01:53<1:38:32,  9.52s/it] 21%|        | 160/780 [01:53<1:09:46,  6.75s/it] 21%|        | 161/780 [01:54<49:39,  4.81s/it]   21%|        | 162/780 [01:54<35:36,  3.46s/it] 21%|        | 163/780 [01:54<25:46,  2.51s/it] 21%|        | 164/780 [01:54<18:55,  1.84s/it] 21%|        | 165/780 [01:55<14:06,  1.38s/it] 21%|       | 166/780 [01:55<10:45,  1.05s/it] 21%|       | 167/780 [01:55<08:24,  1.21it/s] 22%|       | 168/780 [01:56<07:42,  1.32it/s] 22%|       | 169/780 [01:56<06:16,  1.62it/s] 22%|       | 170/780 [01:57<05:16,  1.93it/s] 22%|       | 171/780 [01:57<04:34,  2.22it/s] 22%|       | 172/780 [01:57<04:04,  2.48it/s] 22%|       | 173/780 [01:57<03:44,  2.71it/s] 22%|       | 174/780 [01:58<03:29,  2.89it/s] 22%|       | 175/780 [01:58<03:19,  3.04it/s] 23%|       | 176/780 [01:58<03:12,  3.14it/s] 23%|       | 177/780 [01:59<03:06,  3.23it/s] 23%|       | 178/780 [01:59<03:47,  2.65it/s] 23%|       | 179/780 [01:59<03:31,  2.84it/s] 23%|       | 180/780 [02:00<03:20,  3.00it/s] 23%|       | 181/780 [02:00<03:12,  3.12it/s] 23%|       | 182/780 [02:00<03:06,  3.21it/s] 23%|       | 183/780 [02:01<03:02,  3.27it/s] 24%|       | 184/780 [02:01<02:59,  3.32it/s] 24%|       | 185/780 [02:01<02:57,  3.35it/s] 24%|       | 186/780 [02:01<02:56,  3.37it/s] 24%|       | 187/780 [02:02<02:54,  3.39it/s] 24%|       | 188/780 [02:02<04:13,  2.34it/s] 24%|       | 189/780 [02:03<04:00,  2.45it/s] 24%|       | 190/780 [02:03<03:40,  2.68it/s] 24%|       | 191/780 [02:03<03:25,  2.87it/s] 25%|       | 192/780 [02:04<03:22,  2.90it/s] 25%|       | 193/780 [02:04<03:12,  3.04it/s] 25%|       | 194/780 [02:04<03:05,  3.15it/s] 25%|       | 195/780 [02:05<03:01,  3.23it/s] 25%|       | 196/780 [02:05<02:57,  3.29it/s] 25%|       | 197/780 [02:05<02:55,  3.33it/s] 25%|       | 198/780 [02:06<02:53,  3.36it/s] 26%|       | 199/780 [02:06<02:51,  3.38it/s] 26%|       | 200/780 [02:06<02:50,  3.39it/s] 26%|       | 201/780 [02:06<02:49,  3.41it/s] 26%|       | 202/780 [02:07<04:08,  2.32it/s] 26%|       | 203/780 [02:07<03:44,  2.57it/s] 26%|       | 204/780 [02:08<03:27,  2.78it/s] 26%|       | 205/780 [02:08<03:15,  2.95it/s] 26%|       | 206/780 [02:08<03:06,  3.07it/s] 27%|       | 207/780 [02:09<03:00,  3.17it/s] 27%|       | 208/780 [02:09<02:56,  3.24it/s] 27%|       | 209/780 [02:09<02:53,  3.30it/s] 27%|       | 210/780 [02:09<02:51,  3.33it/s] 27%|       | 211/780 [02:10<03:04,  3.08it/s] 27%|       | 212/780 [02:10<02:58,  3.18it/s] 27%|       | 213/780 [02:10<02:54,  3.25it/s] 27%|       | 214/780 [02:11<02:51,  3.29it/s] 28%|       | 215/780 [02:11<02:49,  3.33it/s] 28%|       | 216/780 [02:11<02:48,  3.36it/s] 28%|       | 217/780 [02:12<02:46,  3.37it/s] 28%|       | 218/780 [02:12<02:46,  3.38it/s] 28%|       | 219/780 [02:12<02:45,  3.39it/s] 28%|       | 220/780 [02:12<02:44,  3.40it/s] 28%|       | 221/780 [02:13<04:19,  2.16it/s] 28%|       | 222/780 [02:14<06:02,  1.54it/s] 29%|       | 223/780 [02:15<06:03,  1.53it/s] 29%|       | 224/780 [02:16<05:40,  1.63it/s] 29%|       | 225/780 [02:16<05:39,  1.63it/s] 29%|       | 226/780 [02:17<04:46,  1.94it/s] 29%|       | 227/780 [02:17<04:08,  2.23it/s] 29%|       | 228/780 [02:17<03:41,  2.49it/s] 29%|       | 229/780 [02:17<03:23,  2.71it/s] 29%|       | 230/780 [02:18<03:10,  2.89it/s] 30%|       | 231/780 [02:18<03:01,  3.03it/s] 30%|       | 232/780 [02:18<02:54,  3.14it/s] 30%|       | 233/780 [02:19<02:49,  3.23it/s] 30%|       | 234/780 [02:19<02:45,  3.30it/s] 30%|       | 235/780 [02:19<03:40,  2.47it/s] 30%|       | 236/780 [02:20<03:20,  2.71it/s] 30%|       | 237/780 [02:20<03:07,  2.90it/s] 31%|       | 238/780 [02:20<02:57,  3.05it/s] 31%|       | 239/780 [02:21<02:50,  3.17it/s] 31%|       | 240/780 [02:21<02:45,  3.25it/s] 31%|       | 241/780 [02:21<02:42,  3.31it/s] 31%|       | 242/780 [02:21<02:40,  3.36it/s] 31%|       | 243/780 [02:22<02:38,  3.39it/s] 31%|      | 244/780 [02:22<02:36,  3.42it/s] 31%|      | 245/780 [02:23<03:03,  2.91it/s] 32%|      | 246/780 [02:23<02:54,  3.05it/s] 32%|      | 247/780 [02:23<02:48,  3.17it/s] 32%|      | 248/780 [02:23<02:43,  3.25it/s] 32%|      | 249/780 [02:24<02:40,  3.31it/s] 32%|      | 250/780 [02:24<02:37,  3.36it/s] 32%|      | 251/780 [02:24<02:36,  3.39it/s] 32%|      | 252/780 [02:25<02:34,  3.41it/s] 32%|      | 253/780 [02:25<02:33,  3.43it/s] 33%|      | 254/780 [02:25<02:32,  3.44it/s] 33%|      | 255/780 [02:26<03:34,  2.45it/s] 33%|      | 256/780 [02:26<03:15,  2.68it/s] 33%|      | 257/780 [02:26<03:01,  2.88it/s] 33%|      | 258/780 [02:27<03:23,  2.56it/s] 33%|      | 259/780 [02:27<03:07,  2.78it/s] 33%|      | 260/780 [02:27<02:55,  2.96it/s] 33%|      | 261/780 [02:28<02:47,  3.09it/s] 34%|      | 262/780 [02:28<02:42,  3.19it/s] 34%|      | 263/780 [02:28<02:38,  3.27it/s] 34%|      | 264/780 [02:29<02:48,  3.06it/s] 34%|      | 265/780 [02:29<02:42,  3.17it/s] 34%|      | 266/780 [02:29<02:37,  3.25it/s] 34%|      | 267/780 [02:30<02:34,  3.32it/s] 34%|      | 268/780 [02:30<02:32,  3.36it/s] 34%|      | 269/780 [02:30<02:31,  3.38it/s] 35%|      | 270/780 [02:30<02:29,  3.41it/s] 35%|      | 271/780 [02:31<02:28,  3.42it/s] 35%|      | 272/780 [02:31<02:27,  3.44it/s] 35%|      | 273/780 [02:31<02:27,  3.44it/s] 35%|      | 274/780 [02:32<02:26,  3.45it/s] 35%|      | 275/780 [02:32<03:09,  2.66it/s] 35%|      | 276/780 [02:32<02:56,  2.86it/s] 36%|      | 277/780 [02:33<02:46,  3.02it/s] 36%|      | 278/780 [02:33<02:39,  3.14it/s] 36%|      | 279/780 [02:33<02:34,  3.23it/s] 36%|      | 280/780 [02:34<02:31,  3.30it/s] 36%|      | 281/780 [02:34<02:29,  3.35it/s] 36%|      | 282/780 [02:34<02:27,  3.38it/s] 36%|      | 283/780 [02:34<02:25,  3.41it/s] 36%|      | 284/780 [02:35<02:24,  3.42it/s] 37%|      | 285/780 [02:35<02:23,  3.44it/s] 37%|      | 286/780 [02:35<02:23,  3.45it/s] 37%|      | 287/780 [02:36<02:22,  3.45it/s] 37%|      | 288/780 [02:36<02:22,  3.45it/s] 37%|      | 289/780 [02:36<02:22,  3.46it/s] 37%|      | 290/780 [02:36<02:21,  3.46it/s] 37%|      | 291/780 [02:37<02:21,  3.46it/s] 37%|      | 292/780 [02:37<02:21,  3.46it/s] 38%|      | 293/780 [02:37<02:31,  3.22it/s] 38%|      | 294/780 [02:38<02:27,  3.29it/s] 38%|      | 295/780 [02:38<02:25,  3.34it/s] 38%|      | 296/780 [02:38<02:23,  3.37it/s] 38%|      | 297/780 [02:39<02:22,  3.40it/s] 38%|      | 298/780 [02:39<02:20,  3.42it/s] 38%|      | 299/780 [02:39<02:20,  3.43it/s] 38%|      | 300/780 [02:39<02:19,  3.44it/s] 39%|      | 301/780 [02:40<02:19,  3.45it/s] 39%|      | 302/780 [02:40<02:18,  3.45it/s] 39%|      | 303/780 [02:40<02:18,  3.45it/s] 39%|      | 304/780 [02:41<02:31,  3.14it/s] 39%|      | 305/780 [02:41<02:26,  3.23it/s] 39%|      | 306/780 [02:41<02:23,  3.30it/s] 39%|      | 307/780 [02:42<02:21,  3.35it/s] 39%|      | 308/780 [02:42<02:19,  3.38it/s] 40%|      | 309/780 [02:42<02:18,  3.41it/s] 40%|      | 310/780 [02:42<02:17,  3.42it/s] 40%|      | 311/780 [02:43<02:16,  3.44it/s] 40%|      | 312/780 [02:43<02:15,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:25:15,281 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:25:15,281 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:25:15,281 >>   Batch size = 8
{'eval_loss': 0.9351065754890442, 'eval_runtime': 9.7918, 'eval_samples_per_second': 356.318, 'eval_steps_per_second': 44.629, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 55.98it/s][A
  3%|         | 12/437 [00:00<00:08, 49.18it/s][A
  4%|         | 17/437 [00:00<00:09, 42.96it/s][A
  5%|         | 22/437 [00:00<00:09, 43.90it/s][A
  6%|         | 27/437 [00:00<00:09, 44.52it/s][A
  7%|         | 32/437 [00:00<00:09, 44.73it/s][A
  8%|         | 37/437 [00:00<00:08, 44.96it/s][A
 10%|         | 42/437 [00:00<00:08, 45.05it/s][A
 11%|         | 47/437 [00:01<00:08, 45.08it/s][A
 12%|        | 52/437 [00:01<00:08, 45.22it/s][A
 13%|        | 57/437 [00:01<00:08, 45.01it/s][A
 14%|        | 62/437 [00:01<00:08, 45.27it/s][A
 15%|        | 67/437 [00:01<00:08, 45.41it/s][A
 16%|        | 72/437 [00:01<00:08, 45.44it/s][A
 18%|        | 77/437 [00:01<00:07, 45.55it/s][A
 19%|        | 82/437 [00:01<00:07, 45.52it/s][A
 20%|        | 87/437 [00:01<00:07, 45.54it/s][A
 21%|        | 92/437 [00:02<00:07, 45.43it/s][A
 22%|       | 97/437 [00:02<00:07, 45.33it/s][A
 23%|       | 102/437 [00:02<00:07, 45.29it/s][A
 24%|       | 107/437 [00:02<00:07, 45.18it/s][A
 26%|       | 112/437 [00:02<00:07, 45.33it/s][A
 27%|       | 117/437 [00:02<00:07, 45.34it/s][A
 28%|       | 122/437 [00:02<00:06, 45.53it/s][A
 29%|       | 127/437 [00:02<00:06, 45.43it/s][A
 30%|       | 132/437 [00:02<00:06, 45.56it/s][A
 31%|      | 137/437 [00:03<00:06, 45.43it/s][A
 32%|      | 142/437 [00:03<00:06, 45.40it/s][A
 34%|      | 147/437 [00:03<00:06, 45.27it/s][A
 35%|      | 152/437 [00:03<00:11, 25.26it/s][A
 36%|      | 157/437 [00:03<00:09, 29.19it/s][A
 37%|      | 162/437 [00:03<00:08, 32.78it/s][A
 38%|      | 167/437 [00:03<00:07, 35.85it/s][A
 39%|      | 172/437 [00:04<00:06, 38.15it/s][A
 41%|      | 177/437 [00:04<00:06, 40.16it/s][A
 42%|     | 182/437 [00:04<00:06, 41.77it/s][A
 43%|     | 187/437 [00:04<00:05, 42.84it/s][A
 44%|     | 192/437 [00:04<00:05, 43.15it/s][A
 45%|     | 197/437 [00:04<00:05, 43.79it/s][A
 46%|     | 202/437 [00:04<00:05, 44.28it/s][A
 47%|     | 207/437 [00:04<00:05, 44.67it/s][A
 49%|     | 212/437 [00:04<00:05, 44.93it/s][A
 50%|     | 217/437 [00:05<00:04, 45.18it/s][A
 51%|     | 222/437 [00:05<00:04, 45.33it/s][A
 52%|    | 227/437 [00:05<00:04, 45.46it/s][A
 53%|    | 232/437 [00:05<00:04, 45.29it/s][A
 54%|    | 237/437 [00:05<00:04, 45.16it/s][A
 55%|    | 242/437 [00:05<00:04, 45.08it/s][A
 57%|    | 247/437 [00:05<00:04, 45.30it/s][A
 58%|    | 252/437 [00:05<00:04, 45.35it/s][A
 59%|    | 257/437 [00:05<00:03, 45.38it/s][A
 60%|    | 262/437 [00:06<00:03, 45.38it/s][A
 61%|    | 267/437 [00:06<00:03, 45.45it/s][A
 62%|   | 272/437 [00:06<00:03, 45.45it/s][A
 63%|   | 277/437 [00:06<00:04, 39.29it/s][A
 65%|   | 282/437 [00:06<00:03, 41.00it/s][A
 66%|   | 287/437 [00:06<00:03, 42.35it/s][A
 67%|   | 292/437 [00:06<00:03, 43.30it/s][A
 68%|   | 297/437 [00:06<00:03, 43.90it/s][A
 69%|   | 302/437 [00:06<00:03, 44.45it/s][A
 70%|   | 307/437 [00:07<00:02, 44.84it/s][A
 71%|  | 312/437 [00:07<00:02, 45.07it/s][A
 73%|  | 317/437 [00:07<00:02, 44.81it/s][A
 74%|  | 322/437 [00:07<00:02, 44.92it/s][A
 75%|  | 327/437 [00:07<00:02, 45.03it/s][A
 76%|  | 332/437 [00:07<00:02, 45.12it/s][A
 77%|  | 337/437 [00:07<00:02, 45.34it/s][A
 78%|  | 342/437 [00:07<00:02, 45.39it/s][A
 79%|  | 347/437 [00:07<00:01, 45.45it/s][A
 81%|  | 352/437 [00:08<00:01, 45.53it/s][A
 82%| | 357/437 [00:08<00:01, 45.56it/s][A
 83%| | 362/437 [00:08<00:01, 45.32it/s][A
 84%| | 367/437 [00:08<00:01, 45.25it/s][A
 85%| | 372/437 [00:08<00:01, 45.16it/s][A
 86%| | 377/437 [00:08<00:01, 45.30it/s][A
 87%| | 382/437 [00:08<00:01, 45.27it/s][A
 89%| | 387/437 [00:08<00:01, 45.32it/s][A
 90%| | 392/437 [00:08<00:00, 45.36it/s][A
 91%| | 397/437 [00:09<00:00, 45.49it/s][A
 92%|| 402/437 [00:09<00:00, 45.37it/s][A
 93%|| 407/437 [00:09<00:00, 45.28it/s][A
 94%|| 412/437 [00:09<00:00, 36.27it/s][A
 95%|| 417/437 [00:09<00:00, 38.69it/s][A
 97%|| 422/437 [00:09<00:00, 40.56it/s][A
 98%|| 427/437 [00:09<00:00, 41.93it/s][A
 99%|| 432/437 [00:09<00:00, 43.01it/s][A
100%|| 437/437 [00:10<00:00, 43.75it/s][A                                                 
                                                 [A 40%|      | 312/780 [02:53<02:15,  3.44it/s]
100%|| 437/437 [00:10<00:00, 43.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:25:26,199 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:25:26,682 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:25:39,285 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:25:39,525 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:25:39,632 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [03:39<2:11:52, 16.94s/it] 40%|      | 314/780 [03:39<1:33:15, 12.01s/it] 40%|      | 315/780 [03:40<1:05:49,  8.49s/it] 41%|      | 316/780 [03:40<46:39,  6.03s/it]   41%|      | 317/780 [03:40<33:15,  4.31s/it] 41%|      | 318/780 [03:40<23:54,  3.10s/it] 41%|      | 319/780 [03:41<17:22,  2.26s/it] 41%|      | 320/780 [03:41<12:48,  1.67s/it] 41%|      | 321/780 [03:41<09:36,  1.26s/it] 41%|     | 322/780 [03:42<07:22,  1.04it/s] 41%|     | 323/780 [03:42<05:48,  1.31it/s] 42%|     | 324/780 [03:42<05:16,  1.44it/s] 42%|     | 325/780 [03:43<04:20,  1.74it/s] 42%|     | 326/780 [03:43<03:41,  2.05it/s] 42%|     | 327/780 [03:43<03:14,  2.33it/s] 42%|     | 328/780 [03:44<02:55,  2.58it/s] 42%|     | 329/780 [03:44<02:41,  2.78it/s] 42%|     | 330/780 [03:44<02:32,  2.95it/s] 42%|     | 331/780 [03:44<02:25,  3.08it/s] 43%|     | 332/780 [03:45<02:20,  3.18it/s] 43%|     | 333/780 [03:45<02:17,  3.25it/s] 43%|     | 334/780 [03:46<03:53,  1.91it/s] 43%|     | 335/780 [03:46<03:22,  2.20it/s] 43%|     | 336/780 [03:47<03:00,  2.46it/s] 43%|     | 337/780 [03:47<02:44,  2.69it/s] 43%|     | 338/780 [03:47<02:33,  2.88it/s] 43%|     | 339/780 [03:48<02:25,  3.03it/s] 44%|     | 340/780 [03:48<02:20,  3.13it/s] 44%|     | 341/780 [03:48<02:16,  3.22it/s] 44%|     | 342/780 [03:49<02:41,  2.72it/s] 44%|     | 343/780 [03:49<02:30,  2.90it/s] 44%|     | 344/780 [03:49<02:23,  3.04it/s] 44%|     | 345/780 [03:50<02:18,  3.15it/s] 44%|     | 346/780 [03:50<02:14,  3.23it/s] 44%|     | 347/780 [03:50<02:11,  3.29it/s] 45%|     | 348/780 [03:50<02:09,  3.33it/s] 45%|     | 349/780 [03:51<02:08,  3.36it/s] 45%|     | 350/780 [03:51<02:07,  3.38it/s] 45%|     | 351/780 [03:51<02:06,  3.39it/s] 45%|     | 352/780 [03:52<02:45,  2.58it/s] 45%|     | 353/780 [03:52<02:33,  2.78it/s] 45%|     | 354/780 [03:52<02:24,  2.95it/s] 46%|     | 355/780 [03:53<02:18,  3.07it/s] 46%|     | 356/780 [03:53<02:13,  3.17it/s] 46%|     | 357/780 [03:53<02:10,  3.24it/s] 46%|     | 358/780 [03:54<02:08,  3.29it/s] 46%|     | 359/780 [03:54<02:06,  3.33it/s] 46%|     | 360/780 [03:54<02:05,  3.36it/s] 46%|     | 361/780 [03:55<02:04,  3.37it/s] 46%|     | 362/780 [03:55<02:50,  2.45it/s] 47%|     | 363/780 [03:55<02:35,  2.68it/s] 47%|     | 364/780 [03:56<02:25,  2.87it/s] 47%|     | 365/780 [03:56<02:17,  3.01it/s] 47%|     | 366/780 [03:56<02:12,  3.12it/s] 47%|     | 367/780 [03:57<02:08,  3.21it/s] 47%|     | 368/780 [03:57<02:06,  3.27it/s] 47%|     | 369/780 [03:57<02:04,  3.31it/s] 47%|     | 370/780 [03:58<02:02,  3.34it/s] 48%|     | 371/780 [03:58<02:10,  3.14it/s] 48%|     | 372/780 [03:58<02:06,  3.22it/s] 48%|     | 373/780 [03:58<02:04,  3.28it/s] 48%|     | 374/780 [03:59<02:02,  3.32it/s] 48%|     | 375/780 [03:59<02:01,  3.34it/s] 48%|     | 376/780 [03:59<01:59,  3.37it/s] 48%|     | 377/780 [04:00<01:59,  3.39it/s] 48%|     | 378/780 [04:00<01:58,  3.39it/s] 49%|     | 379/780 [04:00<01:57,  3.40it/s] 49%|     | 380/780 [04:01<01:57,  3.41it/s] 49%|     | 381/780 [04:01<01:56,  3.41it/s] 49%|     | 382/780 [04:01<02:27,  2.70it/s] 49%|     | 383/780 [04:02<02:17,  2.88it/s] 49%|     | 384/780 [04:02<02:10,  3.03it/s] 49%|     | 385/780 [04:02<02:06,  3.13it/s] 49%|     | 386/780 [04:03<02:02,  3.21it/s] 50%|     | 387/780 [04:03<02:00,  3.27it/s] 50%|     | 388/780 [04:03<02:12,  2.95it/s] 50%|     | 389/780 [04:04<02:07,  3.08it/s] 50%|     | 390/780 [04:04<02:02,  3.17it/s] 50%|     | 391/780 [04:05<02:54,  2.22it/s] 50%|     | 392/780 [04:05<02:36,  2.48it/s] 50%|     | 393/780 [04:05<02:23,  2.71it/s] 51%|     | 394/780 [04:05<02:13,  2.89it/s] 51%|     | 395/780 [04:06<02:07,  3.03it/s] 51%|     | 396/780 [04:06<02:02,  3.14it/s] 51%|     | 397/780 [04:06<01:59,  3.21it/s] 51%|     | 398/780 [04:07<02:32,  2.51it/s] 51%|     | 399/780 [04:07<02:19,  2.73it/s] 51%|    | 400/780 [04:08<02:10,  2.90it/s] 51%|    | 401/780 [04:08<02:04,  3.04it/s] 52%|    | 402/780 [04:08<02:00,  3.14it/s] 52%|    | 403/780 [04:08<01:56,  3.22it/s] 52%|    | 404/780 [04:09<01:54,  3.28it/s] 52%|    | 405/780 [04:09<01:53,  3.31it/s] 52%|    | 406/780 [04:09<01:51,  3.34it/s] 52%|    | 407/780 [04:10<02:00,  3.10it/s] 52%|    | 408/780 [04:10<01:56,  3.18it/s] 52%|    | 409/780 [04:10<01:54,  3.25it/s] 53%|    | 410/780 [04:11<01:52,  3.30it/s] 53%|    | 411/780 [04:11<01:50,  3.33it/s] 53%|    | 412/780 [04:11<01:49,  3.35it/s] 53%|    | 413/780 [04:11<01:48,  3.37it/s] 53%|    | 414/780 [04:12<01:48,  3.38it/s] 53%|    | 415/780 [04:12<01:47,  3.39it/s] 53%|    | 416/780 [04:12<01:47,  3.40it/s] 53%|    | 417/780 [04:13<02:49,  2.14it/s] 54%|    | 418/780 [04:13<02:30,  2.41it/s] 54%|    | 419/780 [04:15<04:42,  1.28it/s] 54%|    | 420/780 [04:15<03:48,  1.57it/s] 54%|    | 421/780 [04:17<05:12,  1.15it/s] 54%|    | 422/780 [04:17<04:09,  1.43it/s] 54%|    | 423/780 [04:17<03:25,  1.73it/s] 54%|    | 424/780 [04:18<02:54,  2.04it/s] 54%|    | 425/780 [04:18<02:33,  2.32it/s] 55%|    | 426/780 [04:18<02:18,  2.56it/s] 55%|    | 427/780 [04:19<02:07,  2.77it/s] 55%|    | 428/780 [04:19<02:08,  2.73it/s] 55%|    | 429/780 [04:19<02:00,  2.91it/s] 55%|    | 430/780 [04:20<01:55,  3.04it/s] 55%|    | 431/780 [04:20<01:50,  3.15it/s] 55%|    | 432/780 [04:20<01:48,  3.22it/s] 56%|    | 433/780 [04:20<01:45,  3.27it/s] 56%|    | 434/780 [04:21<01:44,  3.32it/s] 56%|    | 435/780 [04:21<01:43,  3.34it/s] 56%|    | 436/780 [04:21<01:42,  3.36it/s] 56%|    | 437/780 [04:22<01:41,  3.38it/s] 56%|    | 438/780 [04:22<02:15,  2.52it/s] 56%|    | 439/780 [04:23<02:04,  2.73it/s] 56%|    | 440/780 [04:23<01:56,  2.91it/s] 57%|    | 441/780 [04:23<01:51,  3.04it/s] 57%|    | 442/780 [04:23<01:47,  3.14it/s] 57%|    | 443/780 [04:24<01:44,  3.22it/s] 57%|    | 444/780 [04:24<01:42,  3.28it/s] 57%|    | 445/780 [04:24<01:40,  3.32it/s] 57%|    | 446/780 [04:25<01:39,  3.35it/s] 57%|    | 447/780 [04:25<01:38,  3.37it/s] 57%|    | 448/780 [04:25<01:42,  3.23it/s] 58%|    | 449/780 [04:25<01:40,  3.29it/s] 58%|    | 450/780 [04:26<01:39,  3.32it/s] 58%|    | 451/780 [04:26<01:38,  3.35it/s] 58%|    | 452/780 [04:26<01:37,  3.37it/s] 58%|    | 453/780 [04:27<01:36,  3.38it/s] 58%|    | 454/780 [04:27<01:36,  3.39it/s] 58%|    | 455/780 [04:27<01:35,  3.40it/s] 58%|    | 456/780 [04:28<01:35,  3.40it/s] 59%|    | 457/780 [04:28<01:34,  3.41it/s] 59%|    | 458/780 [04:28<01:34,  3.41it/s] 59%|    | 459/780 [04:29<02:28,  2.16it/s] 59%|    | 460/780 [04:29<02:12,  2.42it/s] 59%|    | 461/780 [04:30<02:00,  2.65it/s] 59%|    | 462/780 [04:30<01:51,  2.84it/s] 59%|    | 463/780 [04:30<01:46,  2.99it/s] 59%|    | 464/780 [04:30<01:41,  3.11it/s] 60%|    | 465/780 [04:31<01:38,  3.19it/s] 60%|    | 466/780 [04:31<01:36,  3.25it/s] 60%|    | 467/780 [04:31<01:34,  3.30it/s] 60%|    | 468/780 [04:32<01:56,  2.68it/s][INFO|trainer.py:2140] 2023-08-28 21:27:04,145 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:27:04,145 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:27:04,145 >>   Batch size = 8
{'eval_loss': 0.928965151309967, 'eval_runtime': 10.0763, 'eval_samples_per_second': 346.257, 'eval_steps_per_second': 43.369, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.75it/s][A
  3%|         | 12/437 [00:00<00:08, 49.87it/s][A
  4%|         | 18/437 [00:00<00:08, 47.93it/s][A
  5%|         | 23/437 [00:00<00:08, 47.08it/s][A
  6%|         | 28/437 [00:00<00:08, 46.09it/s][A
  8%|         | 33/437 [00:00<00:08, 45.60it/s][A
  9%|         | 38/437 [00:00<00:08, 45.37it/s][A
 10%|         | 43/437 [00:00<00:08, 45.34it/s][A
 11%|         | 48/437 [00:01<00:08, 45.43it/s][A
 12%|        | 53/437 [00:01<00:08, 45.57it/s][A
 13%|        | 58/437 [00:01<00:08, 45.53it/s][A
 14%|        | 63/437 [00:01<00:08, 45.59it/s][A
 16%|        | 68/437 [00:01<00:08, 45.69it/s][A
 17%|        | 73/437 [00:01<00:07, 45.62it/s][A
 18%|        | 78/437 [00:01<00:07, 45.42it/s][A
 19%|        | 83/437 [00:01<00:07, 45.25it/s][A
 20%|        | 88/437 [00:01<00:07, 45.19it/s][A
 21%|       | 93/437 [00:02<00:07, 45.25it/s][A
 22%|       | 98/437 [00:02<00:07, 45.46it/s][A
 24%|       | 103/437 [00:02<00:07, 45.55it/s][A
 25%|       | 108/437 [00:02<00:07, 45.61it/s][A
 26%|       | 113/437 [00:02<00:14, 22.13it/s][A
 27%|       | 118/437 [00:02<00:12, 26.21it/s][A
 28%|       | 123/437 [00:03<00:10, 30.04it/s][A
 29%|       | 128/437 [00:03<00:09, 33.56it/s][A
 30%|       | 133/437 [00:03<00:08, 36.50it/s][A
 32%|      | 138/437 [00:03<00:07, 38.88it/s][A
 33%|      | 143/437 [00:03<00:07, 40.69it/s][A
 34%|      | 148/437 [00:03<00:06, 42.01it/s][A
 35%|      | 153/437 [00:03<00:06, 42.68it/s][A
 36%|      | 158/437 [00:03<00:06, 43.33it/s][A
 37%|      | 163/437 [00:03<00:06, 43.94it/s][A
 38%|      | 168/437 [00:04<00:06, 44.47it/s][A
 40%|      | 173/437 [00:04<00:05, 44.92it/s][A
 41%|      | 178/437 [00:04<00:05, 45.23it/s][A
 42%|     | 183/437 [00:04<00:05, 45.26it/s][A
 43%|     | 188/437 [00:04<00:05, 45.40it/s][A
 44%|     | 193/437 [00:04<00:05, 45.37it/s][A
 45%|     | 198/437 [00:04<00:05, 45.21it/s][A
 46%|     | 203/437 [00:04<00:05, 45.10it/s][A
 48%|     | 208/437 [00:05<00:06, 34.68it/s][A
 49%|     | 213/437 [00:05<00:05, 37.49it/s][A
 50%|     | 218/437 [00:05<00:05, 39.71it/s][A
 51%|     | 223/437 [00:05<00:05, 41.38it/s][A
 52%|    | 228/437 [00:05<00:04, 42.56it/s][A
 53%|    | 233/437 [00:05<00:04, 43.51it/s][A
 54%|    | 238/437 [00:05<00:04, 44.07it/s][A
 56%|    | 243/437 [00:05<00:04, 44.64it/s][A
 57%|    | 248/437 [00:05<00:04, 44.55it/s][A
 58%|    | 253/437 [00:06<00:04, 44.54it/s][A
 59%|    | 258/437 [00:06<00:03, 44.89it/s][A
 60%|    | 263/437 [00:06<00:03, 45.18it/s][A
 61%|   | 268/437 [00:06<00:03, 45.32it/s][A
 62%|   | 273/437 [00:06<00:03, 45.45it/s][A
 64%|   | 278/437 [00:06<00:03, 45.43it/s][A
 65%|   | 283/437 [00:06<00:03, 45.60it/s][A
 66%|   | 288/437 [00:06<00:03, 45.52it/s][A
 67%|   | 293/437 [00:06<00:03, 45.19it/s][A
 68%|   | 298/437 [00:07<00:03, 45.02it/s][A
 69%|   | 303/437 [00:07<00:02, 45.13it/s][A
 70%|   | 308/437 [00:07<00:02, 45.32it/s][A
 72%|  | 313/437 [00:07<00:02, 45.13it/s][A
 73%|  | 318/437 [00:07<00:02, 45.54it/s][A
 74%|  | 323/437 [00:07<00:02, 45.67it/s][A
 75%|  | 328/437 [00:07<00:02, 45.67it/s][A
 76%|  | 333/437 [00:07<00:02, 45.65it/s][A
 77%|  | 338/437 [00:07<00:02, 43.14it/s][A
 78%|  | 343/437 [00:08<00:02, 43.87it/s][A
 80%|  | 348/437 [00:08<00:02, 44.34it/s][A
 81%|  | 353/437 [00:08<00:01, 44.69it/s][A
 82%| | 358/437 [00:08<00:01, 44.96it/s][A
 83%| | 363/437 [00:08<00:01, 45.18it/s][A
 84%| | 368/437 [00:08<00:01, 45.41it/s][A
 85%| | 373/437 [00:08<00:01, 45.53it/s][A
 86%| | 378/437 [00:08<00:01, 45.16it/s][A
 88%| | 383/437 [00:08<00:01, 45.02it/s][A
 89%| | 388/437 [00:09<00:01, 45.12it/s][A
 90%| | 393/437 [00:09<00:00, 45.20it/s][A
 91%| | 398/437 [00:09<00:00, 45.31it/s][A
 92%|| 403/437 [00:09<00:00, 45.46it/s][A
 93%|| 408/437 [00:09<00:00, 45.49it/s][A
 95%|| 413/437 [00:09<00:00, 45.57it/s][A
 96%|| 418/437 [00:09<00:00, 45.67it/s][A
 97%|| 423/437 [00:09<00:00, 45.42it/s][A
 98%|| 428/437 [00:09<00:00, 45.33it/s][A
 99%|| 433/437 [00:10<00:00, 45.35it/s][A                                                 
                                                 [A 60%|    | 468/780 [04:42<01:56,  2.68it/s]
100%|| 437/437 [00:10<00:00, 45.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:27:14,905 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:27:15,285 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:27:23,820 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:27:24,462 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:27:24,601 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [05:39<1:46:12, 20.49s/it] 60%|    | 470/780 [05:40<1:14:44, 14.46s/it] 60%|    | 471/780 [05:40<52:35, 10.21s/it]   61%|    | 472/780 [05:40<37:08,  7.24s/it] 61%|    | 473/780 [05:41<26:21,  5.15s/it] 61%|    | 474/780 [05:41<18:50,  3.69s/it] 61%|    | 475/780 [05:41<13:35,  2.67s/it] 61%|    | 476/780 [05:41<09:55,  1.96s/it] 61%|    | 477/780 [05:42<07:21,  1.46s/it] 61%|   | 478/780 [05:42<05:34,  1.11s/it] 61%|   | 479/780 [05:42<04:19,  1.16it/s] 62%|   | 480/780 [05:43<03:39,  1.37it/s] 62%|   | 481/780 [05:43<02:59,  1.67it/s] 62%|   | 482/780 [05:43<02:31,  1.97it/s] 62%|   | 483/780 [05:44<02:19,  2.13it/s] 62%|   | 484/780 [05:44<02:02,  2.41it/s] 62%|   | 485/780 [05:44<01:51,  2.64it/s] 62%|   | 486/780 [05:45<01:43,  2.84it/s] 62%|   | 487/780 [05:45<01:37,  2.99it/s] 63%|   | 488/780 [05:45<01:33,  3.11it/s] 63%|   | 489/780 [05:45<01:30,  3.20it/s] 63%|   | 490/780 [05:46<01:28,  3.27it/s] 63%|   | 491/780 [05:46<01:27,  3.32it/s] 63%|   | 492/780 [05:46<01:26,  3.35it/s] 63%|   | 493/780 [05:47<01:42,  2.79it/s] 63%|   | 494/780 [05:47<01:36,  2.96it/s] 63%|   | 495/780 [05:47<01:32,  3.08it/s] 64%|   | 496/780 [05:48<01:29,  3.18it/s] 64%|   | 497/780 [05:48<01:27,  3.25it/s] 64%|   | 498/780 [05:48<01:25,  3.30it/s] 64%|   | 499/780 [05:49<01:24,  3.34it/s] 64%|   | 500/780 [05:49<01:23,  3.37it/s]                                                  64%|   | 500/780 [05:49<01:23,  3.37it/s] 64%|   | 501/780 [05:49<01:22,  3.38it/s] 64%|   | 502/780 [05:49<01:21,  3.39it/s] 64%|   | 503/780 [05:50<01:48,  2.54it/s] 65%|   | 504/780 [05:50<01:40,  2.76it/s] 65%|   | 505/780 [05:51<01:33,  2.93it/s] 65%|   | 506/780 [05:51<01:29,  3.07it/s] 65%|   | 507/780 [05:51<01:26,  3.16it/s] 65%|   | 508/780 [05:52<01:24,  3.24it/s] 65%|   | 509/780 [05:52<01:22,  3.29it/s] 65%|   | 510/780 [05:52<01:21,  3.33it/s] 66%|   | 511/780 [05:52<01:20,  3.36it/s] 66%|   | 512/780 [05:53<01:19,  3.38it/s] 66%|   | 513/780 [05:53<01:24,  3.17it/s] 66%|   | 514/780 [05:53<01:21,  3.25it/s] 66%|   | 515/780 [05:54<01:19,  3.32it/s] 66%|   | 516/780 [05:54<01:18,  3.36it/s] 66%|   | 517/780 [05:54<01:17,  3.40it/s] 66%|   | 518/780 [05:55<01:16,  3.42it/s] 67%|   | 519/780 [05:55<01:15,  3.43it/s] 67%|   | 520/780 [05:55<01:15,  3.45it/s] 67%|   | 521/780 [05:55<01:14,  3.45it/s] 67%|   | 522/780 [05:56<01:14,  3.46it/s] 67%|   | 523/780 [05:56<01:14,  3.46it/s] 67%|   | 524/780 [05:57<02:04,  2.06it/s] 67%|   | 525/780 [05:57<01:48,  2.35it/s] 67%|   | 526/780 [05:57<01:37,  2.60it/s] 68%|   | 527/780 [05:58<01:30,  2.81it/s] 68%|   | 528/780 [05:58<01:24,  2.98it/s] 68%|   | 529/780 [05:58<01:20,  3.11it/s] 68%|   | 530/780 [05:59<01:17,  3.21it/s] 68%|   | 531/780 [05:59<01:15,  3.28it/s] 68%|   | 532/780 [05:59<01:14,  3.34it/s] 68%|   | 533/780 [06:00<01:22,  2.99it/s] 68%|   | 534/780 [06:00<01:18,  3.12it/s] 69%|   | 535/780 [06:00<01:16,  3.22it/s] 69%|   | 536/780 [06:00<01:14,  3.29it/s] 69%|   | 537/780 [06:01<01:12,  3.34it/s] 69%|   | 538/780 [06:01<01:11,  3.38it/s] 69%|   | 539/780 [06:01<01:10,  3.40it/s] 69%|   | 540/780 [06:02<01:10,  3.42it/s] 69%|   | 541/780 [06:02<01:09,  3.43it/s] 69%|   | 542/780 [06:02<01:09,  3.44it/s] 70%|   | 543/780 [06:03<01:48,  2.18it/s] 70%|   | 544/780 [06:03<01:36,  2.45it/s] 70%|   | 545/780 [06:04<01:27,  2.69it/s] 70%|   | 546/780 [06:04<01:21,  2.88it/s] 70%|   | 547/780 [06:04<01:16,  3.04it/s] 70%|   | 548/780 [06:05<01:13,  3.15it/s] 70%|   | 549/780 [06:05<01:11,  3.24it/s] 71%|   | 550/780 [06:05<01:09,  3.31it/s] 71%|   | 551/780 [06:05<01:08,  3.35it/s] 71%|   | 552/780 [06:06<01:12,  3.15it/s] 71%|   | 553/780 [06:06<01:10,  3.24it/s] 71%|   | 554/780 [06:06<01:08,  3.30it/s] 71%|   | 555/780 [06:07<01:07,  3.35it/s] 71%|  | 556/780 [06:07<01:06,  3.38it/s] 71%|  | 557/780 [06:07<01:05,  3.41it/s] 72%|  | 558/780 [06:07<01:04,  3.43it/s] 72%|  | 559/780 [06:08<01:04,  3.44it/s] 72%|  | 560/780 [06:08<01:03,  3.44it/s] 72%|  | 561/780 [06:08<01:03,  3.45it/s] 72%|  | 562/780 [06:09<01:03,  3.45it/s] 72%|  | 563/780 [06:09<01:07,  3.22it/s] 72%|  | 564/780 [06:09<01:05,  3.29it/s] 72%|  | 565/780 [06:10<01:04,  3.34it/s] 73%|  | 566/780 [06:10<01:03,  3.38it/s] 73%|  | 567/780 [06:10<01:02,  3.40it/s] 73%|  | 568/780 [06:10<01:01,  3.42it/s] 73%|  | 569/780 [06:11<01:01,  3.44it/s] 73%|  | 570/780 [06:11<01:00,  3.44it/s] 73%|  | 571/780 [06:11<01:00,  3.45it/s] 73%|  | 572/780 [06:12<01:00,  3.45it/s] 73%|  | 573/780 [06:12<00:59,  3.46it/s] 74%|  | 574/780 [06:12<01:01,  3.33it/s] 74%|  | 575/780 [06:12<01:00,  3.37it/s] 74%|  | 576/780 [06:13<01:00,  3.39it/s] 74%|  | 577/780 [06:13<00:59,  3.42it/s] 74%|  | 578/780 [06:13<00:58,  3.43it/s] 74%|  | 579/780 [06:14<00:58,  3.44it/s] 74%|  | 580/780 [06:14<00:57,  3.45it/s] 74%|  | 581/780 [06:14<00:57,  3.45it/s] 75%|  | 582/780 [06:15<00:57,  3.44it/s] 75%|  | 583/780 [06:15<00:57,  3.45it/s] 75%|  | 584/780 [06:15<00:56,  3.45it/s] 75%|  | 585/780 [06:15<00:56,  3.46it/s] 75%|  | 586/780 [06:16<00:56,  3.46it/s] 75%|  | 587/780 [06:16<00:55,  3.46it/s] 75%|  | 588/780 [06:16<00:55,  3.46it/s] 76%|  | 589/780 [06:17<00:55,  3.46it/s] 76%|  | 590/780 [06:17<00:54,  3.46it/s] 76%|  | 591/780 [06:17<00:54,  3.46it/s] 76%|  | 592/780 [06:18<01:00,  3.10it/s] 76%|  | 593/780 [06:18<00:58,  3.20it/s] 76%|  | 594/780 [06:18<00:56,  3.27it/s] 76%|  | 595/780 [06:18<00:55,  3.33it/s] 76%|  | 596/780 [06:19<00:54,  3.36it/s] 77%|  | 597/780 [06:19<00:53,  3.39it/s] 77%|  | 598/780 [06:19<00:53,  3.41it/s] 77%|  | 599/780 [06:20<00:52,  3.42it/s] 77%|  | 600/780 [06:20<00:52,  3.44it/s] 77%|  | 601/780 [06:20<00:51,  3.44it/s] 77%|  | 602/780 [06:21<01:09,  2.55it/s] 77%|  | 603/780 [06:21<01:03,  2.77it/s] 77%|  | 604/780 [06:21<00:59,  2.95it/s] 78%|  | 605/780 [06:22<00:56,  3.09it/s] 78%|  | 606/780 [06:22<00:54,  3.19it/s] 78%|  | 607/780 [06:22<00:52,  3.27it/s] 78%|  | 608/780 [06:22<00:51,  3.32it/s] 78%|  | 609/780 [06:23<00:50,  3.36it/s] 78%|  | 610/780 [06:23<00:50,  3.39it/s] 78%|  | 611/780 [06:23<00:49,  3.42it/s] 78%|  | 612/780 [06:24<00:57,  2.91it/s] 79%|  | 613/780 [06:24<00:54,  3.05it/s] 79%|  | 614/780 [06:24<00:52,  3.16it/s] 79%|  | 615/780 [06:25<00:50,  3.25it/s] 79%|  | 616/780 [06:25<00:49,  3.31it/s] 79%|  | 617/780 [06:25<00:48,  3.35it/s] 79%|  | 618/780 [06:26<00:47,  3.39it/s] 79%|  | 619/780 [06:26<00:47,  3.40it/s] 79%|  | 620/780 [06:26<00:46,  3.42it/s] 80%|  | 621/780 [06:26<00:46,  3.43it/s] 80%|  | 622/780 [06:27<00:52,  2.98it/s] 80%|  | 623/780 [06:27<00:50,  3.11it/s] 80%|  | 624/780 [06:28<01:12,  2.16it/s][INFO|trainer.py:2140] 2023-08-28 21:29:00,193 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:29:00,193 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:29:00,193 >>   Batch size = 8
{'eval_loss': 0.9293436408042908, 'eval_runtime': 10.1186, 'eval_samples_per_second': 344.811, 'eval_steps_per_second': 43.188, 'epoch': 3.0}
{'loss': 0.8541, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.05it/s][A
  3%|         | 12/437 [00:00<00:08, 49.76it/s][A
  4%|         | 18/437 [00:00<00:08, 47.82it/s][A
  5%|         | 23/437 [00:00<00:08, 47.15it/s][A
  6%|         | 28/437 [00:00<00:08, 46.67it/s][A
  8%|         | 33/437 [00:00<00:08, 46.24it/s][A
  9%|         | 38/437 [00:00<00:08, 45.71it/s][A
 10%|         | 43/437 [00:00<00:08, 45.33it/s][A
 11%|         | 48/437 [00:01<00:08, 45.35it/s][A
 12%|        | 53/437 [00:01<00:08, 45.48it/s][A
 13%|        | 58/437 [00:01<00:08, 45.55it/s][A
 14%|        | 63/437 [00:01<00:08, 45.54it/s][A
 16%|        | 68/437 [00:01<00:12, 30.27it/s][A
 17%|        | 73/437 [00:01<00:10, 33.77it/s][A
 18%|        | 78/437 [00:01<00:09, 36.72it/s][A
 19%|        | 83/437 [00:01<00:09, 38.95it/s][A
 20%|        | 88/437 [00:02<00:08, 40.80it/s][A
 21%|       | 93/437 [00:02<00:08, 42.19it/s][A
 22%|       | 98/437 [00:02<00:07, 43.19it/s][A
 24%|       | 103/437 [00:02<00:07, 43.85it/s][A
 25%|       | 108/437 [00:02<00:07, 43.92it/s][A
 26%|       | 113/437 [00:02<00:07, 44.10it/s][A
 27%|       | 118/437 [00:02<00:07, 44.49it/s][A
 28%|       | 123/437 [00:02<00:07, 44.83it/s][A
 29%|       | 128/437 [00:02<00:06, 45.04it/s][A
 30%|       | 133/437 [00:03<00:06, 45.21it/s][A
 32%|      | 138/437 [00:03<00:06, 45.34it/s][A
 33%|      | 143/437 [00:03<00:06, 45.49it/s][A
 34%|      | 148/437 [00:03<00:06, 45.54it/s][A
 35%|      | 153/437 [00:03<00:06, 45.42it/s][A
 36%|      | 158/437 [00:03<00:06, 45.23it/s][A
 37%|      | 163/437 [00:03<00:06, 45.19it/s][A
 38%|      | 168/437 [00:03<00:05, 45.19it/s][A
 40%|      | 173/437 [00:03<00:05, 45.43it/s][A
 41%|      | 178/437 [00:04<00:05, 45.45it/s][A
 42%|     | 183/437 [00:04<00:05, 45.50it/s][A
 43%|     | 188/437 [00:04<00:05, 45.57it/s][A
 44%|     | 193/437 [00:04<00:05, 45.54it/s][A
 45%|     | 198/437 [00:04<00:10, 22.45it/s][A
 46%|     | 203/437 [00:04<00:08, 26.50it/s][A
 48%|     | 208/437 [00:05<00:07, 30.35it/s][A
 49%|     | 213/437 [00:05<00:06, 33.75it/s][A
 50%|     | 218/437 [00:05<00:05, 36.64it/s][A
 51%|     | 223/437 [00:05<00:05, 38.92it/s][A
 52%|    | 228/437 [00:05<00:05, 40.76it/s][A
 53%|    | 233/437 [00:05<00:04, 42.10it/s][A
 54%|    | 238/437 [00:05<00:04, 42.66it/s][A
 56%|    | 243/437 [00:05<00:04, 43.32it/s][A
 57%|    | 248/437 [00:05<00:04, 44.00it/s][A
 58%|    | 253/437 [00:06<00:04, 44.50it/s][A
 59%|    | 258/437 [00:06<00:03, 44.86it/s][A
 60%|    | 263/437 [00:06<00:03, 45.17it/s][A
 61%|   | 268/437 [00:06<00:03, 45.30it/s][A
 62%|   | 273/437 [00:07<00:15, 10.80it/s][A
 63%|   | 277/437 [00:07<00:12, 13.06it/s][A
 65%|   | 282/437 [00:07<00:09, 16.85it/s][A
 66%|   | 287/437 [00:08<00:07, 20.95it/s][A
 67%|   | 292/437 [00:08<00:05, 25.13it/s][A
 68%|   | 297/437 [00:08<00:09, 14.02it/s][A
 69%|   | 302/437 [00:08<00:07, 17.77it/s][A
 70%|   | 307/437 [00:09<00:05, 21.79it/s][A
 71%|  | 312/437 [00:09<00:04, 25.84it/s][A
 73%|  | 317/437 [00:09<00:04, 29.75it/s][A
 74%|  | 322/437 [00:09<00:03, 33.28it/s][A
 75%|  | 327/437 [00:09<00:03, 36.21it/s][A
 76%|  | 332/437 [00:09<00:02, 38.64it/s][A
 77%|  | 337/437 [00:09<00:02, 40.36it/s][A
 78%|  | 342/437 [00:09<00:02, 41.53it/s][A
 79%|  | 347/437 [00:09<00:02, 42.63it/s][A
 81%|  | 352/437 [00:10<00:01, 43.45it/s][A
 82%| | 357/437 [00:10<00:01, 44.12it/s][A
 83%| | 362/437 [00:10<00:01, 44.58it/s][A
 84%| | 367/437 [00:10<00:01, 44.94it/s][A
 85%| | 372/437 [00:10<00:01, 45.07it/s][A
 86%| | 377/437 [00:10<00:01, 45.13it/s][A
 87%| | 382/437 [00:11<00:01, 45.10it/s][A
 89%| | 387/437 [00:11<00:02, 22.28it/s][A
 90%| | 392/437 [00:11<00:01, 26.33it/s][A
 91%| | 397/437 [00:11<00:01, 30.15it/s][A
 92%|| 402/437 [00:11<00:01, 33.59it/s][A
 93%|| 407/437 [00:11<00:00, 36.49it/s][A
 94%|| 412/437 [00:11<00:00, 38.88it/s][A
 95%|| 417/437 [00:11<00:00, 40.66it/s][A
 97%|| 422/437 [00:11<00:00, 42.02it/s][A
 98%|| 427/437 [00:12<00:00, 42.70it/s][A
 99%|| 432/437 [00:12<00:00, 43.34it/s][A
100%|| 437/437 [00:12<00:00, 44.15it/s][A                                                 
                                                 [A 80%|  | 624/780 [06:40<01:12,  2.16it/s]
100%|| 437/437 [00:12<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:29:13,526 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:29:15,146 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:28,858 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:29,435 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:29,679 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [07:33<51:02, 19.76s/it] 80%|  | 626/780 [07:34<36:22, 14.17s/it] 80%|  | 627/780 [07:34<25:31, 10.01s/it] 81%|  | 628/780 [07:34<17:58,  7.09s/it] 81%|  | 629/780 [07:38<15:02,  5.98s/it] 81%|  | 630/780 [07:38<10:49,  4.33s/it] 81%|  | 631/780 [07:39<07:44,  3.12s/it] 81%|  | 632/780 [07:39<05:35,  2.27s/it] 81%|  | 633/780 [07:39<04:06,  1.68s/it] 81%| | 634/780 [07:39<03:04,  1.26s/it] 81%| | 635/780 [07:40<02:20,  1.03it/s] 82%| | 636/780 [07:40<01:50,  1.30it/s] 82%| | 637/780 [07:40<01:29,  1.60it/s] 82%| | 638/780 [07:41<01:14,  1.91it/s] 82%| | 639/780 [07:41<01:04,  2.20it/s] 82%| | 640/780 [07:41<01:06,  2.11it/s] 82%| | 641/780 [07:42<00:58,  2.39it/s] 82%| | 642/780 [07:42<00:52,  2.63it/s] 82%| | 643/780 [07:42<00:48,  2.82it/s] 83%| | 644/780 [07:43<00:45,  2.98it/s] 83%| | 645/780 [07:43<00:43,  3.10it/s] 83%| | 646/780 [07:43<00:41,  3.20it/s] 83%| | 647/780 [07:43<00:40,  3.26it/s] 83%| | 648/780 [07:44<00:39,  3.31it/s] 83%| | 649/780 [07:44<00:39,  3.35it/s] 83%| | 650/780 [07:44<00:40,  3.21it/s] 83%| | 651/780 [07:45<00:39,  3.27it/s] 84%| | 652/780 [07:45<00:38,  3.32it/s] 84%| | 653/780 [07:45<00:37,  3.35it/s] 84%| | 654/780 [07:46<00:37,  3.37it/s] 84%| | 655/780 [07:46<00:36,  3.39it/s] 84%| | 656/780 [07:46<00:36,  3.40it/s] 84%| | 657/780 [07:46<00:36,  3.41it/s] 84%| | 658/780 [07:47<00:55,  2.21it/s] 84%| | 659/780 [07:48<00:48,  2.47it/s] 85%| | 660/780 [07:48<00:44,  2.70it/s] 85%| | 661/780 [07:48<00:41,  2.88it/s] 85%| | 662/780 [07:48<00:39,  3.02it/s] 85%| | 663/780 [07:49<00:37,  3.13it/s] 85%| | 664/780 [07:49<00:36,  3.21it/s] 85%| | 665/780 [07:49<00:35,  3.27it/s] 85%| | 666/780 [07:50<00:45,  2.50it/s] 86%| | 667/780 [07:50<00:41,  2.72it/s] 86%| | 668/780 [07:51<00:38,  2.90it/s] 86%| | 669/780 [07:51<00:36,  3.03it/s] 86%| | 670/780 [07:51<00:35,  3.14it/s] 86%| | 671/780 [07:51<00:33,  3.22it/s] 86%| | 672/780 [07:52<00:32,  3.27it/s] 86%| | 673/780 [07:52<00:32,  3.32it/s] 86%| | 674/780 [07:52<00:31,  3.34it/s] 87%| | 675/780 [07:53<00:31,  3.37it/s] 87%| | 676/780 [07:53<00:40,  2.55it/s] 87%| | 677/780 [07:53<00:37,  2.76it/s] 87%| | 678/780 [07:54<00:34,  2.93it/s] 87%| | 679/780 [07:54<00:32,  3.06it/s] 87%| | 680/780 [07:54<00:31,  3.16it/s] 87%| | 681/780 [07:55<00:30,  3.23it/s] 87%| | 682/780 [07:55<00:29,  3.29it/s] 88%| | 683/780 [07:55<00:29,  3.32it/s] 88%| | 684/780 [07:56<00:28,  3.35it/s] 88%| | 685/780 [07:56<00:28,  3.37it/s] 88%| | 686/780 [07:57<00:43,  2.15it/s] 88%| | 687/780 [07:57<00:38,  2.42it/s] 88%| | 688/780 [07:57<00:34,  2.65it/s] 88%| | 689/780 [07:58<00:32,  2.84it/s] 88%| | 690/780 [07:58<00:30,  2.99it/s] 89%| | 691/780 [07:58<00:28,  3.10it/s] 89%| | 692/780 [07:58<00:27,  3.19it/s] 89%| | 693/780 [07:59<00:26,  3.26it/s] 89%| | 694/780 [07:59<00:26,  3.30it/s] 89%| | 695/780 [08:00<00:30,  2.75it/s] 89%| | 696/780 [08:00<00:28,  2.93it/s] 89%| | 697/780 [08:00<00:27,  3.07it/s] 89%| | 698/780 [08:00<00:25,  3.18it/s] 90%| | 699/780 [08:01<00:24,  3.26it/s] 90%| | 700/780 [08:01<00:24,  3.32it/s] 90%| | 701/780 [08:01<00:23,  3.36it/s] 90%| | 702/780 [08:02<00:22,  3.39it/s] 90%| | 703/780 [08:02<00:22,  3.42it/s] 90%| | 704/780 [08:02<00:22,  3.43it/s] 90%| | 705/780 [08:02<00:23,  3.22it/s] 91%| | 706/780 [08:03<00:22,  3.29it/s] 91%| | 707/780 [08:03<00:21,  3.34it/s] 91%| | 708/780 [08:03<00:21,  3.38it/s] 91%| | 709/780 [08:04<00:20,  3.40it/s] 91%| | 710/780 [08:04<00:20,  3.42it/s] 91%| | 711/780 [08:04<00:20,  3.43it/s] 91%|| 712/780 [08:05<00:24,  2.79it/s] 91%|| 713/780 [08:05<00:22,  2.96it/s] 92%|| 714/780 [08:05<00:21,  3.09it/s] 92%|| 715/780 [08:06<00:21,  3.03it/s] 92%|| 716/780 [08:06<00:20,  3.14it/s] 92%|| 717/780 [08:06<00:19,  3.23it/s] 92%|| 718/780 [08:07<00:18,  3.30it/s] 92%|| 719/780 [08:07<00:18,  3.35it/s] 92%|| 720/780 [08:07<00:17,  3.38it/s] 92%|| 721/780 [08:07<00:17,  3.40it/s] 93%|| 722/780 [08:08<00:16,  3.41it/s] 93%|| 723/780 [08:08<00:16,  3.43it/s] 93%|| 724/780 [08:08<00:16,  3.44it/s] 93%|| 725/780 [08:09<00:15,  3.44it/s] 93%|| 726/780 [08:09<00:16,  3.20it/s] 93%|| 727/780 [08:09<00:16,  3.28it/s] 93%|| 728/780 [08:09<00:15,  3.33it/s] 93%|| 729/780 [08:10<00:15,  3.36it/s] 94%|| 730/780 [08:10<00:14,  3.39it/s] 94%|| 731/780 [08:10<00:14,  3.41it/s] 94%|| 732/780 [08:11<00:14,  3.42it/s] 94%|| 733/780 [08:11<00:13,  3.44it/s] 94%|| 734/780 [08:11<00:13,  3.44it/s] 94%|| 735/780 [08:12<00:13,  3.45it/s] 94%|| 736/780 [08:12<00:12,  3.45it/s] 94%|| 737/780 [08:12<00:16,  2.67it/s] 95%|| 738/780 [08:13<00:14,  2.87it/s] 95%|| 739/780 [08:13<00:13,  3.02it/s] 95%|| 740/780 [08:13<00:12,  3.14it/s] 95%|| 741/780 [08:14<00:12,  3.23it/s] 95%|| 742/780 [08:14<00:11,  3.30it/s] 95%|| 743/780 [08:14<00:11,  3.34it/s] 95%|| 744/780 [08:14<00:10,  3.37it/s] 96%|| 745/780 [08:15<00:10,  3.39it/s] 96%|| 746/780 [08:15<00:09,  3.41it/s] 96%|| 747/780 [08:15<00:09,  3.31it/s] 96%|| 748/780 [08:16<00:09,  3.35it/s] 96%|| 749/780 [08:16<00:09,  3.38it/s] 96%|| 750/780 [08:16<00:08,  3.41it/s] 96%|| 751/780 [08:16<00:08,  3.42it/s] 96%|| 752/780 [08:17<00:08,  3.43it/s] 97%|| 753/780 [08:17<00:07,  3.44it/s] 97%|| 754/780 [08:18<00:09,  2.89it/s] 97%|| 755/780 [08:18<00:08,  3.04it/s] 97%|| 756/780 [08:18<00:07,  3.15it/s] 97%|| 757/780 [08:18<00:07,  3.24it/s] 97%|| 758/780 [08:19<00:06,  3.30it/s] 97%|| 759/780 [08:19<00:06,  3.35it/s] 97%|| 760/780 [08:19<00:05,  3.38it/s] 98%|| 761/780 [08:20<00:05,  3.41it/s] 98%|| 762/780 [08:20<00:05,  3.42it/s] 98%|| 763/780 [08:20<00:04,  3.43it/s] 98%|| 764/780 [08:21<00:05,  2.93it/s] 98%|| 765/780 [08:21<00:04,  3.07it/s] 98%|| 766/780 [08:21<00:04,  3.18it/s] 98%|| 767/780 [08:21<00:03,  3.26it/s] 98%|| 768/780 [08:22<00:03,  3.32it/s] 99%|| 769/780 [08:22<00:03,  3.36it/s] 99%|| 770/780 [08:22<00:02,  3.39it/s] 99%|| 771/780 [08:23<00:02,  3.41it/s] 99%|| 772/780 [08:23<00:02,  3.43it/s] 99%|| 773/780 [08:23<00:02,  3.44it/s] 99%|| 774/780 [08:24<00:01,  3.18it/s] 99%|| 775/780 [08:24<00:01,  3.26it/s] 99%|| 776/780 [08:24<00:01,  3.32it/s]100%|| 777/780 [08:24<00:00,  3.36it/s]100%|| 778/780 [08:25<00:00,  3.39it/s]100%|| 779/780 [08:25<00:00,  3.41it/s]100%|| 780/780 [08:25<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 21:30:57,417 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:30:57,417 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:30:57,417 >>   Batch size = 8
{'eval_loss': 0.9285356402397156, 'eval_runtime': 12.3413, 'eval_samples_per_second': 282.71, 'eval_steps_per_second': 35.41, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.70it/s][A
  3%|         | 12/437 [00:00<00:08, 49.19it/s][A
  4%|         | 17/437 [00:00<00:08, 47.31it/s][A
  5%|         | 22/437 [00:00<00:08, 46.44it/s][A
  6%|         | 27/437 [00:00<00:08, 45.91it/s][A
  7%|         | 32/437 [00:00<00:08, 45.63it/s][A
  8%|         | 37/437 [00:00<00:08, 45.51it/s][A
 10%|         | 42/437 [00:01<00:08, 45.29it/s][A
 11%|         | 47/437 [00:01<00:14, 26.11it/s][A
 12%|        | 52/437 [00:01<00:12, 30.04it/s][A
 13%|        | 57/437 [00:01<00:11, 33.51it/s][A
 14%|        | 62/437 [00:01<00:10, 36.46it/s][A
 15%|        | 67/437 [00:01<00:09, 38.82it/s][A
 16%|        | 72/437 [00:01<00:08, 40.62it/s][A
 18%|        | 77/437 [00:01<00:08, 42.05it/s][A
 19%|        | 82/437 [00:02<00:08, 43.02it/s][A
 20%|        | 87/437 [00:02<00:08, 43.33it/s][A
 21%|        | 92/437 [00:02<00:07, 43.70it/s][A
 22%|       | 97/437 [00:02<00:07, 43.94it/s][A
 23%|       | 102/437 [00:02<00:07, 44.43it/s][A
 24%|       | 107/437 [00:02<00:07, 44.77it/s][A
 26%|       | 112/437 [00:03<00:13, 24.29it/s][A
 27%|       | 118/437 [00:03<00:10, 29.53it/s][A
 28%|       | 123/437 [00:03<00:09, 32.85it/s][A
 29%|       | 128/437 [00:03<00:08, 35.80it/s][A
 30%|       | 133/437 [00:03<00:07, 38.23it/s][A
 32%|      | 138/437 [00:03<00:07, 40.16it/s][A
 33%|      | 143/437 [00:03<00:07, 41.68it/s][A
 34%|      | 148/437 [00:03<00:06, 42.72it/s][A
 35%|      | 153/437 [00:04<00:06, 43.47it/s][A
 36%|      | 158/437 [00:04<00:09, 30.18it/s][A
 37%|      | 163/437 [00:04<00:08, 33.60it/s][A
 38%|      | 168/437 [00:04<00:07, 36.51it/s][A
 40%|      | 173/437 [00:04<00:06, 38.84it/s][A
 41%|      | 178/437 [00:04<00:06, 40.69it/s][A
 42%|     | 183/437 [00:04<00:06, 42.02it/s][A
 43%|     | 188/437 [00:04<00:05, 43.08it/s][A
 44%|     | 193/437 [00:04<00:05, 43.79it/s][A
 45%|     | 198/437 [00:05<00:05, 43.91it/s][A
 46%|     | 203/437 [00:05<00:05, 43.92it/s][A
 48%|     | 208/437 [00:05<00:05, 44.15it/s][A
 49%|     | 213/437 [00:05<00:05, 44.55it/s][A
 50%|     | 218/437 [00:05<00:04, 44.83it/s][A
 51%|     | 223/437 [00:05<00:04, 45.09it/s][A
 52%|    | 228/437 [00:05<00:04, 45.25it/s][A
 53%|    | 233/437 [00:05<00:04, 45.47it/s][A
 54%|    | 238/437 [00:05<00:04, 45.39it/s][A
 56%|    | 243/437 [00:06<00:04, 45.13it/s][A
 57%|    | 248/437 [00:06<00:04, 44.89it/s][A
 58%|    | 253/437 [00:06<00:04, 44.81it/s][A
 59%|    | 258/437 [00:06<00:03, 45.00it/s][A
 60%|    | 263/437 [00:06<00:03, 45.09it/s][A
 61%|   | 268/437 [00:06<00:03, 45.27it/s][A
 62%|   | 273/437 [00:06<00:03, 45.49it/s][A
 64%|   | 278/437 [00:06<00:03, 45.51it/s][A
 65%|   | 283/437 [00:07<00:03, 45.46it/s][A
 66%|   | 288/437 [00:07<00:06, 24.06it/s][A
 67%|   | 293/437 [00:07<00:05, 28.04it/s][A
 68%|   | 298/437 [00:07<00:04, 31.71it/s][A
 69%|   | 303/437 [00:07<00:03, 34.93it/s][A
 70%|   | 308/437 [00:07<00:03, 37.60it/s][A
 72%|  | 313/437 [00:07<00:03, 39.68it/s][A
 73%|  | 318/437 [00:08<00:02, 41.33it/s][A
 74%|  | 323/437 [00:08<00:02, 42.52it/s][A
 75%|  | 328/437 [00:08<00:02, 43.03it/s][A
 76%|  | 333/437 [00:08<00:02, 43.37it/s][A
 77%|  | 338/437 [00:08<00:02, 43.80it/s][A
 78%|  | 343/437 [00:08<00:02, 44.30it/s][A
 80%|  | 348/437 [00:08<00:01, 44.65it/s][A
 81%|  | 353/437 [00:08<00:01, 44.89it/s][A
 82%| | 358/437 [00:08<00:01, 45.15it/s][A
 83%| | 363/437 [00:09<00:01, 45.32it/s][A
 84%| | 368/437 [00:09<00:01, 45.27it/s][A
 85%| | 373/437 [00:09<00:01, 45.07it/s][A
 86%| | 378/437 [00:09<00:01, 44.81it/s][A
 88%| | 383/437 [00:09<00:01, 44.93it/s][A
 89%| | 388/437 [00:09<00:01, 44.98it/s][A
 90%| | 393/437 [00:09<00:00, 45.21it/s][A
 91%| | 398/437 [00:09<00:00, 45.34it/s][A
 92%|| 403/437 [00:09<00:00, 45.47it/s][A
 93%|| 408/437 [00:10<00:00, 45.54it/s][A
 95%|| 413/437 [00:10<00:00, 26.77it/s][A
 96%|| 418/437 [00:10<00:00, 30.90it/s][A
 97%|| 423/437 [00:10<00:00, 34.24it/s][A
 98%|| 428/437 [00:11<00:00, 11.65it/s][A
 99%|| 433/437 [00:11<00:00, 15.07it/s][A                                                 
                                                 [A100%|| 780/780 [08:37<00:00,  3.42it/s]
100%|| 437/437 [00:11<00:00, 15.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:31:09,600 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:31:11,164 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:31:22,762 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:31:24,008 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:31:24,551 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:32:03,208 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:32:03,260 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624 (score: 0.9285356402397156).
                                                 100%|| 780/780 [10:15<00:00,  3.42it/s]100%|| 780/780 [10:15<00:00,  1.27it/s]
[INFO|trainer.py:1894] 2023-08-28 21:32:48,234 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 21:32:48,601 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:32:57,736 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:32:58,840 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:32:58,988 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:33:01,987 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,068 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,068 >>   train_loss               =     0.8383
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,068 >>   train_runtime            = 0:10:15.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,068 >>   train_samples            =      10011
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,069 >>   train_samples_per_second =     81.302
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:02,069 >>   train_steps_per_second   =      1.267
{'eval_loss': 0.9305135011672974, 'eval_runtime': 11.9136, 'eval_samples_per_second': 292.857, 'eval_steps_per_second': 36.681, 'epoch': 5.0}
{'train_runtime': 615.6702, 'train_samples_per_second': 81.302, 'train_steps_per_second': 1.267, 'train_loss': 0.8383415222167969, 'epoch': 5.0}
08/28/2023 21:33:04 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:33:05,801 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:33:05,801 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-28 21:33:05,801 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 56.28it/s]  3%|         | 12/437 [00:00<00:08, 49.88it/s]  4%|         | 18/437 [00:00<00:08, 48.28it/s]  5%|         | 23/437 [00:00<00:08, 47.44it/s]  6%|         | 28/437 [00:00<00:08, 47.03it/s]  8%|         | 33/437 [00:00<00:08, 46.70it/s]  9%|         | 38/437 [00:00<00:08, 46.57it/s] 10%|         | 43/437 [00:00<00:08, 46.28it/s] 11%|         | 48/437 [00:01<00:08, 45.84it/s] 12%|        | 53/437 [00:01<00:08, 45.58it/s] 13%|        | 58/437 [00:01<00:08, 45.74it/s] 14%|        | 63/437 [00:01<00:08, 45.92it/s] 16%|        | 68/437 [00:01<00:08, 46.07it/s] 17%|        | 73/437 [00:01<00:07, 46.04it/s] 18%|        | 78/437 [00:01<00:07, 45.99it/s] 19%|        | 83/437 [00:01<00:07, 46.01it/s] 20%|        | 88/437 [00:01<00:07, 45.82it/s] 21%|       | 93/437 [00:02<00:07, 45.58it/s] 22%|       | 98/437 [00:02<00:07, 45.47it/s] 24%|       | 103/437 [00:02<00:07, 45.58it/s] 25%|       | 108/437 [00:02<00:08, 37.43it/s] 26%|       | 113/437 [00:02<00:08, 39.82it/s] 27%|       | 118/437 [00:02<00:07, 41.54it/s] 28%|       | 123/437 [00:02<00:07, 42.85it/s] 29%|       | 128/437 [00:02<00:07, 43.87it/s] 30%|       | 133/437 [00:02<00:06, 44.58it/s] 32%|      | 138/437 [00:03<00:06, 45.12it/s] 33%|      | 143/437 [00:03<00:06, 45.43it/s] 34%|      | 148/437 [00:03<00:06, 45.16it/s] 35%|      | 153/437 [00:03<00:06, 44.83it/s] 36%|      | 158/437 [00:03<00:06, 44.81it/s] 37%|      | 163/437 [00:03<00:06, 45.17it/s] 38%|      | 168/437 [00:03<00:05, 45.56it/s] 40%|      | 173/437 [00:03<00:05, 45.74it/s] 41%|      | 178/437 [00:03<00:05, 45.94it/s] 42%|     | 183/437 [00:04<00:05, 46.03it/s] 43%|     | 188/437 [00:04<00:05, 45.96it/s] 44%|     | 193/437 [00:04<00:05, 45.64it/s] 45%|     | 198/437 [00:04<00:05, 45.28it/s] 46%|     | 203/437 [00:04<00:05, 45.02it/s] 48%|     | 208/437 [00:04<00:05, 45.30it/s] 49%|     | 213/437 [00:04<00:04, 45.38it/s] 50%|     | 218/437 [00:04<00:04, 45.65it/s] 51%|     | 223/437 [00:04<00:04, 45.89it/s] 52%|    | 228/437 [00:05<00:04, 46.01it/s] 53%|    | 233/437 [00:05<00:04, 46.04it/s] 54%|    | 238/437 [00:05<00:04, 45.77it/s] 56%|    | 243/437 [00:05<00:04, 41.68it/s] 57%|    | 248/437 [00:05<00:04, 42.92it/s] 58%|    | 253/437 [00:05<00:04, 43.76it/s] 59%|    | 258/437 [00:05<00:04, 44.34it/s] 60%|    | 263/437 [00:05<00:03, 44.69it/s] 61%|   | 268/437 [00:05<00:03, 45.17it/s] 62%|   | 273/437 [00:06<00:03, 45.46it/s] 64%|   | 278/437 [00:06<00:03, 45.64it/s] 65%|   | 283/437 [00:06<00:03, 45.22it/s] 66%|   | 288/437 [00:06<00:03, 45.15it/s] 67%|   | 293/437 [00:06<00:03, 45.30it/s] 68%|   | 298/437 [00:06<00:03, 45.58it/s] 69%|   | 303/437 [00:06<00:02, 45.61it/s] 70%|   | 308/437 [00:06<00:02, 45.61it/s] 72%|  | 313/437 [00:06<00:02, 45.71it/s] 73%|  | 318/437 [00:07<00:02, 45.84it/s] 74%|  | 323/437 [00:07<00:02, 45.83it/s] 75%|  | 328/437 [00:07<00:02, 45.53it/s] 76%|  | 333/437 [00:07<00:02, 45.40it/s] 77%|  | 338/437 [00:07<00:02, 45.37it/s] 78%|  | 343/437 [00:07<00:02, 45.49it/s] 80%|  | 348/437 [00:07<00:01, 45.56it/s] 81%|  | 353/437 [00:07<00:01, 45.73it/s] 82%| | 358/437 [00:07<00:01, 45.77it/s] 83%| | 363/437 [00:08<00:01, 45.86it/s] 84%| | 368/437 [00:08<00:01, 45.82it/s] 85%| | 373/437 [00:08<00:01, 45.70it/s] 86%| | 378/437 [00:08<00:01, 45.50it/s] 88%| | 383/437 [00:08<00:02, 25.69it/s] 89%| | 388/437 [00:08<00:01, 29.64it/s] 90%| | 393/437 [00:08<00:01, 33.21it/s] 91%| | 398/437 [00:09<00:01, 36.23it/s] 92%|| 403/437 [00:09<00:00, 38.65it/s] 93%|| 408/437 [00:09<00:00, 40.62it/s] 95%|| 413/437 [00:09<00:00, 42.13it/s] 96%|| 418/437 [00:09<00:00, 43.16it/s] 97%|| 423/437 [00:09<00:00, 43.53it/s] 98%|| 428/437 [00:09<00:00, 43.97it/s] 99%|| 433/437 [00:09<00:00, 44.47it/s]100%|| 437/437 [00:09<00:00, 44.03it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:33:15,746 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   eval_loss               =     0.9285
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   eval_runtime            = 0:00:09.94
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   eval_samples_per_second =    350.828
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   eval_steps_per_second   =     43.941
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:33:15,746 >>   perplexity              =     2.5308
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:42,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:43,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:43,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:43,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:43,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:33:44,306 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:33:44,307 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:45,125 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:46,644 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:46,686 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:51,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:51,145 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:51,145 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:51,145 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:33:51,145 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:33:52,225 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:33:52,238 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:33:53,043 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:33:53,390 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:33:53,391 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:04,  1.38it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:11,  1.19it/s]Extractor Predicting: 17it [00:11,  1.29it/s]Extractor Predicting: 18it [00:12,  1.37it/s]Extractor Predicting: 19it [00:13,  1.40it/s]Extractor Predicting: 20it [00:14,  1.26it/s]Extractor Predicting: 21it [00:14,  1.35it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.47it/s]Extractor Predicting: 25it [00:17,  1.28it/s]Extractor Predicting: 26it [00:18,  1.36it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.46it/s]Extractor Predicting: 29it [00:20,  1.49it/s]Extractor Predicting: 30it [00:21,  1.31it/s]Extractor Predicting: 31it [00:21,  1.38it/s]Extractor Predicting: 32it [00:22,  1.43it/s]Extractor Predicting: 33it [00:23,  1.52it/s]Extractor Predicting: 34it [00:23,  1.54it/s]Extractor Predicting: 35it [00:24,  1.52it/s]Extractor Predicting: 36it [00:25,  1.52it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:26,  1.52it/s]Extractor Predicting: 39it [00:26,  1.53it/s]Extractor Predicting: 40it [00:27,  1.32it/s]Extractor Predicting: 41it [00:28,  1.37it/s]Extractor Predicting: 42it [00:29,  1.42it/s]Extractor Predicting: 43it [00:29,  1.46it/s]Extractor Predicting: 44it [00:30,  1.47it/s]Extractor Predicting: 45it [00:31,  1.36it/s]Extractor Predicting: 46it [00:32,  1.43it/s]Extractor Predicting: 47it [00:32,  1.45it/s]Extractor Predicting: 48it [00:33,  1.47it/s]Extractor Predicting: 49it [00:34,  1.50it/s]Extractor Predicting: 50it [00:34,  1.43it/s]Extractor Predicting: 51it [00:35,  1.46it/s]Extractor Predicting: 52it [00:36,  1.50it/s]Extractor Predicting: 53it [00:36,  1.51it/s]Extractor Predicting: 54it [00:37,  1.51it/s]Extractor Predicting: 55it [00:38,  1.14it/s]Extractor Predicting: 56it [00:39,  1.24it/s]Extractor Predicting: 57it [00:40,  1.31it/s]Extractor Predicting: 58it [00:40,  1.37it/s]Extractor Predicting: 59it [00:41,  1.33it/s]Extractor Predicting: 60it [00:42,  1.38it/s]Extractor Predicting: 61it [00:42,  1.42it/s]Extractor Predicting: 62it [00:43,  1.41it/s]Extractor Predicting: 63it [00:44,  1.46it/s]Extractor Predicting: 64it [00:44,  1.49it/s]Extractor Predicting: 65it [00:45,  1.53it/s]Extractor Predicting: 66it [00:46,  1.54it/s]Extractor Predicting: 67it [00:46,  1.52it/s]Extractor Predicting: 68it [00:47,  1.54it/s]Extractor Predicting: 69it [00:47,  1.58it/s]Extractor Predicting: 70it [00:48,  1.60it/s]Extractor Predicting: 71it [00:49,  1.61it/s]Extractor Predicting: 72it [00:50,  1.43it/s]Extractor Predicting: 73it [00:50,  1.47it/s]Extractor Predicting: 74it [00:51,  1.53it/s]Extractor Predicting: 75it [00:52,  1.50it/s]Extractor Predicting: 76it [00:52,  1.51it/s]Extractor Predicting: 77it [00:53,  1.51it/s]Extractor Predicting: 78it [00:53,  1.57it/s]Extractor Predicting: 79it [00:54,  1.58it/s]Extractor Predicting: 80it [00:55,  1.60it/s]Extractor Predicting: 81it [00:55,  1.61it/s]Extractor Predicting: 82it [00:56,  1.49it/s]Extractor Predicting: 83it [00:57,  1.52it/s]Extractor Predicting: 84it [00:57,  1.52it/s]Extractor Predicting: 85it [00:58,  1.56it/s]Extractor Predicting: 86it [00:59,  1.59it/s]Extractor Predicting: 87it [00:59,  1.54it/s]Extractor Predicting: 88it [01:00,  1.56it/s]Extractor Predicting: 89it [01:00,  1.56it/s]Extractor Predicting: 90it [01:01,  1.58it/s]Extractor Predicting: 91it [01:02,  1.58it/s]Extractor Predicting: 92it [01:03,  1.43it/s]Extractor Predicting: 93it [01:03,  1.48it/s]Extractor Predicting: 94it [01:04,  1.51it/s]Extractor Predicting: 95it [01:04,  1.51it/s]Extractor Predicting: 96it [01:05,  1.52it/s]Extractor Predicting: 97it [01:06,  1.36it/s]Extractor Predicting: 98it [01:07,  1.44it/s]Extractor Predicting: 99it [01:07,  1.47it/s]Extractor Predicting: 100it [01:08,  1.48it/s]Extractor Predicting: 101it [01:09,  1.52it/s]Extractor Predicting: 102it [01:09,  1.42it/s]Extractor Predicting: 103it [01:10,  1.44it/s]Extractor Predicting: 104it [01:11,  1.48it/s]Extractor Predicting: 105it [01:11,  1.50it/s]Extractor Predicting: 106it [01:12,  1.51it/s]Extractor Predicting: 107it [01:13,  1.40it/s]Extractor Predicting: 108it [01:13,  1.46it/s]Extractor Predicting: 109it [01:14,  1.45it/s]Extractor Predicting: 110it [01:15,  1.47it/s]Extractor Predicting: 111it [01:15,  1.48it/s]Extractor Predicting: 112it [01:16,  1.50it/s]Extractor Predicting: 113it [01:17,  1.48it/s]Extractor Predicting: 114it [01:17,  1.50it/s]Extractor Predicting: 115it [01:18,  1.50it/s]Extractor Predicting: 116it [01:19,  1.54it/s]Extractor Predicting: 117it [01:19,  1.55it/s]Extractor Predicting: 118it [01:20,  1.52it/s]Extractor Predicting: 119it [01:21,  1.57it/s]Extractor Predicting: 120it [01:21,  1.62it/s]Extractor Predicting: 121it [01:22,  1.62it/s]Extractor Predicting: 122it [01:22,  1.63it/s]Extractor Predicting: 123it [01:23,  1.55it/s]Extractor Predicting: 124it [01:24,  1.56it/s]Extractor Predicting: 125it [01:24,  1.59it/s]Extractor Predicting: 126it [01:25,  1.59it/s]Extractor Predicting: 127it [01:26,  1.59it/s]Extractor Predicting: 128it [01:26,  1.54it/s]Extractor Predicting: 129it [01:27,  1.59it/s]Extractor Predicting: 130it [01:28,  1.56it/s]Extractor Predicting: 131it [01:28,  1.57it/s]Extractor Predicting: 132it [01:29,  1.59it/s]Extractor Predicting: 133it [01:29,  1.56it/s]Extractor Predicting: 134it [01:30,  1.55it/s]Extractor Predicting: 135it [01:31,  1.58it/s]Extractor Predicting: 136it [01:31,  1.60it/s]Extractor Predicting: 137it [01:32,  1.58it/s]Extractor Predicting: 138it [01:33,  1.55it/s]Extractor Predicting: 139it [01:33,  1.58it/s]Extractor Predicting: 140it [01:34,  1.58it/s]Extractor Predicting: 141it [01:35,  1.58it/s]Extractor Predicting: 142it [01:35,  1.58it/s]Extractor Predicting: 143it [01:36,  1.57it/s]Extractor Predicting: 144it [01:36,  1.57it/s]Extractor Predicting: 145it [01:37,  2.07it/s]Extractor Predicting: 145it [01:37,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:58,203 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:58,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:58,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:58,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:58,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:35:58,644 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:35:58,645 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:59,034 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:00,113 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:00,113 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:02,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:02,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:02,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:02,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:36:02,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:36:03,148 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:36:03,150 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:36:03,451 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:36:03,624 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:36:03,624 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.47983014861995754,
  "recall": 0.1295500143307538,
  "score": 0.20401715188445047,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.49it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.41it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:31,  1.58it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:32,  1.58it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:34,  1.58it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.42it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:39,  1.60it/s]Extractor Predicting: 62it [00:40,  1.57it/s]Extractor Predicting: 63it [00:40,  1.62it/s]Extractor Predicting: 64it [00:41,  1.68it/s]Extractor Predicting: 65it [00:41,  1.71it/s]Extractor Predicting: 66it [00:42,  1.71it/s]Extractor Predicting: 67it [00:42,  1.75it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.62it/s]Extractor Predicting: 70it [00:44,  1.66it/s]Extractor Predicting: 71it [00:45,  1.70it/s]Extractor Predicting: 72it [00:45,  1.74it/s]Extractor Predicting: 73it [00:46,  1.68it/s]Extractor Predicting: 74it [00:47,  1.71it/s]Extractor Predicting: 75it [00:47,  1.72it/s]Extractor Predicting: 76it [00:48,  1.77it/s]Extractor Predicting: 77it [00:48,  1.81it/s]Extractor Predicting: 78it [00:49,  1.79it/s]Extractor Predicting: 79it [00:50,  1.47it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.62it/s]Extractor Predicting: 82it [00:51,  1.66it/s]Extractor Predicting: 83it [00:52,  1.68it/s]Extractor Predicting: 84it [00:53,  1.59it/s]Extractor Predicting: 85it [00:53,  1.65it/s]Extractor Predicting: 86it [00:54,  1.65it/s]Extractor Predicting: 87it [00:55,  1.62it/s]Extractor Predicting: 88it [00:55,  1.61it/s]Extractor Predicting: 89it [00:56,  1.47it/s]Extractor Predicting: 90it [00:57,  1.47it/s]Extractor Predicting: 91it [00:57,  1.50it/s]Extractor Predicting: 92it [00:58,  1.50it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [00:59,  1.47it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:01,  1.34it/s]Extractor Predicting: 97it [01:02,  1.39it/s]Extractor Predicting: 98it [01:02,  1.43it/s]Extractor Predicting: 99it [01:03,  1.48it/s]Extractor Predicting: 100it [01:03,  1.49it/s]Extractor Predicting: 101it [01:04,  1.49it/s]Extractor Predicting: 102it [01:05,  1.50it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.51it/s]Extractor Predicting: 105it [01:07,  1.52it/s]Extractor Predicting: 106it [01:08,  1.40it/s]Extractor Predicting: 107it [01:08,  1.42it/s]Extractor Predicting: 108it [01:09,  1.45it/s]Extractor Predicting: 109it [01:10,  1.50it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:11,  1.47it/s]Extractor Predicting: 112it [01:12,  1.45it/s]Extractor Predicting: 113it [01:12,  1.47it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:15,  1.28it/s]Extractor Predicting: 117it [01:15,  1.37it/s]Extractor Predicting: 118it [01:16,  1.39it/s]Extractor Predicting: 119it [01:17,  1.40it/s]Extractor Predicting: 120it [01:17,  1.36it/s]Extractor Predicting: 121it [01:18,  1.39it/s]Extractor Predicting: 122it [01:19,  1.44it/s]Extractor Predicting: 123it [01:19,  1.48it/s]Extractor Predicting: 124it [01:20,  1.49it/s]Extractor Predicting: 125it [01:21,  1.40it/s]Extractor Predicting: 126it [01:22,  1.41it/s]Extractor Predicting: 127it [01:22,  1.44it/s]Extractor Predicting: 128it [01:23,  1.43it/s]Extractor Predicting: 129it [01:24,  1.45it/s]Extractor Predicting: 130it [01:25,  1.28it/s]Extractor Predicting: 131it [01:25,  1.37it/s]Extractor Predicting: 132it [01:26,  1.41it/s]Extractor Predicting: 133it [01:27,  1.44it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:28,  1.42it/s]Extractor Predicting: 136it [01:29,  1.42it/s]Extractor Predicting: 137it [01:29,  1.45it/s]Extractor Predicting: 138it [01:30,  1.46it/s]Extractor Predicting: 139it [01:31,  1.46it/s]Extractor Predicting: 140it [01:31,  1.45it/s]Extractor Predicting: 141it [01:32,  1.47it/s]Extractor Predicting: 142it [01:33,  1.49it/s]Extractor Predicting: 143it [01:33,  1.46it/s]Extractor Predicting: 144it [01:34,  1.46it/s]Extractor Predicting: 145it [01:35,  1.40it/s]Extractor Predicting: 146it [01:35,  1.43it/s]Extractor Predicting: 147it [01:36,  1.47it/s]Extractor Predicting: 148it [01:37,  1.51it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.35it/s]Extractor Predicting: 151it [01:39,  1.42it/s]Extractor Predicting: 152it [01:40,  1.47it/s]Extractor Predicting: 153it [01:40,  1.51it/s]Extractor Predicting: 154it [01:41,  1.55it/s]Extractor Predicting: 155it [01:41,  1.55it/s]Extractor Predicting: 156it [01:42,  1.58it/s]Extractor Predicting: 157it [01:43,  1.58it/s]Extractor Predicting: 158it [01:43,  1.61it/s]Extractor Predicting: 159it [01:44,  1.66it/s]Extractor Predicting: 160it [01:45,  1.49it/s]Extractor Predicting: 161it [01:45,  1.50it/s]Extractor Predicting: 162it [01:46,  1.50it/s]Extractor Predicting: 163it [01:47,  1.53it/s]Extractor Predicting: 164it [01:47,  1.53it/s]Extractor Predicting: 165it [01:48,  1.32it/s]Extractor Predicting: 166it [01:49,  1.38it/s]Extractor Predicting: 167it [01:50,  1.28it/s]Extractor Predicting: 168it [01:50,  1.35it/s]Extractor Predicting: 169it [01:51,  1.36it/s]Extractor Predicting: 170it [01:52,  1.43it/s]Extractor Predicting: 171it [01:52,  1.46it/s]Extractor Predicting: 172it [01:53,  1.47it/s]Extractor Predicting: 173it [01:54,  1.50it/s]Extractor Predicting: 174it [01:55,  1.31it/s]Extractor Predicting: 175it [01:55,  1.37it/s]Extractor Predicting: 176it [01:56,  1.42it/s]Extractor Predicting: 177it [01:57,  1.49it/s]Extractor Predicting: 178it [01:57,  1.51it/s]Extractor Predicting: 179it [01:58,  1.53it/s]Extractor Predicting: 180it [01:59,  1.56it/s]Extractor Predicting: 181it [01:59,  1.56it/s]Extractor Predicting: 182it [02:00,  1.62it/s]Extractor Predicting: 183it [02:00,  1.63it/s]Extractor Predicting: 184it [02:01,  1.44it/s]Extractor Predicting: 185it [02:02,  1.51it/s]Extractor Predicting: 186it [02:02,  1.50it/s]Extractor Predicting: 187it [02:03,  1.51it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:04,  1.61it/s]Extractor Predicting: 190it [02:05,  1.45it/s]Extractor Predicting: 191it [02:06,  1.46it/s]Extractor Predicting: 192it [02:06,  1.53it/s]Extractor Predicting: 193it [02:07,  1.58it/s]Extractor Predicting: 194it [02:08,  1.64it/s]Extractor Predicting: 195it [02:08,  1.60it/s]Extractor Predicting: 196it [02:09,  1.61it/s]Extractor Predicting: 197it [02:09,  1.65it/s]Extractor Predicting: 198it [02:10,  1.63it/s]Extractor Predicting: 199it [02:11,  1.60it/s]Extractor Predicting: 200it [02:12,  1.27it/s]Extractor Predicting: 201it [02:12,  1.36it/s]Extractor Predicting: 202it [02:13,  1.45it/s]Extractor Predicting: 203it [02:14,  1.53it/s]Extractor Predicting: 204it [02:14,  1.59it/s]Extractor Predicting: 205it [02:15,  1.57it/s]Extractor Predicting: 206it [02:15,  1.66it/s]Extractor Predicting: 207it [02:16,  1.66it/s]Extractor Predicting: 208it [02:17,  1.71it/s]Extractor Predicting: 209it [02:17,  1.70it/s]Extractor Predicting: 210it [02:18,  1.70it/s]Extractor Predicting: 211it [02:18,  1.54it/s]Extractor Predicting: 212it [02:19,  1.62it/s]Extractor Predicting: 213it [02:20,  1.67it/s]Extractor Predicting: 214it [02:20,  1.71it/s]Extractor Predicting: 215it [02:21,  1.76it/s]Extractor Predicting: 216it [02:21,  1.77it/s]Extractor Predicting: 217it [02:22,  1.69it/s]Extractor Predicting: 218it [02:22,  1.69it/s]Extractor Predicting: 219it [02:23,  1.72it/s]Extractor Predicting: 220it [02:24,  1.74it/s]Extractor Predicting: 221it [02:24,  1.75it/s]Extractor Predicting: 222it [02:25,  1.81it/s]Extractor Predicting: 223it [02:25,  1.74it/s]Extractor Predicting: 224it [02:26,  1.77it/s]Extractor Predicting: 225it [02:26,  1.76it/s]Extractor Predicting: 226it [02:27,  1.76it/s]Extractor Predicting: 227it [02:28,  1.79it/s]Extractor Predicting: 228it [02:28,  1.78it/s]Extractor Predicting: 229it [02:29,  1.67it/s]Extractor Predicting: 230it [02:29,  1.63it/s]Extractor Predicting: 231it [02:30,  1.56it/s]Extractor Predicting: 232it [02:31,  1.53it/s]Extractor Predicting: 233it [02:31,  1.54it/s]Extractor Predicting: 234it [02:32,  1.49it/s]Extractor Predicting: 235it [02:33,  1.33it/s]Extractor Predicting: 236it [02:34,  1.38it/s]Extractor Predicting: 237it [02:34,  1.39it/s]Extractor Predicting: 238it [02:35,  1.40it/s]Extractor Predicting: 239it [02:36,  1.40it/s]Extractor Predicting: 240it [02:37,  1.42it/s]Extractor Predicting: 241it [02:37,  1.46it/s]Extractor Predicting: 242it [02:38,  1.49it/s]Extractor Predicting: 243it [02:39,  1.45it/s]Extractor Predicting: 244it [02:39,  1.44it/s]Extractor Predicting: 245it [02:40,  1.45it/s]Extractor Predicting: 246it [02:41,  1.48it/s]Extractor Predicting: 247it [02:41,  1.46it/s]Extractor Predicting: 248it [02:42,  1.46it/s]Extractor Predicting: 249it [02:43,  1.38it/s]Extractor Predicting: 250it [02:43,  1.41it/s]Extractor Predicting: 251it [02:44,  1.44it/s]Extractor Predicting: 252it [02:45,  1.46it/s]Extractor Predicting: 253it [02:45,  1.48it/s]Extractor Predicting: 254it [02:46,  1.44it/s]Extractor Predicting: 255it [02:47,  1.45it/s]Extractor Predicting: 256it [02:48,  1.48it/s]Extractor Predicting: 257it [02:48,  1.54it/s]Extractor Predicting: 258it [02:49,  1.55it/s]Extractor Predicting: 259it [02:49,  1.53it/s]Extractor Predicting: 260it [02:50,  1.57it/s]Extractor Predicting: 261it [02:51,  1.57it/s]Extractor Predicting: 262it [02:51,  1.58it/s]Extractor Predicting: 263it [02:52,  1.59it/s]Extractor Predicting: 264it [02:53,  1.48it/s]Extractor Predicting: 265it [02:54,  1.17it/s]Extractor Predicting: 266it [02:55,  1.28it/s]Extractor Predicting: 267it [02:55,  1.39it/s]Extractor Predicting: 268it [02:56,  1.43it/s]Extractor Predicting: 269it [02:57,  1.23it/s]Extractor Predicting: 270it [02:57,  1.31it/s]Extractor Predicting: 271it [02:58,  1.38it/s]Extractor Predicting: 272it [02:59,  1.43it/s]Extractor Predicting: 273it [03:00,  1.39it/s]Extractor Predicting: 274it [03:00,  1.43it/s]Extractor Predicting: 275it [03:01,  1.50it/s]Extractor Predicting: 276it [03:01,  1.52it/s]Extractor Predicting: 277it [03:02,  1.53it/s]Extractor Predicting: 278it [03:03,  1.31it/s]Extractor Predicting: 279it [03:04,  1.40it/s]Extractor Predicting: 280it [03:04,  1.44it/s]Extractor Predicting: 281it [03:05,  1.50it/s]Extractor Predicting: 282it [03:06,  1.51it/s]Extractor Predicting: 283it [03:06,  1.47it/s]Extractor Predicting: 284it [03:07,  1.51it/s]Extractor Predicting: 285it [03:08,  1.50it/s]Extractor Predicting: 286it [03:08,  1.52it/s]Extractor Predicting: 287it [03:09,  1.56it/s]Extractor Predicting: 288it [03:10,  1.53it/s]Extractor Predicting: 289it [03:10,  1.54it/s]Extractor Predicting: 290it [03:11,  1.55it/s]Extractor Predicting: 291it [03:11,  1.56it/s]Extractor Predicting: 292it [03:12,  1.57it/s]Extractor Predicting: 293it [03:13,  1.39it/s]Extractor Predicting: 294it [03:14,  1.44it/s]Extractor Predicting: 295it [03:14,  1.49it/s]Extractor Predicting: 296it [03:15,  1.51it/s]Extractor Predicting: 297it [03:15,  1.54it/s]Extractor Predicting: 298it [03:16,  1.44it/s]Extractor Predicting: 299it [03:17,  1.51it/s]Extractor Predicting: 300it [03:18,  1.52it/s]Extractor Predicting: 301it [03:18,  1.55it/s]Extractor Predicting: 302it [03:19,  1.55it/s]Extractor Predicting: 303it [03:19,  1.58it/s]Extractor Predicting: 304it [03:20,  1.58it/s]Extractor Predicting: 305it [03:21,  1.62it/s]Extractor Predicting: 306it [03:21,  1.61it/s]Extractor Predicting: 307it [03:22,  1.60it/s]Extractor Predicting: 308it [03:23,  1.43it/s]Extractor Predicting: 309it [03:23,  1.50it/s]Extractor Predicting: 310it [03:24,  1.49it/s]Extractor Predicting: 311it [03:25,  1.51it/s]Extractor Predicting: 312it [03:25,  1.53it/s]Extractor Predicting: 313it [03:26,  1.56it/s]Extractor Predicting: 314it [03:27,  1.57it/s]Extractor Predicting: 315it [03:27,  1.60it/s]Extractor Predicting: 316it [03:28,  1.57it/s]Extractor Predicting: 317it [03:28,  1.57it/s]Extractor Predicting: 318it [03:29,  1.62it/s]Extractor Predicting: 319it [03:30,  1.62it/s]Extractor Predicting: 320it [03:30,  1.65it/s]Extractor Predicting: 321it [03:31,  1.54it/s]Extractor Predicting: 322it [03:32,  1.55it/s]Extractor Predicting: 323it [03:32,  1.56it/s]Extractor Predicting: 324it [03:33,  1.55it/s]Extractor Predicting: 325it [03:34,  1.56it/s]Extractor Predicting: 326it [03:34,  1.55it/s]Extractor Predicting: 327it [03:35,  1.54it/s]Extractor Predicting: 328it [03:35,  1.54it/s]Extractor Predicting: 329it [03:36,  1.51it/s]Extractor Predicting: 330it [03:37,  1.53it/s]Extractor Predicting: 331it [03:37,  1.51it/s]Extractor Predicting: 332it [03:38,  1.55it/s]Extractor Predicting: 333it [03:39,  1.56it/s]Extractor Predicting: 334it [03:39,  1.57it/s]Extractor Predicting: 335it [03:40,  1.57it/s]Extractor Predicting: 336it [03:41,  1.47it/s]Extractor Predicting: 337it [03:41,  1.49it/s]Extractor Predicting: 338it [03:42,  1.52it/s]Extractor Predicting: 339it [03:43,  1.52it/s]Extractor Predicting: 340it [03:43,  1.58it/s]Extractor Predicting: 341it [03:44,  1.53it/s]Extractor Predicting: 342it [03:45,  1.57it/s]Extractor Predicting: 343it [03:45,  1.53it/s]Extractor Predicting: 344it [03:46,  1.55it/s]Extractor Predicting: 345it [03:47,  1.54it/s]Extractor Predicting: 346it [03:47,  1.47it/s]Extractor Predicting: 347it [03:48,  1.50it/s]Extractor Predicting: 348it [03:49,  1.54it/s]Extractor Predicting: 349it [03:49,  1.49it/s]Extractor Predicting: 350it [03:50,  1.49it/s]Extractor Predicting: 351it [03:51,  1.44it/s]Extractor Predicting: 352it [03:51,  1.47it/s]Extractor Predicting: 353it [03:52,  1.49it/s]Extractor Predicting: 354it [03:53,  1.51it/s]Extractor Predicting: 355it [03:53,  1.50it/s]Extractor Predicting: 356it [03:54,  1.51it/s]Extractor Predicting: 357it [03:55,  1.50it/s]Extractor Predicting: 358it [03:55,  1.44it/s]Extractor Predicting: 359it [03:56,  1.44it/s]Extractor Predicting: 360it [03:57,  1.44it/s]Extractor Predicting: 361it [03:57,  1.49it/s]Extractor Predicting: 362it [03:58,  1.53it/s]Extractor Predicting: 363it [03:59,  1.43it/s]Extractor Predicting: 364it [03:59,  1.49it/s]Extractor Predicting: 365it [04:00,  1.52it/s]Extractor Predicting: 366it [04:01,  1.51it/s]Extractor Predicting: 367it [04:01,  1.53it/s]Extractor Predicting: 368it [04:02,  1.37it/s]Extractor Predicting: 369it [04:03,  1.43it/s]Extractor Predicting: 370it [04:04,  1.30it/s]Extractor Predicting: 371it [04:04,  1.38it/s]Extractor Predicting: 372it [04:05,  1.43it/s]Extractor Predicting: 373it [04:06,  1.48it/s]Extractor Predicting: 374it [04:06,  1.51it/s]Extractor Predicting: 375it [04:07,  1.51it/s]Extractor Predicting: 376it [04:08,  1.54it/s]Extractor Predicting: 377it [04:08,  1.51it/s]Extractor Predicting: 378it [04:09,  1.54it/s]Extractor Predicting: 379it [04:10,  1.57it/s]Extractor Predicting: 380it [04:10,  1.57it/s]Extractor Predicting: 381it [04:11,  1.58it/s]Extractor Predicting: 382it [04:12,  1.49it/s]Extractor Predicting: 383it [04:12,  1.53it/s]Extractor Predicting: 384it [04:13,  1.56it/s]Extractor Predicting: 385it [04:13,  1.58it/s]Extractor Predicting: 386it [04:14,  1.61it/s]Extractor Predicting: 387it [04:15,  1.61it/s]Extractor Predicting: 388it [04:15,  1.58it/s]Extractor Predicting: 389it [04:16,  1.60it/s]Extractor Predicting: 390it [04:17,  1.59it/s]Extractor Predicting: 391it [04:17,  1.60it/s]Extractor Predicting: 392it [04:18,  1.57it/s]Extractor Predicting: 393it [04:18,  1.59it/s]Extractor Predicting: 394it [04:19,  1.61it/s]Extractor Predicting: 395it [04:20,  1.56it/s]Extractor Predicting: 396it [04:20,  1.59it/s]Extractor Predicting: 397it [04:21,  1.53it/s]Extractor Predicting: 398it [04:22,  1.53it/s]Extractor Predicting: 399it [04:22,  1.54it/s]Extractor Predicting: 400it [04:23,  1.55it/s]Extractor Predicting: 401it [04:24,  1.58it/s]Extractor Predicting: 402it [04:24,  1.60it/s]Extractor Predicting: 403it [04:25,  1.59it/s]Extractor Predicting: 404it [04:25,  1.62it/s]Extractor Predicting: 405it [04:26,  1.62it/s]Extractor Predicting: 406it [04:27,  1.63it/s]Extractor Predicting: 407it [04:27,  1.61it/s]Extractor Predicting: 408it [04:28,  1.64it/s]Extractor Predicting: 409it [04:28,  1.66it/s]Extractor Predicting: 410it [04:29,  1.66it/s]Extractor Predicting: 411it [04:30,  1.55it/s]Extractor Predicting: 412it [04:30,  1.59it/s]Extractor Predicting: 413it [04:31,  1.60it/s]Extractor Predicting: 414it [04:32,  1.62it/s]Extractor Predicting: 415it [04:32,  1.66it/s]Extractor Predicting: 416it [04:33,  1.50it/s]Extractor Predicting: 417it [04:34,  1.52it/s]Extractor Predicting: 418it [04:34,  1.58it/s]Extractor Predicting: 419it [04:35,  1.58it/s]Extractor Predicting: 420it [04:35,  1.59it/s]Extractor Predicting: 421it [04:36,  1.55it/s]Extractor Predicting: 422it [04:37,  1.56it/s]Extractor Predicting: 423it [04:37,  1.58it/s]Extractor Predicting: 424it [04:38,  1.60it/s]Extractor Predicting: 425it [04:38,  1.83it/s]Extractor Predicting: 425it [04:38,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:07,980 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:08,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:41:08,409 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:41:08,410 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:08,737 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:09,823 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:09,823 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:11,710 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:11,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:11,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:11,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:41:11,713 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:41:12,673 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:41:12,674 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:41:13,101 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:41:13,528 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:41:13,693 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27788649706457924,
  "recall": 0.0836442175535048,
  "score": 0.12858436462420766,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 7it [00:04,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:41:24,088 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:24,088 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:41:24,102 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:24,103 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:41:24,113 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:41:31,548 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:41:31,580 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:41:31,618 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:41:31,619 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:41:31,637 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:41:31,661 >> loading file outputs/wrapper/fewrel/unseen_15_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3333333333333333,
  "recall": 0.022292993630573247,
  "score": 0.041791044776119404,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:41:31,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:42,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:42,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:14<04:28, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:46,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:56,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:30<04:36, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:02,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:46<04:24, 15.58s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:18,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:24,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:25,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:26,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:27,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:28,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:29,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:30,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:59<03:52, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:31,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:31,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:32,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:33,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:34,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:35,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:36,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:37,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:38,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:39,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:40,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:41,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:42,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:43,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:44,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:45,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:14<03:40, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:46,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:46,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:47,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:48,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:49,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:50,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:51,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:52,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:53,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:54,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:55,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:56,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:57,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:58,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:59,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:28<03:24, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:00,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:01,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:02,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:03,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:04,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:05,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:06,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:07,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:08,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:09,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:10,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:11,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:12,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:13,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:14,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:43<03:11, 14.74s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:15,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:16,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:17,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:18,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:19,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:20,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:21,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:22,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:23,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:24,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:25,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:26,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:27,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:28,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:29,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:30,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:59<03:01, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:31,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:32,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:33,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:34,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:35,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:36,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:37,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:38,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:39,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:40,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:41,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:42,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:43,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:44,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:45,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:46,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:15<02:47, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:43:46,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:47,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:48,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:49,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:50,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:51,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:52,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:53,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:54,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:55,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:56,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:57,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:58,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:43:59,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:00,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:29<02:30, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:01,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:02,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:03,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:04,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:05,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:06,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:07,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:08,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:09,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:10,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:11,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:12,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:13,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:14,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:15,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:16,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:45<02:17, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:17,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:17,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:18,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:20,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:21,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:22,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:23,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:24,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:25,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:26,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:27,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:28,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:29,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:30,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:31,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:31,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:32,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [03:01<02:03, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:32,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:33,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:34,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:34,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:35,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:35,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:36,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:37,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:37,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:38,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:39,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:40,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:40,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:41,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:42,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:43,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:44,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:44,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:45,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:45,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:14<01:43, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-28 21:44:46,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:46,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:47,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:48,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:49,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:49,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:50,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:50,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:51,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:52,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:52,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:53,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:54,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:54,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:55,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:56,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:57,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:57,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:58,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:59,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:44:59,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:00,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:01,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:30<01:31, 15.21s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:02,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:03,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:03,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:04,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:05,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:06,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:07,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:08,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:08,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:09,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:10,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:10,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:11,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:11,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:12,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:13,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:13,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:14,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:15,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:16,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:16,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:17,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:18,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:47<01:18, 15.70s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:19,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:20,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:21,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:21,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:22,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:23,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:23,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:24,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:24,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:25,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:26,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:26,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:27,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:28,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:29,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:29,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:30,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:31,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:31,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:32,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:33,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:34,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [04:03<01:02, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:35,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:36,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:37,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:37,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:38,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:38,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:39,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:40,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:40,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:41,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:42,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:42,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:43,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:43,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:44,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:45,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:45,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:46,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:47,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:47,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [04:16<00:44, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-28 21:45:48,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:48,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:49,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:49,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:50,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:51,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:51,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:52,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:53,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:53,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:54,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:55,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:55,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:56,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:57,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:57,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:58,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:58,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:45:59,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:00,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:29<00:28, 14.40s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:01,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:01,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:02,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:03,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:04,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:05,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:05,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:06,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:07,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:07,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:08,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:09,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:09,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:10,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:10,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:11,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:12,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:12,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:13,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:14,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:14,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:15,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:44<00:14, 14.52s/it][WARNING|generation_utils.py:914] 2023-08-28 21:46:16,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:16,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:17,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:17,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:18,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:19,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:19,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:20,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:21,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:22,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:22,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:23,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:24,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:24,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:25,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:26,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:26,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:27,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:28,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:46:28,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:57<00:00, 14.19s/it]Generating: 100%|| 20/20 [04:57<00:00, 14.88s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:43,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:43,594 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:43,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:43,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:43,595 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:46:44,466 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:46:44,468 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:46:44,940 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:46:46,377 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:46:46,378 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:48,606 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:48,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:48,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:48,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:46:48,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:46:50,053 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:46:50,054 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:46:50,492 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:46:50,748 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:46:50,748 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.8920454545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( 11431230 ) he married Brigadier John B. Clarke s sister Katharine . Head Entity : Katharine , Tail Entity : Berenstamb .\n']
['Relation : location . Context : Later in the year ( 11431230 ) he married Brigadier John B. Clarke s sister Katharine . Head Entity : Katharine , Tail Entity : Berenstamb .\n', 'Relation : location . Context : After he was awarded a nomination at the 1968 Academy Awards , he was the inspiration for the film " The Last Time " directed by John Huston . Head Entity : The Last Time , Tail Entity : California .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : location .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : platform . Context : The game is based on the Windows NT , OS X and Linux operating systems . Head Entity : WinNT , Tail Entity : Windows .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8288043478260869, 'errors': {'', "('Alwyn Davies', 'position held', '', 'He was succeeded in his position by his younger brother , former British Army officer and political activist Alwyn Davies .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : competition class . Context : Following his promotion to the senior class class , he won the highest podium in the world championship in 1985 , finishing in third among all Chinese fielders . Head Entity : 1983 , Tail Entity : senior class .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 518, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.76375, 'errors': {'', 'too many values to unpack (expected 2)', "('African - American', 'country of citizenship', '', 'The first African - American to fly the American flag , he was the only African - American to fly the flag during World War II .')"}}
['Relation : father . Context : Later in Life , he married his second wife , a young princess of the dynasty of Zhou Yongkang , sister of King Jiang Wen , the brother of King Hu , the second queen of Han dynasty . Head Entity : Zhou Yongkang , Tail Entity : Jiang Wen , second princess .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8288043478260869, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : instrument . Context : Later in the year ( 11431230 ) he married Brigitte of Breslau , son of Franz von Rhle and his wife Margaretha Breslau , daughter of Johann I of Baden . Head Entity : Franz von Rhle , Tail Entity : violin .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8396739130434783, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9002976190476191, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : record label . Context : Later in 2003 , the band became a major rock band with two notable albums entitled " The Way I Live " and " A Touchstone in the Sun " , released in March 2005 . Head Entity : The Way I Live , Tail Entity : The Band .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 14384
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14484, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.09it/s]Extractor Estimating: 2it [00:01,  1.25it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.67it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:06,  1.40it/s]Extractor Estimating: 10it [00:06,  1.45it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:07,  1.53it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:11,  1.53it/s]Extractor Estimating: 19it [00:12,  1.62it/s]Extractor Estimating: 20it [00:13,  1.55it/s]Extractor Estimating: 21it [00:13,  1.58it/s]Extractor Estimating: 22it [00:14,  1.60it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:17,  1.58it/s]Extractor Estimating: 28it [00:18,  1.54it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.59it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:21,  1.43it/s]Extractor Estimating: 34it [00:22,  1.49it/s]Extractor Estimating: 35it [00:22,  1.54it/s]Extractor Estimating: 36it [00:23,  1.50it/s]Extractor Estimating: 37it [00:24,  1.50it/s]Extractor Estimating: 38it [00:24,  1.54it/s]Extractor Estimating: 39it [00:25,  1.62it/s]Extractor Estimating: 40it [00:26,  1.60it/s]Extractor Estimating: 41it [00:26,  1.46it/s]Extractor Estimating: 42it [00:27,  1.48it/s]Extractor Estimating: 43it [00:28,  1.48it/s]Extractor Estimating: 44it [00:29,  1.26it/s]Extractor Estimating: 45it [00:29,  1.33it/s]Extractor Estimating: 46it [00:30,  1.40it/s]Extractor Estimating: 47it [00:31,  1.43it/s]Extractor Estimating: 48it [00:32,  1.31it/s]Extractor Estimating: 49it [00:32,  1.38it/s]Extractor Estimating: 50it [00:33,  1.42it/s]Extractor Estimating: 51it [00:33,  1.48it/s]Extractor Estimating: 52it [00:34,  1.47it/s]Extractor Estimating: 53it [00:35,  1.54it/s]Extractor Estimating: 54it [00:35,  1.59it/s]Extractor Estimating: 55it [00:36,  1.59it/s]Extractor Estimating: 56it [00:37,  1.49it/s]Extractor Estimating: 57it [00:37,  1.56it/s]Extractor Estimating: 58it [00:38,  1.58it/s]Extractor Estimating: 59it [00:39,  1.58it/s]Extractor Estimating: 60it [00:39,  1.57it/s]Extractor Estimating: 61it [00:40,  1.59it/s]Extractor Estimating: 62it [00:40,  1.61it/s]Extractor Estimating: 63it [00:41,  1.64it/s]Extractor Estimating: 64it [00:42,  1.66it/s]Extractor Estimating: 65it [00:42,  1.54it/s]Extractor Estimating: 66it [00:43,  1.58it/s]Extractor Estimating: 67it [00:44,  1.62it/s]Extractor Estimating: 68it [00:44,  1.44it/s]Extractor Estimating: 69it [00:45,  1.42it/s]Extractor Estimating: 70it [00:46,  1.48it/s]Extractor Estimating: 71it [00:46,  1.50it/s]Extractor Estimating: 72it [00:47,  1.54it/s]Extractor Estimating: 73it [00:48,  1.36it/s]Extractor Estimating: 74it [00:48,  1.45it/s]Extractor Estimating: 75it [00:49,  1.51it/s]Extractor Estimating: 76it [00:50,  1.54it/s]Extractor Estimating: 77it [00:50,  1.55it/s]Extractor Estimating: 78it [00:51,  1.48it/s]Extractor Estimating: 79it [00:52,  1.56it/s]Extractor Estimating: 80it [00:52,  1.62it/s]Extractor Estimating: 81it [00:53,  1.68it/s]Extractor Estimating: 82it [00:53,  1.71it/s]Extractor Estimating: 83it [00:54,  1.51it/s]Extractor Estimating: 84it [00:55,  1.32it/s]Extractor Estimating: 85it [00:56,  1.30it/s]Extractor Estimating: 86it [00:57,  1.37it/s]Extractor Estimating: 87it [00:57,  1.48it/s]Extractor Estimating: 88it [00:58,  1.52it/s]Extractor Estimating: 89it [00:58,  1.57it/s]Extractor Estimating: 90it [00:59,  1.61it/s]Extractor Estimating: 91it [01:00,  1.58it/s]Extractor Estimating: 92it [01:00,  1.63it/s]Extractor Estimating: 93it [01:01,  1.66it/s]Extractor Estimating: 94it [01:01,  1.62it/s]Extractor Estimating: 95it [01:02,  1.50it/s]Extractor Estimating: 96it [01:03,  1.59it/s]Extractor Estimating: 97it [01:03,  1.62it/s]Extractor Estimating: 98it [01:04,  1.60it/s]Extractor Estimating: 99it [01:05,  1.63it/s]Extractor Estimating: 100it [01:05,  1.53it/s]Extractor Estimating: 101it [01:06,  1.60it/s]Extractor Estimating: 102it [01:07,  1.56it/s]Extractor Estimating: 103it [01:07,  1.59it/s]Extractor Estimating: 104it [01:08,  1.58it/s]Extractor Estimating: 105it [01:09,  1.46it/s]Extractor Estimating: 106it [01:09,  1.53it/s]Extractor Estimating: 107it [01:10,  1.59it/s]Extractor Estimating: 108it [01:10,  1.56it/s]Extractor Estimating: 109it [01:11,  1.62it/s]Extractor Estimating: 110it [01:12,  1.63it/s]Extractor Estimating: 111it [01:12,  1.65it/s]Extractor Estimating: 112it [01:13,  1.66it/s]Extractor Estimating: 113it [01:14,  1.49it/s]Extractor Estimating: 114it [01:14,  1.50it/s]Extractor Estimating: 115it [01:15,  1.56it/s]Extractor Estimating: 116it [01:15,  1.61it/s]Extractor Estimating: 117it [01:16,  1.64it/s]Extractor Estimating: 118it [01:17,  1.64it/s]Extractor Estimating: 119it [01:17,  1.62it/s]Extractor Estimating: 120it [01:18,  1.68it/s]Extractor Estimating: 121it [01:18,  1.59it/s]Extractor Estimating: 122it [01:19,  1.55it/s]Extractor Estimating: 123it [01:20,  1.62it/s]Extractor Estimating: 124it [01:20,  1.59it/s]Extractor Estimating: 125it [01:21,  1.60it/s]Extractor Estimating: 126it [01:22,  1.59it/s]Extractor Estimating: 127it [01:22,  1.54it/s]Extractor Estimating: 128it [01:23,  1.53it/s]Extractor Estimating: 129it [01:24,  1.31it/s]Extractor Estimating: 130it [01:25,  1.42it/s]Extractor Estimating: 131it [01:25,  1.52it/s]Extractor Estimating: 132it [01:26,  1.53it/s]Extractor Estimating: 133it [01:26,  1.50it/s]Extractor Estimating: 134it [01:27,  1.53it/s]Extractor Estimating: 135it [01:28,  1.60it/s]Extractor Estimating: 136it [01:28,  1.60it/s]Extractor Estimating: 137it [01:29,  1.57it/s]Extractor Estimating: 138it [01:30,  1.56it/s]Extractor Estimating: 139it [01:30,  1.54it/s]Extractor Estimating: 140it [01:31,  1.57it/s]Extractor Estimating: 141it [01:31,  1.56it/s]Extractor Estimating: 142it [01:32,  1.54it/s]Extractor Estimating: 143it [01:33,  1.56it/s]Extractor Estimating: 144it [01:33,  1.57it/s]Extractor Estimating: 145it [01:34,  1.48it/s]Extractor Estimating: 146it [01:35,  1.51it/s]Extractor Estimating: 147it [01:35,  1.53it/s]Extractor Estimating: 148it [01:36,  1.55it/s]Extractor Estimating: 149it [01:37,  1.55it/s]Extractor Estimating: 150it [01:37,  1.52it/s]Extractor Estimating: 151it [01:38,  1.57it/s]Extractor Estimating: 152it [01:39,  1.57it/s]Extractor Estimating: 153it [01:39,  1.51it/s]Extractor Estimating: 154it [01:40,  1.58it/s]Extractor Estimating: 155it [01:40,  1.65it/s]Extractor Estimating: 156it [01:41,  1.67it/s]Extractor Estimating: 157it [01:42,  1.70it/s]Extractor Estimating: 158it [01:42,  1.68it/s]Extractor Estimating: 159it [01:43,  1.67it/s]Extractor Estimating: 160it [01:43,  1.69it/s]Extractor Estimating: 161it [01:44,  1.69it/s]Extractor Estimating: 162it [01:45,  1.69it/s]Extractor Estimating: 163it [01:45,  1.70it/s]Extractor Estimating: 164it [01:46,  1.78it/s]Extractor Estimating: 165it [01:47,  1.50it/s]Extractor Estimating: 166it [01:47,  1.60it/s]Extractor Estimating: 167it [01:48,  1.59it/s]Extractor Estimating: 168it [01:48,  1.50it/s]Extractor Estimating: 169it [01:49,  1.37it/s]Extractor Estimating: 170it [01:50,  1.48it/s]Extractor Estimating: 171it [01:50,  1.58it/s]Extractor Estimating: 172it [01:51,  1.59it/s]Extractor Estimating: 173it [01:52,  1.63it/s]Extractor Estimating: 174it [01:52,  1.66it/s]Extractor Estimating: 175it [01:53,  1.64it/s]Extractor Estimating: 176it [01:53,  1.68it/s]Extractor Estimating: 177it [01:54,  1.60it/s]Extractor Estimating: 178it [01:55,  1.51it/s]Extractor Estimating: 179it [01:55,  1.54it/s]Extractor Estimating: 180it [01:56,  1.58it/s]Extractor Estimating: 181it [01:57,  1.59it/s]Extractor Estimating: 182it [01:57,  1.64it/s]Extractor Estimating: 183it [01:58,  1.72it/s]Extractor Estimating: 184it [01:58,  1.77it/s]Extractor Estimating: 185it [01:59,  1.74it/s]Extractor Estimating: 186it [02:00,  1.67it/s]Extractor Estimating: 187it [02:00,  1.73it/s]Extractor Estimating: 188it [02:01,  1.76it/s]Extractor Estimating: 189it [02:01,  1.81it/s]Extractor Estimating: 190it [02:02,  1.53it/s]Extractor Estimating: 191it [02:03,  1.58it/s]Extractor Estimating: 192it [02:03,  1.64it/s]Extractor Estimating: 193it [02:04,  1.65it/s]Extractor Estimating: 194it [02:04,  1.71it/s]Extractor Estimating: 195it [02:05,  1.41it/s]Extractor Estimating: 196it [02:06,  1.46it/s]Extractor Estimating: 197it [02:07,  1.51it/s]Extractor Estimating: 198it [02:07,  1.46it/s]Extractor Estimating: 199it [02:08,  1.51it/s]Extractor Estimating: 200it [02:08,  1.56it/s]Extractor Estimating: 201it [02:09,  1.60it/s]Extractor Estimating: 202it [02:10,  1.61it/s]Extractor Estimating: 203it [02:11,  1.43it/s]Extractor Estimating: 204it [02:11,  1.40it/s]Extractor Estimating: 205it [02:12,  1.51it/s]Extractor Estimating: 206it [02:13,  1.50it/s]Extractor Estimating: 207it [02:13,  1.58it/s]Extractor Estimating: 208it [02:14,  1.58it/s]Extractor Estimating: 209it [02:14,  1.62it/s]Extractor Estimating: 210it [02:15,  1.54it/s]Extractor Estimating: 211it [02:16,  1.52it/s]Extractor Estimating: 212it [02:16,  1.55it/s]Extractor Estimating: 213it [02:17,  1.55it/s]Extractor Estimating: 214it [02:18,  1.54it/s]Extractor Estimating: 215it [02:18,  1.59it/s]Extractor Estimating: 216it [02:19,  1.55it/s]Extractor Estimating: 217it [02:20,  1.53it/s]Extractor Estimating: 218it [02:20,  1.61it/s]Extractor Estimating: 219it [02:21,  1.54it/s]Extractor Estimating: 220it [02:21,  1.57it/s]Extractor Estimating: 221it [02:22,  1.58it/s]Extractor Estimating: 222it [02:23,  1.28it/s]Extractor Estimating: 223it [02:24,  1.40it/s]Extractor Estimating: 224it [02:24,  1.48it/s]Extractor Estimating: 225it [02:25,  1.49it/s]Extractor Estimating: 226it [02:26,  1.49it/s]Extractor Estimating: 227it [02:26,  1.50it/s]Extractor Estimating: 228it [02:27,  1.44it/s]Extractor Estimating: 229it [02:28,  1.46it/s]Extractor Estimating: 230it [02:28,  1.53it/s]Extractor Estimating: 231it [02:29,  1.51it/s]Extractor Estimating: 232it [02:30,  1.54it/s]Extractor Estimating: 233it [02:30,  1.61it/s]Extractor Estimating: 234it [02:31,  1.61it/s]Extractor Estimating: 235it [02:32,  1.50it/s]Extractor Estimating: 236it [02:32,  1.52it/s]Extractor Estimating: 237it [02:33,  1.58it/s]Extractor Estimating: 238it [02:33,  1.51it/s]Extractor Estimating: 239it [02:34,  1.52it/s]Extractor Estimating: 240it [02:35,  1.51it/s]Extractor Estimating: 241it [02:36,  1.48it/s]Extractor Estimating: 242it [02:36,  1.43it/s]Extractor Estimating: 243it [02:37,  1.49it/s]Extractor Estimating: 244it [02:37,  1.54it/s]Extractor Estimating: 245it [02:38,  1.56it/s]Extractor Estimating: 246it [02:39,  1.58it/s]Extractor Estimating: 247it [02:39,  1.57it/s]Extractor Estimating: 248it [02:40,  1.63it/s]Extractor Estimating: 249it [02:40,  1.68it/s]Extractor Estimating: 250it [02:41,  1.61it/s]Extractor Estimating: 251it [02:42,  1.47it/s]Extractor Estimating: 252it [02:43,  1.39it/s]Extractor Estimating: 253it [02:43,  1.44it/s]Extractor Estimating: 254it [02:44,  1.49it/s]Extractor Estimating: 255it [02:45,  1.55it/s]Extractor Estimating: 256it [02:45,  1.59it/s]Extractor Estimating: 257it [02:46,  1.62it/s]Extractor Estimating: 258it [02:47,  1.54it/s]Extractor Estimating: 259it [02:47,  1.42it/s]Extractor Estimating: 260it [02:48,  1.45it/s]Extractor Estimating: 261it [02:49,  1.52it/s]Extractor Estimating: 262it [02:49,  1.57it/s]Extractor Estimating: 263it [02:50,  1.61it/s]Extractor Estimating: 264it [02:51,  1.52it/s]Extractor Estimating: 265it [02:51,  1.56it/s]Extractor Estimating: 266it [02:52,  1.60it/s]Extractor Estimating: 267it [02:52,  1.61it/s]Extractor Estimating: 268it [02:53,  1.61it/s]Extractor Estimating: 269it [02:54,  1.58it/s]Extractor Estimating: 270it [02:54,  1.64it/s]Extractor Estimating: 271it [02:55,  1.68it/s]Extractor Estimating: 272it [02:55,  1.65it/s]Extractor Estimating: 273it [02:56,  1.67it/s]Extractor Estimating: 274it [02:57,  1.65it/s]Extractor Estimating: 275it [02:57,  1.60it/s]Extractor Estimating: 276it [02:58,  1.62it/s]Extractor Estimating: 277it [02:59,  1.56it/s]Extractor Estimating: 278it [02:59,  1.55it/s]Extractor Estimating: 279it [03:00,  1.56it/s]Extractor Estimating: 280it [03:01,  1.41it/s]Extractor Estimating: 281it [03:01,  1.48it/s]Extractor Estimating: 282it [03:02,  1.56it/s]Extractor Estimating: 283it [03:02,  1.57it/s]Extractor Estimating: 284it [03:03,  1.61it/s]Extractor Estimating: 285it [03:04,  1.52it/s]Extractor Estimating: 286it [03:04,  1.53it/s]Extractor Estimating: 287it [03:05,  1.61it/s]Extractor Estimating: 288it [03:06,  1.62it/s]Extractor Estimating: 289it [03:06,  1.61it/s]Extractor Estimating: 290it [03:07,  1.33it/s]Extractor Estimating: 291it [03:08,  1.43it/s]Extractor Estimating: 292it [03:08,  1.47it/s]Extractor Estimating: 293it [03:09,  1.51it/s]Extractor Estimating: 294it [03:10,  1.52it/s]Extractor Estimating: 295it [03:10,  1.47it/s]Extractor Estimating: 296it [03:11,  1.55it/s]Extractor Estimating: 297it [03:12,  1.51it/s]Extractor Estimating: 298it [03:12,  1.56it/s]Extractor Estimating: 299it [03:13,  1.56it/s]Extractor Estimating: 300it [03:14,  1.35it/s]Extractor Estimating: 301it [03:15,  1.39it/s]Extractor Estimating: 302it [03:15,  1.51it/s]Extractor Estimating: 303it [03:16,  1.53it/s]Extractor Estimating: 304it [03:16,  1.57it/s]Extractor Estimating: 305it [03:17,  1.63it/s]Extractor Estimating: 306it [03:18,  1.66it/s]Extractor Estimating: 307it [03:18,  1.68it/s]Extractor Estimating: 308it [03:19,  1.64it/s]Extractor Estimating: 309it [03:19,  1.66it/s]Extractor Estimating: 310it [03:20,  1.61it/s]Extractor Estimating: 311it [03:21,  1.50it/s]Extractor Estimating: 312it [03:21,  1.55it/s]Extractor Estimating: 313it [03:22,  1.59it/s]Extractor Estimating: 314it [03:23,  1.57it/s]Extractor Estimating: 315it [03:23,  1.58it/s]Extractor Estimating: 316it [03:24,  1.44it/s]Extractor Estimating: 317it [03:25,  1.40it/s]Extractor Estimating: 318it [03:25,  1.48it/s]Extractor Estimating: 319it [03:26,  1.48it/s]Extractor Estimating: 320it [03:27,  1.55it/s]Extractor Estimating: 321it [03:27,  1.54it/s]Extractor Estimating: 322it [03:29,  1.14it/s]Extractor Estimating: 323it [03:29,  1.27it/s]Extractor Estimating: 324it [03:31,  1.04s/it]Extractor Estimating: 325it [03:31,  1.13it/s]Extractor Estimating: 326it [03:32,  1.21it/s]Extractor Estimating: 327it [03:33,  1.33it/s]Extractor Estimating: 328it [03:34,  1.31it/s]Extractor Estimating: 329it [03:34,  1.45it/s]Extractor Estimating: 330it [03:35,  1.53it/s]Extractor Estimating: 331it [03:35,  1.61it/s]Extractor Estimating: 332it [03:36,  1.64it/s]Extractor Estimating: 333it [03:36,  1.71it/s]Extractor Estimating: 334it [03:37,  1.68it/s]Extractor Estimating: 335it [03:37,  1.72it/s]Extractor Estimating: 336it [03:38,  1.72it/s]Extractor Estimating: 337it [03:39,  1.73it/s]Extractor Estimating: 338it [03:40,  1.45it/s]Extractor Estimating: 339it [03:40,  1.52it/s]Extractor Estimating: 340it [03:41,  1.60it/s]Extractor Estimating: 341it [03:41,  1.60it/s]Extractor Estimating: 342it [03:42,  1.59it/s]Extractor Estimating: 343it [03:43,  1.56it/s]Extractor Estimating: 344it [03:43,  1.60it/s]Extractor Estimating: 345it [03:44,  1.64it/s]Extractor Estimating: 346it [03:44,  1.65it/s]Extractor Estimating: 347it [03:45,  1.66it/s]Extractor Estimating: 348it [03:46,  1.69it/s]Extractor Estimating: 349it [03:46,  1.68it/s]Extractor Estimating: 350it [03:47,  1.69it/s]Extractor Estimating: 351it [03:47,  1.64it/s]Extractor Estimating: 352it [03:48,  1.62it/s]Extractor Estimating: 353it [03:49,  1.49it/s]Extractor Estimating: 354it [03:50,  1.33it/s]Extractor Estimating: 355it [03:50,  1.37it/s]Extractor Estimating: 356it [03:51,  1.46it/s]Extractor Estimating: 357it [03:52,  1.45it/s]Extractor Estimating: 358it [03:52,  1.52it/s]Extractor Estimating: 359it [03:53,  1.56it/s]Extractor Estimating: 360it [03:53,  1.62it/s]Extractor Estimating: 361it [03:54,  1.66it/s]Extractor Estimating: 362it [03:55,  1.57it/s]Extractor Estimating: 363it [03:55,  1.51it/s]Extractor Estimating: 364it [03:56,  1.52it/s]Extractor Estimating: 365it [03:57,  1.53it/s]Extractor Estimating: 366it [03:57,  1.53it/s]Extractor Estimating: 367it [03:58,  1.53it/s]Extractor Estimating: 368it [03:59,  1.60it/s]Extractor Estimating: 369it [03:59,  1.52it/s]Extractor Estimating: 370it [04:00,  1.59it/s]Extractor Estimating: 371it [04:01,  1.51it/s]Extractor Estimating: 372it [04:01,  1.53it/s]Extractor Estimating: 373it [04:02,  1.50it/s]Extractor Estimating: 374it [04:03,  1.56it/s]Extractor Estimating: 375it [04:03,  1.62it/s]Extractor Estimating: 376it [04:04,  1.61it/s]Extractor Estimating: 377it [04:04,  1.61it/s]Extractor Estimating: 378it [04:05,  1.60it/s]Extractor Estimating: 379it [04:06,  1.53it/s]Extractor Estimating: 380it [04:06,  1.57it/s]Extractor Estimating: 381it [04:07,  1.59it/s]Extractor Estimating: 382it [04:08,  1.60it/s]Extractor Estimating: 383it [04:08,  1.65it/s]Extractor Estimating: 384it [04:09,  1.68it/s]Extractor Estimating: 385it [04:09,  1.69it/s]Extractor Estimating: 386it [04:10,  1.68it/s]Extractor Estimating: 387it [04:11,  1.61it/s]Extractor Estimating: 388it [04:11,  1.52it/s]Extractor Estimating: 389it [04:12,  1.51it/s]Extractor Estimating: 390it [04:13,  1.53it/s]Extractor Estimating: 391it [04:13,  1.61it/s]Extractor Estimating: 392it [04:14,  1.62it/s]Extractor Estimating: 393it [04:14,  1.67it/s]Extractor Estimating: 394it [04:15,  1.70it/s]Extractor Estimating: 395it [04:15,  1.70it/s]Extractor Estimating: 396it [04:16,  1.58it/s]Extractor Estimating: 397it [04:17,  1.57it/s]Extractor Estimating: 398it [04:17,  1.64it/s]Extractor Estimating: 399it [04:18,  1.67it/s]Extractor Estimating: 400it [04:19,  1.69it/s]Extractor Estimating: 401it [04:19,  1.61it/s]Extractor Estimating: 402it [04:20,  1.60it/s]Extractor Estimating: 403it [04:20,  1.64it/s]Extractor Estimating: 404it [04:21,  1.58it/s]Extractor Estimating: 405it [04:22,  1.51it/s]Extractor Estimating: 406it [04:22,  1.55it/s]Extractor Estimating: 407it [04:23,  1.51it/s]Extractor Estimating: 408it [04:24,  1.57it/s]Extractor Estimating: 409it [04:24,  1.60it/s]Extractor Estimating: 410it [04:25,  1.62it/s]Extractor Estimating: 411it [04:26,  1.63it/s]Extractor Estimating: 412it [04:26,  1.57it/s]Extractor Estimating: 413it [04:27,  1.58it/s]Extractor Estimating: 414it [04:27,  1.63it/s]Extractor Estimating: 415it [04:28,  1.65it/s]Extractor Estimating: 416it [04:29,  1.41it/s]Extractor Estimating: 417it [04:29,  1.52it/s]Extractor Estimating: 418it [04:30,  1.53it/s]Extractor Estimating: 419it [04:31,  1.55it/s]Extractor Estimating: 420it [04:31,  1.58it/s]Extractor Estimating: 421it [04:32,  1.35it/s]Extractor Estimating: 422it [04:33,  1.37it/s]Extractor Estimating: 423it [04:34,  1.44it/s]Extractor Estimating: 424it [04:35,  1.35it/s]Extractor Estimating: 425it [04:35,  1.43it/s]Extractor Estimating: 426it [04:36,  1.49it/s]Extractor Estimating: 427it [04:36,  1.55it/s]Extractor Estimating: 428it [04:37,  1.54it/s]Extractor Estimating: 429it [04:38,  1.51it/s]Extractor Estimating: 430it [04:38,  1.52it/s]Extractor Estimating: 431it [04:39,  1.55it/s]Extractor Estimating: 432it [04:40,  1.56it/s]Extractor Estimating: 433it [04:40,  1.54it/s]Extractor Estimating: 434it [04:41,  1.55it/s]Extractor Estimating: 435it [04:41,  1.55it/s]Extractor Estimating: 436it [04:42,  1.51it/s]Extractor Estimating: 437it [04:43,  1.36it/s]Extractor Estimating: 438it [04:44,  1.44it/s]Extractor Estimating: 439it [04:44,  1.48it/s]Extractor Estimating: 440it [04:45,  1.43it/s]Extractor Estimating: 441it [04:46,  1.49it/s]Extractor Estimating: 442it [04:46,  1.50it/s]Extractor Estimating: 443it [04:47,  1.56it/s]Extractor Estimating: 444it [04:48,  1.59it/s]Extractor Estimating: 445it [04:48,  1.53it/s]Extractor Estimating: 446it [04:49,  1.55it/s]Extractor Estimating: 447it [04:50,  1.56it/s]Extractor Estimating: 448it [04:50,  1.50it/s]Extractor Estimating: 449it [04:51,  1.56it/s]Extractor Estimating: 450it [04:51,  1.62it/s]Extractor Estimating: 451it [04:52,  1.48it/s]Extractor Estimating: 452it [04:53,  1.51it/s]Extractor Estimating: 453it [04:54,  1.34it/s]Extractor Estimating: 454it [04:54,  1.46it/s]Extractor Estimating: 455it [04:55,  1.54it/s]Extractor Estimating: 456it [04:56,  1.45it/s]Extractor Estimating: 457it [04:56,  1.50it/s]Extractor Estimating: 458it [04:57,  1.56it/s]Extractor Estimating: 459it [04:57,  1.59it/s]Extractor Estimating: 460it [04:58,  1.65it/s]Extractor Estimating: 461it [04:59,  1.68it/s]Extractor Estimating: 462it [04:59,  1.74it/s]Extractor Estimating: 463it [05:00,  1.77it/s]Extractor Estimating: 464it [05:00,  1.77it/s]Extractor Estimating: 465it [05:01,  1.71it/s]Extractor Estimating: 466it [05:01,  1.74it/s]Extractor Estimating: 467it [05:02,  1.77it/s]Extractor Estimating: 468it [05:02,  1.80it/s]Extractor Estimating: 469it [05:03,  1.74it/s]Extractor Estimating: 470it [05:04,  1.61it/s]Extractor Estimating: 471it [05:04,  1.68it/s]Extractor Estimating: 472it [05:05,  1.72it/s]Extractor Estimating: 473it [05:05,  1.70it/s]Extractor Estimating: 474it [05:06,  1.74it/s]Extractor Estimating: 475it [05:07,  1.73it/s]Extractor Estimating: 476it [05:07,  1.74it/s]Extractor Estimating: 477it [05:08,  1.61it/s]Extractor Estimating: 478it [05:09,  1.63it/s]Extractor Estimating: 479it [05:09,  1.41it/s]Extractor Estimating: 480it [05:10,  1.49it/s]Extractor Estimating: 481it [05:11,  1.48it/s]Extractor Estimating: 482it [05:11,  1.54it/s]Extractor Estimating: 483it [05:12,  1.53it/s]Extractor Estimating: 484it [05:13,  1.59it/s]Extractor Estimating: 485it [05:13,  1.63it/s]Extractor Estimating: 486it [05:14,  1.65it/s]Extractor Estimating: 487it [05:14,  1.57it/s]Extractor Estimating: 488it [05:15,  1.55it/s]Extractor Estimating: 489it [05:16,  1.61it/s]Extractor Estimating: 490it [05:16,  1.66it/s]Extractor Estimating: 491it [05:17,  1.68it/s]Extractor Estimating: 492it [05:17,  1.61it/s]Extractor Estimating: 493it [05:18,  1.62it/s]Extractor Estimating: 494it [05:19,  1.62it/s]Extractor Estimating: 495it [05:19,  1.60it/s]Extractor Estimating: 496it [05:20,  1.57it/s]Extractor Estimating: 497it [05:21,  1.50it/s]Extractor Estimating: 498it [05:22,  1.42it/s]Extractor Estimating: 499it [05:22,  1.48it/s]Extractor Estimating: 500it [05:23,  1.70it/s]Extractor Estimating: 500it [05:23,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:51,598 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:51,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:51,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:51,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:51,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:52:53,292 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:52:53,293 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:52:54,185 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:52:55,660 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:52:55,736 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:58,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:58,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:58,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:58,502 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:52:58,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:52:59,774 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:52:59,909 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:53:00,367 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:53:00,582 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:53:00,582 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:42:27,663 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:42:27,707 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9972 mean pseudo reward: 0.9487582961010836
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 28373
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28473, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28473, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.031, loss:623.4591
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.974, loss:596.3690
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.980, loss:571.4743
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.969, loss:597.2112
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.980, loss:546.2716
>> valid entity prec:0.5501, rec:0.5126, f1:0.5307
>> valid relation prec:0.2681, rec:0.0923, f1:0.1374
>> valid relation with NER prec:0.2681, rec:0.0923, f1:0.1374
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.271, loss:578.9290
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.978, loss:590.5224
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.976, loss:631.4686
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.983, loss:590.2656
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.972, loss:597.4265
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5634, rec:0.5081, f1:0.5343
>> valid relation prec:0.2429, rec:0.0832, f1:0.1239
>> valid relation with NER prec:0.2429, rec:0.0832, f1:0.1239
new max entity f1 on valid!
g_step 1100, step 268, avg_time 2.244, loss:618.8551
g_step 1200, step 368, avg_time 0.980, loss:668.4038
g_step 1300, step 52, avg_time 0.970, loss:605.1578
g_step 1400, step 152, avg_time 1.003, loss:607.6535
g_step 1500, step 252, avg_time 0.964, loss:623.3532
>> valid entity prec:0.5634, rec:0.5391, f1:0.5510
>> valid relation prec:0.3101, rec:0.1227, f1:0.1759
>> valid relation with NER prec:0.3101, rec:0.1227, f1:0.1759
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.283, loss:599.7667
g_step 1700, step 36, avg_time 0.971, loss:585.9744
g_step 1800, step 136, avg_time 0.980, loss:569.9190
g_step 1900, step 236, avg_time 0.974, loss:600.2946
g_step 2000, step 336, avg_time 0.980, loss:590.1850
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4932, rec:0.5467, f1:0.5186
>> valid relation prec:0.2450, rec:0.1164, f1:0.1579
>> valid relation with NER prec:0.2450, rec:0.1164, f1:0.1579
g_step 2100, step 20, avg_time 2.233, loss:578.8780
g_step 2200, step 120, avg_time 0.969, loss:526.7215
g_step 2300, step 220, avg_time 0.983, loss:553.8900
g_step 2400, step 320, avg_time 0.980, loss:578.1947
g_step 2500, step 4, avg_time 0.970, loss:575.0568
>> valid entity prec:0.5607, rec:0.4752, f1:0.5144
>> valid relation prec:0.2601, rec:0.0938, f1:0.1379
>> valid relation with NER prec:0.2601, rec:0.0938, f1:0.1379
g_step 2600, step 104, avg_time 2.242, loss:505.0447
g_step 2700, step 204, avg_time 0.962, loss:536.0896
g_step 2800, step 304, avg_time 0.972, loss:551.1994
g_step 2900, step 404, avg_time 0.980, loss:533.9676
g_step 3000, step 88, avg_time 0.972, loss:491.8346
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5278, rec:0.4956, f1:0.5112
>> valid relation prec:0.2455, rec:0.1012, f1:0.1434
>> valid relation with NER prec:0.2455, rec:0.1012, f1:0.1434
g_step 3100, step 188, avg_time 2.232, loss:507.4261
g_step 3200, step 288, avg_time 0.989, loss:523.1315
g_step 3300, step 388, avg_time 0.973, loss:529.3689
g_step 3400, step 72, avg_time 0.972, loss:475.3754
g_step 3500, step 172, avg_time 0.979, loss:488.0732
>> valid entity prec:0.5224, rec:0.5497, f1:0.5357
>> valid relation prec:0.2359, rec:0.1058, f1:0.1461
>> valid relation with NER prec:0.2359, rec:0.1058, f1:0.1461
g_step 3600, step 272, avg_time 2.234, loss:492.0542
g_step 3700, step 372, avg_time 0.976, loss:520.6771
g_step 3800, step 56, avg_time 0.968, loss:482.9947
g_step 3900, step 156, avg_time 0.973, loss:480.6809
g_step 4000, step 256, avg_time 0.972, loss:502.8770
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5540, rec:0.5365, f1:0.5451
>> valid relation prec:0.2775, rec:0.1144, f1:0.1620
>> valid relation with NER prec:0.2775, rec:0.1144, f1:0.1620
g_step 4100, step 356, avg_time 2.248, loss:518.7938
g_step 4200, step 40, avg_time 0.965, loss:460.6121
g_step 4300, step 140, avg_time 0.970, loss:475.8855
g_step 4400, step 240, avg_time 0.983, loss:459.8467
g_step 4500, step 340, avg_time 0.977, loss:469.1586
>> valid entity prec:0.5497, rec:0.5316, f1:0.5405
>> valid relation prec:0.2496, rec:0.1268, f1:0.1681
>> valid relation with NER prec:0.2496, rec:0.1268, f1:0.1681
g_step 4600, step 24, avg_time 2.240, loss:459.9253
g_step 4700, step 124, avg_time 0.979, loss:440.8189
g_step 4800, step 224, avg_time 0.971, loss:450.4390
g_step 4900, step 324, avg_time 0.980, loss:470.4743
g_step 5000, step 8, avg_time 0.967, loss:450.1634
learning rate was adjusted to 0.0008
>> valid entity prec:0.5410, rec:0.5050, f1:0.5223
>> valid relation prec:0.2255, rec:0.0952, f1:0.1339
>> valid relation with NER prec:0.2255, rec:0.0952, f1:0.1339
g_step 5100, step 108, avg_time 2.233, loss:403.9538
g_step 5200, step 208, avg_time 0.990, loss:447.0236
g_step 5300, step 308, avg_time 0.977, loss:446.7606
g_step 5400, step 408, avg_time 0.975, loss:465.7569
g_step 5500, step 92, avg_time 0.982, loss:402.3452
>> valid entity prec:0.5293, rec:0.4851, f1:0.5062
>> valid relation prec:0.2280, rec:0.0964, f1:0.1355
>> valid relation with NER prec:0.2280, rec:0.0964, f1:0.1355
g_step 5600, step 192, avg_time 2.231, loss:409.6316
g_step 5700, step 292, avg_time 0.980, loss:435.2095
g_step 5800, step 392, avg_time 0.980, loss:449.2849
g_step 5900, step 76, avg_time 0.978, loss:419.4296
g_step 6000, step 176, avg_time 0.975, loss:411.9467
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5510, rec:0.5083, f1:0.5288
>> valid relation prec:0.2483, rec:0.1024, f1:0.1450
>> valid relation with NER prec:0.2483, rec:0.1024, f1:0.1450
g_step 6100, step 276, avg_time 2.250, loss:395.9435
g_step 6200, step 376, avg_time 0.972, loss:428.9556
g_step 6300, step 60, avg_time 0.975, loss:397.3824
g_step 6400, step 160, avg_time 0.969, loss:379.6282
g_step 6500, step 260, avg_time 0.970, loss:379.7615
>> valid entity prec:0.5322, rec:0.4989, f1:0.5150
>> valid relation prec:0.1998, rec:0.1001, f1:0.1334
>> valid relation with NER prec:0.1998, rec:0.1001, f1:0.1334
g_step 6600, step 360, avg_time 2.242, loss:428.9357
g_step 6700, step 44, avg_time 0.965, loss:404.0462
g_step 6800, step 144, avg_time 0.978, loss:392.0827
g_step 6900, step 244, avg_time 0.974, loss:383.2769
g_step 7000, step 344, avg_time 0.974, loss:398.4520
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5466, rec:0.4770, f1:0.5095
>> valid relation prec:0.2367, rec:0.1107, f1:0.1508
>> valid relation with NER prec:0.2367, rec:0.1107, f1:0.1508
g_step 7100, step 28, avg_time 2.247, loss:379.3992
g_step 7200, step 128, avg_time 0.970, loss:355.9210
g_step 7300, step 228, avg_time 0.976, loss:381.1350
g_step 7400, step 328, avg_time 0.982, loss:405.5320
g_step 7500, step 12, avg_time 0.974, loss:389.0183
>> valid entity prec:0.5181, rec:0.4916, f1:0.5045
>> valid relation prec:0.2428, rec:0.1113, f1:0.1526
>> valid relation with NER prec:0.2428, rec:0.1113, f1:0.1526
g_step 7600, step 112, avg_time 2.242, loss:352.8641
g_step 7700, step 212, avg_time 0.968, loss:347.9188
g_step 7800, step 312, avg_time 0.979, loss:375.4854
g_step 7900, step 412, avg_time 0.971, loss:385.0906
g_step 8000, step 96, avg_time 0.970, loss:347.7218
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5561, rec:0.4743, f1:0.5120
>> valid relation prec:0.2463, rec:0.1061, f1:0.1483
>> valid relation with NER prec:0.2463, rec:0.1061, f1:0.1483
g_step 8100, step 196, avg_time 2.232, loss:351.7251
g_step 8200, step 296, avg_time 0.987, loss:356.2394
g_step 8300, step 396, avg_time 0.973, loss:362.3560
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:42:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:42:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-42-27_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:42:28 - WARNING - datasets.builder -   Using custom data configuration default-c72383d06969334d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c72383d06969334d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:42:32,012 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:42:32,013 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:42:32,014 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:42:32,015 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:42:32,177 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:42:32,275 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:42:32,992 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:42:36,169 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:42:36,222 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c72383d06969334d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.29ba/s] 20%|        | 2/10 [00:00<00:02,  3.31ba/s] 30%|       | 3/10 [00:00<00:01,  3.78ba/s] 40%|      | 4/10 [00:01<00:01,  4.05ba/s] 50%|     | 5/10 [00:01<00:01,  4.22ba/s] 60%|    | 6/10 [00:01<00:00,  4.32ba/s] 70%|   | 7/10 [00:01<00:00,  4.40ba/s] 80%|  | 8/10 [00:01<00:00,  4.44ba/s] 90%| | 9/10 [00:02<00:00,  4.45ba/s]100%|| 10/10 [00:02<00:00,  4.48ba/s]100%|| 10/10 [00:02<00:00,  4.15ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.21ba/s] 50%|     | 2/4 [00:00<00:00,  2.88ba/s] 75%|  | 3/4 [00:00<00:00,  3.42ba/s]100%|| 4/4 [00:01<00:00,  4.50ba/s]100%|| 4/4 [00:01<00:00,  3.90ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.41ba/s] 30%|       | 3/10 [00:00<00:00,  8.61ba/s] 50%|     | 5/10 [00:00<00:00,  9.70ba/s] 70%|   | 7/10 [00:00<00:00, 10.16ba/s] 90%| | 9/10 [00:00<00:00, 10.42ba/s]100%|| 10/10 [00:01<00:00,  9.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.53ba/s] 75%|  | 3/4 [00:00<00:00,  9.30ba/s]100%|| 4/4 [00:00<00:00, 10.45ba/s]
[INFO|trainer.py:414] 2023-08-29 00:42:42,123 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:42:42,146 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:42:42,146 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 00:42:42,146 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:42:42,147 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:42:42,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:42:42,147 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:42:42,147 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:52,  3.36it/s]  0%|          | 2/780 [00:00<03:47,  3.42it/s]  0%|          | 3/780 [00:00<03:45,  3.45it/s]  1%|          | 4/780 [00:01<03:44,  3.46it/s]  1%|          | 5/780 [00:01<03:43,  3.47it/s]  1%|          | 6/780 [00:01<03:43,  3.47it/s]  1%|          | 7/780 [00:02<03:43,  3.45it/s]  1%|          | 8/780 [00:02<03:44,  3.44it/s]  1%|          | 9/780 [00:02<03:44,  3.44it/s]  1%|         | 10/780 [00:02<03:44,  3.43it/s]  1%|         | 11/780 [00:03<03:43,  3.44it/s]  2%|         | 12/780 [00:03<03:42,  3.45it/s]  2%|         | 13/780 [00:03<03:41,  3.46it/s]  2%|         | 14/780 [00:04<03:41,  3.46it/s]  2%|         | 15/780 [00:04<03:40,  3.47it/s]  2%|         | 16/780 [00:04<03:40,  3.47it/s]  2%|         | 17/780 [00:04<03:40,  3.47it/s]  2%|         | 18/780 [00:05<03:44,  3.39it/s]  2%|         | 19/780 [00:05<03:42,  3.41it/s]  3%|         | 20/780 [00:05<03:41,  3.43it/s]  3%|         | 21/780 [00:06<03:40,  3.44it/s]  3%|         | 22/780 [00:06<03:39,  3.45it/s]  3%|         | 23/780 [00:06<03:38,  3.46it/s]  3%|         | 24/780 [00:06<03:38,  3.46it/s]  3%|         | 25/780 [00:07<03:44,  3.37it/s]  3%|         | 26/780 [00:07<03:41,  3.40it/s]  3%|         | 27/780 [00:07<03:40,  3.42it/s]  4%|         | 28/780 [00:08<03:38,  3.43it/s]  4%|         | 29/780 [00:08<03:37,  3.45it/s]  4%|         | 30/780 [00:08<03:37,  3.45it/s]  4%|         | 31/780 [00:09<03:36,  3.46it/s]  4%|         | 32/780 [00:09<03:35,  3.46it/s]  4%|         | 33/780 [00:09<03:35,  3.46it/s]  4%|         | 34/780 [00:09<03:35,  3.47it/s]  4%|         | 35/780 [00:10<03:34,  3.47it/s]  5%|         | 36/780 [00:10<03:44,  3.31it/s]  5%|         | 37/780 [00:10<03:41,  3.36it/s]  5%|         | 38/780 [00:11<03:38,  3.39it/s]  5%|         | 39/780 [00:11<03:37,  3.41it/s]  5%|         | 40/780 [00:11<03:35,  3.43it/s]  5%|         | 41/780 [00:11<03:34,  3.44it/s]  5%|         | 42/780 [00:12<03:33,  3.45it/s]  6%|         | 43/780 [00:12<03:33,  3.46it/s]  6%|         | 44/780 [00:12<03:32,  3.46it/s]  6%|         | 45/780 [00:13<03:32,  3.46it/s]  6%|         | 46/780 [00:13<03:31,  3.47it/s]  6%|         | 47/780 [00:13<03:31,  3.47it/s]  6%|         | 48/780 [00:13<03:31,  3.47it/s]  6%|         | 49/780 [00:14<03:30,  3.47it/s]  6%|         | 50/780 [00:14<03:30,  3.47it/s]  7%|         | 51/780 [00:14<03:30,  3.47it/s]  7%|         | 52/780 [00:15<03:29,  3.47it/s]  7%|         | 53/780 [00:15<03:37,  3.35it/s]  7%|         | 54/780 [00:15<03:34,  3.38it/s]  7%|         | 55/780 [00:16<03:32,  3.41it/s]  7%|         | 56/780 [00:16<03:31,  3.43it/s]  7%|         | 57/780 [00:16<03:30,  3.44it/s]  7%|         | 58/780 [00:16<03:29,  3.45it/s]  8%|         | 59/780 [00:17<03:28,  3.45it/s]  8%|         | 60/780 [00:17<03:28,  3.46it/s]  8%|         | 61/780 [00:17<03:27,  3.46it/s]  8%|         | 62/780 [00:18<03:27,  3.46it/s]  8%|         | 63/780 [00:18<03:27,  3.46it/s]  8%|         | 64/780 [00:18<03:26,  3.46it/s]  8%|         | 65/780 [00:18<03:26,  3.47it/s]  8%|         | 66/780 [00:19<03:25,  3.47it/s]  9%|         | 67/780 [00:19<03:25,  3.47it/s]  9%|         | 68/780 [00:19<03:25,  3.47it/s]  9%|         | 69/780 [00:20<03:24,  3.47it/s]  9%|         | 70/780 [00:20<03:24,  3.47it/s]  9%|         | 71/780 [00:20<03:30,  3.37it/s]  9%|         | 72/780 [00:20<03:28,  3.40it/s]  9%|         | 73/780 [00:21<03:48,  3.09it/s]  9%|         | 74/780 [00:21<03:40,  3.20it/s] 10%|         | 75/780 [00:21<03:35,  3.27it/s] 10%|         | 76/780 [00:22<03:31,  3.33it/s] 10%|         | 77/780 [00:22<03:28,  3.37it/s] 10%|         | 78/780 [00:22<03:26,  3.40it/s] 10%|         | 79/780 [00:23<03:25,  3.42it/s] 10%|         | 80/780 [00:23<03:23,  3.43it/s] 10%|         | 81/780 [00:23<03:23,  3.44it/s] 11%|         | 82/780 [00:23<03:22,  3.45it/s] 11%|         | 83/780 [00:24<03:21,  3.46it/s] 11%|         | 84/780 [00:24<03:21,  3.46it/s] 11%|         | 85/780 [00:24<03:20,  3.46it/s] 11%|         | 86/780 [00:25<03:20,  3.46it/s] 11%|         | 87/780 [00:25<03:20,  3.46it/s] 11%|        | 88/780 [00:25<03:24,  3.38it/s] 11%|        | 89/780 [00:25<03:22,  3.40it/s] 12%|        | 90/780 [00:26<03:21,  3.42it/s] 12%|        | 91/780 [00:26<03:20,  3.44it/s] 12%|        | 92/780 [00:26<03:19,  3.45it/s] 12%|        | 93/780 [00:27<03:19,  3.45it/s] 12%|        | 94/780 [00:27<03:18,  3.45it/s] 12%|        | 95/780 [00:27<03:18,  3.46it/s] 12%|        | 96/780 [00:27<03:17,  3.46it/s] 12%|        | 97/780 [00:28<03:17,  3.46it/s] 13%|        | 98/780 [00:28<03:16,  3.46it/s] 13%|        | 99/780 [00:28<03:16,  3.46it/s] 13%|        | 100/780 [00:29<03:16,  3.46it/s] 13%|        | 101/780 [00:29<03:16,  3.46it/s] 13%|        | 102/780 [00:29<03:15,  3.46it/s] 13%|        | 103/780 [00:30<03:15,  3.47it/s] 13%|        | 104/780 [00:30<03:15,  3.46it/s] 13%|        | 105/780 [00:30<03:14,  3.46it/s] 14%|        | 106/780 [00:30<03:25,  3.28it/s] 14%|        | 107/780 [00:31<03:21,  3.33it/s] 14%|        | 108/780 [00:31<03:19,  3.37it/s] 14%|        | 109/780 [00:31<03:17,  3.40it/s] 14%|        | 110/780 [00:32<03:16,  3.42it/s] 14%|        | 111/780 [00:32<03:15,  3.43it/s] 14%|        | 112/780 [00:32<03:14,  3.44it/s] 14%|        | 113/780 [00:32<03:13,  3.44it/s] 15%|        | 114/780 [00:33<03:13,  3.45it/s] 15%|        | 115/780 [00:33<03:12,  3.45it/s] 15%|        | 116/780 [00:33<03:12,  3.45it/s] 15%|        | 117/780 [00:34<03:21,  3.29it/s] 15%|        | 118/780 [00:34<03:17,  3.34it/s] 15%|        | 119/780 [00:34<03:15,  3.38it/s] 15%|        | 120/780 [00:35<03:13,  3.40it/s] 16%|        | 121/780 [00:35<03:12,  3.42it/s] 16%|        | 122/780 [00:35<03:11,  3.43it/s] 16%|        | 123/780 [00:35<03:10,  3.44it/s] 16%|        | 124/780 [00:36<03:10,  3.45it/s] 16%|        | 125/780 [00:36<03:09,  3.45it/s] 16%|        | 126/780 [00:36<03:09,  3.45it/s] 16%|        | 127/780 [00:37<03:08,  3.46it/s] 16%|        | 128/780 [00:37<03:18,  3.29it/s] 17%|        | 129/780 [00:37<03:15,  3.34it/s] 17%|        | 130/780 [00:37<03:12,  3.37it/s] 17%|        | 131/780 [00:38<03:10,  3.40it/s] 17%|        | 132/780 [00:38<03:09,  3.42it/s] 17%|        | 133/780 [00:38<03:08,  3.43it/s] 17%|        | 134/780 [00:39<03:07,  3.44it/s] 17%|        | 135/780 [00:39<03:06,  3.45it/s] 17%|        | 136/780 [00:39<03:06,  3.45it/s] 18%|        | 137/780 [00:39<03:05,  3.46it/s] 18%|        | 138/780 [00:40<03:05,  3.46it/s] 18%|        | 139/780 [00:40<03:11,  3.35it/s] 18%|        | 140/780 [00:40<03:08,  3.39it/s] 18%|        | 141/780 [00:41<03:07,  3.41it/s] 18%|        | 142/780 [00:41<03:06,  3.42it/s] 18%|        | 143/780 [00:41<03:05,  3.44it/s] 18%|        | 144/780 [00:42<03:04,  3.44it/s] 19%|        | 145/780 [00:42<03:03,  3.45it/s] 19%|        | 146/780 [00:42<03:03,  3.45it/s] 19%|        | 147/780 [00:42<03:03,  3.46it/s] 19%|        | 148/780 [00:43<03:02,  3.46it/s] 19%|        | 149/780 [00:43<03:02,  3.46it/s] 19%|        | 150/780 [00:43<03:10,  3.31it/s] 19%|        | 151/780 [00:44<03:07,  3.36it/s] 19%|        | 152/780 [00:44<03:05,  3.39it/s] 20%|        | 153/780 [00:44<03:03,  3.41it/s] 20%|        | 154/780 [00:44<03:02,  3.43it/s] 20%|        | 155/780 [00:45<03:01,  3.44it/s] 20%|        | 156/780 [00:45<03:01,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:43:27,722 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:43:27,722 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:43:27,722 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.14it/s][A
  3%|         | 12/437 [00:00<00:08, 49.38it/s][A
  4%|         | 17/437 [00:00<00:08, 47.52it/s][A
  5%|         | 22/437 [00:00<00:08, 46.39it/s][A
  6%|         | 27/437 [00:00<00:08, 45.89it/s][A
  7%|         | 32/437 [00:00<00:08, 45.67it/s][A
  8%|         | 37/437 [00:00<00:08, 45.64it/s][A
 10%|         | 42/437 [00:00<00:08, 45.32it/s][A
 11%|         | 47/437 [00:01<00:09, 42.42it/s][A
 12%|        | 52/437 [00:01<00:08, 43.39it/s][A
 13%|        | 57/437 [00:01<00:08, 44.10it/s][A
 14%|        | 62/437 [00:01<00:08, 44.54it/s][A
 15%|        | 67/437 [00:01<00:08, 44.95it/s][A
 16%|        | 72/437 [00:01<00:08, 45.11it/s][A
 18%|        | 77/437 [00:01<00:07, 45.17it/s][A
 19%|        | 82/437 [00:01<00:07, 45.14it/s][A
 20%|        | 87/437 [00:01<00:07, 44.89it/s][A
 21%|        | 92/437 [00:02<00:07, 44.89it/s][A
 22%|       | 97/437 [00:02<00:07, 45.08it/s][A
 23%|       | 102/437 [00:02<00:07, 45.23it/s][A
 24%|       | 107/437 [00:02<00:07, 45.36it/s][A
 26%|       | 112/437 [00:02<00:07, 45.47it/s][A
 27%|       | 117/437 [00:02<00:07, 45.59it/s][A
 28%|       | 122/437 [00:02<00:06, 45.53it/s][A
 29%|       | 127/437 [00:02<00:06, 45.46it/s][A
 30%|       | 132/437 [00:02<00:06, 45.34it/s][A
 31%|      | 137/437 [00:03<00:06, 45.16it/s][A
 32%|      | 142/437 [00:03<00:06, 45.22it/s][A
 34%|      | 147/437 [00:03<00:06, 45.19it/s][A
 35%|      | 152/437 [00:03<00:06, 45.36it/s][A
 36%|      | 157/437 [00:03<00:06, 45.46it/s][A
 37%|      | 162/437 [00:03<00:06, 45.53it/s][A
 38%|      | 167/437 [00:03<00:05, 45.55it/s][A
 39%|      | 172/437 [00:03<00:05, 45.52it/s][A
 41%|      | 177/437 [00:03<00:05, 45.32it/s][A
 42%|     | 182/437 [00:04<00:05, 45.38it/s][A
 43%|     | 187/437 [00:04<00:06, 39.81it/s][A
 44%|     | 192/437 [00:04<00:05, 41.45it/s][A
 45%|     | 197/437 [00:04<00:05, 42.70it/s][A
 46%|     | 202/437 [00:04<00:05, 43.62it/s][A
 47%|     | 207/437 [00:04<00:05, 44.16it/s][A
 49%|     | 212/437 [00:04<00:05, 44.63it/s][A
 50%|     | 217/437 [00:04<00:04, 44.97it/s][A
 51%|     | 222/437 [00:04<00:04, 45.15it/s][A
 52%|    | 227/437 [00:05<00:04, 44.77it/s][A
 53%|    | 232/437 [00:05<00:04, 44.58it/s][A
 54%|    | 237/437 [00:05<00:04, 44.81it/s][A
 55%|    | 242/437 [00:05<00:04, 45.06it/s][A
 57%|    | 247/437 [00:05<00:04, 45.30it/s][A
 58%|    | 252/437 [00:05<00:04, 45.39it/s][A
 59%|    | 257/437 [00:05<00:03, 45.59it/s][A
 60%|    | 262/437 [00:05<00:03, 45.60it/s][A
 61%|    | 267/437 [00:05<00:03, 45.49it/s][A
 62%|   | 272/437 [00:06<00:03, 45.29it/s][A
 63%|   | 277/437 [00:06<00:03, 45.03it/s][A
 65%|   | 282/437 [00:06<00:03, 44.89it/s][A
 66%|   | 287/437 [00:06<00:03, 45.08it/s][A
 67%|   | 292/437 [00:06<00:03, 45.29it/s][A
 68%|   | 297/437 [00:06<00:03, 45.46it/s][A
 69%|   | 302/437 [00:06<00:02, 45.56it/s][A
 70%|   | 307/437 [00:06<00:02, 45.58it/s][A
 71%|  | 312/437 [00:06<00:02, 45.50it/s][A
 73%|  | 317/437 [00:07<00:02, 45.23it/s][A
 74%|  | 322/437 [00:07<00:02, 41.70it/s][A
 75%|  | 327/437 [00:07<00:02, 42.85it/s][A
 76%|  | 332/437 [00:07<00:02, 43.69it/s][A
 77%|  | 337/437 [00:07<00:02, 44.22it/s][A
 78%|  | 342/437 [00:07<00:02, 44.60it/s][A
 79%|  | 347/437 [00:07<00:02, 44.87it/s][A
 81%|  | 352/437 [00:07<00:01, 45.15it/s][A
 82%| | 357/437 [00:07<00:01, 45.24it/s][A
 83%| | 362/437 [00:08<00:01, 44.99it/s][A
 84%| | 367/437 [00:08<00:01, 44.89it/s][A
 85%| | 372/437 [00:08<00:01, 45.03it/s][A
 86%| | 377/437 [00:08<00:01, 45.15it/s][A
 87%| | 382/437 [00:08<00:01, 45.23it/s][A
 89%| | 387/437 [00:08<00:01, 45.32it/s][A
 90%| | 392/437 [00:08<00:00, 45.39it/s][A
 91%| | 397/437 [00:08<00:00, 45.57it/s][A
 92%|| 402/437 [00:08<00:00, 45.35it/s][A
 93%|| 407/437 [00:09<00:00, 45.28it/s][A
 94%|| 412/437 [00:09<00:00, 45.08it/s][A
 95%|| 417/437 [00:09<00:00, 45.19it/s][A
 97%|| 422/437 [00:09<00:00, 45.23it/s][A
 98%|| 427/437 [00:09<00:00, 45.34it/s][A
 99%|| 432/437 [00:09<00:00, 45.30it/s][A
100%|| 437/437 [00:09<00:00, 45.45it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.45it/s][A 20%|        | 156/780 [00:55<03:01,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:43:38,053 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:43:38,629 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:43:41,987 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:43:42,125 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:43:42,213 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:06<1:08:43,  6.62s/it] 20%|        | 158/780 [01:07<48:59,  4.73s/it]   20%|        | 159/780 [01:07<35:08,  3.40s/it] 21%|        | 160/780 [01:07<25:28,  2.46s/it] 21%|        | 161/780 [01:08<18:42,  1.81s/it] 21%|        | 162/780 [01:08<13:58,  1.36s/it] 21%|        | 163/780 [01:08<10:40,  1.04s/it] 21%|        | 164/780 [01:08<08:21,  1.23it/s] 21%|        | 165/780 [01:09<06:44,  1.52it/s] 21%|       | 166/780 [01:09<05:36,  1.82it/s] 21%|       | 167/780 [01:09<04:49,  2.12it/s] 22%|       | 168/780 [01:10<04:15,  2.39it/s] 22%|       | 169/780 [01:10<03:56,  2.58it/s] 22%|       | 170/780 [01:10<03:39,  2.78it/s] 22%|       | 171/780 [01:11<03:26,  2.95it/s] 22%|       | 172/780 [01:11<03:17,  3.07it/s] 22%|       | 173/780 [01:11<03:11,  3.17it/s] 22%|       | 174/780 [01:11<03:07,  3.24it/s] 22%|       | 175/780 [01:12<03:03,  3.29it/s] 23%|       | 176/780 [01:12<03:01,  3.32it/s] 23%|       | 177/780 [01:12<02:59,  3.35it/s] 23%|       | 178/780 [01:13<02:58,  3.37it/s] 23%|       | 179/780 [01:13<02:57,  3.38it/s] 23%|       | 180/780 [01:13<03:04,  3.26it/s] 23%|       | 181/780 [01:14<03:01,  3.31it/s] 23%|       | 182/780 [01:14<02:59,  3.33it/s] 23%|       | 183/780 [01:14<02:57,  3.36it/s] 24%|       | 184/780 [01:14<02:56,  3.37it/s] 24%|       | 185/780 [01:15<02:55,  3.38it/s] 24%|       | 186/780 [01:15<02:55,  3.39it/s] 24%|       | 187/780 [01:15<02:54,  3.40it/s] 24%|       | 188/780 [01:16<02:54,  3.40it/s] 24%|       | 189/780 [01:16<02:53,  3.41it/s] 24%|       | 190/780 [01:16<02:52,  3.41it/s] 24%|       | 191/780 [01:16<02:56,  3.33it/s] 25%|       | 192/780 [01:17<02:55,  3.36it/s] 25%|       | 193/780 [01:17<02:53,  3.38it/s] 25%|       | 194/780 [01:17<02:53,  3.38it/s] 25%|       | 195/780 [01:18<02:52,  3.39it/s] 25%|       | 196/780 [01:18<02:51,  3.40it/s] 25%|       | 197/780 [01:18<02:51,  3.40it/s] 25%|       | 198/780 [01:19<02:50,  3.41it/s] 26%|       | 199/780 [01:19<02:50,  3.41it/s] 26%|       | 200/780 [01:19<02:50,  3.41it/s] 26%|       | 201/780 [01:19<02:49,  3.41it/s] 26%|       | 202/780 [01:20<02:54,  3.31it/s] 26%|       | 203/780 [01:20<02:52,  3.34it/s] 26%|       | 204/780 [01:20<02:51,  3.36it/s] 26%|       | 205/780 [01:21<03:43,  2.57it/s] 26%|       | 206/780 [01:21<03:26,  2.78it/s] 27%|       | 207/780 [01:22<03:14,  2.94it/s] 27%|       | 208/780 [01:22<03:06,  3.07it/s] 27%|       | 209/780 [01:22<03:00,  3.16it/s] 27%|       | 210/780 [01:22<02:56,  3.24it/s] 27%|       | 211/780 [01:23<02:53,  3.28it/s] 27%|       | 212/780 [01:23<02:56,  3.22it/s] 27%|       | 213/780 [01:23<02:53,  3.27it/s] 27%|       | 214/780 [01:24<02:50,  3.31it/s] 28%|       | 215/780 [01:24<02:49,  3.34it/s] 28%|       | 216/780 [01:24<02:47,  3.36it/s] 28%|       | 217/780 [01:24<02:46,  3.38it/s] 28%|       | 218/780 [01:25<02:45,  3.39it/s] 28%|       | 219/780 [01:25<02:45,  3.39it/s] 28%|       | 220/780 [01:25<02:44,  3.40it/s] 28%|       | 221/780 [01:26<02:44,  3.41it/s] 28%|       | 222/780 [01:26<02:43,  3.41it/s] 29%|       | 223/780 [01:26<02:48,  3.31it/s] 29%|       | 224/780 [01:27<02:46,  3.34it/s] 29%|       | 225/780 [01:27<02:45,  3.36it/s] 29%|       | 226/780 [01:27<02:44,  3.38it/s] 29%|       | 227/780 [01:27<02:43,  3.39it/s] 29%|       | 228/780 [01:28<02:42,  3.40it/s] 29%|       | 229/780 [01:28<02:42,  3.40it/s] 29%|       | 230/780 [01:28<02:41,  3.40it/s] 30%|       | 231/780 [01:29<02:41,  3.41it/s] 30%|       | 232/780 [01:29<02:40,  3.41it/s] 30%|       | 233/780 [01:29<02:40,  3.41it/s] 30%|       | 234/780 [01:29<02:40,  3.41it/s] 30%|       | 235/780 [01:30<02:39,  3.41it/s] 30%|       | 236/780 [01:30<02:39,  3.41it/s] 30%|       | 237/780 [01:30<02:39,  3.41it/s] 31%|       | 238/780 [01:31<02:38,  3.41it/s] 31%|       | 239/780 [01:31<02:38,  3.41it/s] 31%|       | 240/780 [01:31<02:38,  3.41it/s] 31%|       | 241/780 [01:32<02:38,  3.41it/s] 31%|       | 242/780 [01:32<02:41,  3.33it/s] 31%|       | 243/780 [01:32<02:39,  3.36it/s] 31%|      | 244/780 [01:32<02:38,  3.37it/s] 31%|      | 245/780 [01:33<02:38,  3.38it/s] 32%|      | 246/780 [01:33<02:37,  3.39it/s] 32%|      | 247/780 [01:33<02:36,  3.40it/s] 32%|      | 248/780 [01:34<02:36,  3.40it/s] 32%|      | 249/780 [01:34<02:35,  3.41it/s] 32%|      | 250/780 [01:34<02:35,  3.41it/s] 32%|      | 251/780 [01:34<02:35,  3.41it/s] 32%|      | 252/780 [01:35<02:34,  3.41it/s] 32%|      | 253/780 [01:35<02:41,  3.26it/s] 33%|      | 254/780 [01:35<02:39,  3.30it/s] 33%|      | 255/780 [01:36<02:37,  3.34it/s] 33%|      | 256/780 [01:36<02:36,  3.36it/s] 33%|      | 257/780 [01:36<02:35,  3.37it/s] 33%|      | 258/780 [01:37<02:34,  3.38it/s] 33%|      | 259/780 [01:37<02:33,  3.39it/s] 33%|      | 260/780 [01:37<02:33,  3.39it/s] 33%|      | 261/780 [01:37<02:32,  3.40it/s] 34%|      | 262/780 [01:38<02:32,  3.40it/s] 34%|      | 263/780 [01:38<02:31,  3.40it/s] 34%|      | 264/780 [01:38<02:36,  3.29it/s] 34%|      | 265/780 [01:39<02:34,  3.33it/s] 34%|      | 266/780 [01:39<02:33,  3.35it/s] 34%|      | 267/780 [01:39<02:32,  3.37it/s] 34%|      | 268/780 [01:40<02:31,  3.37it/s] 34%|      | 269/780 [01:40<02:31,  3.38it/s] 35%|      | 270/780 [01:40<02:30,  3.39it/s] 35%|      | 271/780 [01:40<02:29,  3.40it/s] 35%|      | 272/780 [01:41<02:29,  3.40it/s] 35%|      | 273/780 [01:41<02:28,  3.41it/s] 35%|      | 274/780 [01:41<02:28,  3.40it/s] 35%|      | 275/780 [01:42<02:34,  3.27it/s] 35%|      | 276/780 [01:42<02:32,  3.31it/s] 36%|      | 277/780 [01:42<02:30,  3.34it/s] 36%|      | 278/780 [01:43<02:29,  3.36it/s] 36%|      | 279/780 [01:43<02:28,  3.38it/s] 36%|      | 280/780 [01:43<02:27,  3.39it/s] 36%|      | 281/780 [01:43<02:27,  3.39it/s] 36%|      | 282/780 [01:44<02:26,  3.40it/s] 36%|      | 283/780 [01:44<02:26,  3.40it/s] 36%|      | 284/780 [01:44<02:25,  3.41it/s] 37%|      | 285/780 [01:45<02:25,  3.41it/s] 37%|      | 286/780 [01:45<02:27,  3.35it/s] 37%|      | 287/780 [01:45<02:26,  3.37it/s] 37%|      | 288/780 [01:45<02:25,  3.38it/s] 37%|      | 289/780 [01:46<02:24,  3.39it/s] 37%|      | 290/780 [01:46<02:24,  3.40it/s] 37%|      | 291/780 [01:46<02:23,  3.40it/s] 37%|      | 292/780 [01:47<02:23,  3.40it/s] 38%|      | 293/780 [01:47<02:23,  3.40it/s] 38%|      | 294/780 [01:47<02:22,  3.40it/s] 38%|      | 295/780 [01:48<02:22,  3.40it/s] 38%|      | 296/780 [01:48<02:22,  3.41it/s] 38%|      | 297/780 [01:48<02:26,  3.30it/s] 38%|      | 298/780 [01:48<02:24,  3.34it/s] 38%|      | 299/780 [01:49<02:23,  3.35it/s] 38%|      | 300/780 [01:49<02:22,  3.37it/s] 39%|      | 301/780 [01:49<02:21,  3.38it/s] 39%|      | 302/780 [01:50<02:21,  3.39it/s] 39%|      | 303/780 [01:50<02:20,  3.39it/s] 39%|      | 304/780 [01:50<02:20,  3.40it/s] 39%|      | 305/780 [01:50<02:19,  3.40it/s] 39%|      | 306/780 [01:51<02:19,  3.40it/s] 39%|      | 307/780 [01:51<02:19,  3.40it/s] 39%|      | 308/780 [01:51<02:24,  3.27it/s] 40%|      | 309/780 [01:52<02:22,  3.31it/s] 40%|      | 310/780 [01:52<02:20,  3.33it/s] 40%|      | 311/780 [01:52<02:19,  3.36it/s] 40%|      | 312/780 [01:53<02:18,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 00:44:35,285 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:44:35,285 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:44:35,285 >>   Batch size = 8
{'eval_loss': 0.9271476864814758, 'eval_runtime': 9.7623, 'eval_samples_per_second': 357.394, 'eval_steps_per_second': 44.764, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.13it/s][A
  3%|         | 12/437 [00:00<00:08, 49.02it/s][A
  4%|         | 17/437 [00:00<00:08, 47.35it/s][A
  5%|         | 22/437 [00:00<00:08, 46.49it/s][A
  6%|         | 27/437 [00:00<00:08, 45.96it/s][A
  7%|         | 32/437 [00:00<00:08, 45.68it/s][A
  8%|         | 37/437 [00:00<00:08, 45.40it/s][A
 10%|         | 42/437 [00:00<00:08, 45.13it/s][A
 11%|         | 47/437 [00:01<00:08, 45.16it/s][A
 12%|        | 52/437 [00:01<00:08, 45.21it/s][A
 13%|        | 57/437 [00:01<00:08, 45.32it/s][A
 14%|        | 62/437 [00:01<00:08, 45.22it/s][A
 15%|        | 67/437 [00:01<00:08, 45.34it/s][A
 16%|        | 72/437 [00:01<00:09, 40.16it/s][A
 18%|        | 77/437 [00:01<00:08, 41.70it/s][A
 19%|        | 82/437 [00:01<00:08, 42.74it/s][A
 20%|        | 87/437 [00:01<00:08, 43.51it/s][A
 21%|        | 92/437 [00:02<00:07, 44.08it/s][A
 22%|       | 97/437 [00:02<00:07, 44.46it/s][A
 23%|       | 102/437 [00:02<00:07, 44.73it/s][A
 24%|       | 107/437 [00:02<00:07, 44.91it/s][A
 26%|       | 112/437 [00:02<00:07, 44.62it/s][A
 27%|       | 117/437 [00:02<00:07, 44.75it/s][A
 28%|       | 122/437 [00:02<00:07, 44.81it/s][A
 29%|       | 127/437 [00:02<00:06, 45.01it/s][A
 30%|       | 132/437 [00:02<00:06, 45.17it/s][A
 31%|      | 137/437 [00:03<00:06, 45.26it/s][A
 32%|      | 142/437 [00:03<00:06, 45.18it/s][A
 34%|      | 147/437 [00:03<00:06, 45.24it/s][A
 35%|      | 152/437 [00:03<00:06, 45.16it/s][A
 36%|      | 157/437 [00:03<00:06, 44.97it/s][A
 37%|      | 162/437 [00:03<00:06, 44.91it/s][A
 38%|      | 167/437 [00:03<00:06, 44.72it/s][A
 39%|      | 172/437 [00:03<00:05, 45.00it/s][A
 41%|      | 177/437 [00:03<00:05, 45.08it/s][A
 42%|     | 182/437 [00:04<00:05, 45.12it/s][A
 43%|     | 187/437 [00:04<00:05, 45.32it/s][A
 44%|     | 192/437 [00:04<00:05, 45.25it/s][A
 45%|     | 197/437 [00:04<00:05, 45.25it/s][A
 46%|     | 202/437 [00:04<00:05, 45.21it/s][A
 47%|     | 207/437 [00:04<00:05, 44.66it/s][A
 49%|     | 212/437 [00:04<00:05, 44.71it/s][A
 50%|     | 217/437 [00:04<00:04, 44.83it/s][A
 51%|     | 222/437 [00:04<00:04, 44.97it/s][A
 52%|    | 227/437 [00:05<00:04, 45.14it/s][A
 53%|    | 232/437 [00:05<00:04, 45.20it/s][A
 54%|    | 237/437 [00:05<00:04, 45.19it/s][A
 55%|    | 242/437 [00:05<00:04, 45.15it/s][A
 57%|    | 247/437 [00:05<00:04, 45.00it/s][A
 58%|    | 252/437 [00:05<00:04, 44.99it/s][A
 59%|    | 257/437 [00:05<00:04, 44.85it/s][A
 60%|    | 262/437 [00:05<00:03, 44.95it/s][A
 61%|    | 267/437 [00:05<00:03, 45.00it/s][A
 62%|   | 272/437 [00:06<00:03, 45.21it/s][A
 63%|   | 277/437 [00:06<00:03, 45.22it/s][A
 65%|   | 282/437 [00:06<00:03, 45.23it/s][A
 66%|   | 287/437 [00:06<00:03, 45.04it/s][A
 67%|   | 292/437 [00:06<00:03, 45.07it/s][A
 68%|   | 297/437 [00:06<00:03, 45.08it/s][A
 69%|   | 302/437 [00:06<00:02, 45.01it/s][A
 70%|   | 307/437 [00:06<00:02, 45.03it/s][A
 71%|  | 312/437 [00:06<00:02, 45.12it/s][A
 73%|  | 317/437 [00:07<00:02, 45.23it/s][A
 74%|  | 322/437 [00:07<00:02, 45.30it/s][A
 75%|  | 327/437 [00:07<00:02, 45.25it/s][A
 76%|  | 332/437 [00:07<00:02, 45.10it/s][A
 77%|  | 337/437 [00:07<00:02, 45.07it/s][A
 78%|  | 342/437 [00:07<00:02, 45.00it/s][A
 79%|  | 347/437 [00:07<00:02, 44.96it/s][A
 81%|  | 352/437 [00:07<00:01, 44.97it/s][A
 82%| | 357/437 [00:07<00:01, 45.07it/s][A
 83%| | 362/437 [00:08<00:01, 45.19it/s][A
 84%| | 367/437 [00:08<00:01, 45.22it/s][A
 85%| | 372/437 [00:08<00:01, 45.23it/s][A
 86%| | 377/437 [00:08<00:01, 45.19it/s][A
 87%| | 382/437 [00:08<00:01, 45.06it/s][A
 89%| | 387/437 [00:08<00:01, 44.98it/s][A
 90%| | 392/437 [00:08<00:01, 44.93it/s][A
 91%| | 397/437 [00:08<00:00, 44.98it/s][A
 92%|| 402/437 [00:08<00:00, 45.19it/s][A
 93%|| 407/437 [00:09<00:00, 45.28it/s][A
 94%|| 412/437 [00:09<00:00, 45.23it/s][A
 95%|| 417/437 [00:09<00:00, 45.24it/s][A
 97%|| 422/437 [00:09<00:00, 45.09it/s][A
 98%|| 427/437 [00:09<00:00, 44.95it/s][A
 99%|| 432/437 [00:09<00:00, 42.46it/s][A
100%|| 437/437 [00:09<00:00, 43.39it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 43.39it/s][A 40%|      | 312/780 [02:02<02:18,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:44:45,206 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:44:45,345 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:44:48,280 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:44:48,418 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:44:48,508 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:14<50:53,  6.54s/it] 40%|      | 314/780 [02:14<36:19,  4.68s/it] 40%|      | 315/780 [02:14<26:02,  3.36s/it] 41%|      | 316/780 [02:15<18:52,  2.44s/it] 41%|      | 317/780 [02:15<13:51,  1.80s/it] 41%|      | 318/780 [02:15<10:21,  1.35s/it] 41%|      | 319/780 [02:15<07:54,  1.03s/it] 41%|      | 320/780 [02:16<06:12,  1.24it/s] 41%|      | 321/780 [02:16<05:00,  1.53it/s] 41%|     | 322/780 [02:16<04:09,  1.83it/s] 41%|     | 323/780 [02:17<03:34,  2.13it/s] 42%|     | 324/780 [02:17<03:10,  2.40it/s] 42%|     | 325/780 [02:17<02:56,  2.58it/s] 42%|     | 326/780 [02:18<02:43,  2.78it/s] 42%|     | 327/780 [02:18<02:33,  2.94it/s] 42%|     | 328/780 [02:18<02:27,  3.07it/s] 42%|     | 329/780 [02:18<02:22,  3.17it/s] 42%|     | 330/780 [02:19<02:22,  3.17it/s] 42%|     | 331/780 [02:19<02:18,  3.24it/s] 43%|     | 332/780 [02:19<02:16,  3.29it/s] 43%|     | 333/780 [02:20<02:14,  3.32it/s] 43%|     | 334/780 [02:20<02:13,  3.35it/s] 43%|     | 335/780 [02:20<02:11,  3.37it/s] 43%|     | 336/780 [02:21<02:14,  3.31it/s] 43%|     | 337/780 [02:21<02:50,  2.59it/s] 43%|     | 338/780 [02:21<02:37,  2.80it/s] 43%|     | 339/780 [02:22<02:29,  2.96it/s] 44%|     | 340/780 [02:22<02:22,  3.08it/s] 44%|     | 341/780 [02:22<02:18,  3.18it/s] 44%|     | 342/780 [02:23<02:14,  3.26it/s] 44%|     | 343/780 [02:23<02:11,  3.32it/s] 44%|     | 344/780 [02:23<02:09,  3.36it/s] 44%|     | 345/780 [02:23<02:08,  3.39it/s] 44%|     | 346/780 [02:24<02:10,  3.32it/s] 44%|     | 347/780 [02:24<02:09,  3.35it/s] 45%|     | 348/780 [02:24<02:08,  3.37it/s] 45%|     | 349/780 [02:25<02:07,  3.38it/s] 45%|     | 350/780 [02:25<02:06,  3.40it/s] 45%|     | 351/780 [02:25<02:06,  3.40it/s] 45%|     | 352/780 [02:26<02:05,  3.41it/s] 45%|     | 353/780 [02:26<02:05,  3.41it/s] 45%|     | 354/780 [02:26<02:04,  3.41it/s] 46%|     | 355/780 [02:26<02:04,  3.41it/s] 46%|     | 356/780 [02:27<02:04,  3.41it/s] 46%|     | 357/780 [02:27<02:06,  3.36it/s] 46%|     | 358/780 [02:27<02:05,  3.38it/s] 46%|     | 359/780 [02:28<02:04,  3.39it/s] 46%|     | 360/780 [02:28<02:03,  3.39it/s] 46%|     | 361/780 [02:28<02:03,  3.40it/s] 46%|     | 362/780 [02:28<02:02,  3.41it/s] 47%|     | 363/780 [02:29<02:02,  3.41it/s] 47%|     | 364/780 [02:29<02:01,  3.41it/s] 47%|     | 365/780 [02:29<02:01,  3.42it/s] 47%|     | 366/780 [02:30<02:00,  3.43it/s] 47%|     | 367/780 [02:30<02:00,  3.44it/s] 47%|     | 368/780 [02:30<01:59,  3.45it/s] 47%|     | 369/780 [02:30<01:59,  3.45it/s] 47%|     | 370/780 [02:31<01:58,  3.46it/s] 48%|     | 371/780 [02:31<01:58,  3.46it/s] 48%|     | 372/780 [02:31<01:57,  3.46it/s] 48%|     | 373/780 [02:32<01:57,  3.46it/s] 48%|     | 374/780 [02:32<01:57,  3.46it/s] 48%|     | 375/780 [02:32<01:56,  3.46it/s] 48%|     | 376/780 [02:33<01:56,  3.46it/s] 48%|     | 377/780 [02:33<01:56,  3.47it/s] 48%|     | 378/780 [02:33<02:00,  3.35it/s] 49%|     | 379/780 [02:33<01:58,  3.38it/s] 49%|     | 380/780 [02:34<01:57,  3.41it/s] 49%|     | 381/780 [02:34<01:56,  3.43it/s] 49%|     | 382/780 [02:34<01:55,  3.44it/s] 49%|     | 383/780 [02:35<01:55,  3.45it/s] 49%|     | 384/780 [02:35<01:54,  3.45it/s] 49%|     | 385/780 [02:35<01:54,  3.46it/s] 49%|     | 386/780 [02:35<01:53,  3.46it/s] 50%|     | 387/780 [02:36<01:53,  3.46it/s] 50%|     | 388/780 [02:36<01:53,  3.46it/s] 50%|     | 389/780 [02:36<01:55,  3.39it/s] 50%|     | 390/780 [02:37<01:54,  3.41it/s] 50%|     | 391/780 [02:37<01:53,  3.43it/s] 50%|     | 392/780 [02:37<01:52,  3.44it/s] 50%|     | 393/780 [02:37<01:52,  3.45it/s] 51%|     | 394/780 [02:38<01:51,  3.45it/s] 51%|     | 395/780 [02:38<01:51,  3.46it/s] 51%|     | 396/780 [02:38<01:51,  3.46it/s] 51%|     | 397/780 [02:39<01:50,  3.46it/s] 51%|     | 398/780 [02:39<01:50,  3.46it/s] 51%|     | 399/780 [02:39<01:49,  3.46it/s] 51%|    | 400/780 [02:40<01:53,  3.34it/s] 51%|    | 401/780 [02:40<01:52,  3.38it/s] 52%|    | 402/780 [02:40<01:51,  3.40it/s] 52%|    | 403/780 [02:40<01:50,  3.42it/s] 52%|    | 404/780 [02:41<01:49,  3.43it/s] 52%|    | 405/780 [02:41<01:48,  3.44it/s] 52%|    | 406/780 [02:41<01:48,  3.45it/s] 52%|    | 407/780 [02:42<01:48,  3.45it/s] 52%|    | 408/780 [02:42<01:47,  3.45it/s] 52%|    | 409/780 [02:42<01:47,  3.45it/s] 53%|    | 410/780 [02:42<01:47,  3.45it/s] 53%|    | 411/780 [02:43<01:51,  3.31it/s] 53%|    | 412/780 [02:43<01:49,  3.35it/s] 53%|    | 413/780 [02:43<01:48,  3.38it/s] 53%|    | 414/780 [02:44<01:47,  3.41it/s] 53%|    | 415/780 [02:44<01:46,  3.42it/s] 53%|    | 416/780 [02:44<01:46,  3.43it/s] 53%|    | 417/780 [02:44<01:45,  3.44it/s] 54%|    | 418/780 [02:45<01:45,  3.45it/s] 54%|    | 419/780 [02:45<01:44,  3.45it/s] 54%|    | 420/780 [02:45<01:44,  3.45it/s] 54%|    | 421/780 [02:46<01:43,  3.45it/s] 54%|    | 422/780 [02:46<01:46,  3.37it/s] 54%|    | 423/780 [02:46<01:45,  3.39it/s] 54%|    | 424/780 [02:47<01:44,  3.41it/s] 54%|    | 425/780 [02:47<01:43,  3.43it/s] 55%|    | 426/780 [02:47<01:43,  3.43it/s] 55%|    | 427/780 [02:47<01:42,  3.44it/s] 55%|    | 428/780 [02:48<01:42,  3.45it/s] 55%|    | 429/780 [02:48<01:41,  3.45it/s] 55%|    | 430/780 [02:48<01:41,  3.46it/s] 55%|    | 431/780 [02:49<01:40,  3.46it/s] 55%|    | 432/780 [02:49<01:40,  3.46it/s] 56%|    | 433/780 [02:49<01:46,  3.26it/s] 56%|    | 434/780 [02:49<01:44,  3.32it/s] 56%|    | 435/780 [02:50<01:42,  3.36it/s] 56%|    | 436/780 [02:50<01:41,  3.39it/s] 56%|    | 437/780 [02:50<01:40,  3.41it/s] 56%|    | 438/780 [02:51<01:39,  3.43it/s] 56%|    | 439/780 [02:51<01:39,  3.44it/s] 56%|    | 440/780 [02:51<01:38,  3.44it/s] 57%|    | 441/780 [02:52<01:38,  3.45it/s] 57%|    | 442/780 [02:52<01:37,  3.45it/s] 57%|    | 443/780 [02:52<01:37,  3.46it/s] 57%|    | 444/780 [02:52<01:43,  3.25it/s] 57%|    | 445/780 [02:53<01:41,  3.31it/s] 57%|    | 446/780 [02:53<01:39,  3.36it/s] 57%|    | 447/780 [02:53<01:38,  3.39it/s] 57%|    | 448/780 [02:54<01:37,  3.41it/s] 58%|    | 449/780 [02:54<01:36,  3.42it/s] 58%|    | 450/780 [02:54<01:36,  3.43it/s] 58%|    | 451/780 [02:54<01:35,  3.44it/s] 58%|    | 452/780 [02:55<01:35,  3.44it/s] 58%|    | 453/780 [02:55<01:34,  3.45it/s] 58%|    | 454/780 [02:55<01:34,  3.45it/s] 58%|    | 455/780 [02:56<01:40,  3.24it/s] 58%|    | 456/780 [02:56<01:38,  3.30it/s] 59%|    | 457/780 [02:56<01:36,  3.35it/s] 59%|    | 458/780 [02:57<01:35,  3.38it/s] 59%|    | 459/780 [02:57<01:34,  3.40it/s] 59%|    | 460/780 [02:57<01:33,  3.42it/s] 59%|    | 461/780 [02:57<01:32,  3.43it/s] 59%|    | 462/780 [02:58<01:32,  3.44it/s] 59%|    | 463/780 [02:58<01:31,  3.45it/s] 59%|    | 464/780 [02:58<01:31,  3.45it/s] 60%|    | 465/780 [02:59<01:31,  3.45it/s] 60%|    | 466/780 [02:59<01:33,  3.36it/s] 60%|    | 467/780 [02:59<01:32,  3.39it/s] 60%|    | 468/780 [02:59<01:31,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 00:45:42,144 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:45:42,145 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:45:42,145 >>   Batch size = 8
{'eval_loss': 0.9367855191230774, 'eval_runtime': 9.7733, 'eval_samples_per_second': 356.992, 'eval_steps_per_second': 44.714, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.29it/s][A
  3%|         | 12/437 [00:00<00:08, 49.03it/s][A
  4%|         | 17/437 [00:00<00:08, 47.16it/s][A
  5%|         | 22/437 [00:00<00:08, 46.29it/s][A
  6%|         | 27/437 [00:00<00:08, 45.69it/s][A
  7%|         | 32/437 [00:00<00:08, 45.50it/s][A
  8%|         | 37/437 [00:00<00:08, 45.35it/s][A
 10%|         | 42/437 [00:00<00:08, 45.27it/s][A
 11%|         | 47/437 [00:01<00:08, 45.18it/s][A
 12%|        | 52/437 [00:01<00:08, 43.87it/s][A
 13%|        | 57/437 [00:01<00:08, 44.38it/s][A
 14%|        | 62/437 [00:01<00:08, 44.68it/s][A
 15%|        | 67/437 [00:01<00:08, 44.83it/s][A
 16%|        | 72/437 [00:01<00:08, 44.72it/s][A
 18%|        | 77/437 [00:01<00:08, 44.81it/s][A
 19%|        | 82/437 [00:01<00:07, 44.90it/s][A
 20%|        | 87/437 [00:01<00:07, 44.86it/s][A
 21%|        | 92/437 [00:02<00:07, 44.95it/s][A
 22%|       | 97/437 [00:02<00:07, 45.06it/s][A
 23%|       | 102/437 [00:02<00:07, 45.18it/s][A
 24%|       | 107/437 [00:02<00:07, 45.17it/s][A
 26%|       | 112/437 [00:02<00:07, 45.29it/s][A
 27%|       | 117/437 [00:02<00:07, 45.19it/s][A
 28%|       | 122/437 [00:02<00:06, 45.23it/s][A
 29%|       | 127/437 [00:02<00:06, 45.10it/s][A
 30%|       | 132/437 [00:02<00:06, 44.99it/s][A
 31%|      | 137/437 [00:03<00:06, 45.06it/s][A
 32%|      | 142/437 [00:03<00:06, 45.03it/s][A
 34%|      | 147/437 [00:03<00:06, 45.13it/s][A
 35%|      | 152/437 [00:03<00:06, 45.15it/s][A
 36%|      | 157/437 [00:03<00:06, 45.21it/s][A
 37%|      | 162/437 [00:03<00:06, 45.25it/s][A
 38%|      | 167/437 [00:03<00:05, 45.17it/s][A
 39%|      | 172/437 [00:03<00:05, 45.05it/s][A
 41%|      | 177/437 [00:03<00:05, 44.94it/s][A
 42%|     | 182/437 [00:04<00:05, 44.93it/s][A
 43%|     | 187/437 [00:04<00:05, 43.86it/s][A
 44%|     | 192/437 [00:04<00:05, 44.41it/s][A
 45%|     | 197/437 [00:04<00:05, 44.66it/s][A
 46%|     | 202/437 [00:04<00:05, 44.94it/s][A
 47%|     | 207/437 [00:04<00:05, 44.96it/s][A
 49%|     | 212/437 [00:04<00:05, 44.97it/s][A
 50%|     | 217/437 [00:04<00:04, 44.94it/s][A
 51%|     | 222/437 [00:04<00:04, 44.89it/s][A
 52%|    | 227/437 [00:05<00:04, 44.80it/s][A
 53%|    | 232/437 [00:05<00:04, 44.89it/s][A
 54%|    | 237/437 [00:05<00:04, 45.11it/s][A
 55%|    | 242/437 [00:05<00:04, 45.20it/s][A
 57%|    | 247/437 [00:05<00:04, 45.29it/s][A
 58%|    | 252/437 [00:05<00:04, 45.19it/s][A
 59%|    | 257/437 [00:05<00:03, 45.20it/s][A
 60%|    | 262/437 [00:05<00:03, 45.08it/s][A
 61%|    | 267/437 [00:05<00:03, 45.00it/s][A
 62%|   | 272/437 [00:06<00:03, 45.01it/s][A
 63%|   | 277/437 [00:06<00:03, 44.98it/s][A
 65%|   | 282/437 [00:06<00:03, 45.17it/s][A
 66%|   | 287/437 [00:06<00:03, 45.23it/s][A
 67%|   | 292/437 [00:06<00:03, 45.28it/s][A
 68%|   | 297/437 [00:06<00:03, 45.17it/s][A
 69%|   | 302/437 [00:06<00:02, 45.18it/s][A
 70%|   | 307/437 [00:06<00:02, 45.11it/s][A
 71%|  | 312/437 [00:06<00:02, 44.95it/s][A
 73%|  | 317/437 [00:07<00:02, 44.94it/s][A
 74%|  | 322/437 [00:07<00:02, 43.11it/s][A
 75%|  | 327/437 [00:07<00:02, 43.83it/s][A
 76%|  | 332/437 [00:07<00:02, 44.34it/s][A
 77%|  | 337/437 [00:07<00:02, 44.70it/s][A
 78%|  | 342/437 [00:07<00:02, 44.92it/s][A
 79%|  | 347/437 [00:07<00:01, 45.03it/s][A
 81%|  | 352/437 [00:07<00:01, 45.07it/s][A
 82%| | 357/437 [00:07<00:01, 45.00it/s][A
 83%| | 362/437 [00:08<00:01, 44.75it/s][A
 84%| | 367/437 [00:08<00:01, 44.79it/s][A
 85%| | 372/437 [00:08<00:01, 44.95it/s][A
 86%| | 377/437 [00:08<00:01, 45.18it/s][A
 87%| | 382/437 [00:08<00:01, 45.19it/s][A
 89%| | 387/437 [00:08<00:01, 45.24it/s][A
 90%| | 392/437 [00:08<00:00, 45.11it/s][A
 91%| | 397/437 [00:08<00:00, 45.21it/s][A
 92%|| 402/437 [00:08<00:00, 45.07it/s][A
 93%|| 407/437 [00:09<00:00, 44.89it/s][A
 94%|| 412/437 [00:09<00:00, 44.86it/s][A
 95%|| 417/437 [00:09<00:00, 44.91it/s][A
 97%|| 422/437 [00:09<00:00, 45.12it/s][A
 98%|| 427/437 [00:09<00:00, 45.25it/s][A
 99%|| 432/437 [00:09<00:00, 45.25it/s][A
100%|| 437/437 [00:09<00:00, 45.22it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.22it/s][A 60%|    | 468/780 [03:09<01:31,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:45:52,050 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:45:52,213 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:45:55,225 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:45:55,386 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:45:55,455 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:20<33:18,  6.43s/it] 60%|    | 470/780 [03:21<23:43,  4.59s/it] 60%|    | 471/780 [03:21<17:00,  3.30s/it] 61%|    | 472/780 [03:21<12:38,  2.46s/it] 61%|    | 473/780 [03:22<09:15,  1.81s/it] 61%|    | 474/780 [03:22<06:54,  1.35s/it] 61%|    | 475/780 [03:22<05:15,  1.03s/it] 61%|    | 476/780 [03:22<04:06,  1.23it/s] 61%|    | 477/780 [03:23<03:18,  1.53it/s] 61%|   | 478/780 [03:23<02:44,  1.84it/s] 61%|   | 479/780 [03:23<02:20,  2.14it/s] 62%|   | 480/780 [03:24<02:06,  2.37it/s] 62%|   | 481/780 [03:24<01:54,  2.62it/s] 62%|   | 482/780 [03:24<01:45,  2.82it/s] 62%|   | 483/780 [03:25<01:39,  2.99it/s] 62%|   | 484/780 [03:25<01:34,  3.12it/s] 62%|   | 485/780 [03:25<01:31,  3.21it/s] 62%|   | 486/780 [03:25<01:29,  3.29it/s] 62%|   | 487/780 [03:26<01:27,  3.34it/s] 63%|   | 488/780 [03:26<01:26,  3.37it/s] 63%|   | 489/780 [03:26<01:25,  3.40it/s] 63%|   | 490/780 [03:27<01:24,  3.42it/s] 63%|   | 491/780 [03:27<01:26,  3.34it/s] 63%|   | 492/780 [03:27<01:25,  3.38it/s] 63%|   | 493/780 [03:27<01:24,  3.40it/s] 63%|   | 494/780 [03:28<01:23,  3.42it/s] 63%|   | 495/780 [03:28<01:23,  3.43it/s] 64%|   | 496/780 [03:28<01:22,  3.44it/s] 64%|   | 497/780 [03:29<01:22,  3.45it/s] 64%|   | 498/780 [03:29<01:21,  3.45it/s] 64%|   | 499/780 [03:29<01:21,  3.46it/s] 64%|   | 500/780 [03:29<01:21,  3.45it/s]                                                  64%|   | 500/780 [03:29<01:21,  3.45it/s] 64%|   | 501/780 [03:30<01:20,  3.46it/s] 64%|   | 502/780 [03:30<01:22,  3.37it/s] 64%|   | 503/780 [03:30<01:21,  3.40it/s] 65%|   | 504/780 [03:31<01:20,  3.42it/s] 65%|   | 505/780 [03:31<01:20,  3.43it/s] 65%|   | 506/780 [03:31<01:19,  3.44it/s] 65%|   | 507/780 [03:31<01:19,  3.45it/s] 65%|   | 508/780 [03:32<01:18,  3.45it/s] 65%|   | 509/780 [03:32<01:18,  3.46it/s] 65%|   | 510/780 [03:32<01:18,  3.46it/s] 66%|   | 511/780 [03:33<01:17,  3.46it/s] 66%|   | 512/780 [03:33<01:17,  3.46it/s] 66%|   | 513/780 [03:33<01:17,  3.46it/s] 66%|   | 514/780 [03:34<01:16,  3.46it/s] 66%|   | 515/780 [03:34<01:16,  3.46it/s] 66%|   | 516/780 [03:34<01:16,  3.46it/s] 66%|   | 517/780 [03:34<01:15,  3.46it/s] 66%|   | 518/780 [03:35<01:17,  3.38it/s] 67%|   | 519/780 [03:35<01:16,  3.41it/s] 67%|   | 520/780 [03:35<01:15,  3.42it/s] 67%|   | 521/780 [03:36<01:15,  3.44it/s] 67%|   | 522/780 [03:36<01:14,  3.44it/s] 67%|   | 523/780 [03:36<01:14,  3.45it/s] 67%|   | 524/780 [03:36<01:14,  3.45it/s] 67%|   | 525/780 [03:37<01:13,  3.46it/s] 67%|   | 526/780 [03:37<01:13,  3.46it/s] 68%|   | 527/780 [03:37<01:13,  3.46it/s] 68%|   | 528/780 [03:38<01:12,  3.46it/s] 68%|   | 529/780 [03:38<01:14,  3.37it/s] 68%|   | 530/780 [03:38<01:13,  3.39it/s] 68%|   | 531/780 [03:38<01:12,  3.42it/s] 68%|   | 532/780 [03:39<01:12,  3.43it/s] 68%|   | 533/780 [03:39<01:11,  3.44it/s] 68%|   | 534/780 [03:39<01:11,  3.45it/s] 69%|   | 535/780 [03:40<01:10,  3.45it/s] 69%|   | 536/780 [03:40<01:10,  3.46it/s] 69%|   | 537/780 [03:40<01:10,  3.46it/s] 69%|   | 538/780 [03:40<01:09,  3.46it/s] 69%|   | 539/780 [03:41<01:09,  3.46it/s] 69%|   | 540/780 [03:41<01:10,  3.39it/s] 69%|   | 541/780 [03:41<01:10,  3.41it/s] 69%|   | 542/780 [03:42<01:09,  3.43it/s] 70%|   | 543/780 [03:42<01:08,  3.44it/s] 70%|   | 544/780 [03:42<01:08,  3.45it/s] 70%|   | 545/780 [03:43<01:08,  3.45it/s] 70%|   | 546/780 [03:43<01:07,  3.45it/s] 70%|   | 547/780 [03:43<01:07,  3.46it/s] 70%|   | 548/780 [03:43<01:07,  3.46it/s] 70%|   | 549/780 [03:44<01:06,  3.46it/s] 71%|   | 550/780 [03:44<01:06,  3.46it/s] 71%|   | 551/780 [03:44<01:07,  3.40it/s] 71%|   | 552/780 [03:45<01:06,  3.41it/s] 71%|   | 553/780 [03:45<01:06,  3.43it/s] 71%|   | 554/780 [03:45<01:05,  3.44it/s] 71%|   | 555/780 [03:45<01:05,  3.44it/s] 71%|  | 556/780 [03:46<01:04,  3.45it/s] 71%|  | 557/780 [03:46<01:04,  3.45it/s] 72%|  | 558/780 [03:46<01:04,  3.46it/s] 72%|  | 559/780 [03:47<01:03,  3.46it/s] 72%|  | 560/780 [03:47<01:03,  3.46it/s] 72%|  | 561/780 [03:47<01:03,  3.46it/s] 72%|  | 562/780 [03:47<01:04,  3.40it/s] 72%|  | 563/780 [03:48<01:03,  3.42it/s] 72%|  | 564/780 [03:48<01:02,  3.43it/s] 72%|  | 565/780 [03:48<01:02,  3.44it/s] 73%|  | 566/780 [03:49<01:02,  3.45it/s] 73%|  | 567/780 [03:49<01:01,  3.45it/s] 73%|  | 568/780 [03:49<01:01,  3.46it/s] 73%|  | 569/780 [03:49<01:01,  3.46it/s] 73%|  | 570/780 [03:50<01:00,  3.46it/s] 73%|  | 571/780 [03:50<01:00,  3.46it/s] 73%|  | 572/780 [03:50<01:00,  3.46it/s] 73%|  | 573/780 [03:51<01:04,  3.23it/s] 74%|  | 574/780 [03:51<01:02,  3.30it/s] 74%|  | 575/780 [03:51<01:01,  3.35it/s] 74%|  | 576/780 [03:52<01:00,  3.38it/s] 74%|  | 577/780 [03:52<00:59,  3.41it/s] 74%|  | 578/780 [03:52<00:58,  3.42it/s] 74%|  | 579/780 [03:52<00:58,  3.43it/s] 74%|  | 580/780 [03:53<00:58,  3.45it/s] 74%|  | 581/780 [03:53<00:57,  3.45it/s] 75%|  | 582/780 [03:53<00:57,  3.45it/s] 75%|  | 583/780 [03:54<00:56,  3.46it/s] 75%|  | 584/780 [03:54<01:00,  3.25it/s] 75%|  | 585/780 [03:54<00:58,  3.31it/s] 75%|  | 586/780 [03:55<00:57,  3.35it/s] 75%|  | 587/780 [03:55<00:57,  3.38it/s] 75%|  | 588/780 [03:55<00:56,  3.40it/s] 76%|  | 589/780 [03:55<00:55,  3.42it/s] 76%|  | 590/780 [03:56<00:55,  3.43it/s] 76%|  | 591/780 [03:56<00:55,  3.43it/s] 76%|  | 592/780 [03:56<00:54,  3.44it/s] 76%|  | 593/780 [03:57<00:54,  3.44it/s] 76%|  | 594/780 [03:57<00:53,  3.44it/s] 76%|  | 595/780 [03:57<00:56,  3.26it/s] 76%|  | 596/780 [03:57<00:55,  3.31it/s] 77%|  | 597/780 [03:58<00:54,  3.35it/s] 77%|  | 598/780 [03:58<00:53,  3.38it/s] 77%|  | 599/780 [03:58<00:53,  3.41it/s] 77%|  | 600/780 [03:59<00:52,  3.42it/s] 77%|  | 601/780 [03:59<00:52,  3.43it/s] 77%|  | 602/780 [03:59<00:51,  3.44it/s] 77%|  | 603/780 [04:00<00:51,  3.44it/s] 77%|  | 604/780 [04:00<00:51,  3.45it/s] 78%|  | 605/780 [04:00<00:50,  3.45it/s] 78%|  | 606/780 [04:00<00:55,  3.15it/s] 78%|  | 607/780 [04:01<00:53,  3.23it/s] 78%|  | 608/780 [04:01<00:52,  3.30it/s] 78%|  | 609/780 [04:01<00:51,  3.34it/s] 78%|  | 610/780 [04:02<00:50,  3.38it/s] 78%|  | 611/780 [04:02<00:49,  3.40it/s] 78%|  | 612/780 [04:02<00:49,  3.42it/s] 79%|  | 613/780 [04:03<00:48,  3.43it/s] 79%|  | 614/780 [04:03<00:48,  3.44it/s] 79%|  | 615/780 [04:03<00:47,  3.44it/s] 79%|  | 616/780 [04:03<00:47,  3.45it/s] 79%|  | 617/780 [04:04<00:47,  3.45it/s] 79%|  | 618/780 [04:04<00:46,  3.45it/s] 79%|  | 619/780 [04:04<00:46,  3.45it/s] 79%|  | 620/780 [04:05<00:46,  3.46it/s] 80%|  | 621/780 [04:05<00:46,  3.46it/s] 80%|  | 622/780 [04:05<00:45,  3.46it/s] 80%|  | 623/780 [04:05<00:47,  3.28it/s] 80%|  | 624/780 [04:06<00:46,  3.33it/s][INFO|trainer.py:2140] 2023-08-29 00:46:48,422 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:46:48,422 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:46:48,422 >>   Batch size = 8
{'eval_loss': 0.9467291235923767, 'eval_runtime': 9.7453, 'eval_samples_per_second': 358.018, 'eval_steps_per_second': 44.842, 'epoch': 3.0}
{'loss': 0.7227, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.76it/s][A
  3%|         | 12/437 [00:00<00:08, 49.31it/s][A
  4%|         | 17/437 [00:00<00:08, 47.37it/s][A
  5%|         | 22/437 [00:00<00:08, 46.39it/s][A
  6%|         | 27/437 [00:00<00:08, 45.90it/s][A
  7%|         | 32/437 [00:00<00:08, 45.62it/s][A
  8%|         | 37/437 [00:00<00:08, 45.49it/s][A
 10%|         | 42/437 [00:00<00:08, 45.11it/s][A
 11%|         | 47/437 [00:01<00:08, 45.19it/s][A
 12%|        | 52/437 [00:01<00:08, 45.25it/s][A
 13%|        | 57/437 [00:01<00:08, 45.36it/s][A
 14%|        | 62/437 [00:01<00:08, 45.33it/s][A
 15%|        | 67/437 [00:01<00:08, 45.21it/s][A
 16%|        | 72/437 [00:01<00:08, 45.15it/s][A
 18%|        | 77/437 [00:01<00:07, 45.01it/s][A
 19%|        | 82/437 [00:01<00:07, 45.08it/s][A
 20%|        | 87/437 [00:01<00:07, 44.92it/s][A
 21%|        | 92/437 [00:02<00:07, 45.02it/s][A
 22%|       | 97/437 [00:02<00:07, 45.12it/s][A
 23%|       | 102/437 [00:02<00:07, 45.20it/s][A
 24%|       | 107/437 [00:02<00:07, 45.33it/s][A
 26%|       | 112/437 [00:02<00:07, 44.41it/s][A
 27%|       | 117/437 [00:02<00:07, 44.63it/s][A
 28%|       | 122/437 [00:02<00:07, 44.76it/s][A
 29%|       | 127/437 [00:02<00:06, 44.85it/s][A
 30%|       | 132/437 [00:02<00:06, 44.88it/s][A
 31%|      | 137/437 [00:03<00:06, 44.89it/s][A
 32%|      | 142/437 [00:03<00:06, 45.05it/s][A
 34%|      | 147/437 [00:03<00:06, 45.10it/s][A
 35%|      | 152/437 [00:03<00:06, 45.07it/s][A
 36%|      | 157/437 [00:03<00:06, 45.06it/s][A
 37%|      | 162/437 [00:03<00:06, 45.07it/s][A
 38%|      | 167/437 [00:03<00:05, 45.05it/s][A
 39%|      | 172/437 [00:03<00:05, 45.11it/s][A
 41%|      | 177/437 [00:03<00:05, 45.05it/s][A
 42%|     | 182/437 [00:04<00:05, 45.07it/s][A
 43%|     | 187/437 [00:04<00:05, 45.20it/s][A
 44%|     | 192/437 [00:04<00:05, 45.11it/s][A
 45%|     | 197/437 [00:04<00:05, 45.04it/s][A
 46%|     | 202/437 [00:04<00:05, 45.04it/s][A
 47%|     | 207/437 [00:04<00:05, 45.07it/s][A
 49%|     | 212/437 [00:04<00:04, 45.00it/s][A
 50%|     | 217/437 [00:04<00:04, 44.97it/s][A
 51%|     | 222/437 [00:04<00:04, 45.08it/s][A
 52%|    | 227/437 [00:05<00:04, 45.11it/s][A
 53%|    | 232/437 [00:05<00:04, 45.15it/s][A
 54%|    | 237/437 [00:05<00:04, 45.12it/s][A
 55%|    | 242/437 [00:05<00:04, 45.11it/s][A
 57%|    | 247/437 [00:05<00:04, 42.43it/s][A
 58%|    | 252/437 [00:05<00:04, 43.36it/s][A
 59%|    | 257/437 [00:05<00:04, 43.64it/s][A
 60%|    | 262/437 [00:05<00:03, 44.22it/s][A
 61%|    | 267/437 [00:05<00:03, 44.61it/s][A
 62%|   | 272/437 [00:06<00:03, 44.69it/s][A
 63%|   | 277/437 [00:06<00:03, 44.91it/s][A
 65%|   | 282/437 [00:06<00:03, 44.93it/s][A
 66%|   | 287/437 [00:06<00:03, 44.65it/s][A
 67%|   | 292/437 [00:06<00:03, 44.77it/s][A
 68%|   | 297/437 [00:06<00:03, 44.97it/s][A
 69%|   | 302/437 [00:06<00:02, 45.11it/s][A
 70%|   | 307/437 [00:06<00:02, 45.14it/s][A
 71%|  | 312/437 [00:06<00:02, 45.14it/s][A
 73%|  | 317/437 [00:07<00:02, 45.17it/s][A
 74%|  | 322/437 [00:07<00:02, 45.10it/s][A
 75%|  | 327/437 [00:07<00:02, 45.04it/s][A
 76%|  | 332/437 [00:07<00:02, 44.90it/s][A
 77%|  | 337/437 [00:07<00:02, 44.86it/s][A
 78%|  | 342/437 [00:07<00:02, 45.01it/s][A
 79%|  | 347/437 [00:07<00:01, 45.06it/s][A
 81%|  | 352/437 [00:07<00:01, 45.13it/s][A
 82%| | 357/437 [00:07<00:01, 45.22it/s][A
 83%| | 362/437 [00:08<00:01, 45.20it/s][A
 84%| | 367/437 [00:08<00:01, 45.19it/s][A
 85%| | 372/437 [00:08<00:01, 45.14it/s][A
 86%| | 377/437 [00:08<00:01, 44.90it/s][A
 87%| | 382/437 [00:08<00:01, 43.26it/s][A
 89%| | 387/437 [00:08<00:01, 43.95it/s][A
 90%| | 392/437 [00:08<00:01, 44.33it/s][A
 91%| | 397/437 [00:08<00:00, 44.53it/s][A
 92%|| 402/437 [00:08<00:00, 44.88it/s][A
 93%|| 407/437 [00:09<00:00, 45.07it/s][A
 94%|| 412/437 [00:09<00:00, 45.14it/s][A
 95%|| 417/437 [00:09<00:00, 45.05it/s][A
 97%|| 422/437 [00:09<00:00, 44.83it/s][A
 98%|| 427/437 [00:09<00:00, 44.88it/s][A
 99%|| 432/437 [00:09<00:00, 44.94it/s][A
100%|| 437/437 [00:09<00:00, 45.12it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.12it/s][A 80%|  | 624/780 [04:16<00:46,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:46:58,453 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 00:46:58,781 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:47:02,083 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:47:02,282 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:47:02,363 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:27<16:47,  6.50s/it] 80%|  | 626/780 [04:27<11:54,  4.64s/it] 80%|  | 627/780 [04:27<08:30,  3.34s/it] 81%|  | 628/780 [04:28<06:08,  2.42s/it] 81%|  | 629/780 [04:28<04:29,  1.78s/it] 81%|  | 630/780 [04:28<03:20,  1.34s/it] 81%|  | 631/780 [04:28<02:32,  1.02s/it] 81%|  | 632/780 [04:29<01:58,  1.25it/s] 81%|  | 633/780 [04:29<01:35,  1.54it/s] 81%| | 634/780 [04:29<01:18,  1.85it/s] 81%| | 635/780 [04:30<01:07,  2.15it/s] 82%| | 636/780 [04:30<00:59,  2.42it/s] 82%| | 637/780 [04:30<00:54,  2.62it/s] 82%| | 638/780 [04:31<00:50,  2.82it/s] 82%| | 639/780 [04:31<00:47,  2.99it/s] 82%| | 640/780 [04:31<00:44,  3.12it/s] 82%| | 641/780 [04:31<00:43,  3.21it/s] 82%| | 642/780 [04:32<00:42,  3.29it/s] 82%| | 643/780 [04:32<00:41,  3.34it/s] 83%| | 644/780 [04:32<00:40,  3.37it/s] 83%| | 645/780 [04:33<00:39,  3.39it/s] 83%| | 646/780 [04:33<00:39,  3.39it/s] 83%| | 647/780 [04:33<00:39,  3.40it/s] 83%| | 648/780 [04:33<00:38,  3.40it/s] 83%| | 649/780 [04:34<00:38,  3.41it/s] 83%| | 650/780 [04:34<00:38,  3.41it/s] 83%| | 651/780 [04:34<00:37,  3.41it/s] 84%| | 652/780 [04:35<00:37,  3.41it/s] 84%| | 653/780 [04:35<00:37,  3.41it/s] 84%| | 654/780 [04:35<00:36,  3.41it/s] 84%| | 655/780 [04:35<00:36,  3.41it/s] 84%| | 656/780 [04:36<00:36,  3.41it/s] 84%| | 657/780 [04:36<00:36,  3.35it/s] 84%| | 658/780 [04:36<00:36,  3.37it/s] 84%| | 659/780 [04:37<00:35,  3.38it/s] 85%| | 660/780 [04:37<00:35,  3.39it/s] 85%| | 661/780 [04:37<00:35,  3.40it/s] 85%| | 662/780 [04:38<00:34,  3.40it/s] 85%| | 663/780 [04:38<00:34,  3.41it/s] 85%| | 664/780 [04:38<00:34,  3.41it/s] 85%| | 665/780 [04:38<00:33,  3.41it/s] 85%| | 666/780 [04:39<00:33,  3.41it/s] 86%| | 667/780 [04:39<00:33,  3.41it/s] 86%| | 668/780 [04:39<00:33,  3.34it/s] 86%| | 669/780 [04:40<00:33,  3.36it/s] 86%| | 670/780 [04:40<00:32,  3.38it/s] 86%| | 671/780 [04:40<00:32,  3.38it/s] 86%| | 672/780 [04:40<00:31,  3.40it/s] 86%| | 673/780 [04:41<00:31,  3.42it/s] 86%| | 674/780 [04:41<00:30,  3.43it/s] 87%| | 675/780 [04:41<00:30,  3.44it/s] 87%| | 676/780 [04:42<00:30,  3.45it/s] 87%| | 677/780 [04:42<00:29,  3.45it/s] 87%| | 678/780 [04:42<00:29,  3.46it/s] 87%| | 679/780 [04:43<00:30,  3.35it/s] 87%| | 680/780 [04:43<00:29,  3.38it/s] 87%| | 681/780 [04:43<00:29,  3.40it/s] 87%| | 682/780 [04:43<00:28,  3.42it/s] 88%| | 683/780 [04:44<00:28,  3.43it/s] 88%| | 684/780 [04:44<00:27,  3.44it/s] 88%| | 685/780 [04:44<00:27,  3.44it/s] 88%| | 686/780 [04:45<00:27,  3.45it/s] 88%| | 687/780 [04:45<00:26,  3.45it/s] 88%| | 688/780 [04:45<00:26,  3.46it/s] 88%| | 689/780 [04:45<00:26,  3.46it/s] 88%| | 690/780 [04:46<00:26,  3.35it/s] 89%| | 691/780 [04:46<00:26,  3.38it/s] 89%| | 692/780 [04:46<00:25,  3.41it/s] 89%| | 693/780 [04:47<00:25,  3.42it/s] 89%| | 694/780 [04:47<00:25,  3.43it/s] 89%| | 695/780 [04:47<00:24,  3.44it/s] 89%| | 696/780 [04:47<00:24,  3.44it/s] 89%| | 697/780 [04:48<00:24,  3.45it/s] 89%| | 698/780 [04:48<00:23,  3.46it/s] 90%| | 699/780 [04:48<00:23,  3.45it/s] 90%| | 700/780 [04:49<00:23,  3.46it/s] 90%| | 701/780 [04:49<00:23,  3.37it/s] 90%| | 702/780 [04:49<00:22,  3.40it/s] 90%| | 703/780 [04:50<00:22,  3.41it/s] 90%| | 704/780 [04:50<00:22,  3.43it/s] 90%| | 705/780 [04:50<00:21,  3.43it/s] 91%| | 706/780 [04:50<00:21,  3.44it/s] 91%| | 707/780 [04:51<00:21,  3.45it/s] 91%| | 708/780 [04:51<00:20,  3.45it/s] 91%| | 709/780 [04:51<00:20,  3.45it/s] 91%| | 710/780 [04:52<00:20,  3.46it/s] 91%| | 711/780 [04:52<00:19,  3.46it/s] 91%|| 712/780 [04:52<00:20,  3.25it/s] 91%|| 713/780 [04:52<00:20,  3.31it/s] 92%|| 714/780 [04:53<00:19,  3.35it/s] 92%|| 715/780 [04:53<00:19,  3.39it/s] 92%|| 716/780 [04:53<00:18,  3.41it/s] 92%|| 717/780 [04:54<00:18,  3.42it/s] 92%|| 718/780 [04:54<00:18,  3.43it/s] 92%|| 719/780 [04:54<00:17,  3.45it/s] 92%|| 720/780 [04:55<00:17,  3.45it/s] 92%|| 721/780 [04:55<00:17,  3.45it/s] 93%|| 722/780 [04:55<00:16,  3.46it/s] 93%|| 723/780 [04:55<00:17,  3.23it/s] 93%|| 724/780 [04:56<00:16,  3.30it/s] 93%|| 725/780 [04:56<00:16,  3.35it/s] 93%|| 726/780 [04:56<00:15,  3.38it/s] 93%|| 727/780 [04:57<00:15,  3.41it/s] 93%|| 728/780 [04:57<00:15,  3.42it/s] 93%|| 729/780 [04:57<00:14,  3.44it/s] 94%|| 730/780 [04:57<00:14,  3.45it/s] 94%|| 731/780 [04:58<00:14,  3.45it/s] 94%|| 732/780 [04:58<00:13,  3.46it/s] 94%|| 733/780 [04:58<00:13,  3.46it/s] 94%|| 734/780 [04:59<00:13,  3.31it/s] 94%|| 735/780 [04:59<00:13,  3.35it/s] 94%|| 736/780 [04:59<00:13,  3.38it/s] 94%|| 737/780 [05:00<00:12,  3.40it/s] 95%|| 738/780 [05:00<00:12,  3.42it/s] 95%|| 739/780 [05:00<00:11,  3.44it/s] 95%|| 740/780 [05:00<00:11,  3.44it/s] 95%|| 741/780 [05:01<00:11,  3.45it/s] 95%|| 742/780 [05:01<00:10,  3.45it/s] 95%|| 743/780 [05:01<00:10,  3.46it/s] 95%|| 744/780 [05:02<00:10,  3.46it/s] 96%|| 745/780 [05:02<00:10,  3.29it/s] 96%|| 746/780 [05:02<00:10,  3.34it/s] 96%|| 747/780 [05:02<00:09,  3.37it/s] 96%|| 748/780 [05:03<00:09,  3.40it/s] 96%|| 749/780 [05:03<00:09,  3.42it/s] 96%|| 750/780 [05:03<00:08,  3.43it/s] 96%|| 751/780 [05:04<00:08,  3.44it/s] 96%|| 752/780 [05:04<00:08,  3.44it/s] 97%|| 753/780 [05:04<00:07,  3.45it/s] 97%|| 754/780 [05:04<00:07,  3.45it/s] 97%|| 755/780 [05:05<00:07,  3.45it/s] 97%|| 756/780 [05:05<00:06,  3.45it/s] 97%|| 757/780 [05:05<00:06,  3.45it/s] 97%|| 758/780 [05:06<00:06,  3.45it/s] 97%|| 759/780 [05:06<00:06,  3.45it/s] 97%|| 760/780 [05:06<00:05,  3.45it/s] 98%|| 761/780 [05:07<00:05,  3.45it/s] 98%|| 762/780 [05:07<00:05,  3.36it/s] 98%|| 763/780 [05:07<00:05,  3.39it/s] 98%|| 764/780 [05:07<00:04,  3.41it/s] 98%|| 765/780 [05:08<00:04,  3.42it/s] 98%|| 766/780 [05:08<00:04,  3.43it/s] 98%|| 767/780 [05:08<00:03,  3.44it/s] 98%|| 768/780 [05:09<00:03,  3.45it/s] 99%|| 769/780 [05:09<00:03,  3.45it/s] 99%|| 770/780 [05:09<00:02,  3.45it/s] 99%|| 771/780 [05:09<00:02,  3.46it/s] 99%|| 772/780 [05:10<00:02,  3.46it/s] 99%|| 773/780 [05:10<00:02,  3.31it/s] 99%|| 774/780 [05:10<00:01,  3.35it/s] 99%|| 775/780 [05:11<00:01,  3.38it/s] 99%|| 776/780 [05:11<00:01,  3.40it/s]100%|| 777/780 [05:11<00:00,  3.42it/s]100%|| 778/780 [05:12<00:00,  3.43it/s]100%|| 779/780 [05:12<00:00,  3.44it/s]100%|| 780/780 [05:12<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:47:54,737 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:47:54,737 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:47:54,737 >>   Batch size = 8
{'eval_loss': 0.9525221586227417, 'eval_runtime': 9.754, 'eval_samples_per_second': 357.698, 'eval_steps_per_second': 44.802, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.17it/s][A
  3%|         | 12/437 [00:00<00:08, 49.28it/s][A
  4%|         | 17/437 [00:00<00:08, 47.53it/s][A
  5%|         | 22/437 [00:00<00:08, 46.50it/s][A
  6%|         | 27/437 [00:00<00:08, 46.08it/s][A
  7%|         | 32/437 [00:00<00:08, 45.59it/s][A
  8%|         | 37/437 [00:00<00:09, 43.56it/s][A
 10%|         | 42/437 [00:00<00:08, 44.10it/s][A
 11%|         | 47/437 [00:01<00:08, 44.50it/s][A
 12%|        | 52/437 [00:01<00:08, 44.75it/s][A
 13%|        | 57/437 [00:01<00:08, 44.98it/s][A
 14%|        | 62/437 [00:01<00:08, 45.11it/s][A
 15%|        | 67/437 [00:01<00:08, 45.24it/s][A
 16%|        | 72/437 [00:01<00:08, 45.18it/s][A
 18%|        | 77/437 [00:01<00:08, 44.91it/s][A
 19%|        | 82/437 [00:01<00:07, 44.97it/s][A
 20%|        | 87/437 [00:01<00:07, 45.08it/s][A
 21%|        | 92/437 [00:02<00:07, 45.10it/s][A
 22%|       | 97/437 [00:02<00:07, 45.23it/s][A
 23%|       | 102/437 [00:02<00:07, 45.21it/s][A
 24%|       | 107/437 [00:02<00:07, 45.27it/s][A
 26%|       | 112/437 [00:02<00:07, 45.31it/s][A
 27%|       | 117/437 [00:02<00:07, 45.06it/s][A
 28%|       | 122/437 [00:02<00:07, 44.98it/s][A
 29%|       | 127/437 [00:02<00:06, 44.93it/s][A
 30%|       | 132/437 [00:02<00:06, 45.03it/s][A
 31%|      | 137/437 [00:03<00:06, 45.12it/s][A
 32%|      | 142/437 [00:03<00:06, 45.20it/s][A
 34%|      | 147/437 [00:03<00:06, 45.25it/s][A
 35%|      | 152/437 [00:03<00:06, 45.21it/s][A
 36%|      | 157/437 [00:03<00:06, 45.22it/s][A
 37%|      | 162/437 [00:03<00:06, 45.11it/s][A
 38%|      | 167/437 [00:03<00:05, 45.03it/s][A
 39%|      | 172/437 [00:03<00:06, 41.52it/s][A
 41%|      | 177/437 [00:03<00:06, 42.64it/s][A
 42%|     | 182/437 [00:04<00:05, 43.55it/s][A
 43%|     | 187/437 [00:04<00:05, 44.22it/s][A
 44%|     | 192/437 [00:04<00:05, 44.60it/s][A
 45%|     | 197/437 [00:04<00:05, 44.77it/s][A
 46%|     | 202/437 [00:04<00:05, 44.86it/s][A
 47%|     | 207/437 [00:04<00:05, 44.85it/s][A
 49%|     | 212/437 [00:04<00:05, 44.63it/s][A
 50%|     | 217/437 [00:04<00:04, 44.64it/s][A
 51%|     | 222/437 [00:04<00:04, 44.77it/s][A
 52%|    | 227/437 [00:05<00:04, 45.05it/s][A
 53%|    | 232/437 [00:05<00:04, 45.17it/s][A
 54%|    | 237/437 [00:05<00:04, 45.35it/s][A
 55%|    | 242/437 [00:05<00:04, 45.39it/s][A
 57%|    | 247/437 [00:05<00:04, 45.32it/s][A
 58%|    | 252/437 [00:05<00:04, 45.08it/s][A
 59%|    | 257/437 [00:05<00:04, 44.87it/s][A
 60%|    | 262/437 [00:05<00:03, 44.82it/s][A
 61%|    | 267/437 [00:05<00:03, 44.86it/s][A
 62%|   | 272/437 [00:06<00:03, 45.13it/s][A
 63%|   | 277/437 [00:06<00:03, 45.23it/s][A
 65%|   | 282/437 [00:06<00:03, 45.39it/s][A
 66%|   | 287/437 [00:06<00:03, 45.37it/s][A
 67%|   | 292/437 [00:06<00:03, 45.29it/s][A
 68%|   | 297/437 [00:06<00:03, 45.04it/s][A
 69%|   | 302/437 [00:06<00:03, 44.80it/s][A
 70%|   | 307/437 [00:07<00:05, 25.97it/s][A
 72%|  | 313/437 [00:07<00:03, 31.06it/s][A
 73%|  | 318/437 [00:07<00:03, 34.18it/s][A
 74%|  | 323/437 [00:07<00:03, 36.92it/s][A
 75%|  | 328/437 [00:07<00:02, 39.10it/s][A
 76%|  | 333/437 [00:07<00:02, 40.77it/s][A
 77%|  | 338/437 [00:07<00:02, 42.05it/s][A
 78%|  | 343/437 [00:07<00:02, 43.01it/s][A
 80%|  | 348/437 [00:07<00:02, 43.64it/s][A
 81%|  | 353/437 [00:08<00:01, 43.75it/s][A
 82%| | 358/437 [00:08<00:01, 44.06it/s][A
 83%| | 363/437 [00:08<00:01, 44.38it/s][A
 84%| | 368/437 [00:08<00:01, 44.75it/s][A
 85%| | 373/437 [00:08<00:01, 44.97it/s][A
 86%| | 378/437 [00:08<00:01, 45.09it/s][A
 88%| | 383/437 [00:08<00:01, 45.10it/s][A
 89%| | 388/437 [00:08<00:01, 45.29it/s][A
 90%| | 393/437 [00:09<00:01, 40.60it/s][A
 91%| | 398/437 [00:09<00:00, 42.08it/s][A
 92%|| 403/437 [00:09<00:01, 17.58it/s][A
 93%|| 407/437 [00:09<00:01, 19.66it/s][A
 94%|| 412/437 [00:10<00:01, 23.92it/s][A
 95%|| 417/437 [00:10<00:00, 28.06it/s][A
 97%|| 422/437 [00:10<00:00, 31.86it/s][A
 98%|| 427/437 [00:10<00:00, 35.10it/s][A
 99%|| 432/437 [00:10<00:00, 37.74it/s][A
100%|| 437/437 [00:10<00:00, 39.90it/s][A
                                                 [A                                                 
100%|| 437/437 [00:10<00:00, 39.90it/s][A100%|| 780/780 [05:23<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:48:05,579 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 00:48:05,824 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:48:10,407 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:48:10,696 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:48:10,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:48:17,414 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:48:17,414 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156 (score: 0.9271476864814758).
                                                 100%|| 780/780 [05:44<00:00,  3.45it/s]100%|| 780/780 [05:44<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-29 00:48:26,476 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:48:26,633 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:48:29,643 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:48:29,811 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:48:29,895 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:48:30,348 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   train_loss               =     0.7114
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   train_runtime            = 0:05:44.29
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   train_samples_per_second =    145.226
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:30,348 >>   train_steps_per_second   =      2.266
{'eval_loss': 0.9545684456825256, 'eval_runtime': 10.5993, 'eval_samples_per_second': 329.173, 'eval_steps_per_second': 41.229, 'epoch': 5.0}
{'train_runtime': 344.2901, 'train_samples_per_second': 145.226, 'train_steps_per_second': 2.266, 'train_loss': 0.7113636505909455, 'epoch': 5.0}
08/29/2023 00:48:30 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:48:30,531 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:48:30,531 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 00:48:30,531 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 55.91it/s]  3%|         | 12/437 [00:00<00:08, 49.50it/s]  4%|         | 17/437 [00:00<00:08, 48.16it/s]  5%|         | 22/437 [00:00<00:08, 47.40it/s]  6%|         | 27/437 [00:00<00:08, 46.94it/s]  7%|         | 32/437 [00:00<00:08, 46.67it/s]  8%|         | 37/437 [00:00<00:08, 46.42it/s] 10%|         | 42/437 [00:00<00:08, 46.17it/s] 11%|         | 47/437 [00:01<00:08, 45.47it/s] 12%|        | 52/437 [00:01<00:08, 45.28it/s] 13%|        | 57/437 [00:01<00:08, 45.26it/s] 14%|        | 62/437 [00:01<00:08, 45.53it/s] 15%|        | 67/437 [00:01<00:08, 45.65it/s] 16%|        | 72/437 [00:01<00:07, 45.67it/s] 18%|        | 77/437 [00:01<00:07, 45.76it/s] 19%|        | 82/437 [00:01<00:07, 45.80it/s] 20%|        | 87/437 [00:01<00:07, 45.65it/s] 21%|        | 92/437 [00:01<00:07, 45.32it/s] 22%|       | 97/437 [00:02<00:07, 45.14it/s] 23%|       | 102/437 [00:02<00:07, 44.41it/s] 24%|       | 107/437 [00:02<00:07, 44.89it/s] 26%|       | 112/437 [00:02<00:07, 45.20it/s] 27%|       | 117/437 [00:02<00:07, 45.39it/s] 28%|       | 122/437 [00:02<00:06, 45.54it/s] 29%|       | 127/437 [00:02<00:06, 45.57it/s] 30%|       | 132/437 [00:02<00:06, 45.58it/s] 31%|      | 137/437 [00:02<00:06, 45.45it/s] 32%|      | 142/437 [00:03<00:06, 45.25it/s] 34%|      | 147/437 [00:03<00:06, 45.20it/s] 35%|      | 152/437 [00:03<00:06, 45.26it/s] 36%|      | 157/437 [00:03<00:06, 45.42it/s] 37%|      | 162/437 [00:03<00:06, 45.61it/s] 38%|      | 167/437 [00:03<00:05, 45.65it/s] 39%|      | 172/437 [00:03<00:05, 45.54it/s] 41%|      | 177/437 [00:03<00:05, 45.46it/s] 42%|     | 182/437 [00:03<00:05, 45.31it/s] 43%|     | 187/437 [00:04<00:05, 44.98it/s] 44%|     | 192/437 [00:04<00:05, 44.97it/s] 45%|     | 197/437 [00:04<00:05, 45.08it/s] 46%|     | 202/437 [00:04<00:05, 45.24it/s] 47%|     | 207/437 [00:04<00:05, 45.39it/s] 49%|     | 212/437 [00:04<00:04, 45.57it/s] 50%|     | 217/437 [00:04<00:04, 45.63it/s] 51%|     | 222/437 [00:04<00:04, 45.65it/s] 52%|    | 227/437 [00:04<00:04, 45.67it/s] 53%|    | 232/437 [00:05<00:04, 45.63it/s] 54%|    | 237/437 [00:05<00:04, 45.43it/s] 55%|    | 242/437 [00:05<00:04, 41.62it/s] 57%|    | 247/437 [00:05<00:04, 42.79it/s] 58%|    | 252/437 [00:05<00:04, 43.81it/s] 59%|    | 257/437 [00:05<00:04, 44.38it/s] 60%|    | 262/437 [00:05<00:03, 44.90it/s] 61%|    | 267/437 [00:05<00:03, 45.24it/s] 62%|   | 272/437 [00:05<00:03, 45.29it/s] 63%|   | 277/437 [00:06<00:03, 45.34it/s] 65%|   | 282/437 [00:06<00:03, 45.00it/s] 66%|   | 287/437 [00:06<00:03, 44.95it/s] 67%|   | 292/437 [00:06<00:03, 45.16it/s] 68%|   | 297/437 [00:06<00:03, 45.33it/s] 69%|   | 302/437 [00:06<00:02, 45.52it/s] 70%|   | 307/437 [00:06<00:02, 45.74it/s] 71%|  | 312/437 [00:06<00:02, 45.79it/s] 73%|  | 317/437 [00:06<00:02, 45.89it/s] 74%|  | 322/437 [00:07<00:02, 45.67it/s] 75%|  | 327/437 [00:07<00:02, 45.44it/s] 76%|  | 332/437 [00:07<00:02, 45.27it/s] 77%|  | 337/437 [00:07<00:02, 45.27it/s] 78%|  | 342/437 [00:07<00:02, 45.30it/s] 79%|  | 347/437 [00:07<00:01, 45.47it/s] 81%|  | 352/437 [00:07<00:01, 45.49it/s] 82%| | 357/437 [00:07<00:01, 45.64it/s] 83%| | 362/437 [00:07<00:01, 45.55it/s] 84%| | 367/437 [00:08<00:01, 45.71it/s] 85%| | 372/437 [00:08<00:01, 45.65it/s] 86%| | 377/437 [00:08<00:01, 45.54it/s] 87%| | 382/437 [00:08<00:01, 42.18it/s] 89%| | 387/437 [00:08<00:01, 43.18it/s] 90%| | 392/437 [00:08<00:01, 43.78it/s] 91%| | 397/437 [00:08<00:00, 44.42it/s] 92%|| 402/437 [00:08<00:00, 44.85it/s] 93%|| 407/437 [00:08<00:00, 45.10it/s] 94%|| 412/437 [00:09<00:00, 45.32it/s] 95%|| 417/437 [00:09<00:00, 45.37it/s] 97%|| 422/437 [00:09<00:00, 45.09it/s] 98%|| 427/437 [00:09<00:00, 44.90it/s] 99%|| 432/437 [00:09<00:00, 45.03it/s]100%|| 437/437 [00:09<00:00, 45.27it/s]100%|| 437/437 [00:09<00:00, 45.31it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:48:40,194 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   eval_loss               =     0.9271
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   eval_runtime            = 0:00:09.66
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   eval_samples_per_second =    361.072
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   eval_steps_per_second   =     45.225
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:48:40,194 >>   perplexity              =     2.5273
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:54,613 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:54,626 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:54,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:54,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:48:54,627 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:48:55,451 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:48:55,452 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:48:56,077 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:48:57,122 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:48:57,122 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:49:00,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:49:00,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:49:00,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:49:00,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:49:00,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:49:00,747 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:49:00,748 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:49:01,391 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:49:01,563 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:49:01,563 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.63it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.45it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:32,  1.51it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.57it/s]Extractor Predicting: 68it [00:43,  1.58it/s]Extractor Predicting: 69it [00:44,  1.61it/s]Extractor Predicting: 70it [00:45,  1.63it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:46,  1.59it/s]Extractor Predicting: 73it [00:47,  1.60it/s]Extractor Predicting: 74it [00:47,  1.62it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:50,  1.61it/s]Extractor Predicting: 79it [00:50,  1.62it/s]Extractor Predicting: 80it [00:51,  1.62it/s]Extractor Predicting: 81it [00:51,  1.62it/s]Extractor Predicting: 82it [00:52,  1.62it/s]Extractor Predicting: 83it [00:53,  1.62it/s]Extractor Predicting: 84it [00:53,  1.59it/s]Extractor Predicting: 85it [00:54,  1.62it/s]Extractor Predicting: 86it [00:55,  1.63it/s]Extractor Predicting: 87it [00:55,  1.63it/s]Extractor Predicting: 88it [00:56,  1.63it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:57,  1.62it/s]Extractor Predicting: 91it [00:58,  1.61it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:00,  1.56it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:02,  1.57it/s]Extractor Predicting: 99it [01:03,  1.56it/s]Extractor Predicting: 100it [01:04,  1.54it/s]Extractor Predicting: 101it [01:04,  1.56it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:09,  1.56it/s]Extractor Predicting: 109it [01:09,  1.52it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:11,  1.52it/s]Extractor Predicting: 113it [01:12,  1.54it/s]Extractor Predicting: 114it [01:13,  1.54it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.56it/s]Extractor Predicting: 117it [01:15,  1.57it/s]Extractor Predicting: 118it [01:15,  1.57it/s]Extractor Predicting: 119it [01:16,  1.61it/s]Extractor Predicting: 120it [01:16,  1.63it/s]Extractor Predicting: 121it [01:17,  1.51it/s]Extractor Predicting: 122it [01:18,  1.54it/s]Extractor Predicting: 123it [01:18,  1.54it/s]Extractor Predicting: 124it [01:19,  1.56it/s]Extractor Predicting: 125it [01:20,  1.55it/s]Extractor Predicting: 126it [01:20,  1.57it/s]Extractor Predicting: 127it [01:21,  1.58it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:22,  1.60it/s]Extractor Predicting: 130it [01:23,  1.55it/s]Extractor Predicting: 131it [01:23,  1.57it/s]Extractor Predicting: 132it [01:24,  1.59it/s]Extractor Predicting: 133it [01:25,  1.57it/s]Extractor Predicting: 134it [01:25,  1.56it/s]Extractor Predicting: 135it [01:26,  1.57it/s]Extractor Predicting: 136it [01:27,  1.60it/s]Extractor Predicting: 137it [01:27,  1.58it/s]Extractor Predicting: 138it [01:28,  1.56it/s]Extractor Predicting: 139it [01:29,  1.59it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:30,  1.58it/s]Extractor Predicting: 142it [01:30,  1.57it/s]Extractor Predicting: 143it [01:31,  1.59it/s]Extractor Predicting: 144it [01:32,  1.58it/s]Extractor Predicting: 145it [01:32,  2.02it/s]Extractor Predicting: 145it [01:32,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:51,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:51,343 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:51,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:51,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:51,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:50:51,992 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:50:51,993 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:52,606 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:53,655 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:53,696 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:56,704 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:56,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:56,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:56,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:56,738 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:50:57,428 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:50:57,430 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:58,032 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:58,199 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:58,199 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5026511134676565,
  "recall": 0.1358555460017197,
  "score": 0.21389891696750904,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.62it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.59it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.59it/s]Extractor Predicting: 48it [00:30,  1.59it/s]Extractor Predicting: 49it [00:31,  1.58it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:35,  1.53it/s]Extractor Predicting: 57it [00:36,  1.57it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:38,  1.69it/s]Extractor Predicting: 61it [00:38,  1.68it/s]Extractor Predicting: 62it [00:39,  1.70it/s]Extractor Predicting: 63it [00:39,  1.72it/s]Extractor Predicting: 64it [00:40,  1.75it/s]Extractor Predicting: 65it [00:41,  1.76it/s]Extractor Predicting: 66it [00:41,  1.75it/s]Extractor Predicting: 67it [00:42,  1.74it/s]Extractor Predicting: 68it [00:42,  1.79it/s]Extractor Predicting: 69it [00:43,  1.83it/s]Extractor Predicting: 70it [00:43,  1.81it/s]Extractor Predicting: 71it [00:44,  1.80it/s]Extractor Predicting: 72it [00:44,  1.81it/s]Extractor Predicting: 73it [00:45,  1.78it/s]Extractor Predicting: 74it [00:46,  1.77it/s]Extractor Predicting: 75it [00:46,  1.76it/s]Extractor Predicting: 76it [00:47,  1.80it/s]Extractor Predicting: 77it [00:47,  1.84it/s]Extractor Predicting: 78it [00:48,  1.82it/s]Extractor Predicting: 79it [00:48,  1.83it/s]Extractor Predicting: 80it [00:49,  1.84it/s]Extractor Predicting: 81it [00:49,  1.82it/s]Extractor Predicting: 82it [00:50,  1.81it/s]Extractor Predicting: 83it [00:51,  1.76it/s]Extractor Predicting: 84it [00:51,  1.76it/s]Extractor Predicting: 85it [00:52,  1.77it/s]Extractor Predicting: 86it [00:52,  1.73it/s]Extractor Predicting: 87it [00:53,  1.68it/s]Extractor Predicting: 88it [00:54,  1.65it/s]Extractor Predicting: 89it [00:54,  1.61it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:56,  1.55it/s]Extractor Predicting: 92it [00:56,  1.53it/s]Extractor Predicting: 93it [00:57,  1.57it/s]Extractor Predicting: 94it [00:58,  1.56it/s]Extractor Predicting: 95it [00:58,  1.56it/s]Extractor Predicting: 96it [00:59,  1.53it/s]Extractor Predicting: 97it [00:59,  1.54it/s]Extractor Predicting: 98it [01:00,  1.54it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:01,  1.56it/s]Extractor Predicting: 101it [01:02,  1.50it/s]Extractor Predicting: 102it [01:03,  1.51it/s]Extractor Predicting: 103it [01:03,  1.55it/s]Extractor Predicting: 104it [01:04,  1.52it/s]Extractor Predicting: 105it [01:05,  1.52it/s]Extractor Predicting: 106it [01:05,  1.47it/s]Extractor Predicting: 107it [01:06,  1.47it/s]Extractor Predicting: 108it [01:07,  1.48it/s]Extractor Predicting: 109it [01:07,  1.52it/s]Extractor Predicting: 110it [01:08,  1.52it/s]Extractor Predicting: 111it [01:09,  1.48it/s]Extractor Predicting: 112it [01:10,  1.46it/s]Extractor Predicting: 113it [01:10,  1.48it/s]Extractor Predicting: 114it [01:11,  1.53it/s]Extractor Predicting: 115it [01:11,  1.52it/s]Extractor Predicting: 116it [01:12,  1.52it/s]Extractor Predicting: 117it [01:13,  1.56it/s]Extractor Predicting: 118it [01:13,  1.52it/s]Extractor Predicting: 119it [01:14,  1.49it/s]Extractor Predicting: 120it [01:15,  1.50it/s]Extractor Predicting: 121it [01:15,  1.48it/s]Extractor Predicting: 122it [01:16,  1.51it/s]Extractor Predicting: 123it [01:17,  1.53it/s]Extractor Predicting: 124it [01:17,  1.52it/s]Extractor Predicting: 125it [01:18,  1.51it/s]Extractor Predicting: 126it [01:19,  1.48it/s]Extractor Predicting: 127it [01:19,  1.48it/s]Extractor Predicting: 128it [01:20,  1.46it/s]Extractor Predicting: 129it [01:21,  1.48it/s]Extractor Predicting: 130it [01:21,  1.46it/s]Extractor Predicting: 131it [01:22,  1.49it/s]Extractor Predicting: 132it [01:23,  1.50it/s]Extractor Predicting: 133it [01:23,  1.51it/s]Extractor Predicting: 134it [01:24,  1.53it/s]Extractor Predicting: 135it [01:25,  1.52it/s]Extractor Predicting: 136it [01:25,  1.48it/s]Extractor Predicting: 137it [01:26,  1.47it/s]Extractor Predicting: 138it [01:27,  1.47it/s]Extractor Predicting: 139it [01:28,  1.47it/s]Extractor Predicting: 140it [01:28,  1.49it/s]Extractor Predicting: 141it [01:29,  1.50it/s]Extractor Predicting: 142it [01:29,  1.50it/s]Extractor Predicting: 143it [01:30,  1.47it/s]Extractor Predicting: 144it [01:31,  1.47it/s]Extractor Predicting: 145it [01:32,  1.48it/s]Extractor Predicting: 146it [01:32,  1.34it/s]Extractor Predicting: 147it [01:33,  1.39it/s]Extractor Predicting: 148it [01:34,  1.45it/s]Extractor Predicting: 149it [01:34,  1.44it/s]Extractor Predicting: 150it [01:35,  1.46it/s]Extractor Predicting: 151it [01:36,  1.51it/s]Extractor Predicting: 152it [01:36,  1.52it/s]Extractor Predicting: 153it [01:37,  1.54it/s]Extractor Predicting: 154it [01:38,  1.57it/s]Extractor Predicting: 155it [01:38,  1.59it/s]Extractor Predicting: 156it [01:39,  1.61it/s]Extractor Predicting: 157it [01:39,  1.58it/s]Extractor Predicting: 158it [01:40,  1.60it/s]Extractor Predicting: 159it [01:41,  1.66it/s]Extractor Predicting: 160it [01:41,  1.62it/s]Extractor Predicting: 161it [01:42,  1.59it/s]Extractor Predicting: 162it [01:43,  1.54it/s]Extractor Predicting: 163it [01:43,  1.56it/s]Extractor Predicting: 164it [01:44,  1.55it/s]Extractor Predicting: 165it [01:45,  1.54it/s]Extractor Predicting: 166it [01:45,  1.55it/s]Extractor Predicting: 167it [01:46,  1.53it/s]Extractor Predicting: 168it [01:47,  1.53it/s]Extractor Predicting: 169it [01:47,  1.53it/s]Extractor Predicting: 170it [01:48,  1.56it/s]Extractor Predicting: 171it [01:48,  1.55it/s]Extractor Predicting: 172it [01:49,  1.51it/s]Extractor Predicting: 173it [01:50,  1.54it/s]Extractor Predicting: 174it [01:50,  1.55it/s]Extractor Predicting: 175it [01:51,  1.54it/s]Extractor Predicting: 176it [01:52,  1.54it/s]Extractor Predicting: 177it [01:52,  1.57it/s]Extractor Predicting: 178it [01:53,  1.56it/s]Extractor Predicting: 179it [01:54,  1.59it/s]Extractor Predicting: 180it [01:54,  1.61it/s]Extractor Predicting: 181it [01:55,  1.59it/s]Extractor Predicting: 182it [01:55,  1.64it/s]Extractor Predicting: 183it [01:56,  1.64it/s]Extractor Predicting: 184it [01:57,  1.62it/s]Extractor Predicting: 185it [01:57,  1.61it/s]Extractor Predicting: 186it [01:58,  1.62it/s]Extractor Predicting: 187it [01:59,  1.59it/s]Extractor Predicting: 188it [01:59,  1.63it/s]Extractor Predicting: 189it [02:00,  1.65it/s]Extractor Predicting: 190it [02:00,  1.68it/s]Extractor Predicting: 191it [02:01,  1.62it/s]Extractor Predicting: 192it [02:02,  1.64it/s]Extractor Predicting: 193it [02:02,  1.66it/s]Extractor Predicting: 194it [02:03,  1.69it/s]Extractor Predicting: 195it [02:03,  1.66it/s]Extractor Predicting: 196it [02:04,  1.61it/s]Extractor Predicting: 197it [02:05,  1.64it/s]Extractor Predicting: 198it [02:05,  1.63it/s]Extractor Predicting: 199it [02:06,  1.60it/s]Extractor Predicting: 200it [02:06,  1.62it/s]Extractor Predicting: 201it [02:07,  1.59it/s]Extractor Predicting: 202it [02:08,  1.63it/s]Extractor Predicting: 203it [02:08,  1.67it/s]Extractor Predicting: 204it [02:09,  1.69it/s]Extractor Predicting: 205it [02:09,  1.68it/s]Extractor Predicting: 206it [02:10,  1.74it/s]Extractor Predicting: 207it [02:11,  1.67it/s]Extractor Predicting: 208it [02:11,  1.73it/s]Extractor Predicting: 209it [02:12,  1.71it/s]Extractor Predicting: 210it [02:12,  1.70it/s]Extractor Predicting: 211it [02:13,  1.70it/s]Extractor Predicting: 212it [02:13,  1.75it/s]Extractor Predicting: 213it [02:14,  1.73it/s]Extractor Predicting: 214it [02:15,  1.75it/s]Extractor Predicting: 215it [02:15,  1.79it/s]Extractor Predicting: 216it [02:16,  1.79it/s]Extractor Predicting: 217it [02:16,  1.75it/s]Extractor Predicting: 218it [02:17,  1.73it/s]Extractor Predicting: 219it [02:17,  1.72it/s]Extractor Predicting: 220it [02:18,  1.74it/s]Extractor Predicting: 221it [02:19,  1.76it/s]Extractor Predicting: 222it [02:19,  1.81it/s]Extractor Predicting: 223it [02:20,  1.75it/s]Extractor Predicting: 224it [02:20,  1.79it/s]Extractor Predicting: 225it [02:21,  1.75it/s]Extractor Predicting: 226it [02:21,  1.77it/s]Extractor Predicting: 227it [02:22,  1.80it/s]Extractor Predicting: 228it [02:22,  1.79it/s]Extractor Predicting: 229it [02:23,  1.70it/s]Extractor Predicting: 230it [02:24,  1.65it/s]Extractor Predicting: 231it [02:25,  1.51it/s]Extractor Predicting: 232it [02:25,  1.49it/s]Extractor Predicting: 233it [02:26,  1.51it/s]Extractor Predicting: 234it [02:27,  1.50it/s]Extractor Predicting: 235it [02:27,  1.51it/s]Extractor Predicting: 236it [02:28,  1.50it/s]Extractor Predicting: 237it [02:29,  1.47it/s]Extractor Predicting: 238it [02:29,  1.46it/s]Extractor Predicting: 239it [02:30,  1.47it/s]Extractor Predicting: 240it [02:31,  1.48it/s]Extractor Predicting: 241it [02:31,  1.49it/s]Extractor Predicting: 242it [02:32,  1.51it/s]Extractor Predicting: 243it [02:33,  1.47it/s]Extractor Predicting: 244it [02:33,  1.49it/s]Extractor Predicting: 245it [02:34,  1.50it/s]Extractor Predicting: 246it [02:35,  1.50it/s]Extractor Predicting: 247it [02:35,  1.46it/s]Extractor Predicting: 248it [02:36,  1.47it/s]Extractor Predicting: 249it [02:37,  1.43it/s]Extractor Predicting: 250it [02:37,  1.45it/s]Extractor Predicting: 251it [02:38,  1.46it/s]Extractor Predicting: 252it [02:39,  1.48it/s]Extractor Predicting: 253it [02:39,  1.50it/s]Extractor Predicting: 254it [02:40,  1.48it/s]Extractor Predicting: 255it [02:41,  1.48it/s]Extractor Predicting: 256it [02:41,  1.47it/s]Extractor Predicting: 257it [02:42,  1.54it/s]Extractor Predicting: 258it [02:43,  1.56it/s]Extractor Predicting: 259it [02:43,  1.57it/s]Extractor Predicting: 260it [02:44,  1.60it/s]Extractor Predicting: 261it [02:45,  1.57it/s]Extractor Predicting: 262it [02:45,  1.57it/s]Extractor Predicting: 263it [02:46,  1.60it/s]Extractor Predicting: 264it [02:46,  1.61it/s]Extractor Predicting: 265it [02:47,  1.58it/s]Extractor Predicting: 266it [02:48,  1.56it/s]Extractor Predicting: 267it [02:48,  1.60it/s]Extractor Predicting: 268it [02:49,  1.58it/s]Extractor Predicting: 269it [02:50,  1.60it/s]Extractor Predicting: 270it [02:50,  1.58it/s]Extractor Predicting: 271it [02:51,  1.54it/s]Extractor Predicting: 272it [02:52,  1.55it/s]Extractor Predicting: 273it [02:52,  1.39it/s]Extractor Predicting: 274it [02:53,  1.43it/s]Extractor Predicting: 275it [02:54,  1.49it/s]Extractor Predicting: 276it [02:54,  1.51it/s]Extractor Predicting: 277it [02:55,  1.51it/s]Extractor Predicting: 278it [02:56,  1.49it/s]Extractor Predicting: 279it [02:56,  1.53it/s]Extractor Predicting: 280it [02:57,  1.53it/s]Extractor Predicting: 281it [02:58,  1.57it/s]Extractor Predicting: 282it [02:58,  1.54it/s]Extractor Predicting: 283it [02:59,  1.55it/s]Extractor Predicting: 284it [03:00,  1.57it/s]Extractor Predicting: 285it [03:00,  1.54it/s]Extractor Predicting: 286it [03:01,  1.54it/s]Extractor Predicting: 287it [03:01,  1.55it/s]Extractor Predicting: 288it [03:02,  1.56it/s]Extractor Predicting: 289it [03:03,  1.54it/s]Extractor Predicting: 290it [03:03,  1.55it/s]Extractor Predicting: 291it [03:04,  1.55it/s]Extractor Predicting: 292it [03:05,  1.51it/s]Extractor Predicting: 293it [03:05,  1.52it/s]Extractor Predicting: 294it [03:06,  1.54it/s]Extractor Predicting: 295it [03:07,  1.55it/s]Extractor Predicting: 296it [03:07,  1.56it/s]Extractor Predicting: 297it [03:08,  1.55it/s]Extractor Predicting: 298it [03:09,  1.53it/s]Extractor Predicting: 299it [03:09,  1.57it/s]Extractor Predicting: 300it [03:10,  1.57it/s]Extractor Predicting: 301it [03:10,  1.58it/s]Extractor Predicting: 302it [03:11,  1.55it/s]Extractor Predicting: 303it [03:12,  1.60it/s]Extractor Predicting: 304it [03:12,  1.60it/s]Extractor Predicting: 305it [03:13,  1.63it/s]Extractor Predicting: 306it [03:14,  1.61it/s]Extractor Predicting: 307it [03:14,  1.58it/s]Extractor Predicting: 308it [03:15,  1.58it/s]Extractor Predicting: 309it [03:15,  1.60it/s]Extractor Predicting: 310it [03:16,  1.56it/s]Extractor Predicting: 311it [03:17,  1.56it/s]Extractor Predicting: 312it [03:17,  1.54it/s]Extractor Predicting: 313it [03:18,  1.57it/s]Extractor Predicting: 314it [03:19,  1.57it/s]Extractor Predicting: 315it [03:19,  1.60it/s]Extractor Predicting: 316it [03:20,  1.59it/s]Extractor Predicting: 317it [03:21,  1.56it/s]Extractor Predicting: 318it [03:21,  1.62it/s]Extractor Predicting: 319it [03:22,  1.62it/s]Extractor Predicting: 320it [03:22,  1.65it/s]Extractor Predicting: 321it [03:23,  1.62it/s]Extractor Predicting: 322it [03:24,  1.58it/s]Extractor Predicting: 323it [03:24,  1.59it/s]Extractor Predicting: 324it [03:25,  1.58it/s]Extractor Predicting: 325it [03:26,  1.57it/s]Extractor Predicting: 326it [03:26,  1.58it/s]Extractor Predicting: 327it [03:27,  1.56it/s]Extractor Predicting: 328it [03:28,  1.56it/s]Extractor Predicting: 329it [03:28,  1.52it/s]Extractor Predicting: 330it [03:29,  1.53it/s]Extractor Predicting: 331it [03:30,  1.53it/s]Extractor Predicting: 332it [03:30,  1.58it/s]Extractor Predicting: 333it [03:31,  1.58it/s]Extractor Predicting: 334it [03:31,  1.59it/s]Extractor Predicting: 335it [03:32,  1.58it/s]Extractor Predicting: 336it [03:33,  1.56it/s]Extractor Predicting: 337it [03:33,  1.56it/s]Extractor Predicting: 338it [03:34,  1.56it/s]Extractor Predicting: 339it [03:35,  1.55it/s]Extractor Predicting: 340it [03:35,  1.60it/s]Extractor Predicting: 341it [03:36,  1.55it/s]Extractor Predicting: 342it [03:36,  1.59it/s]Extractor Predicting: 343it [03:37,  1.54it/s]Extractor Predicting: 344it [03:38,  1.57it/s]Extractor Predicting: 345it [03:38,  1.54it/s]Extractor Predicting: 346it [03:39,  1.56it/s]Extractor Predicting: 347it [03:40,  1.57it/s]Extractor Predicting: 348it [03:40,  1.59it/s]Extractor Predicting: 349it [03:41,  1.53it/s]Extractor Predicting: 350it [03:42,  1.52it/s]Extractor Predicting: 351it [03:42,  1.47it/s]Extractor Predicting: 352it [03:43,  1.50it/s]Extractor Predicting: 353it [03:44,  1.51it/s]Extractor Predicting: 354it [03:44,  1.53it/s]Extractor Predicting: 355it [03:45,  1.52it/s]Extractor Predicting: 356it [03:46,  1.52it/s]Extractor Predicting: 357it [03:46,  1.51it/s]Extractor Predicting: 358it [03:47,  1.51it/s]Extractor Predicting: 359it [03:48,  1.50it/s]Extractor Predicting: 360it [03:48,  1.48it/s]Extractor Predicting: 361it [03:49,  1.49it/s]Extractor Predicting: 362it [03:50,  1.53it/s]Extractor Predicting: 363it [03:50,  1.53it/s]Extractor Predicting: 364it [03:51,  1.56it/s]Extractor Predicting: 365it [03:52,  1.40it/s]Extractor Predicting: 366it [03:52,  1.42it/s]Extractor Predicting: 367it [03:53,  1.46it/s]Extractor Predicting: 368it [03:54,  1.46it/s]Extractor Predicting: 369it [03:54,  1.50it/s]Extractor Predicting: 370it [03:55,  1.50it/s]Extractor Predicting: 371it [03:56,  1.52it/s]Extractor Predicting: 372it [03:56,  1.54it/s]Extractor Predicting: 373it [03:57,  1.57it/s]Extractor Predicting: 374it [03:58,  1.57it/s]Extractor Predicting: 375it [03:58,  1.56it/s]Extractor Predicting: 376it [03:59,  1.58it/s]Extractor Predicting: 377it [03:59,  1.59it/s]Extractor Predicting: 378it [04:00,  1.58it/s]Extractor Predicting: 379it [04:01,  1.60it/s]Extractor Predicting: 380it [04:01,  1.59it/s]Extractor Predicting: 381it [04:02,  1.60it/s]Extractor Predicting: 382it [04:03,  1.57it/s]Extractor Predicting: 383it [04:03,  1.58it/s]Extractor Predicting: 384it [04:04,  1.59it/s]Extractor Predicting: 385it [04:05,  1.60it/s]Extractor Predicting: 386it [04:05,  1.63it/s]Extractor Predicting: 387it [04:06,  1.65it/s]Extractor Predicting: 388it [04:06,  1.58it/s]Extractor Predicting: 389it [04:07,  1.60it/s]Extractor Predicting: 390it [04:08,  1.59it/s]Extractor Predicting: 391it [04:08,  1.59it/s]Extractor Predicting: 392it [04:09,  1.60it/s]Extractor Predicting: 393it [04:10,  1.58it/s]Extractor Predicting: 394it [04:10,  1.60it/s]Extractor Predicting: 395it [04:11,  1.55it/s]Extractor Predicting: 396it [04:11,  1.58it/s]Extractor Predicting: 397it [04:12,  1.57it/s]Extractor Predicting: 398it [04:13,  1.55it/s]Extractor Predicting: 399it [04:13,  1.57it/s]Extractor Predicting: 400it [04:14,  1.57it/s]Extractor Predicting: 401it [04:15,  1.59it/s]Extractor Predicting: 402it [04:15,  1.61it/s]Extractor Predicting: 403it [04:16,  1.60it/s]Extractor Predicting: 404it [04:16,  1.63it/s]Extractor Predicting: 405it [04:17,  1.63it/s]Extractor Predicting: 406it [04:18,  1.63it/s]Extractor Predicting: 407it [04:18,  1.62it/s]Extractor Predicting: 408it [04:19,  1.64it/s]Extractor Predicting: 409it [04:19,  1.66it/s]Extractor Predicting: 410it [04:20,  1.63it/s]Extractor Predicting: 411it [04:21,  1.60it/s]Extractor Predicting: 412it [04:21,  1.63it/s]Extractor Predicting: 413it [04:22,  1.63it/s]Extractor Predicting: 414it [04:23,  1.64it/s]Extractor Predicting: 415it [04:23,  1.67it/s]Extractor Predicting: 416it [04:24,  1.67it/s]Extractor Predicting: 417it [04:24,  1.64it/s]Extractor Predicting: 418it [04:25,  1.66it/s]Extractor Predicting: 419it [04:26,  1.62it/s]Extractor Predicting: 420it [04:26,  1.62it/s]Extractor Predicting: 421it [04:27,  1.62it/s]Extractor Predicting: 422it [04:27,  1.61it/s]Extractor Predicting: 423it [04:28,  1.62it/s]Extractor Predicting: 424it [04:29,  1.63it/s]Extractor Predicting: 425it [04:29,  1.85it/s]Extractor Predicting: 425it [04:29,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:41,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:41,243 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:41,656 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:41,657 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:42,355 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:43,423 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:43,423 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:46,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:46,531 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:46,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:46,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:46,532 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:47,241 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:47,242 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:47,877 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:48,047 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:48,047 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35763511062749365,
  "recall": 0.09679952876497153,
  "score": 0.1523603492235185,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.73it/s]Extractor Predicting: 7it [00:04,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:55:54,744 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:55:54,745 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:55:54,835 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:55:54,836 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:55:54,872 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:56:04,864 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:56:04,893 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:56:05,028 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:56:05,029 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:56:05,104 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,176 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,176 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,177 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:56:05,177 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6875,
  "recall": 0.03503184713375796,
  "score": 0.06666666666666667,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:56:05,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:06,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:06,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:07,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:08,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:08,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:09,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:10,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:10,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:11,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:12,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:12,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:13,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:14,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:14,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:15,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:15,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:16,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:16,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:17,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:18,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:18,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:13<04:21, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:19,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:19,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:20,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:21,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:21,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:22,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:23,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:23,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:24,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:25,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:25,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:26,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:27,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:27,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:28,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:29,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:29,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:30,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:31,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:31,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:32,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:32,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:28<04:22, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:34,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:35,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:35,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:36,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:37,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:39,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:39,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:40,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:41,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:41,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:42,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:42,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:43,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:43,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:44,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:45,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:45,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:46,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:47,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:47,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:48,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:43<04:06, 14.52s/it][WARNING|generation_utils.py:914] 2023-08-29 00:56:48,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:49,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:50,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:50,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:51,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:51,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:52,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:52,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:53,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:54,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:55,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:55,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:56,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:56,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:57,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:58,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:58,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:59,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:56:59,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:00,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:00,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:01,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:56<03:42, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:01,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:02,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:03,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:04,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:04,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:05,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:05,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:06,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:07,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:07,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:08,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:08,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:09,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:10,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:11,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:11,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:12,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:13,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:13,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:14,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:14,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:15,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:10<03:32, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:16,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:16,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:17,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:18,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:18,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:19,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:20,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:20,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:21,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:21,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:22,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:23,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:23,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:24,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:24,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:25,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:26,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:26,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:27,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:28,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:28,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:23<03:12, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:29,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:30,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:30,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:31,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:32,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:32,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:33,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:33,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:34,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:34,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:35,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:36,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:36,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:37,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:37,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:38,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:39,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:39,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:40,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:41,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:41,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:42,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:43,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:38<03:01, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:43,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:44,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:44,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:45,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:46,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:46,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:47,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:48,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:48,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:49,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:49,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:50,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:51,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:51,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:52,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:52,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:53,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:54,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:54,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:55,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:56,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:56,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:57,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:57,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:58,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:53<02:52, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-29 00:57:59,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:57:59,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:00,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:00,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:02,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:02,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:03,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:04,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:04,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:05,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:06,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:06,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:07,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:08,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:09,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:09,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:10,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:11,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:12,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:12,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:13,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:13,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [02:09<02:42, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:14,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:15,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:15,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:16,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:17,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:17,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:18,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:19,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:19,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:20,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:21,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:21,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:22,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:23,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:24,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:24,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:25,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:26,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:26,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:27,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:28,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:23<02:27, 14.78s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:29,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:30,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:30,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:31,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:31,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:32,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:33,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:33,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:34,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:35,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:35,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:36,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:37,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:37,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:38,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:38,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:39,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:39,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:40,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:41,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:41,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:42,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:37<02:09, 14.40s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:43,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:43,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:44,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:44,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:45,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:46,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:46,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:47,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:47,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:48,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:49,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:49,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:50,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:50,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:51,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:52,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:53,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:53,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:54,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:54,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:55,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:50<01:52, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-29 00:58:56,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:56,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:57,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:57,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:58,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:59,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:58:59,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:00,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:01,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:01,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:02,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:02,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:03,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:03,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:04,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:05,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:05,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:06,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:06,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:07,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [03:02<01:32, 13.25s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:07,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:08,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:09,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:10,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:11,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:11,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:12,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:13,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:14,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:14,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:15,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:16,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:17,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:17,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:18,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:19,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:19,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:20,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:21,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:21,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [03:17<01:22, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:22,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:23,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:23,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:24,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:25,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:25,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:26,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:27,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:27,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:28,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:29,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:29,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:30,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:31,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:31,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:32,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:33,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:33,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:34,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:35,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:35,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:36,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:37,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:32<01:11, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:38,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:39,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:40,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:40,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:41,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:42,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:42,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:43,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:44,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:45,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:45,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:46,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:47,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:47,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:48,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:49,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:49,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:50,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:50,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:51,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:52,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:52,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:47<00:58, 14.59s/it][WARNING|generation_utils.py:914] 2023-08-29 00:59:53,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:53,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:54,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:55,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:55,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:56,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:56,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:57,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:58,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:58,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:59,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:59:59,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:00,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:00,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:01,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:01,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:02,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:03,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:03,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:04,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:04,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:59<00:41, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:05,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:05,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:06,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:07,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:07,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:08,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:08,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:09,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:10,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:10,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:11,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:11,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:12,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:13,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:14,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:15,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:15,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:16,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:16,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:17,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [04:12<00:26, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:18,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:18,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:19,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:19,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:20,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:20,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:21,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:22,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:22,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:23,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:23,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:24,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:25,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:25,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:26,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:26,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:27,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:29,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [04:26<00:13, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:31,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:32,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:32,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:33,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:33,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:34,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:34,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:35,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:36,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:36,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:37,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:38,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:38,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:39,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:39,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:40,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:40,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:41,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:42,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:42,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:43,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:38<00:00, 13.14s/it]Generating: 100%|| 20/20 [04:38<00:00, 13.92s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:52,514 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:52,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:52,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:52,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:52,555 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:00:53,189 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:00:53,190 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:53,803 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:54,880 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:54,880 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:58,053 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:58,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:58,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:58,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:58,087 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:00:58,762 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:00:58,763 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:59,389 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:59,563 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:59,563 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8778409090909091, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in the year ( October 1887 ) , a young French politician , Louis Baudelaire , was expelled from Parliament for being in favour of a reform of the constitution , and he was later convicted of conspiring to overthrow the government . Head Entity : Louis Baudelaire , Tail Entity : parliamentary .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8464673913043478, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 165, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 234, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 481, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.75375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : father .', 'success_rate': 0.8478260869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8693181818181818, 'errors': {''}}
['Relation : heritage designation . Context : Later in the year ( 186869 ) a monument to Thomas Jefferson was constructed in the middle of town . Head Entity : Thomas Jefferson , Tail Entity : city .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.90625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8260869565217391, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9285714285714286, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Armenian national team returned to their regular match against the Copa Libertadores in Las Palmas . Head Entity : Copa Libertadores , Tail Entity : regular game .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 13412
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13512, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.51it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.56it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:10,  1.56it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.61it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.61it/s]Extractor Estimating: 24it [00:15,  1.59it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:16,  1.61it/s]Extractor Estimating: 27it [00:17,  1.61it/s]Extractor Estimating: 28it [00:17,  1.67it/s]Extractor Estimating: 29it [00:18,  1.66it/s]Extractor Estimating: 30it [00:18,  1.67it/s]Extractor Estimating: 31it [00:19,  1.62it/s]Extractor Estimating: 32it [00:20,  1.56it/s]Extractor Estimating: 33it [00:20,  1.54it/s]Extractor Estimating: 34it [00:21,  1.57it/s]Extractor Estimating: 35it [00:22,  1.61it/s]Extractor Estimating: 36it [00:22,  1.60it/s]Extractor Estimating: 37it [00:23,  1.57it/s]Extractor Estimating: 38it [00:23,  1.58it/s]Extractor Estimating: 39it [00:24,  1.54it/s]Extractor Estimating: 40it [00:25,  1.49it/s]Extractor Estimating: 41it [00:26,  1.47it/s]Extractor Estimating: 42it [00:26,  1.42it/s]Extractor Estimating: 43it [00:27,  1.43it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:28,  1.54it/s]Extractor Estimating: 46it [00:29,  1.56it/s]Extractor Estimating: 47it [00:29,  1.55it/s]Extractor Estimating: 48it [00:30,  1.58it/s]Extractor Estimating: 49it [00:31,  1.55it/s]Extractor Estimating: 50it [00:31,  1.53it/s]Extractor Estimating: 51it [00:32,  1.52it/s]Extractor Estimating: 52it [00:33,  1.53it/s]Extractor Estimating: 53it [00:33,  1.57it/s]Extractor Estimating: 54it [00:34,  1.62it/s]Extractor Estimating: 55it [00:35,  1.63it/s]Extractor Estimating: 56it [00:35,  1.56it/s]Extractor Estimating: 57it [00:36,  1.59it/s]Extractor Estimating: 58it [00:36,  1.61it/s]Extractor Estimating: 59it [00:37,  1.58it/s]Extractor Estimating: 60it [00:38,  1.61it/s]Extractor Estimating: 61it [00:38,  1.62it/s]Extractor Estimating: 62it [00:39,  1.65it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:40,  1.63it/s]Extractor Estimating: 65it [00:41,  1.66it/s]Extractor Estimating: 66it [00:41,  1.69it/s]Extractor Estimating: 67it [00:42,  1.70it/s]Extractor Estimating: 68it [00:42,  1.75it/s]Extractor Estimating: 69it [00:43,  1.77it/s]Extractor Estimating: 70it [00:44,  1.75it/s]Extractor Estimating: 71it [00:44,  1.72it/s]Extractor Estimating: 72it [00:45,  1.69it/s]Extractor Estimating: 73it [00:45,  1.70it/s]Extractor Estimating: 74it [00:46,  1.69it/s]Extractor Estimating: 75it [00:47,  1.65it/s]Extractor Estimating: 76it [00:47,  1.68it/s]Extractor Estimating: 77it [00:48,  1.72it/s]Extractor Estimating: 78it [00:48,  1.64it/s]Extractor Estimating: 79it [00:49,  1.62it/s]Extractor Estimating: 80it [00:50,  1.68it/s]Extractor Estimating: 81it [00:50,  1.70it/s]Extractor Estimating: 82it [00:51,  1.72it/s]Extractor Estimating: 83it [00:51,  1.71it/s]Extractor Estimating: 84it [00:52,  1.69it/s]Extractor Estimating: 85it [00:52,  1.69it/s]Extractor Estimating: 86it [00:53,  1.69it/s]Extractor Estimating: 87it [00:54,  1.71it/s]Extractor Estimating: 88it [00:54,  1.72it/s]Extractor Estimating: 89it [00:55,  1.60it/s]Extractor Estimating: 90it [00:56,  1.65it/s]Extractor Estimating: 91it [00:56,  1.47it/s]Extractor Estimating: 92it [00:57,  1.48it/s]Extractor Estimating: 93it [00:58,  1.53it/s]Extractor Estimating: 94it [00:58,  1.56it/s]Extractor Estimating: 95it [00:59,  1.59it/s]Extractor Estimating: 96it [00:59,  1.63it/s]Extractor Estimating: 97it [01:00,  1.66it/s]Extractor Estimating: 98it [01:01,  1.61it/s]Extractor Estimating: 99it [01:01,  1.64it/s]Extractor Estimating: 100it [01:02,  1.65it/s]Extractor Estimating: 101it [01:03,  1.60it/s]Extractor Estimating: 102it [01:03,  1.54it/s]Extractor Estimating: 103it [01:04,  1.53it/s]Extractor Estimating: 104it [01:05,  1.54it/s]Extractor Estimating: 105it [01:05,  1.57it/s]Extractor Estimating: 106it [01:06,  1.56it/s]Extractor Estimating: 107it [01:06,  1.60it/s]Extractor Estimating: 108it [01:07,  1.63it/s]Extractor Estimating: 109it [01:07,  1.70it/s]Extractor Estimating: 110it [01:08,  1.68it/s]Extractor Estimating: 111it [01:09,  1.65it/s]Extractor Estimating: 112it [01:09,  1.68it/s]Extractor Estimating: 113it [01:10,  1.69it/s]Extractor Estimating: 114it [01:11,  1.65it/s]Extractor Estimating: 115it [01:11,  1.66it/s]Extractor Estimating: 116it [01:12,  1.64it/s]Extractor Estimating: 117it [01:12,  1.62it/s]Extractor Estimating: 118it [01:13,  1.66it/s]Extractor Estimating: 119it [01:14,  1.63it/s]Extractor Estimating: 120it [01:14,  1.69it/s]Extractor Estimating: 121it [01:15,  1.76it/s]Extractor Estimating: 122it [01:15,  1.79it/s]Extractor Estimating: 123it [01:16,  1.76it/s]Extractor Estimating: 124it [01:16,  1.71it/s]Extractor Estimating: 125it [01:17,  1.56it/s]Extractor Estimating: 126it [01:18,  1.49it/s]Extractor Estimating: 127it [01:19,  1.49it/s]Extractor Estimating: 128it [01:19,  1.52it/s]Extractor Estimating: 129it [01:20,  1.56it/s]Extractor Estimating: 130it [01:20,  1.59it/s]Extractor Estimating: 131it [01:21,  1.52it/s]Extractor Estimating: 132it [01:22,  1.50it/s]Extractor Estimating: 133it [01:22,  1.51it/s]Extractor Estimating: 134it [01:23,  1.56it/s]Extractor Estimating: 135it [01:24,  1.57it/s]Extractor Estimating: 136it [01:24,  1.60it/s]Extractor Estimating: 137it [01:25,  1.57it/s]Extractor Estimating: 138it [01:26,  1.60it/s]Extractor Estimating: 139it [01:26,  1.59it/s]Extractor Estimating: 140it [01:27,  1.48it/s]Extractor Estimating: 141it [01:28,  1.48it/s]Extractor Estimating: 142it [01:28,  1.50it/s]Extractor Estimating: 143it [01:29,  1.47it/s]Extractor Estimating: 144it [01:30,  1.50it/s]Extractor Estimating: 145it [01:30,  1.54it/s]Extractor Estimating: 146it [01:31,  1.54it/s]Extractor Estimating: 147it [01:32,  1.53it/s]Extractor Estimating: 148it [01:32,  1.61it/s]Extractor Estimating: 149it [01:33,  1.61it/s]Extractor Estimating: 150it [01:33,  1.58it/s]Extractor Estimating: 151it [01:34,  1.63it/s]Extractor Estimating: 152it [01:35,  1.61it/s]Extractor Estimating: 153it [01:35,  1.66it/s]Extractor Estimating: 154it [01:36,  1.64it/s]Extractor Estimating: 155it [01:36,  1.64it/s]Extractor Estimating: 156it [01:37,  1.69it/s]Extractor Estimating: 157it [01:38,  1.69it/s]Extractor Estimating: 158it [01:38,  1.71it/s]Extractor Estimating: 159it [01:39,  1.72it/s]Extractor Estimating: 160it [01:39,  1.70it/s]Extractor Estimating: 161it [01:40,  1.74it/s]Extractor Estimating: 162it [01:40,  1.75it/s]Extractor Estimating: 163it [01:41,  1.80it/s]Extractor Estimating: 164it [01:42,  1.72it/s]Extractor Estimating: 165it [01:42,  1.67it/s]Extractor Estimating: 166it [01:43,  1.67it/s]Extractor Estimating: 167it [01:43,  1.65it/s]Extractor Estimating: 168it [01:44,  1.69it/s]Extractor Estimating: 169it [01:45,  1.67it/s]Extractor Estimating: 170it [01:45,  1.70it/s]Extractor Estimating: 171it [01:46,  1.69it/s]Extractor Estimating: 172it [01:46,  1.69it/s]Extractor Estimating: 173it [01:47,  1.70it/s]Extractor Estimating: 174it [01:48,  1.65it/s]Extractor Estimating: 175it [01:48,  1.65it/s]Extractor Estimating: 176it [01:49,  1.68it/s]Extractor Estimating: 177it [01:49,  1.68it/s]Extractor Estimating: 178it [01:50,  1.72it/s]Extractor Estimating: 179it [01:50,  1.72it/s]Extractor Estimating: 180it [01:51,  1.70it/s]Extractor Estimating: 181it [01:52,  1.75it/s]Extractor Estimating: 182it [01:52,  1.71it/s]Extractor Estimating: 183it [01:53,  1.74it/s]Extractor Estimating: 184it [01:53,  1.71it/s]Extractor Estimating: 185it [01:54,  1.70it/s]Extractor Estimating: 186it [01:54,  1.76it/s]Extractor Estimating: 187it [01:55,  1.75it/s]Extractor Estimating: 188it [01:56,  1.74it/s]Extractor Estimating: 189it [01:56,  1.72it/s]Extractor Estimating: 190it [01:57,  1.72it/s]Extractor Estimating: 191it [01:57,  1.73it/s]Extractor Estimating: 192it [01:58,  1.65it/s]Extractor Estimating: 193it [01:59,  1.61it/s]Extractor Estimating: 194it [01:59,  1.63it/s]Extractor Estimating: 195it [02:00,  1.66it/s]Extractor Estimating: 196it [02:01,  1.50it/s]Extractor Estimating: 197it [02:01,  1.56it/s]Extractor Estimating: 198it [02:02,  1.57it/s]Extractor Estimating: 199it [02:03,  1.55it/s]Extractor Estimating: 200it [02:03,  1.53it/s]Extractor Estimating: 201it [02:04,  1.55it/s]Extractor Estimating: 202it [02:04,  1.57it/s]Extractor Estimating: 203it [02:05,  1.59it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:06,  1.62it/s]Extractor Estimating: 206it [02:07,  1.62it/s]Extractor Estimating: 207it [02:08,  1.65it/s]Extractor Estimating: 208it [02:08,  1.70it/s]Extractor Estimating: 209it [02:09,  1.68it/s]Extractor Estimating: 210it [02:09,  1.71it/s]Extractor Estimating: 211it [02:10,  1.65it/s]Extractor Estimating: 212it [02:11,  1.64it/s]Extractor Estimating: 213it [02:11,  1.62it/s]Extractor Estimating: 214it [02:12,  1.59it/s]Extractor Estimating: 215it [02:12,  1.55it/s]Extractor Estimating: 216it [02:13,  1.60it/s]Extractor Estimating: 217it [02:14,  1.61it/s]Extractor Estimating: 218it [02:14,  1.65it/s]Extractor Estimating: 219it [02:15,  1.67it/s]Extractor Estimating: 220it [02:16,  1.53it/s]Extractor Estimating: 221it [02:16,  1.55it/s]Extractor Estimating: 222it [02:17,  1.58it/s]Extractor Estimating: 223it [02:17,  1.59it/s]Extractor Estimating: 224it [02:18,  1.69it/s]Extractor Estimating: 225it [02:19,  1.64it/s]Extractor Estimating: 226it [02:19,  1.65it/s]Extractor Estimating: 227it [02:20,  1.65it/s]Extractor Estimating: 228it [02:20,  1.67it/s]Extractor Estimating: 229it [02:21,  1.69it/s]Extractor Estimating: 230it [02:22,  1.62it/s]Extractor Estimating: 231it [02:22,  1.61it/s]Extractor Estimating: 232it [02:23,  1.68it/s]Extractor Estimating: 233it [02:23,  1.65it/s]Extractor Estimating: 234it [02:24,  1.68it/s]Extractor Estimating: 235it [02:25,  1.66it/s]Extractor Estimating: 236it [02:25,  1.63it/s]Extractor Estimating: 237it [02:26,  1.64it/s]Extractor Estimating: 238it [02:27,  1.56it/s]Extractor Estimating: 239it [02:27,  1.50it/s]Extractor Estimating: 240it [02:28,  1.53it/s]Extractor Estimating: 241it [02:28,  1.61it/s]Extractor Estimating: 242it [02:29,  1.66it/s]Extractor Estimating: 243it [02:30,  1.61it/s]Extractor Estimating: 244it [02:30,  1.60it/s]Extractor Estimating: 245it [02:31,  1.57it/s]Extractor Estimating: 246it [02:32,  1.57it/s]Extractor Estimating: 247it [02:32,  1.54it/s]Extractor Estimating: 248it [02:33,  1.54it/s]Extractor Estimating: 249it [02:34,  1.57it/s]Extractor Estimating: 250it [02:34,  1.55it/s]Extractor Estimating: 251it [02:35,  1.58it/s]Extractor Estimating: 252it [02:35,  1.67it/s]Extractor Estimating: 253it [02:36,  1.64it/s]Extractor Estimating: 254it [02:37,  1.65it/s]Extractor Estimating: 255it [02:37,  1.65it/s]Extractor Estimating: 256it [02:38,  1.65it/s]Extractor Estimating: 257it [02:38,  1.66it/s]Extractor Estimating: 258it [02:39,  1.66it/s]Extractor Estimating: 259it [02:40,  1.59it/s]Extractor Estimating: 260it [02:40,  1.61it/s]Extractor Estimating: 261it [02:41,  1.59it/s]Extractor Estimating: 262it [02:42,  1.61it/s]Extractor Estimating: 263it [02:42,  1.58it/s]Extractor Estimating: 264it [02:43,  1.62it/s]Extractor Estimating: 265it [02:43,  1.61it/s]Extractor Estimating: 266it [02:44,  1.65it/s]Extractor Estimating: 267it [02:45,  1.68it/s]Extractor Estimating: 268it [02:45,  1.69it/s]Extractor Estimating: 269it [02:46,  1.69it/s]Extractor Estimating: 270it [02:46,  1.69it/s]Extractor Estimating: 271it [02:47,  1.66it/s]Extractor Estimating: 272it [02:48,  1.67it/s]Extractor Estimating: 273it [02:48,  1.57it/s]Extractor Estimating: 274it [02:49,  1.62it/s]Extractor Estimating: 275it [02:49,  1.65it/s]Extractor Estimating: 276it [02:50,  1.68it/s]Extractor Estimating: 277it [02:51,  1.72it/s]Extractor Estimating: 278it [02:51,  1.73it/s]Extractor Estimating: 279it [02:52,  1.71it/s]Extractor Estimating: 280it [02:52,  1.66it/s]Extractor Estimating: 281it [02:53,  1.65it/s]Extractor Estimating: 282it [02:54,  1.68it/s]Extractor Estimating: 283it [02:54,  1.66it/s]Extractor Estimating: 284it [02:55,  1.65it/s]Extractor Estimating: 285it [02:55,  1.61it/s]Extractor Estimating: 286it [02:56,  1.47it/s]Extractor Estimating: 287it [02:57,  1.52it/s]Extractor Estimating: 288it [02:57,  1.56it/s]Extractor Estimating: 289it [02:58,  1.56it/s]Extractor Estimating: 290it [02:59,  1.59it/s]Extractor Estimating: 291it [02:59,  1.66it/s]Extractor Estimating: 292it [03:00,  1.64it/s]Extractor Estimating: 293it [03:00,  1.64it/s]Extractor Estimating: 294it [03:01,  1.65it/s]Extractor Estimating: 295it [03:02,  1.61it/s]Extractor Estimating: 296it [03:02,  1.67it/s]Extractor Estimating: 297it [03:03,  1.65it/s]Extractor Estimating: 298it [03:04,  1.60it/s]Extractor Estimating: 299it [03:04,  1.59it/s]Extractor Estimating: 300it [03:05,  1.61it/s]Extractor Estimating: 301it [03:05,  1.71it/s]Extractor Estimating: 302it [03:06,  1.73it/s]Extractor Estimating: 303it [03:06,  1.75it/s]Extractor Estimating: 304it [03:07,  1.66it/s]Extractor Estimating: 305it [03:08,  1.73it/s]Extractor Estimating: 306it [03:08,  1.77it/s]Extractor Estimating: 307it [03:09,  1.75it/s]Extractor Estimating: 308it [03:09,  1.70it/s]Extractor Estimating: 309it [03:10,  1.76it/s]Extractor Estimating: 310it [03:11,  1.69it/s]Extractor Estimating: 311it [03:11,  1.65it/s]Extractor Estimating: 312it [03:12,  1.66it/s]Extractor Estimating: 313it [03:12,  1.70it/s]Extractor Estimating: 314it [03:13,  1.71it/s]Extractor Estimating: 315it [03:13,  1.73it/s]Extractor Estimating: 316it [03:14,  1.72it/s]Extractor Estimating: 317it [03:15,  1.64it/s]Extractor Estimating: 318it [03:15,  1.68it/s]Extractor Estimating: 319it [03:16,  1.74it/s]Extractor Estimating: 320it [03:16,  1.72it/s]Extractor Estimating: 321it [03:17,  1.72it/s]Extractor Estimating: 322it [03:18,  1.65it/s]Extractor Estimating: 323it [03:18,  1.71it/s]Extractor Estimating: 324it [03:19,  1.77it/s]Extractor Estimating: 325it [03:19,  1.82it/s]Extractor Estimating: 326it [03:20,  1.83it/s]Extractor Estimating: 327it [03:20,  1.69it/s]Extractor Estimating: 328it [03:21,  1.70it/s]Extractor Estimating: 329it [03:22,  1.76it/s]Extractor Estimating: 330it [03:22,  1.79it/s]Extractor Estimating: 331it [03:23,  1.79it/s]Extractor Estimating: 332it [03:23,  1.84it/s]Extractor Estimating: 333it [03:24,  1.69it/s]Extractor Estimating: 334it [03:25,  1.63it/s]Extractor Estimating: 335it [03:25,  1.61it/s]Extractor Estimating: 336it [03:26,  1.64it/s]Extractor Estimating: 337it [03:26,  1.68it/s]Extractor Estimating: 338it [03:27,  1.67it/s]Extractor Estimating: 339it [03:27,  1.69it/s]Extractor Estimating: 340it [03:28,  1.73it/s]Extractor Estimating: 341it [03:29,  1.66it/s]Extractor Estimating: 342it [03:29,  1.60it/s]Extractor Estimating: 343it [03:30,  1.62it/s]Extractor Estimating: 344it [03:31,  1.63it/s]Extractor Estimating: 345it [03:31,  1.69it/s]Extractor Estimating: 346it [03:32,  1.74it/s]Extractor Estimating: 347it [03:32,  1.76it/s]Extractor Estimating: 348it [03:33,  1.67it/s]Extractor Estimating: 349it [03:34,  1.57it/s]Extractor Estimating: 350it [03:34,  1.57it/s]Extractor Estimating: 351it [03:35,  1.55it/s]Extractor Estimating: 352it [03:36,  1.57it/s]Extractor Estimating: 353it [03:36,  1.64it/s]Extractor Estimating: 354it [03:37,  1.65it/s]Extractor Estimating: 355it [03:37,  1.65it/s]Extractor Estimating: 356it [03:38,  1.59it/s]Extractor Estimating: 357it [03:39,  1.63it/s]Extractor Estimating: 358it [03:39,  1.58it/s]Extractor Estimating: 359it [03:40,  1.64it/s]Extractor Estimating: 360it [03:41,  1.55it/s]Extractor Estimating: 361it [03:41,  1.58it/s]Extractor Estimating: 362it [03:42,  1.58it/s]Extractor Estimating: 363it [03:42,  1.60it/s]Extractor Estimating: 364it [03:43,  1.66it/s]Extractor Estimating: 365it [03:43,  1.73it/s]Extractor Estimating: 366it [03:44,  1.72it/s]Extractor Estimating: 367it [03:45,  1.70it/s]Extractor Estimating: 368it [03:45,  1.61it/s]Extractor Estimating: 369it [03:46,  1.67it/s]Extractor Estimating: 370it [03:46,  1.67it/s]Extractor Estimating: 371it [03:47,  1.66it/s]Extractor Estimating: 372it [03:48,  1.64it/s]Extractor Estimating: 373it [03:48,  1.59it/s]Extractor Estimating: 374it [03:49,  1.65it/s]Extractor Estimating: 375it [03:49,  1.67it/s]Extractor Estimating: 376it [03:50,  1.63it/s]Extractor Estimating: 377it [03:51,  1.56it/s]Extractor Estimating: 378it [03:52,  1.40it/s]Extractor Estimating: 379it [03:52,  1.44it/s]Extractor Estimating: 380it [03:53,  1.54it/s]Extractor Estimating: 381it [03:54,  1.51it/s]Extractor Estimating: 382it [03:54,  1.53it/s]Extractor Estimating: 383it [03:55,  1.53it/s]Extractor Estimating: 384it [03:56,  1.53it/s]Extractor Estimating: 385it [03:56,  1.55it/s]Extractor Estimating: 386it [03:57,  1.57it/s]Extractor Estimating: 387it [03:58,  1.51it/s]Extractor Estimating: 388it [03:58,  1.55it/s]Extractor Estimating: 389it [03:59,  1.59it/s]Extractor Estimating: 390it [03:59,  1.51it/s]Extractor Estimating: 391it [04:00,  1.53it/s]Extractor Estimating: 392it [04:01,  1.58it/s]Extractor Estimating: 393it [04:01,  1.61it/s]Extractor Estimating: 394it [04:02,  1.61it/s]Extractor Estimating: 395it [04:02,  1.70it/s]Extractor Estimating: 396it [04:03,  1.71it/s]Extractor Estimating: 397it [04:04,  1.66it/s]Extractor Estimating: 398it [04:04,  1.68it/s]Extractor Estimating: 399it [04:05,  1.63it/s]Extractor Estimating: 400it [04:05,  1.62it/s]Extractor Estimating: 401it [04:06,  1.61it/s]Extractor Estimating: 402it [04:07,  1.53it/s]Extractor Estimating: 403it [04:07,  1.63it/s]Extractor Estimating: 404it [04:08,  1.68it/s]Extractor Estimating: 405it [04:09,  1.64it/s]Extractor Estimating: 406it [04:09,  1.69it/s]Extractor Estimating: 407it [04:10,  1.68it/s]Extractor Estimating: 408it [04:10,  1.75it/s]Extractor Estimating: 409it [04:11,  1.74it/s]Extractor Estimating: 410it [04:11,  1.66it/s]Extractor Estimating: 411it [04:12,  1.75it/s]Extractor Estimating: 412it [04:13,  1.81it/s]Extractor Estimating: 413it [04:13,  1.73it/s]Extractor Estimating: 414it [04:14,  1.76it/s]Extractor Estimating: 415it [04:14,  1.72it/s]Extractor Estimating: 416it [04:15,  1.71it/s]Extractor Estimating: 417it [04:15,  1.71it/s]Extractor Estimating: 418it [04:16,  1.70it/s]Extractor Estimating: 419it [04:17,  1.76it/s]Extractor Estimating: 420it [04:17,  1.75it/s]Extractor Estimating: 421it [04:18,  1.71it/s]Extractor Estimating: 422it [04:18,  1.69it/s]Extractor Estimating: 423it [04:19,  1.73it/s]Extractor Estimating: 424it [04:19,  1.75it/s]Extractor Estimating: 425it [04:20,  1.64it/s]Extractor Estimating: 426it [04:21,  1.65it/s]Extractor Estimating: 427it [04:21,  1.63it/s]Extractor Estimating: 428it [04:22,  1.62it/s]Extractor Estimating: 429it [04:23,  1.66it/s]Extractor Estimating: 430it [04:23,  1.64it/s]Extractor Estimating: 431it [04:24,  1.62it/s]Extractor Estimating: 432it [04:25,  1.61it/s]Extractor Estimating: 433it [04:25,  1.62it/s]Extractor Estimating: 434it [04:26,  1.64it/s]Extractor Estimating: 435it [04:26,  1.64it/s]Extractor Estimating: 436it [04:27,  1.64it/s]Extractor Estimating: 437it [04:28,  1.60it/s]Extractor Estimating: 438it [04:28,  1.61it/s]Extractor Estimating: 439it [04:29,  1.59it/s]Extractor Estimating: 440it [04:30,  1.57it/s]Extractor Estimating: 441it [04:30,  1.61it/s]Extractor Estimating: 442it [04:31,  1.56it/s]Extractor Estimating: 443it [04:31,  1.55it/s]Extractor Estimating: 444it [04:32,  1.56it/s]Extractor Estimating: 445it [04:33,  1.58it/s]Extractor Estimating: 446it [04:33,  1.58it/s]Extractor Estimating: 447it [04:34,  1.53it/s]Extractor Estimating: 448it [04:35,  1.60it/s]Extractor Estimating: 449it [04:35,  1.67it/s]Extractor Estimating: 450it [04:36,  1.69it/s]Extractor Estimating: 451it [04:36,  1.73it/s]Extractor Estimating: 452it [04:37,  1.76it/s]Extractor Estimating: 453it [04:37,  1.75it/s]Extractor Estimating: 454it [04:38,  1.75it/s]Extractor Estimating: 455it [04:38,  1.81it/s]Extractor Estimating: 456it [04:39,  1.81it/s]Extractor Estimating: 457it [04:40,  1.83it/s]Extractor Estimating: 458it [04:40,  1.82it/s]Extractor Estimating: 459it [04:41,  1.63it/s]Extractor Estimating: 460it [04:41,  1.64it/s]Extractor Estimating: 461it [04:42,  1.69it/s]Extractor Estimating: 462it [04:43,  1.70it/s]Extractor Estimating: 463it [04:43,  1.75it/s]Extractor Estimating: 464it [04:44,  1.75it/s]Extractor Estimating: 465it [04:44,  1.74it/s]Extractor Estimating: 466it [04:45,  1.70it/s]Extractor Estimating: 467it [04:45,  1.72it/s]Extractor Estimating: 468it [04:46,  1.72it/s]Extractor Estimating: 469it [04:47,  1.72it/s]Extractor Estimating: 470it [04:47,  1.78it/s]Extractor Estimating: 471it [04:48,  1.74it/s]Extractor Estimating: 472it [04:48,  1.73it/s]Extractor Estimating: 473it [04:49,  1.70it/s]Extractor Estimating: 474it [04:49,  1.74it/s]Extractor Estimating: 475it [04:50,  1.67it/s]Extractor Estimating: 476it [04:51,  1.59it/s]Extractor Estimating: 477it [04:51,  1.62it/s]Extractor Estimating: 478it [04:52,  1.63it/s]Extractor Estimating: 479it [04:53,  1.68it/s]Extractor Estimating: 480it [04:53,  1.65it/s]Extractor Estimating: 481it [04:54,  1.65it/s]Extractor Estimating: 482it [04:54,  1.62it/s]Extractor Estimating: 483it [04:55,  1.61it/s]Extractor Estimating: 484it [04:56,  1.58it/s]Extractor Estimating: 485it [04:56,  1.51it/s]Extractor Estimating: 486it [04:57,  1.54it/s]Extractor Estimating: 487it [04:58,  1.54it/s]Extractor Estimating: 488it [04:58,  1.55it/s]Extractor Estimating: 489it [04:59,  1.61it/s]Extractor Estimating: 490it [05:00,  1.60it/s]Extractor Estimating: 491it [05:00,  1.64it/s]Extractor Estimating: 492it [05:01,  1.68it/s]Extractor Estimating: 493it [05:01,  1.58it/s]Extractor Estimating: 494it [05:02,  1.58it/s]Extractor Estimating: 495it [05:03,  1.68it/s]Extractor Estimating: 496it [05:03,  1.70it/s]Extractor Estimating: 497it [05:04,  1.69it/s]Extractor Estimating: 498it [05:04,  1.72it/s]Extractor Estimating: 499it [05:05,  1.68it/s]Extractor Estimating: 500it [05:05,  1.83it/s]Extractor Estimating: 500it [05:05,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:24,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:24,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:24,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:24,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:24,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:06:24,579 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:06:24,580 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:06:25,313 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:06:26,482 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:06:26,483 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:30,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:30,126 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:30,126 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:30,126 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:06:30,126 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:06:31,061 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:06:31,062 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:06:31,786 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:06:32,023 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:06:32,023 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:56:25,226 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:56:25,399 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9982 mean pseudo reward: 0.9413834444720666
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 25423
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25523, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25523, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.980, loss:665.9532
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.990, loss:657.3449
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.998, loss:633.3634
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.972, loss:632.3208
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.979, loss:568.3876
>> valid entity prec:0.5942, rec:0.4613, f1:0.5194
>> valid relation prec:0.3128, rec:0.1007, f1:0.1523
>> valid relation with NER prec:0.3128, rec:0.1007, f1:0.1523
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.264, loss:627.4786
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.994, loss:634.5487
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.000, loss:653.1953
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.992, loss:609.5089
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.975, loss:623.8626
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5400, rec:0.5653, f1:0.5523
>> valid relation prec:0.2962, rec:0.1508, f1:0.1999
>> valid relation with NER prec:0.2962, rec:0.1508, f1:0.1999
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.274, loss:645.7790
g_step 1200, step 368, avg_time 0.999, loss:666.4845
g_step 1300, step 52, avg_time 0.992, loss:602.8790
g_step 1400, step 152, avg_time 0.991, loss:585.5987
g_step 1500, step 252, avg_time 0.992, loss:616.7590
>> valid entity prec:0.5442, rec:0.5482, f1:0.5462
>> valid relation prec:0.2547, rec:0.1095, f1:0.1532
>> valid relation with NER prec:0.2547, rec:0.1095, f1:0.1532
g_step 1600, step 352, avg_time 2.259, loss:630.7464
g_step 1700, step 36, avg_time 0.992, loss:614.6724
g_step 1800, step 136, avg_time 0.978, loss:559.3712
g_step 1900, step 236, avg_time 0.987, loss:593.4508
g_step 2000, step 336, avg_time 0.998, loss:608.5728
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5870, rec:0.5279, f1:0.5559
>> valid relation prec:0.3317, rec:0.1187, f1:0.1749
>> valid relation with NER prec:0.3317, rec:0.1187, f1:0.1749
new max entity f1 on valid!
g_step 2100, step 20, avg_time 2.255, loss:561.4544
g_step 2200, step 120, avg_time 0.978, loss:538.5142
g_step 2300, step 220, avg_time 0.988, loss:576.6173
g_step 2400, step 320, avg_time 0.999, loss:553.9320
g_step 2500, step 4, avg_time 0.981, loss:567.2011
>> valid entity prec:0.5409, rec:0.5623, f1:0.5514
>> valid relation prec:0.3033, rec:0.1334, f1:0.1853
>> valid relation with NER prec:0.3033, rec:0.1334, f1:0.1853
g_step 2600, step 104, avg_time 2.243, loss:513.0459
g_step 2700, step 204, avg_time 0.990, loss:510.4302
g_step 2800, step 304, avg_time 0.990, loss:553.8727
g_step 2900, step 404, avg_time 0.989, loss:558.7748
g_step 3000, step 88, avg_time 0.991, loss:510.3043
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5279, rec:0.5230, f1:0.5254
>> valid relation prec:0.2536, rec:0.1273, f1:0.1695
>> valid relation with NER prec:0.2536, rec:0.1273, f1:0.1695
g_step 3100, step 188, avg_time 2.261, loss:517.1510
g_step 3200, step 288, avg_time 0.983, loss:517.7730
g_step 3300, step 388, avg_time 0.986, loss:519.9795
g_step 3400, step 72, avg_time 0.987, loss:474.9835
g_step 3500, step 172, avg_time 0.981, loss:500.4449
>> valid entity prec:0.5499, rec:0.4986, f1:0.5230
>> valid relation prec:0.2583, rec:0.1161, f1:0.1602
>> valid relation with NER prec:0.2583, rec:0.1161, f1:0.1602
g_step 3600, step 272, avg_time 2.245, loss:484.7794
g_step 3700, step 372, avg_time 0.977, loss:527.8538
g_step 3800, step 56, avg_time 1.001, loss:500.5220
g_step 3900, step 156, avg_time 0.986, loss:475.7185
g_step 4000, step 256, avg_time 0.987, loss:473.0067
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5276, rec:0.5431, f1:0.5353
>> valid relation prec:0.2735, rec:0.1391, f1:0.1844
>> valid relation with NER prec:0.2735, rec:0.1391, f1:0.1844
g_step 4100, step 356, avg_time 2.237, loss:485.9862
g_step 4200, step 40, avg_time 0.983, loss:452.9983
g_step 4300, step 140, avg_time 0.982, loss:450.2324
g_step 4400, step 240, avg_time 0.985, loss:486.2149
g_step 4500, step 340, avg_time 0.998, loss:467.1168
>> valid entity prec:0.5685, rec:0.4631, f1:0.5104
>> valid relation prec:0.3103, rec:0.0966, f1:0.1474
>> valid relation with NER prec:0.3103, rec:0.0966, f1:0.1474
g_step 4600, step 24, avg_time 2.240, loss:480.8129
g_step 4700, step 124, avg_time 0.974, loss:417.4122
g_step 4800, step 224, avg_time 0.981, loss:441.4146
g_step 4900, step 324, avg_time 0.997, loss:448.6808
g_step 5000, step 8, avg_time 0.988, loss:458.5115
learning rate was adjusted to 0.0008
>> valid entity prec:0.5534, rec:0.5140, f1:0.5330
>> valid relation prec:0.2715, rec:0.1256, f1:0.1718
>> valid relation with NER prec:0.2715, rec:0.1256, f1:0.1718
g_step 5100, step 108, avg_time 2.261, loss:424.1268
g_step 5200, step 208, avg_time 0.988, loss:419.4066
g_step 5300, step 308, avg_time 0.980, loss:439.5748
g_step 5400, step 408, avg_time 0.978, loss:473.6223
g_step 5500, step 92, avg_time 0.977, loss:395.8619
>> valid entity prec:0.5754, rec:0.4932, f1:0.5311
>> valid relation prec:0.2978, rec:0.1305, f1:0.1815
>> valid relation with NER prec:0.2978, rec:0.1305, f1:0.1815
g_step 5600, step 192, avg_time 2.248, loss:418.8468
g_step 5700, step 292, avg_time 0.989, loss:436.7312
g_step 5800, step 392, avg_time 0.987, loss:420.7512
g_step 5900, step 76, avg_time 0.990, loss:411.2898
g_step 6000, step 176, avg_time 0.990, loss:394.6586
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5356, rec:0.5215, f1:0.5285
>> valid relation prec:0.2696, rec:0.1342, f1:0.1792
>> valid relation with NER prec:0.2696, rec:0.1342, f1:0.1792
g_step 6100, step 276, avg_time 2.245, loss:404.3301
g_step 6200, step 376, avg_time 0.987, loss:424.9613
g_step 6300, step 60, avg_time 0.987, loss:391.2761
g_step 6400, step 160, avg_time 0.980, loss:376.6839
g_step 6500, step 260, avg_time 0.979, loss:399.0956
>> valid entity prec:0.5524, rec:0.5021, f1:0.5260
>> valid relation prec:0.2799, rec:0.1233, f1:0.1712
>> valid relation with NER prec:0.2799, rec:0.1233, f1:0.1712
g_step 6600, step 360, avg_time 2.229, loss:411.8908
g_step 6700, step 44, avg_time 0.998, loss:392.5569
g_step 6800, step 144, avg_time 0.971, loss:368.8149
g_step 6900, step 244, avg_time 0.990, loss:380.1248
g_step 7000, step 344, avg_time 0.987, loss:387.2416
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5421, rec:0.5051, f1:0.5230
>> valid relation prec:0.2575, rec:0.1276, f1:0.1707
>> valid relation with NER prec:0.2575, rec:0.1276, f1:0.1707
g_step 7100, step 28, avg_time 2.262, loss:372.8855
g_step 7200, step 128, avg_time 0.987, loss:368.7688
g_step 7300, step 228, avg_time 0.951, loss:362.6482
g_step 7400, step 328, avg_time 0.947, loss:378.5294
g_step 7500, step 12, avg_time 0.933, loss:362.1685
>> valid entity prec:0.5487, rec:0.5025, f1:0.5246
>> valid relation prec:0.2927, rec:0.1434, f1:0.1925
>> valid relation with NER prec:0.2927, rec:0.1434, f1:0.1925
g_step 7600, step 112, avg_time 2.178, loss:354.8432
g_step 7700, step 212, avg_time 0.946, loss:350.1685
g_step 7800, step 312, avg_time 0.958, loss:363.8370
g_step 7900, step 412, avg_time 0.956, loss:378.5230
g_step 8000, step 96, avg_time 0.964, loss:329.0485
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5539, rec:0.5244, f1:0.5388
>> valid relation prec:0.2695, rec:0.1379, f1:0.1825
>> valid relation with NER prec:0.2695, rec:0.1379, f1:0.1825
g_step 8100, step 196, avg_time 2.173, loss:327.6060
g_step 8200, step 296, avg_time 0.948, loss:350.4430
g_step 8300, step 396, avg_time 0.950, loss:348.9564
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:56:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:56:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-56-25_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:56:26 - WARNING - datasets.builder -   Using custom data configuration default-c9d0c7b5c3fb8004
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c9d0c7b5c3fb8004/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.37 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:56:29,222 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:56:29,224 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:56:29,224 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:56:29,225 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:56:29,357 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:56:29,417 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:56:30,078 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:56:33,142 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:56:33,159 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c9d0c7b5c3fb8004/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.29ba/s] 20%|        | 2/10 [00:00<00:02,  3.33ba/s] 30%|       | 3/10 [00:00<00:01,  3.88ba/s] 40%|      | 4/10 [00:01<00:01,  4.19ba/s] 50%|     | 5/10 [00:01<00:01,  4.35ba/s] 60%|    | 6/10 [00:01<00:00,  4.49ba/s] 70%|   | 7/10 [00:01<00:00,  4.58ba/s] 80%|  | 8/10 [00:01<00:00,  4.63ba/s] 90%| | 9/10 [00:02<00:00,  4.68ba/s]100%|| 10/10 [00:02<00:00,  4.69ba/s]100%|| 10/10 [00:02<00:00,  4.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  3.12ba/s] 50%|     | 2/4 [00:00<00:00,  3.80ba/s] 75%|  | 3/4 [00:00<00:00,  4.10ba/s]100%|| 4/4 [00:00<00:00,  5.23ba/s]100%|| 4/4 [00:00<00:00,  4.57ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  5.03ba/s] 30%|       | 3/10 [00:00<00:00,  8.51ba/s] 50%|     | 5/10 [00:00<00:00,  9.77ba/s] 70%|   | 7/10 [00:00<00:00, 10.32ba/s] 90%| | 9/10 [00:01<00:00,  8.36ba/s]100%|| 10/10 [00:01<00:00,  8.45ba/s]100%|| 10/10 [00:01<00:00,  8.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  6.63ba/s] 75%|  | 3/4 [00:00<00:00,  9.62ba/s]100%|| 4/4 [00:00<00:00, 10.79ba/s]
[INFO|trainer.py:414] 2023-08-29 03:56:38,869 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:56:38,959 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:56:38,959 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 03:56:38,959 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:56:38,959 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:56:38,959 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:56:38,959 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:56:38,959 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<08:46,  1.48it/s]  0%|          | 2/780 [00:00<05:48,  2.23it/s]  0%|          | 3/780 [00:01<04:50,  2.67it/s]  1%|          | 4/780 [00:01<04:24,  2.94it/s]  1%|          | 5/780 [00:01<04:09,  3.11it/s]  1%|          | 6/780 [00:02<03:59,  3.23it/s]  1%|          | 7/780 [00:02<03:53,  3.31it/s]  1%|          | 8/780 [00:02<03:49,  3.36it/s]  1%|          | 9/780 [00:02<03:47,  3.39it/s]  1%|         | 10/780 [00:03<03:45,  3.42it/s]  1%|         | 11/780 [00:03<03:43,  3.44it/s]  2%|         | 12/780 [00:03<03:42,  3.45it/s]  2%|         | 13/780 [00:04<03:41,  3.46it/s]  2%|         | 14/780 [00:04<03:49,  3.33it/s]  2%|         | 15/780 [00:04<03:46,  3.37it/s]  2%|         | 16/780 [00:05<03:44,  3.41it/s]  2%|         | 17/780 [00:05<03:42,  3.43it/s]  2%|         | 18/780 [00:05<03:41,  3.44it/s]  2%|         | 19/780 [00:05<03:40,  3.45it/s]  3%|         | 20/780 [00:06<03:39,  3.46it/s]  3%|         | 21/780 [00:06<03:39,  3.46it/s]  3%|         | 22/780 [00:06<03:38,  3.46it/s]  3%|         | 23/780 [00:07<03:38,  3.47it/s]  3%|         | 24/780 [00:07<03:38,  3.47it/s]  3%|         | 25/780 [00:07<03:37,  3.47it/s]  3%|         | 26/780 [00:07<03:37,  3.47it/s]  3%|         | 27/780 [00:08<03:37,  3.47it/s]  4%|         | 28/780 [00:08<03:36,  3.47it/s]  4%|         | 29/780 [00:08<03:36,  3.47it/s]  4%|         | 30/780 [00:09<03:36,  3.47it/s]  4%|         | 31/780 [00:09<03:35,  3.47it/s]  4%|         | 32/780 [00:09<03:40,  3.40it/s]  4%|         | 33/780 [00:09<03:38,  3.42it/s]  4%|         | 34/780 [00:10<03:37,  3.43it/s]  4%|         | 35/780 [00:10<03:36,  3.44it/s]  5%|         | 36/780 [00:10<03:35,  3.45it/s]  5%|         | 37/780 [00:11<03:35,  3.45it/s]  5%|         | 38/780 [00:11<03:34,  3.46it/s]  5%|         | 39/780 [00:11<03:34,  3.46it/s]  5%|         | 40/780 [00:11<03:33,  3.46it/s]  5%|         | 41/780 [00:12<03:33,  3.47it/s]  5%|         | 42/780 [00:12<03:32,  3.47it/s]  6%|         | 43/780 [00:12<03:32,  3.47it/s]  6%|         | 44/780 [00:13<03:32,  3.47it/s]  6%|         | 45/780 [00:13<03:31,  3.47it/s]  6%|         | 46/780 [00:13<03:31,  3.47it/s]  6%|         | 47/780 [00:13<03:31,  3.47it/s]  6%|         | 48/780 [00:14<03:30,  3.47it/s]  6%|         | 49/780 [00:14<03:35,  3.39it/s]  6%|         | 50/780 [00:14<03:33,  3.41it/s]  7%|         | 51/780 [00:15<03:32,  3.43it/s]  7%|         | 52/780 [00:15<03:31,  3.44it/s]  7%|         | 53/780 [00:15<03:30,  3.45it/s]  7%|         | 54/780 [00:16<03:30,  3.46it/s]  7%|         | 55/780 [00:16<03:29,  3.46it/s]  7%|         | 56/780 [00:16<03:29,  3.46it/s]  7%|         | 57/780 [00:16<03:28,  3.46it/s]  7%|         | 58/780 [00:17<03:28,  3.46it/s]  8%|         | 59/780 [00:17<03:29,  3.45it/s]  8%|         | 60/780 [00:17<03:29,  3.43it/s]  8%|         | 61/780 [00:18<03:29,  3.43it/s]  8%|         | 62/780 [00:18<03:29,  3.43it/s]  8%|         | 63/780 [00:18<03:29,  3.42it/s]  8%|         | 64/780 [00:18<03:29,  3.42it/s]  8%|         | 65/780 [00:19<03:29,  3.42it/s]  8%|         | 66/780 [00:19<03:29,  3.41it/s]  9%|         | 67/780 [00:19<03:34,  3.33it/s]  9%|         | 68/780 [00:20<03:32,  3.36it/s]  9%|         | 69/780 [00:20<03:30,  3.37it/s]  9%|         | 70/780 [00:20<03:29,  3.38it/s]  9%|         | 71/780 [00:21<03:28,  3.40it/s]  9%|         | 72/780 [00:21<03:28,  3.40it/s]  9%|         | 73/780 [00:21<03:27,  3.40it/s]  9%|         | 74/780 [00:21<03:26,  3.41it/s] 10%|         | 75/780 [00:22<03:26,  3.41it/s] 10%|         | 76/780 [00:22<03:26,  3.41it/s] 10%|         | 77/780 [00:22<03:25,  3.42it/s] 10%|         | 78/780 [00:23<03:25,  3.41it/s] 10%|         | 79/780 [00:23<03:25,  3.41it/s] 10%|         | 80/780 [00:23<03:25,  3.41it/s] 10%|         | 81/780 [00:23<03:24,  3.41it/s] 11%|         | 82/780 [00:24<03:24,  3.41it/s] 11%|         | 83/780 [00:24<03:24,  3.41it/s] 11%|         | 84/780 [00:24<03:30,  3.31it/s] 11%|         | 85/780 [00:25<03:28,  3.34it/s] 11%|         | 86/780 [00:25<03:26,  3.36it/s] 11%|         | 87/780 [00:25<03:25,  3.37it/s] 11%|        | 88/780 [00:26<03:24,  3.38it/s] 11%|        | 89/780 [00:26<03:23,  3.39it/s] 12%|        | 90/780 [00:26<03:23,  3.40it/s] 12%|        | 91/780 [00:26<03:35,  3.20it/s] 12%|        | 92/780 [00:27<03:31,  3.26it/s] 12%|        | 93/780 [00:27<03:27,  3.31it/s] 12%|        | 94/780 [00:27<03:25,  3.34it/s] 12%|        | 95/780 [00:28<03:24,  3.36it/s] 12%|        | 96/780 [00:28<03:22,  3.37it/s] 12%|        | 97/780 [00:28<03:21,  3.39it/s] 13%|        | 98/780 [00:29<03:21,  3.39it/s] 13%|        | 99/780 [00:29<03:20,  3.39it/s] 13%|        | 100/780 [00:29<03:20,  3.40it/s] 13%|        | 101/780 [00:29<03:27,  3.27it/s] 13%|        | 102/780 [00:30<03:24,  3.31it/s] 13%|        | 103/780 [00:30<03:22,  3.34it/s] 13%|        | 104/780 [00:30<03:21,  3.36it/s] 13%|        | 105/780 [00:31<03:20,  3.37it/s] 14%|        | 106/780 [00:31<03:19,  3.39it/s] 14%|        | 107/780 [00:31<03:18,  3.39it/s] 14%|        | 108/780 [00:32<03:27,  3.23it/s] 14%|        | 109/780 [00:32<03:24,  3.28it/s] 14%|        | 110/780 [00:32<03:21,  3.32it/s] 14%|        | 111/780 [00:32<03:19,  3.35it/s] 14%|        | 112/780 [00:33<03:18,  3.36it/s] 14%|        | 113/780 [00:33<03:17,  3.38it/s] 15%|        | 114/780 [00:33<03:16,  3.39it/s] 15%|        | 115/780 [00:34<03:16,  3.39it/s] 15%|        | 116/780 [00:34<03:15,  3.40it/s] 15%|        | 117/780 [00:34<03:14,  3.40it/s] 15%|        | 118/780 [00:34<03:14,  3.40it/s] 15%|        | 119/780 [00:35<03:19,  3.31it/s] 15%|        | 120/780 [00:35<03:17,  3.34it/s] 16%|        | 121/780 [00:35<03:15,  3.36it/s] 16%|        | 122/780 [00:36<03:15,  3.37it/s] 16%|        | 123/780 [00:36<03:14,  3.39it/s] 16%|        | 124/780 [00:36<03:13,  3.39it/s] 16%|        | 125/780 [00:37<03:12,  3.40it/s] 16%|        | 126/780 [00:37<03:12,  3.40it/s] 16%|        | 127/780 [00:37<03:11,  3.40it/s] 16%|        | 128/780 [00:37<03:11,  3.41it/s] 17%|        | 129/780 [00:38<03:11,  3.41it/s] 17%|        | 130/780 [00:38<03:10,  3.41it/s] 17%|        | 131/780 [00:38<03:10,  3.41it/s] 17%|        | 132/780 [00:39<03:10,  3.41it/s] 17%|        | 133/780 [00:39<03:09,  3.41it/s] 17%|        | 134/780 [00:39<03:09,  3.41it/s] 17%|        | 135/780 [00:39<03:09,  3.41it/s] 17%|        | 136/780 [00:40<03:13,  3.32it/s] 18%|        | 137/780 [00:40<03:12,  3.35it/s] 18%|        | 138/780 [00:40<03:10,  3.37it/s] 18%|        | 139/780 [00:41<03:10,  3.37it/s] 18%|        | 140/780 [00:41<03:09,  3.38it/s] 18%|        | 141/780 [00:41<03:08,  3.39it/s] 18%|        | 142/780 [00:42<03:07,  3.39it/s] 18%|        | 143/780 [00:42<03:07,  3.40it/s] 18%|        | 144/780 [00:42<03:07,  3.40it/s] 19%|        | 145/780 [00:42<03:06,  3.40it/s] 19%|        | 146/780 [00:43<03:06,  3.40it/s] 19%|        | 147/780 [00:43<03:06,  3.40it/s] 19%|        | 148/780 [00:43<03:05,  3.40it/s] 19%|        | 149/780 [00:44<03:11,  3.30it/s] 19%|        | 150/780 [00:44<03:09,  3.33it/s] 19%|        | 151/780 [00:44<03:07,  3.35it/s] 19%|        | 152/780 [00:45<03:06,  3.37it/s] 20%|        | 153/780 [00:45<03:10,  3.29it/s] 20%|        | 154/780 [00:45<03:08,  3.32it/s] 20%|        | 155/780 [00:45<03:06,  3.35it/s] 20%|        | 156/780 [00:46<03:05,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 03:57:25,714 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:57:25,714 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 03:57:25,715 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.21it/s][A
  3%|         | 12/437 [00:00<00:08, 49.96it/s][A
  4%|         | 18/437 [00:00<00:08, 47.99it/s][A
  5%|         | 23/437 [00:00<00:09, 42.99it/s][A
  6%|         | 28/437 [00:00<00:09, 43.87it/s][A
  8%|         | 33/437 [00:00<00:09, 44.32it/s][A
  9%|         | 38/437 [00:00<00:08, 44.73it/s][A
 10%|         | 43/437 [00:00<00:08, 44.62it/s][A
 11%|         | 48/437 [00:01<00:08, 44.77it/s][A
 12%|        | 53/437 [00:01<00:08, 44.99it/s][A
 13%|        | 58/437 [00:01<00:08, 45.01it/s][A
 14%|        | 63/437 [00:01<00:08, 44.92it/s][A
 16%|        | 68/437 [00:01<00:08, 44.95it/s][A
 17%|        | 73/437 [00:01<00:08, 45.17it/s][A
 18%|        | 78/437 [00:01<00:07, 45.12it/s][A
 19%|        | 83/437 [00:01<00:07, 45.14it/s][A
 20%|        | 88/437 [00:01<00:07, 45.07it/s][A
 21%|       | 93/437 [00:02<00:07, 45.10it/s][A
 22%|       | 98/437 [00:02<00:07, 45.04it/s][A
 24%|       | 103/437 [00:02<00:07, 45.17it/s][A
 25%|       | 108/437 [00:02<00:07, 45.10it/s][A
 26%|       | 113/437 [00:02<00:07, 45.18it/s][A
 27%|       | 118/437 [00:02<00:07, 45.22it/s][A
 28%|       | 123/437 [00:02<00:06, 45.23it/s][A
 29%|       | 128/437 [00:02<00:06, 45.23it/s][A
 30%|       | 133/437 [00:02<00:06, 45.32it/s][A
 32%|      | 138/437 [00:03<00:06, 45.20it/s][A
 33%|      | 143/437 [00:03<00:06, 45.18it/s][A
 34%|      | 148/437 [00:03<00:06, 45.16it/s][A
 35%|      | 153/437 [00:03<00:06, 45.07it/s][A
 36%|      | 158/437 [00:03<00:06, 43.03it/s][A
 37%|      | 163/437 [00:03<00:06, 43.71it/s][A
 38%|      | 168/437 [00:03<00:06, 44.30it/s][A
 40%|      | 173/437 [00:03<00:05, 44.61it/s][A
 41%|      | 178/437 [00:03<00:05, 44.88it/s][A
 42%|     | 183/437 [00:04<00:05, 44.99it/s][A
 43%|     | 188/437 [00:04<00:05, 45.07it/s][A
 44%|     | 193/437 [00:04<00:05, 45.10it/s][A
 45%|     | 198/437 [00:04<00:05, 44.98it/s][A
 46%|     | 203/437 [00:04<00:05, 44.91it/s][A
 48%|     | 208/437 [00:04<00:05, 45.06it/s][A
 49%|     | 213/437 [00:04<00:04, 45.11it/s][A
 50%|     | 218/437 [00:04<00:04, 45.29it/s][A
 51%|     | 223/437 [00:04<00:04, 45.25it/s][A
 52%|    | 228/437 [00:05<00:04, 45.31it/s][A
 53%|    | 233/437 [00:05<00:04, 45.28it/s][A
 54%|    | 238/437 [00:05<00:04, 45.19it/s][A
 56%|    | 243/437 [00:05<00:04, 45.09it/s][A
 57%|    | 248/437 [00:05<00:04, 44.90it/s][A
 58%|    | 253/437 [00:05<00:04, 45.01it/s][A
 59%|    | 258/437 [00:05<00:03, 45.09it/s][A
 60%|    | 263/437 [00:05<00:03, 45.17it/s][A
 61%|   | 268/437 [00:05<00:03, 45.24it/s][A
 62%|   | 273/437 [00:06<00:03, 45.24it/s][A
 64%|   | 278/437 [00:06<00:03, 45.18it/s][A
 65%|   | 283/437 [00:06<00:03, 45.16it/s][A
 66%|   | 288/437 [00:06<00:03, 45.07it/s][A
 67%|   | 293/437 [00:06<00:03, 44.94it/s][A
 68%|   | 298/437 [00:06<00:03, 44.94it/s][A
 69%|   | 303/437 [00:06<00:02, 45.14it/s][A
 70%|   | 308/437 [00:06<00:02, 45.20it/s][A
 72%|  | 313/437 [00:06<00:02, 45.32it/s][A
 73%|  | 318/437 [00:07<00:02, 45.24it/s][A
 74%|  | 323/437 [00:07<00:02, 45.32it/s][A
 75%|  | 328/437 [00:07<00:02, 45.30it/s][A
 76%|  | 333/437 [00:07<00:02, 45.15it/s][A
 77%|  | 338/437 [00:07<00:02, 45.07it/s][A
 78%|  | 343/437 [00:07<00:02, 45.05it/s][A
 80%|  | 348/437 [00:07<00:01, 45.15it/s][A
 81%|  | 353/437 [00:07<00:01, 45.18it/s][A
 82%| | 358/437 [00:07<00:01, 45.21it/s][A
 83%| | 363/437 [00:08<00:01, 45.25it/s][A
 84%| | 368/437 [00:08<00:01, 45.20it/s][A
 85%| | 373/437 [00:08<00:01, 45.24it/s][A
 86%| | 378/437 [00:08<00:01, 45.05it/s][A
 88%| | 383/437 [00:08<00:01, 45.07it/s][A
 89%| | 388/437 [00:08<00:01, 45.04it/s][A
 90%| | 393/437 [00:08<00:00, 45.11it/s][A
 91%| | 398/437 [00:08<00:00, 44.93it/s][A
 92%|| 403/437 [00:08<00:00, 45.04it/s][A
 93%|| 408/437 [00:09<00:00, 45.09it/s][A
 95%|| 413/437 [00:09<00:00, 45.20it/s][A
 96%|| 418/437 [00:09<00:00, 45.11it/s][A
 97%|| 423/437 [00:09<00:00, 45.10it/s][A
 98%|| 428/437 [00:09<00:00, 45.12it/s][A
 99%|| 433/437 [00:09<00:00, 45.10it/s][A                                                 
                                                 [A 20%|        | 156/780 [00:56<03:05,  3.37it/s]
100%|| 437/437 [00:09<00:00, 45.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:57:35,625 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 03:57:35,745 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:57:38,445 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:57:38,595 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:57:38,685 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:06<1:06:09,  6.37s/it] 20%|        | 158/780 [01:07<47:14,  4.56s/it]   20%|        | 159/780 [01:07<33:55,  3.28s/it] 21%|        | 160/780 [01:07<24:36,  2.38s/it] 21%|        | 161/780 [01:07<18:06,  1.76s/it] 21%|        | 162/780 [01:08<13:33,  1.32s/it] 21%|        | 163/780 [01:08<10:22,  1.01s/it] 21%|        | 164/780 [01:08<08:09,  1.26it/s] 21%|        | 165/780 [01:09<06:36,  1.55it/s] 21%|       | 166/780 [01:09<05:30,  1.86it/s] 21%|       | 167/780 [01:09<04:44,  2.15it/s] 22%|       | 168/780 [01:10<04:12,  2.42it/s] 22%|       | 169/780 [01:10<03:56,  2.59it/s] 22%|       | 170/780 [01:10<03:37,  2.80it/s] 22%|       | 171/780 [01:10<03:24,  2.97it/s] 22%|       | 172/780 [01:11<03:15,  3.10it/s] 22%|       | 173/780 [01:11<03:09,  3.20it/s] 22%|       | 174/780 [01:11<03:05,  3.27it/s] 22%|       | 175/780 [01:12<03:01,  3.33it/s] 23%|       | 176/780 [01:12<02:59,  3.37it/s] 23%|       | 177/780 [01:12<02:57,  3.40it/s] 23%|       | 178/780 [01:12<02:56,  3.41it/s] 23%|       | 179/780 [01:13<02:55,  3.43it/s] 23%|       | 180/780 [01:13<02:58,  3.36it/s] 23%|       | 181/780 [01:13<02:56,  3.39it/s] 23%|       | 182/780 [01:14<02:55,  3.41it/s] 23%|       | 183/780 [01:14<02:53,  3.43it/s] 24%|       | 184/780 [01:14<02:53,  3.44it/s] 24%|       | 185/780 [01:15<02:52,  3.45it/s] 24%|       | 186/780 [01:15<02:52,  3.45it/s] 24%|       | 187/780 [01:15<02:51,  3.45it/s] 24%|       | 188/780 [01:15<02:51,  3.46it/s] 24%|       | 189/780 [01:16<02:50,  3.46it/s] 24%|       | 190/780 [01:16<02:50,  3.46it/s] 24%|       | 191/780 [01:16<02:55,  3.36it/s] 25%|       | 192/780 [01:17<02:53,  3.40it/s] 25%|       | 193/780 [01:17<02:51,  3.42it/s] 25%|       | 194/780 [01:17<02:50,  3.43it/s] 25%|       | 195/780 [01:17<02:49,  3.44it/s] 25%|       | 196/780 [01:18<02:49,  3.45it/s] 25%|       | 197/780 [01:18<02:48,  3.45it/s] 25%|       | 198/780 [01:18<02:48,  3.46it/s] 26%|       | 199/780 [01:19<02:47,  3.46it/s] 26%|       | 200/780 [01:19<02:47,  3.46it/s] 26%|       | 201/780 [01:19<02:47,  3.46it/s] 26%|       | 202/780 [01:19<02:51,  3.37it/s] 26%|       | 203/780 [01:20<02:49,  3.40it/s] 26%|       | 204/780 [01:20<02:48,  3.42it/s] 26%|       | 205/780 [01:20<02:47,  3.43it/s] 26%|       | 206/780 [01:21<02:46,  3.45it/s] 27%|       | 207/780 [01:21<02:46,  3.45it/s] 27%|       | 208/780 [01:21<02:45,  3.45it/s] 27%|       | 209/780 [01:21<02:45,  3.46it/s] 27%|       | 210/780 [01:22<02:44,  3.46it/s] 27%|       | 211/780 [01:22<02:44,  3.46it/s] 27%|       | 212/780 [01:22<02:43,  3.46it/s] 27%|       | 213/780 [01:23<02:48,  3.37it/s] 27%|       | 214/780 [01:23<02:46,  3.40it/s] 28%|       | 215/780 [01:23<02:45,  3.42it/s] 28%|       | 216/780 [01:24<02:44,  3.43it/s] 28%|       | 217/780 [01:24<02:43,  3.44it/s] 28%|       | 218/780 [01:24<02:43,  3.45it/s] 28%|       | 219/780 [01:24<02:42,  3.45it/s] 28%|       | 220/780 [01:25<02:41,  3.46it/s] 28%|       | 221/780 [01:25<02:41,  3.46it/s] 28%|       | 222/780 [01:25<02:41,  3.46it/s] 29%|       | 223/780 [01:26<02:40,  3.46it/s] 29%|       | 224/780 [01:26<02:51,  3.24it/s] 29%|       | 225/780 [01:26<02:48,  3.30it/s] 29%|       | 226/780 [01:26<02:45,  3.35it/s] 29%|       | 227/780 [01:27<02:43,  3.38it/s] 29%|       | 228/780 [01:27<02:42,  3.40it/s] 29%|       | 229/780 [01:27<02:40,  3.43it/s] 29%|       | 230/780 [01:28<02:40,  3.44it/s] 30%|       | 231/780 [01:28<02:39,  3.45it/s] 30%|       | 232/780 [01:28<02:38,  3.45it/s] 30%|       | 233/780 [01:29<02:38,  3.46it/s] 30%|       | 234/780 [01:29<02:37,  3.46it/s] 30%|       | 235/780 [01:29<02:37,  3.46it/s] 30%|       | 236/780 [01:29<02:37,  3.46it/s] 30%|       | 237/780 [01:30<02:36,  3.46it/s] 31%|       | 238/780 [01:30<02:36,  3.46it/s] 31%|       | 239/780 [01:30<02:36,  3.46it/s] 31%|       | 240/780 [01:31<02:35,  3.46it/s] 31%|       | 241/780 [01:31<02:43,  3.30it/s] 31%|       | 242/780 [01:31<02:40,  3.35it/s] 31%|       | 243/780 [01:31<02:38,  3.38it/s] 31%|      | 244/780 [01:32<02:37,  3.41it/s] 31%|      | 245/780 [01:32<02:36,  3.42it/s] 32%|      | 246/780 [01:32<02:35,  3.44it/s] 32%|      | 247/780 [01:33<02:34,  3.44it/s] 32%|      | 248/780 [01:33<02:34,  3.45it/s] 32%|      | 249/780 [01:33<02:33,  3.45it/s] 32%|      | 250/780 [01:33<02:33,  3.46it/s] 32%|      | 251/780 [01:34<02:32,  3.46it/s] 32%|      | 252/780 [01:34<02:32,  3.46it/s] 32%|      | 253/780 [01:34<02:32,  3.46it/s] 33%|      | 254/780 [01:35<02:32,  3.46it/s] 33%|      | 255/780 [01:35<02:31,  3.46it/s] 33%|      | 256/780 [01:35<02:31,  3.46it/s] 33%|      | 257/780 [01:35<02:30,  3.46it/s] 33%|      | 258/780 [01:36<02:30,  3.46it/s] 33%|      | 259/780 [01:36<02:33,  3.39it/s] 33%|      | 260/780 [01:36<02:32,  3.41it/s] 33%|      | 261/780 [01:37<02:31,  3.43it/s] 34%|      | 262/780 [01:37<02:30,  3.44it/s] 34%|      | 263/780 [01:37<02:30,  3.44it/s] 34%|      | 264/780 [01:38<02:29,  3.45it/s] 34%|      | 265/780 [01:38<02:29,  3.45it/s] 34%|      | 266/780 [01:38<02:28,  3.45it/s] 34%|      | 267/780 [01:38<02:28,  3.45it/s] 34%|      | 268/780 [01:39<02:28,  3.46it/s] 34%|      | 269/780 [01:39<02:27,  3.46it/s] 35%|      | 270/780 [01:39<02:27,  3.46it/s] 35%|      | 271/780 [01:40<02:27,  3.45it/s] 35%|      | 272/780 [01:40<02:27,  3.45it/s] 35%|      | 273/780 [01:40<02:26,  3.46it/s] 35%|      | 274/780 [01:40<02:26,  3.46it/s] 35%|      | 275/780 [01:41<02:25,  3.46it/s] 35%|      | 276/780 [01:41<02:25,  3.46it/s] 36%|      | 277/780 [01:41<02:28,  3.39it/s] 36%|      | 278/780 [01:42<02:27,  3.40it/s] 36%|      | 279/780 [01:42<02:26,  3.42it/s] 36%|      | 280/780 [01:42<02:25,  3.43it/s] 36%|      | 281/780 [01:42<02:25,  3.44it/s] 36%|      | 282/780 [01:43<02:24,  3.44it/s] 36%|      | 283/780 [01:43<02:24,  3.45it/s] 36%|      | 284/780 [01:43<02:23,  3.45it/s] 37%|      | 285/780 [01:44<02:23,  3.45it/s] 37%|      | 286/780 [01:44<02:25,  3.39it/s] 37%|      | 287/780 [01:44<02:24,  3.41it/s] 37%|      | 288/780 [01:45<02:23,  3.43it/s] 37%|      | 289/780 [01:45<02:23,  3.43it/s] 37%|      | 290/780 [01:45<02:22,  3.44it/s] 37%|      | 291/780 [01:45<02:21,  3.45it/s] 37%|      | 292/780 [01:46<02:21,  3.45it/s] 38%|      | 293/780 [01:46<02:36,  3.11it/s] 38%|      | 294/780 [01:46<02:45,  2.93it/s] 38%|      | 295/780 [01:47<02:37,  3.07it/s] 38%|      | 296/780 [01:47<02:32,  3.18it/s] 38%|      | 297/780 [01:47<02:28,  3.26it/s] 38%|      | 298/780 [01:48<02:25,  3.32it/s] 38%|      | 299/780 [01:48<02:23,  3.35it/s] 38%|      | 300/780 [01:48<02:21,  3.38it/s] 39%|      | 301/780 [01:48<02:20,  3.41it/s] 39%|      | 302/780 [01:49<02:19,  3.42it/s] 39%|      | 303/780 [01:49<02:18,  3.43it/s] 39%|      | 304/780 [01:49<02:18,  3.44it/s] 39%|      | 305/780 [01:50<02:17,  3.45it/s] 39%|      | 306/780 [01:50<02:17,  3.45it/s] 39%|      | 307/780 [01:50<02:16,  3.45it/s] 39%|      | 308/780 [01:50<02:16,  3.45it/s] 40%|      | 309/780 [01:51<02:16,  3.44it/s] 40%|      | 310/780 [01:51<02:17,  3.43it/s] 40%|      | 311/780 [01:51<02:19,  3.36it/s] 40%|      | 312/780 [01:52<02:18,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 03:58:31,184 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:58:31,184 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 03:58:31,184 >>   Batch size = 8
{'eval_loss': 0.9444257020950317, 'eval_runtime': 9.7084, 'eval_samples_per_second': 359.379, 'eval_steps_per_second': 45.012, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.16it/s][A
  3%|         | 12/437 [00:00<00:08, 49.35it/s][A
  4%|         | 17/437 [00:00<00:08, 47.66it/s][A
  5%|         | 22/437 [00:00<00:08, 46.63it/s][A
  6%|         | 27/437 [00:00<00:08, 45.88it/s][A
  7%|         | 32/437 [00:00<00:08, 45.50it/s][A
  8%|         | 37/437 [00:00<00:08, 45.30it/s][A
 10%|         | 42/437 [00:00<00:08, 45.13it/s][A
 11%|         | 47/437 [00:01<00:08, 45.13it/s][A
 12%|        | 52/437 [00:01<00:08, 45.30it/s][A
 13%|        | 57/437 [00:01<00:08, 45.40it/s][A
 14%|        | 62/437 [00:01<00:08, 45.44it/s][A
 15%|        | 67/437 [00:01<00:08, 42.06it/s][A
 16%|        | 72/437 [00:01<00:08, 43.07it/s][A
 18%|        | 77/437 [00:01<00:08, 43.69it/s][A
 19%|        | 82/437 [00:01<00:08, 44.04it/s][A
 20%|        | 87/437 [00:01<00:07, 44.26it/s][A
 21%|        | 92/437 [00:02<00:07, 44.56it/s][A
 22%|       | 97/437 [00:02<00:07, 44.79it/s][A
 23%|       | 102/437 [00:02<00:07, 44.93it/s][A
 24%|       | 107/437 [00:02<00:07, 44.82it/s][A
 26%|       | 112/437 [00:02<00:07, 45.00it/s][A
 27%|       | 117/437 [00:02<00:07, 45.13it/s][A
 28%|       | 122/437 [00:02<00:06, 45.16it/s][A
 29%|       | 127/437 [00:02<00:06, 45.17it/s][A
 30%|       | 132/437 [00:02<00:06, 45.18it/s][A
 31%|      | 137/437 [00:03<00:06, 45.18it/s][A
 32%|      | 142/437 [00:03<00:06, 45.08it/s][A
 34%|      | 147/437 [00:03<00:06, 45.13it/s][A
 35%|      | 152/437 [00:03<00:06, 45.09it/s][A
 36%|      | 157/437 [00:03<00:06, 45.10it/s][A
 37%|      | 162/437 [00:03<00:06, 45.02it/s][A
 38%|      | 167/437 [00:03<00:05, 45.17it/s][A
 39%|      | 172/437 [00:03<00:05, 45.20it/s][A
 41%|      | 177/437 [00:03<00:05, 45.24it/s][A
 42%|     | 182/437 [00:04<00:05, 45.23it/s][A
 43%|     | 187/437 [00:04<00:05, 45.24it/s][A
 44%|     | 192/437 [00:04<00:05, 45.12it/s][A
 45%|     | 197/437 [00:04<00:05, 45.16it/s][A
 46%|     | 202/437 [00:04<00:05, 45.04it/s][A
 47%|     | 207/437 [00:04<00:05, 45.08it/s][A
 49%|     | 212/437 [00:04<00:04, 45.17it/s][A
 50%|     | 217/437 [00:04<00:04, 45.16it/s][A
 51%|     | 222/437 [00:04<00:04, 45.28it/s][A
 52%|    | 227/437 [00:05<00:04, 45.18it/s][A
 53%|    | 232/437 [00:05<00:04, 45.19it/s][A
 54%|    | 237/437 [00:05<00:04, 45.16it/s][A
 55%|    | 242/437 [00:05<00:04, 45.09it/s][A
 57%|    | 247/437 [00:05<00:04, 44.99it/s][A
 58%|    | 252/437 [00:05<00:04, 44.99it/s][A
 59%|    | 257/437 [00:05<00:03, 45.06it/s][A
 60%|    | 262/437 [00:05<00:03, 45.17it/s][A
 61%|    | 267/437 [00:05<00:03, 45.16it/s][A
 62%|   | 272/437 [00:06<00:03, 45.24it/s][A
 63%|   | 277/437 [00:06<00:03, 45.27it/s][A
 65%|   | 282/437 [00:06<00:03, 45.29it/s][A
 66%|   | 287/437 [00:06<00:03, 45.20it/s][A
 67%|   | 292/437 [00:06<00:03, 45.11it/s][A
 68%|   | 297/437 [00:06<00:03, 41.87it/s][A
 69%|   | 302/437 [00:06<00:03, 42.94it/s][A
 70%|   | 307/437 [00:06<00:02, 43.71it/s][A
 71%|  | 312/437 [00:06<00:02, 44.23it/s][A
 73%|  | 317/437 [00:07<00:02, 44.58it/s][A
 74%|  | 322/437 [00:07<00:02, 44.89it/s][A
 75%|  | 327/437 [00:07<00:02, 44.95it/s][A
 76%|  | 332/437 [00:07<00:02, 45.03it/s][A
 77%|  | 337/437 [00:07<00:02, 44.79it/s][A
 78%|  | 342/437 [00:07<00:02, 44.85it/s][A
 79%|  | 347/437 [00:07<00:02, 44.97it/s][A
 81%|  | 352/437 [00:07<00:01, 45.03it/s][A
 82%| | 357/437 [00:07<00:01, 45.04it/s][A
 83%| | 362/437 [00:08<00:01, 45.21it/s][A
 84%| | 367/437 [00:08<00:01, 45.21it/s][A
 85%| | 372/437 [00:08<00:01, 45.29it/s][A
 86%| | 377/437 [00:08<00:01, 45.09it/s][A
 87%| | 382/437 [00:08<00:01, 45.02it/s][A
 89%| | 387/437 [00:08<00:01, 44.94it/s][A
 90%| | 392/437 [00:08<00:01, 44.93it/s][A
 91%| | 397/437 [00:08<00:00, 45.15it/s][A
 92%|| 402/437 [00:08<00:00, 45.09it/s][A
 93%|| 407/437 [00:09<00:00, 45.25it/s][A
 94%|| 412/437 [00:09<00:00, 45.29it/s][A
 95%|| 417/437 [00:09<00:00, 45.26it/s][A
 97%|| 422/437 [00:09<00:00, 45.14it/s][A
 98%|| 427/437 [00:09<00:00, 44.99it/s][A
 99%|| 432/437 [00:09<00:00, 44.58it/s][A
100%|| 437/437 [00:09<00:00, 44.72it/s][A                                                 
                                                 [A 40%|      | 312/780 [02:01<02:18,  3.38it/s]
100%|| 437/437 [00:09<00:00, 44.72it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:58:41,168 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 03:58:41,355 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:58:44,561 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:58:44,698 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:58:44,766 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:12<48:25,  6.22s/it] 40%|      | 314/780 [02:12<34:34,  4.45s/it] 40%|      | 315/780 [02:12<24:50,  3.20s/it] 41%|      | 316/780 [02:13<18:01,  2.33s/it] 41%|      | 317/780 [02:13<13:16,  1.72s/it] 41%|      | 318/780 [02:13<09:56,  1.29s/it] 41%|      | 319/780 [02:14<07:37,  1.01it/s] 41%|      | 320/780 [02:14<05:59,  1.28it/s] 41%|      | 321/780 [02:14<04:51,  1.57it/s] 41%|     | 322/780 [02:14<04:03,  1.88it/s] 41%|     | 323/780 [02:15<03:30,  2.17it/s] 42%|     | 324/780 [02:15<03:07,  2.44it/s] 42%|     | 325/780 [02:15<02:53,  2.62it/s] 42%|     | 326/780 [02:16<02:41,  2.82it/s] 42%|     | 327/780 [02:16<02:32,  2.97it/s] 42%|     | 328/780 [02:16<02:26,  3.09it/s] 42%|     | 329/780 [02:16<02:21,  3.18it/s] 42%|     | 330/780 [02:17<02:18,  3.25it/s] 42%|     | 331/780 [02:17<02:16,  3.29it/s] 43%|     | 332/780 [02:17<02:14,  3.33it/s] 43%|     | 333/780 [02:18<02:13,  3.35it/s] 43%|     | 334/780 [02:18<02:12,  3.37it/s] 43%|     | 335/780 [02:18<02:11,  3.38it/s] 43%|     | 336/780 [02:19<02:12,  3.35it/s] 43%|     | 337/780 [02:19<02:11,  3.37it/s] 43%|     | 338/780 [02:19<02:10,  3.38it/s] 43%|     | 339/780 [02:19<02:09,  3.40it/s] 44%|     | 340/780 [02:20<02:09,  3.40it/s] 44%|     | 341/780 [02:20<02:09,  3.40it/s] 44%|     | 342/780 [02:20<02:08,  3.41it/s] 44%|     | 343/780 [02:21<02:08,  3.41it/s] 44%|     | 344/780 [02:21<02:07,  3.41it/s] 44%|     | 345/780 [02:21<02:07,  3.41it/s] 44%|     | 346/780 [02:21<02:07,  3.41it/s] 44%|     | 347/780 [02:22<02:08,  3.36it/s] 45%|     | 348/780 [02:22<02:08,  3.37it/s] 45%|     | 349/780 [02:22<02:07,  3.38it/s] 45%|     | 350/780 [02:23<02:06,  3.40it/s] 45%|     | 351/780 [02:23<02:06,  3.40it/s] 45%|     | 352/780 [02:23<02:05,  3.41it/s] 45%|     | 353/780 [02:24<02:05,  3.40it/s] 45%|     | 354/780 [02:24<02:05,  3.40it/s] 46%|     | 355/780 [02:24<02:04,  3.41it/s] 46%|     | 356/780 [02:24<02:04,  3.41it/s] 46%|     | 357/780 [02:25<02:04,  3.41it/s] 46%|     | 358/780 [02:25<02:09,  3.25it/s] 46%|     | 359/780 [02:25<02:07,  3.29it/s] 46%|     | 360/780 [02:26<02:06,  3.33it/s] 46%|     | 361/780 [02:26<02:05,  3.35it/s] 46%|     | 362/780 [02:26<02:04,  3.37it/s] 47%|     | 363/780 [02:27<02:03,  3.39it/s] 47%|     | 364/780 [02:27<02:02,  3.39it/s] 47%|     | 365/780 [02:27<02:02,  3.40it/s] 47%|     | 366/780 [02:27<02:01,  3.40it/s] 47%|     | 367/780 [02:28<02:01,  3.40it/s] 47%|     | 368/780 [02:28<02:01,  3.40it/s] 47%|     | 369/780 [02:28<02:05,  3.28it/s] 47%|     | 370/780 [02:29<02:03,  3.32it/s] 48%|     | 371/780 [02:29<02:02,  3.35it/s] 48%|     | 372/780 [02:29<02:01,  3.37it/s] 48%|     | 373/780 [02:29<02:00,  3.38it/s] 48%|     | 374/780 [02:30<01:59,  3.39it/s] 48%|     | 375/780 [02:30<01:59,  3.39it/s] 48%|     | 376/780 [02:30<01:58,  3.40it/s] 48%|     | 377/780 [02:31<01:58,  3.40it/s] 48%|     | 378/780 [02:31<01:58,  3.40it/s] 49%|     | 379/780 [02:31<01:57,  3.40it/s] 49%|     | 380/780 [02:32<01:57,  3.41it/s] 49%|     | 381/780 [02:32<01:57,  3.41it/s] 49%|     | 382/780 [02:32<01:56,  3.41it/s] 49%|     | 383/780 [02:32<02:01,  3.27it/s] 49%|     | 384/780 [02:33<01:59,  3.31it/s] 49%|     | 385/780 [02:33<01:58,  3.34it/s] 49%|     | 386/780 [02:33<01:57,  3.36it/s] 50%|     | 387/780 [02:34<01:56,  3.37it/s] 50%|     | 388/780 [02:34<01:55,  3.38it/s] 50%|     | 389/780 [02:34<01:55,  3.39it/s] 50%|     | 390/780 [02:35<01:54,  3.40it/s] 50%|     | 391/780 [02:35<01:54,  3.40it/s] 50%|     | 392/780 [02:35<01:54,  3.40it/s] 50%|     | 393/780 [02:35<01:53,  3.41it/s] 51%|     | 394/780 [02:36<01:56,  3.32it/s] 51%|     | 395/780 [02:36<01:54,  3.36it/s] 51%|     | 396/780 [02:36<01:53,  3.39it/s] 51%|     | 397/780 [02:37<01:52,  3.41it/s] 51%|     | 398/780 [02:37<01:51,  3.43it/s] 51%|     | 399/780 [02:37<01:50,  3.44it/s] 51%|    | 400/780 [02:37<01:50,  3.45it/s] 51%|    | 401/780 [02:38<01:49,  3.45it/s] 52%|    | 402/780 [02:38<01:49,  3.45it/s] 52%|    | 403/780 [02:38<01:48,  3.46it/s] 52%|    | 404/780 [02:39<01:48,  3.46it/s] 52%|    | 405/780 [02:39<01:49,  3.41it/s] 52%|    | 406/780 [02:39<01:49,  3.43it/s] 52%|    | 407/780 [02:39<01:48,  3.44it/s] 52%|    | 408/780 [02:40<01:47,  3.45it/s] 52%|    | 409/780 [02:40<01:47,  3.45it/s] 53%|    | 410/780 [02:40<01:47,  3.45it/s] 53%|    | 411/780 [02:41<01:46,  3.46it/s] 53%|    | 412/780 [02:41<01:46,  3.46it/s] 53%|    | 413/780 [02:41<01:46,  3.46it/s] 53%|    | 414/780 [02:41<01:45,  3.46it/s] 53%|    | 415/780 [02:42<01:45,  3.46it/s] 53%|    | 416/780 [02:42<01:45,  3.46it/s] 53%|    | 417/780 [02:42<01:44,  3.46it/s] 54%|    | 418/780 [02:43<01:47,  3.36it/s] 54%|    | 419/780 [02:43<01:46,  3.39it/s] 54%|    | 420/780 [02:43<01:45,  3.41it/s] 54%|    | 421/780 [02:44<01:44,  3.42it/s] 54%|    | 422/780 [02:44<01:44,  3.43it/s] 54%|    | 423/780 [02:44<01:46,  3.34it/s] 54%|    | 424/780 [02:44<01:45,  3.38it/s] 54%|    | 425/780 [02:45<01:44,  3.40it/s] 55%|    | 426/780 [02:45<01:43,  3.42it/s] 55%|    | 427/780 [02:45<01:42,  3.43it/s] 55%|    | 428/780 [02:46<01:42,  3.44it/s] 55%|    | 429/780 [02:46<01:41,  3.44it/s] 55%|    | 430/780 [02:46<01:55,  3.03it/s] 55%|    | 431/780 [02:47<01:57,  2.96it/s] 55%|    | 432/780 [02:47<01:52,  3.10it/s] 56%|    | 433/780 [02:47<01:48,  3.20it/s] 56%|    | 434/780 [02:48<01:45,  3.27it/s] 56%|    | 435/780 [02:48<01:46,  3.23it/s] 56%|    | 436/780 [02:48<01:44,  3.30it/s] 56%|    | 437/780 [02:48<01:42,  3.35it/s] 56%|    | 438/780 [02:49<01:41,  3.38it/s] 56%|    | 439/780 [02:49<01:40,  3.40it/s] 56%|    | 440/780 [02:49<01:39,  3.42it/s] 57%|    | 441/780 [02:50<01:38,  3.43it/s] 57%|    | 442/780 [02:50<01:38,  3.44it/s] 57%|    | 443/780 [02:50<01:37,  3.44it/s] 57%|    | 444/780 [02:50<01:37,  3.45it/s] 57%|    | 445/780 [02:51<01:37,  3.45it/s] 57%|    | 446/780 [02:51<01:36,  3.45it/s] 57%|    | 447/780 [02:51<01:36,  3.46it/s] 57%|    | 448/780 [02:52<01:36,  3.46it/s] 58%|    | 449/780 [02:52<01:35,  3.46it/s] 58%|    | 450/780 [02:52<01:35,  3.46it/s] 58%|    | 451/780 [02:52<01:35,  3.46it/s] 58%|    | 452/780 [02:53<01:36,  3.41it/s] 58%|    | 453/780 [02:53<01:35,  3.42it/s] 58%|    | 454/780 [02:53<01:34,  3.44it/s] 58%|    | 455/780 [02:54<01:34,  3.44it/s] 58%|    | 456/780 [02:54<01:33,  3.45it/s] 59%|    | 457/780 [02:54<01:33,  3.45it/s] 59%|    | 458/780 [02:55<01:33,  3.46it/s] 59%|    | 459/780 [02:55<01:32,  3.46it/s] 59%|    | 460/780 [02:55<01:32,  3.46it/s] 59%|    | 461/780 [02:55<01:32,  3.45it/s] 59%|    | 462/780 [02:56<01:31,  3.46it/s] 59%|    | 463/780 [02:56<01:31,  3.46it/s] 59%|    | 464/780 [02:56<01:31,  3.46it/s] 60%|    | 465/780 [02:57<01:31,  3.46it/s] 60%|    | 466/780 [02:57<01:30,  3.46it/s] 60%|    | 467/780 [02:57<01:30,  3.46it/s] 60%|    | 468/780 [02:57<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:59:36,902 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:59:36,903 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 03:59:36,903 >>   Batch size = 8
{'eval_loss': 0.9492586255073547, 'eval_runtime': 9.756, 'eval_samples_per_second': 357.624, 'eval_steps_per_second': 44.793, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.47it/s][A
  3%|         | 12/437 [00:00<00:08, 48.33it/s][A
  4%|         | 17/437 [00:00<00:08, 46.93it/s][A
  5%|         | 22/437 [00:00<00:08, 46.30it/s][A
  6%|         | 27/437 [00:00<00:08, 45.70it/s][A
  7%|         | 32/437 [00:00<00:08, 45.42it/s][A
  8%|         | 37/437 [00:00<00:08, 45.35it/s][A
 10%|         | 42/437 [00:00<00:08, 45.17it/s][A
 11%|         | 47/437 [00:01<00:08, 45.33it/s][A
 12%|        | 52/437 [00:01<00:08, 45.33it/s][A
 13%|        | 57/437 [00:01<00:08, 45.40it/s][A
 14%|        | 62/437 [00:01<00:08, 45.30it/s][A
 15%|        | 67/437 [00:01<00:08, 45.36it/s][A
 16%|        | 72/437 [00:01<00:08, 45.24it/s][A
 18%|        | 77/437 [00:01<00:07, 45.10it/s][A
 19%|        | 82/437 [00:01<00:07, 45.15it/s][A
 20%|        | 87/437 [00:01<00:07, 45.18it/s][A
 21%|        | 92/437 [00:02<00:07, 45.20it/s][A
 22%|       | 97/437 [00:02<00:07, 45.33it/s][A
 23%|       | 102/437 [00:02<00:07, 45.23it/s][A
 24%|       | 107/437 [00:02<00:07, 45.28it/s][A
 26%|       | 112/437 [00:02<00:07, 45.05it/s][A
 27%|       | 117/437 [00:02<00:07, 44.96it/s][A
 28%|       | 122/437 [00:02<00:06, 45.05it/s][A
 29%|       | 127/437 [00:02<00:06, 45.10it/s][A
 30%|       | 132/437 [00:02<00:06, 45.16it/s][A
 31%|      | 137/437 [00:03<00:06, 45.18it/s][A
 32%|      | 142/437 [00:03<00:06, 45.23it/s][A
 34%|      | 147/437 [00:03<00:06, 45.24it/s][A
 35%|      | 152/437 [00:03<00:06, 45.33it/s][A
 36%|      | 157/437 [00:03<00:06, 45.10it/s][A
 37%|      | 162/437 [00:03<00:06, 45.19it/s][A
 38%|      | 167/437 [00:03<00:05, 45.13it/s][A
 39%|      | 172/437 [00:03<00:05, 45.04it/s][A
 41%|      | 177/437 [00:03<00:05, 45.04it/s][A
 42%|     | 182/437 [00:04<00:05, 45.14it/s][A
 43%|     | 187/437 [00:04<00:05, 45.21it/s][A
 44%|     | 192/437 [00:04<00:05, 45.32it/s][A
 45%|     | 197/437 [00:04<00:05, 45.35it/s][A
 46%|     | 202/437 [00:04<00:05, 45.24it/s][A
 47%|     | 207/437 [00:04<00:05, 45.28it/s][A
 49%|     | 212/437 [00:04<00:04, 45.16it/s][A
 50%|     | 217/437 [00:04<00:04, 45.15it/s][A
 51%|     | 222/437 [00:04<00:04, 45.12it/s][A
 52%|    | 227/437 [00:05<00:04, 45.06it/s][A
 53%|    | 232/437 [00:05<00:04, 45.11it/s][A
 54%|    | 237/437 [00:05<00:04, 45.25it/s][A
 55%|    | 242/437 [00:05<00:04, 44.22it/s][A
 57%|    | 247/437 [00:05<00:04, 44.56it/s][A
 58%|    | 252/437 [00:05<00:04, 44.73it/s][A
 59%|    | 257/437 [00:05<00:04, 44.91it/s][A
 60%|    | 262/437 [00:05<00:03, 44.94it/s][A
 61%|    | 267/437 [00:05<00:03, 45.02it/s][A
 62%|   | 272/437 [00:06<00:03, 45.05it/s][A
 63%|   | 277/437 [00:06<00:03, 45.00it/s][A
 65%|   | 282/437 [00:06<00:03, 44.97it/s][A
 66%|   | 287/437 [00:06<00:03, 45.06it/s][A
 67%|   | 292/437 [00:06<00:03, 45.14it/s][A
 68%|   | 297/437 [00:06<00:03, 45.24it/s][A
 69%|   | 302/437 [00:06<00:02, 45.16it/s][A
 70%|   | 307/437 [00:06<00:02, 45.26it/s][A
 71%|  | 312/437 [00:06<00:02, 45.25it/s][A
 73%|  | 317/437 [00:07<00:02, 45.16it/s][A
 74%|  | 322/437 [00:07<00:02, 45.17it/s][A
 75%|  | 327/437 [00:07<00:02, 45.09it/s][A
 76%|  | 332/437 [00:07<00:02, 45.14it/s][A
 77%|  | 337/437 [00:07<00:02, 44.71it/s][A
 78%|  | 342/437 [00:07<00:02, 44.88it/s][A
 79%|  | 347/437 [00:07<00:01, 45.03it/s][A
 81%|  | 352/437 [00:07<00:01, 45.12it/s][A
 82%| | 357/437 [00:07<00:01, 45.23it/s][A
 83%| | 362/437 [00:08<00:01, 45.24it/s][A
 84%| | 367/437 [00:08<00:01, 45.18it/s][A
 85%| | 372/437 [00:08<00:01, 45.14it/s][A
 86%| | 377/437 [00:08<00:01, 44.95it/s][A
 87%| | 382/437 [00:08<00:01, 45.06it/s][A
 89%| | 387/437 [00:08<00:01, 45.14it/s][A
 90%| | 392/437 [00:08<00:00, 45.19it/s][A
 91%| | 397/437 [00:08<00:00, 45.26it/s][A
 92%|| 402/437 [00:08<00:00, 45.11it/s][A
 93%|| 407/437 [00:08<00:00, 45.34it/s][A
 94%|| 412/437 [00:09<00:00, 45.28it/s][A
 95%|| 417/437 [00:09<00:00, 45.31it/s][A
 97%|| 422/437 [00:09<00:00, 45.18it/s][A
 98%|| 427/437 [00:09<00:00, 45.06it/s][A
 99%|| 432/437 [00:09<00:00, 45.12it/s][A
100%|| 437/437 [00:09<00:00, 45.26it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.26it/s][A 60%|    | 468/780 [03:07<01:30,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:59:46,833 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 03:59:47,007 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:59:50,239 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:59:50,406 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:59:50,479 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:18<33:20,  6.43s/it] 60%|    | 470/780 [03:18<23:45,  4.60s/it] 60%|    | 471/780 [03:19<17:01,  3.31s/it] 61%|    | 472/780 [03:19<12:19,  2.40s/it] 61%|    | 473/780 [03:19<09:03,  1.77s/it] 61%|    | 474/780 [03:20<06:45,  1.33s/it] 61%|    | 475/780 [03:20<05:09,  1.02s/it] 61%|    | 476/780 [03:20<04:03,  1.25it/s] 61%|    | 477/780 [03:21<03:16,  1.54it/s] 61%|   | 478/780 [03:21<02:43,  1.85it/s] 61%|   | 479/780 [03:21<02:20,  2.14it/s] 62%|   | 480/780 [03:21<02:04,  2.41it/s] 62%|   | 481/780 [03:22<01:54,  2.60it/s] 62%|   | 482/780 [03:22<01:46,  2.80it/s] 62%|   | 483/780 [03:22<01:40,  2.96it/s] 62%|   | 484/780 [03:23<01:36,  3.08it/s] 62%|   | 485/780 [03:23<01:32,  3.18it/s] 62%|   | 486/780 [03:23<01:30,  3.24it/s] 62%|   | 487/780 [03:23<01:29,  3.29it/s] 63%|   | 488/780 [03:24<01:27,  3.33it/s] 63%|   | 489/780 [03:24<01:26,  3.35it/s] 63%|   | 490/780 [03:24<01:26,  3.37it/s] 63%|   | 491/780 [03:25<01:25,  3.38it/s] 63%|   | 492/780 [03:25<01:27,  3.30it/s] 63%|   | 493/780 [03:25<01:26,  3.33it/s] 63%|   | 494/780 [03:26<01:25,  3.36it/s] 63%|   | 495/780 [03:26<01:24,  3.39it/s] 64%|   | 496/780 [03:26<01:23,  3.42it/s] 64%|   | 497/780 [03:26<01:22,  3.43it/s] 64%|   | 498/780 [03:27<01:21,  3.45it/s] 64%|   | 499/780 [03:27<01:21,  3.45it/s] 64%|   | 500/780 [03:27<01:21,  3.46it/s]                                                  64%|   | 500/780 [03:27<01:21,  3.46it/s] 64%|   | 501/780 [03:28<01:20,  3.46it/s] 64%|   | 502/780 [03:28<01:20,  3.46it/s] 64%|   | 503/780 [03:28<01:24,  3.26it/s] 65%|   | 504/780 [03:29<01:23,  3.32it/s] 65%|   | 505/780 [03:29<01:21,  3.36it/s] 65%|   | 506/780 [03:29<01:20,  3.39it/s] 65%|   | 507/780 [03:29<01:19,  3.41it/s] 65%|   | 508/780 [03:30<01:19,  3.43it/s] 65%|   | 509/780 [03:30<01:18,  3.44it/s] 65%|   | 510/780 [03:30<01:18,  3.44it/s] 66%|   | 511/780 [03:31<01:17,  3.45it/s] 66%|   | 512/780 [03:31<01:17,  3.46it/s] 66%|   | 513/780 [03:31<01:17,  3.46it/s] 66%|   | 514/780 [03:31<01:20,  3.30it/s] 66%|   | 515/780 [03:32<01:19,  3.35it/s] 66%|   | 516/780 [03:32<01:17,  3.39it/s] 66%|   | 517/780 [03:32<01:17,  3.41it/s] 66%|   | 518/780 [03:33<01:16,  3.43it/s] 67%|   | 519/780 [03:33<01:15,  3.44it/s] 67%|   | 520/780 [03:33<01:15,  3.45it/s] 67%|   | 521/780 [03:33<01:15,  3.45it/s] 67%|   | 522/780 [03:34<01:14,  3.45it/s] 67%|   | 523/780 [03:34<01:14,  3.46it/s] 67%|   | 524/780 [03:34<01:13,  3.46it/s] 67%|   | 525/780 [03:35<01:17,  3.29it/s] 67%|   | 526/780 [03:35<01:15,  3.34it/s] 68%|   | 527/780 [03:35<01:14,  3.38it/s] 68%|   | 528/780 [03:36<01:13,  3.41it/s] 68%|   | 529/780 [03:36<01:18,  3.22it/s] 68%|   | 530/780 [03:36<01:16,  3.29it/s] 68%|   | 531/780 [03:36<01:14,  3.34it/s] 68%|   | 532/780 [03:37<01:13,  3.38it/s] 68%|   | 533/780 [03:37<01:12,  3.40it/s] 68%|   | 534/780 [03:37<01:11,  3.42it/s] 69%|   | 535/780 [03:38<01:11,  3.43it/s] 69%|   | 536/780 [03:38<01:10,  3.45it/s] 69%|   | 537/780 [03:38<01:10,  3.45it/s] 69%|   | 538/780 [03:38<01:09,  3.46it/s] 69%|   | 539/780 [03:39<01:09,  3.46it/s] 69%|   | 540/780 [03:39<01:10,  3.39it/s] 69%|   | 541/780 [03:39<01:09,  3.42it/s] 69%|   | 542/780 [03:40<01:09,  3.43it/s] 70%|   | 543/780 [03:40<01:08,  3.44it/s] 70%|   | 544/780 [03:40<01:08,  3.44it/s] 70%|   | 545/780 [03:41<01:08,  3.45it/s] 70%|   | 546/780 [03:41<01:07,  3.46it/s] 70%|   | 547/780 [03:41<01:07,  3.46it/s] 70%|   | 548/780 [03:41<01:06,  3.46it/s] 70%|   | 549/780 [03:42<01:06,  3.46it/s] 71%|   | 550/780 [03:42<01:06,  3.46it/s] 71%|   | 551/780 [03:42<01:07,  3.40it/s] 71%|   | 552/780 [03:43<01:06,  3.41it/s] 71%|   | 553/780 [03:43<01:06,  3.43it/s] 71%|   | 554/780 [03:43<01:05,  3.44it/s] 71%|   | 555/780 [03:43<01:05,  3.43it/s] 71%|  | 556/780 [03:44<01:05,  3.42it/s] 71%|  | 557/780 [03:44<01:05,  3.42it/s] 72%|  | 558/780 [03:44<01:05,  3.40it/s] 72%|  | 559/780 [03:45<01:04,  3.41it/s] 72%|  | 560/780 [03:45<01:04,  3.41it/s] 72%|  | 561/780 [03:45<01:04,  3.40it/s] 72%|  | 562/780 [03:45<01:04,  3.39it/s] 72%|  | 563/780 [03:46<01:03,  3.40it/s] 72%|  | 564/780 [03:46<01:03,  3.40it/s] 72%|  | 565/780 [03:46<01:04,  3.34it/s] 73%|  | 566/780 [03:47<01:05,  3.28it/s] 73%|  | 567/780 [03:47<01:04,  3.32it/s] 73%|  | 568/780 [03:47<01:03,  3.34it/s] 73%|  | 569/780 [03:48<01:02,  3.37it/s] 73%|  | 570/780 [03:48<01:02,  3.38it/s] 73%|  | 571/780 [03:48<01:01,  3.39it/s] 73%|  | 572/780 [03:48<01:01,  3.40it/s] 73%|  | 573/780 [03:49<01:01,  3.38it/s] 74%|  | 574/780 [03:49<01:00,  3.38it/s] 74%|  | 575/780 [03:49<01:00,  3.39it/s] 74%|  | 576/780 [03:50<01:00,  3.40it/s] 74%|  | 577/780 [03:50<00:59,  3.40it/s] 74%|  | 578/780 [03:50<00:59,  3.40it/s] 74%|  | 579/780 [03:51<00:59,  3.41it/s] 74%|  | 580/780 [03:51<00:58,  3.41it/s] 74%|  | 581/780 [03:51<00:58,  3.41it/s] 75%|  | 582/780 [03:51<00:57,  3.41it/s] 75%|  | 583/780 [03:52<00:57,  3.41it/s] 75%|  | 584/780 [03:52<00:57,  3.40it/s] 75%|  | 585/780 [03:52<00:57,  3.40it/s] 75%|  | 586/780 [03:53<00:56,  3.40it/s] 75%|  | 587/780 [03:53<00:56,  3.41it/s] 75%|  | 588/780 [03:53<00:56,  3.41it/s] 76%|  | 589/780 [03:53<00:56,  3.41it/s] 76%|  | 590/780 [03:54<00:55,  3.41it/s] 76%|  | 591/780 [03:54<00:55,  3.41it/s] 76%|  | 592/780 [03:54<00:55,  3.41it/s] 76%|  | 593/780 [03:55<00:54,  3.41it/s] 76%|  | 594/780 [03:55<00:54,  3.41it/s] 76%|  | 595/780 [03:55<00:54,  3.37it/s] 76%|  | 596/780 [03:56<00:54,  3.39it/s] 77%|  | 597/780 [03:56<00:53,  3.39it/s] 77%|  | 598/780 [03:56<00:53,  3.40it/s] 77%|  | 599/780 [03:56<00:53,  3.40it/s] 77%|  | 600/780 [03:57<00:52,  3.40it/s] 77%|  | 601/780 [03:57<00:52,  3.40it/s] 77%|  | 602/780 [03:57<00:52,  3.41it/s] 77%|  | 603/780 [03:58<00:51,  3.41it/s] 77%|  | 604/780 [03:58<00:51,  3.41it/s] 78%|  | 605/780 [03:58<00:51,  3.41it/s] 78%|  | 606/780 [03:58<00:51,  3.35it/s] 78%|  | 607/780 [03:59<00:51,  3.36it/s] 78%|  | 608/780 [03:59<00:50,  3.38it/s] 78%|  | 609/780 [03:59<00:50,  3.39it/s] 78%|  | 610/780 [04:00<00:50,  3.39it/s] 78%|  | 611/780 [04:00<00:49,  3.40it/s] 78%|  | 612/780 [04:00<00:49,  3.40it/s] 79%|  | 613/780 [04:01<00:49,  3.40it/s] 79%|  | 614/780 [04:01<00:48,  3.40it/s] 79%|  | 615/780 [04:01<00:48,  3.41it/s] 79%|  | 616/780 [04:01<00:48,  3.40it/s] 79%|  | 617/780 [04:02<00:48,  3.33it/s] 79%|  | 618/780 [04:02<00:48,  3.35it/s] 79%|  | 619/780 [04:02<00:47,  3.37it/s] 79%|  | 620/780 [04:03<00:47,  3.38it/s] 80%|  | 621/780 [04:03<00:46,  3.39it/s] 80%|  | 622/780 [04:03<00:46,  3.39it/s] 80%|  | 623/780 [04:03<00:46,  3.40it/s] 80%|  | 624/780 [04:04<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 04:00:43,280 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:00:43,280 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 04:00:43,280 >>   Batch size = 8
{'eval_loss': 0.9576515555381775, 'eval_runtime': 9.7092, 'eval_samples_per_second': 359.35, 'eval_steps_per_second': 45.009, 'epoch': 3.0}
{'loss': 0.6422, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.33it/s][A
  3%|         | 12/437 [00:00<00:08, 49.09it/s][A
  4%|         | 17/437 [00:00<00:08, 47.51it/s][A
  5%|         | 22/437 [00:00<00:08, 46.61it/s][A
  6%|         | 27/437 [00:00<00:08, 45.97it/s][A
  7%|         | 32/437 [00:00<00:09, 44.74it/s][A
  8%|         | 37/437 [00:00<00:08, 44.88it/s][A
 10%|         | 42/437 [00:00<00:08, 44.83it/s][A
 11%|         | 47/437 [00:01<00:08, 45.02it/s][A
 12%|        | 52/437 [00:01<00:08, 45.09it/s][A
 13%|        | 57/437 [00:01<00:08, 45.26it/s][A
 14%|        | 62/437 [00:01<00:08, 45.31it/s][A
 15%|        | 67/437 [00:01<00:08, 45.25it/s][A
 16%|        | 72/437 [00:01<00:08, 45.12it/s][A
 18%|        | 77/437 [00:01<00:07, 45.10it/s][A
 19%|        | 82/437 [00:01<00:07, 45.12it/s][A
 20%|        | 87/437 [00:01<00:07, 45.10it/s][A
 21%|        | 92/437 [00:02<00:07, 45.08it/s][A
 22%|       | 97/437 [00:02<00:07, 45.17it/s][A
 23%|       | 102/437 [00:02<00:07, 45.21it/s][A
 24%|       | 107/437 [00:02<00:07, 45.18it/s][A
 26%|       | 112/437 [00:02<00:07, 45.25it/s][A
 27%|       | 117/437 [00:02<00:07, 45.20it/s][A
 28%|       | 122/437 [00:02<00:06, 45.13it/s][A
 29%|       | 127/437 [00:02<00:06, 45.20it/s][A
 30%|       | 132/437 [00:02<00:06, 45.22it/s][A
 31%|      | 137/437 [00:03<00:06, 45.18it/s][A
 32%|      | 142/437 [00:03<00:06, 45.15it/s][A
 34%|      | 147/437 [00:03<00:06, 45.03it/s][A
 35%|      | 152/437 [00:03<00:06, 45.14it/s][A
 36%|      | 157/437 [00:03<00:06, 45.12it/s][A
 37%|      | 162/437 [00:03<00:06, 45.15it/s][A
 38%|      | 167/437 [00:03<00:05, 45.15it/s][A
 39%|      | 172/437 [00:03<00:05, 45.12it/s][A
 41%|      | 177/437 [00:03<00:05, 45.09it/s][A
 42%|     | 182/437 [00:04<00:05, 45.15it/s][A
 43%|     | 187/437 [00:04<00:05, 45.21it/s][A
 44%|     | 192/437 [00:04<00:05, 45.17it/s][A
 45%|     | 197/437 [00:04<00:05, 45.20it/s][A
 46%|     | 202/437 [00:04<00:05, 45.21it/s][A
 47%|     | 207/437 [00:04<00:05, 45.22it/s][A
 49%|     | 212/437 [00:04<00:04, 45.17it/s][A
 50%|     | 217/437 [00:04<00:04, 45.11it/s][A
 51%|     | 222/437 [00:04<00:04, 45.09it/s][A
 52%|    | 227/437 [00:05<00:04, 45.22it/s][A
 53%|    | 232/437 [00:05<00:04, 45.25it/s][A
 54%|    | 237/437 [00:05<00:04, 45.24it/s][A
 55%|    | 242/437 [00:05<00:04, 45.19it/s][A
 57%|    | 247/437 [00:05<00:04, 45.22it/s][A
 58%|    | 252/437 [00:05<00:04, 44.75it/s][A
 59%|    | 257/437 [00:05<00:04, 44.88it/s][A
 60%|    | 262/437 [00:05<00:03, 44.83it/s][A
 61%|    | 267/437 [00:05<00:03, 44.95it/s][A
 62%|   | 272/437 [00:06<00:03, 45.08it/s][A
 63%|   | 277/437 [00:06<00:03, 45.14it/s][A
 65%|   | 282/437 [00:06<00:03, 45.19it/s][A
 66%|   | 287/437 [00:06<00:03, 45.14it/s][A
 67%|   | 292/437 [00:06<00:03, 45.07it/s][A
 68%|   | 297/437 [00:06<00:03, 45.07it/s][A
 69%|   | 302/437 [00:06<00:02, 45.03it/s][A
 70%|   | 307/437 [00:06<00:02, 45.09it/s][A
 71%|  | 312/437 [00:06<00:02, 45.08it/s][A
 73%|  | 317/437 [00:07<00:02, 45.16it/s][A
 74%|  | 322/437 [00:07<00:02, 45.14it/s][A
 75%|  | 327/437 [00:07<00:02, 45.29it/s][A
 76%|  | 332/437 [00:07<00:02, 45.21it/s][A
 77%|  | 337/437 [00:07<00:02, 45.18it/s][A
 78%|  | 342/437 [00:07<00:02, 44.99it/s][A
 79%|  | 347/437 [00:07<00:01, 45.09it/s][A
 81%|  | 352/437 [00:07<00:01, 45.18it/s][A
 82%| | 357/437 [00:07<00:01, 45.13it/s][A
 83%| | 362/437 [00:08<00:01, 45.19it/s][A
 84%| | 367/437 [00:08<00:01, 45.13it/s][A
 85%| | 372/437 [00:08<00:01, 45.21it/s][A
 86%| | 377/437 [00:08<00:01, 45.22it/s][A
 87%| | 382/437 [00:08<00:01, 45.14it/s][A
 89%| | 387/437 [00:08<00:01, 44.98it/s][A
 90%| | 392/437 [00:08<00:01, 43.27it/s][A
 91%| | 397/437 [00:08<00:00, 43.87it/s][A
 92%|| 402/437 [00:08<00:00, 44.36it/s][A
 93%|| 407/437 [00:09<00:00, 44.60it/s][A
 94%|| 412/437 [00:09<00:00, 44.79it/s][A
 95%|| 417/437 [00:09<00:00, 44.90it/s][A
 97%|| 422/437 [00:09<00:00, 45.11it/s][A
 98%|| 427/437 [00:09<00:00, 45.08it/s][A
 99%|| 432/437 [00:09<00:00, 44.95it/s][A
100%|| 437/437 [00:09<00:00, 44.98it/s][A                                                 
                                                 [A 80%|  | 624/780 [04:14<00:45,  3.40it/s]
100%|| 437/437 [00:09<00:00, 44.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:00:53,225 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 04:00:53,415 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:00:56,549 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:00:56,670 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:00:56,720 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:23<15:48,  6.12s/it] 80%|  | 626/780 [04:24<11:14,  4.38s/it] 80%|  | 627/780 [04:24<08:02,  3.15s/it] 81%|  | 628/780 [04:24<05:48,  2.30s/it] 81%|  | 629/780 [04:25<04:15,  1.70s/it] 81%|  | 630/780 [04:25<03:11,  1.27s/it] 81%|  | 631/780 [04:25<02:25,  1.02it/s] 81%|  | 632/780 [04:26<01:54,  1.29it/s] 81%|  | 633/780 [04:26<01:32,  1.59it/s] 81%| | 634/780 [04:26<01:17,  1.89it/s] 81%| | 635/780 [04:26<01:06,  2.18it/s] 82%| | 636/780 [04:27<00:58,  2.45it/s] 82%| | 637/780 [04:27<00:54,  2.65it/s] 82%| | 638/780 [04:27<00:50,  2.83it/s] 82%| | 639/780 [04:28<00:47,  2.99it/s] 82%| | 640/780 [04:28<00:45,  3.10it/s] 82%| | 641/780 [04:28<00:43,  3.19it/s] 82%| | 642/780 [04:29<00:42,  3.26it/s] 82%| | 643/780 [04:29<00:41,  3.30it/s] 83%| | 644/780 [04:29<00:40,  3.33it/s] 83%| | 645/780 [04:29<00:40,  3.36it/s] 83%| | 646/780 [04:30<00:39,  3.38it/s] 83%| | 647/780 [04:30<00:39,  3.38it/s] 83%| | 648/780 [04:30<00:40,  3.29it/s] 83%| | 649/780 [04:31<00:39,  3.33it/s] 83%| | 650/780 [04:31<00:38,  3.35it/s] 83%| | 651/780 [04:31<00:38,  3.37it/s] 84%| | 652/780 [04:31<00:37,  3.38it/s] 84%| | 653/780 [04:32<00:37,  3.39it/s] 84%| | 654/780 [04:32<00:37,  3.40it/s] 84%| | 655/780 [04:32<00:36,  3.40it/s] 84%| | 656/780 [04:33<00:36,  3.41it/s] 84%| | 657/780 [04:33<00:36,  3.41it/s] 84%| | 658/780 [04:33<00:35,  3.41it/s] 84%| | 659/780 [04:34<00:36,  3.27it/s] 85%| | 660/780 [04:34<00:36,  3.31it/s] 85%| | 661/780 [04:34<00:35,  3.34it/s] 85%| | 662/780 [04:34<00:35,  3.36it/s] 85%| | 663/780 [04:35<00:34,  3.37it/s] 85%| | 664/780 [04:35<00:34,  3.39it/s] 85%| | 665/780 [04:35<00:33,  3.40it/s] 85%| | 666/780 [04:36<00:33,  3.40it/s] 86%| | 667/780 [04:36<00:33,  3.41it/s] 86%| | 668/780 [04:36<00:32,  3.40it/s] 86%| | 669/780 [04:36<00:32,  3.40it/s] 86%| | 670/780 [04:37<00:33,  3.27it/s] 86%| | 671/780 [04:37<00:32,  3.31it/s] 86%| | 672/780 [04:37<00:32,  3.34it/s] 86%| | 673/780 [04:38<00:31,  3.36it/s] 86%| | 674/780 [04:38<00:31,  3.38it/s] 87%| | 675/780 [04:38<00:31,  3.39it/s] 87%| | 676/780 [04:39<00:30,  3.39it/s] 87%| | 677/780 [04:39<00:30,  3.40it/s] 87%| | 678/780 [04:39<00:29,  3.40it/s] 87%| | 679/780 [04:39<00:29,  3.41it/s] 87%| | 680/780 [04:40<00:29,  3.41it/s] 87%| | 681/780 [04:40<00:29,  3.41it/s] 87%| | 682/780 [04:40<00:29,  3.33it/s] 88%| | 683/780 [04:41<00:28,  3.35it/s] 88%| | 684/780 [04:41<00:28,  3.37it/s] 88%| | 685/780 [04:41<00:28,  3.39it/s] 88%| | 686/780 [04:42<00:27,  3.40it/s] 88%| | 687/780 [04:42<00:27,  3.40it/s] 88%| | 688/780 [04:42<00:27,  3.40it/s] 88%| | 689/780 [04:42<00:26,  3.41it/s] 88%| | 690/780 [04:43<00:26,  3.41it/s] 89%| | 691/780 [04:43<00:26,  3.40it/s] 89%| | 692/780 [04:43<00:25,  3.40it/s] 89%| | 693/780 [04:44<00:26,  3.32it/s] 89%| | 694/780 [04:44<00:25,  3.34it/s] 89%| | 695/780 [04:44<00:25,  3.36it/s] 89%| | 696/780 [04:45<00:25,  3.31it/s] 89%| | 697/780 [04:45<00:24,  3.34it/s] 89%| | 698/780 [04:45<00:24,  3.36it/s] 90%| | 699/780 [04:45<00:23,  3.38it/s] 90%| | 700/780 [04:46<00:23,  3.40it/s] 90%| | 701/780 [04:46<00:23,  3.42it/s] 90%| | 702/780 [04:46<00:22,  3.43it/s] 90%| | 703/780 [04:47<00:28,  2.72it/s] 90%| | 704/780 [04:47<00:26,  2.82it/s] 90%| | 705/780 [04:47<00:25,  2.98it/s] 91%| | 706/780 [04:48<00:23,  3.10it/s] 91%| | 707/780 [04:48<00:22,  3.19it/s] 91%| | 708/780 [04:48<00:22,  3.25it/s] 91%| | 709/780 [04:49<00:21,  3.30it/s] 91%| | 710/780 [04:49<00:21,  3.33it/s] 91%| | 711/780 [04:49<00:20,  3.35it/s] 91%|| 712/780 [04:49<00:20,  3.37it/s] 91%|| 713/780 [04:50<00:19,  3.38it/s] 92%|| 714/780 [04:50<00:19,  3.39it/s] 92%|| 715/780 [04:50<00:19,  3.31it/s] 92%|| 716/780 [04:51<00:19,  3.34it/s] 92%|| 717/780 [04:51<00:18,  3.36it/s] 92%|| 718/780 [04:51<00:18,  3.38it/s] 92%|| 719/780 [04:52<00:18,  3.39it/s] 92%|| 720/780 [04:52<00:17,  3.39it/s] 92%|| 721/780 [04:52<00:17,  3.40it/s] 93%|| 722/780 [04:52<00:17,  3.40it/s] 93%|| 723/780 [04:53<00:16,  3.40it/s] 93%|| 724/780 [04:53<00:16,  3.41it/s] 93%|| 725/780 [04:53<00:16,  3.40it/s] 93%|| 726/780 [04:54<00:16,  3.28it/s] 93%|| 727/780 [04:54<00:15,  3.33it/s] 93%|| 728/780 [04:54<00:15,  3.37it/s] 93%|| 729/780 [04:55<00:15,  3.39it/s] 94%|| 730/780 [04:55<00:14,  3.41it/s] 94%|| 731/780 [04:55<00:14,  3.43it/s] 94%|| 732/780 [04:55<00:13,  3.44it/s] 94%|| 733/780 [04:56<00:13,  3.45it/s] 94%|| 734/780 [04:56<00:13,  3.45it/s] 94%|| 735/780 [04:56<00:13,  3.45it/s] 94%|| 736/780 [04:57<00:12,  3.46it/s] 94%|| 737/780 [04:57<00:13,  3.30it/s] 95%|| 738/780 [04:57<00:12,  3.35it/s] 95%|| 739/780 [04:57<00:12,  3.39it/s] 95%|| 740/780 [04:58<00:11,  3.41it/s] 95%|| 741/780 [04:58<00:11,  3.42it/s] 95%|| 742/780 [04:58<00:11,  3.44it/s] 95%|| 743/780 [04:59<00:10,  3.44it/s] 95%|| 744/780 [04:59<00:10,  3.45it/s] 96%|| 745/780 [04:59<00:10,  3.45it/s] 96%|| 746/780 [04:59<00:09,  3.45it/s] 96%|| 747/780 [05:00<00:09,  3.45it/s] 96%|| 748/780 [05:00<00:09,  3.36it/s] 96%|| 749/780 [05:00<00:09,  3.39it/s] 96%|| 750/780 [05:01<00:08,  3.41it/s] 96%|| 751/780 [05:01<00:08,  3.43it/s] 96%|| 752/780 [05:01<00:08,  3.44it/s] 97%|| 753/780 [05:02<00:07,  3.44it/s] 97%|| 754/780 [05:02<00:07,  3.45it/s] 97%|| 755/780 [05:02<00:07,  3.45it/s] 97%|| 756/780 [05:02<00:06,  3.46it/s] 97%|| 757/780 [05:03<00:06,  3.46it/s] 97%|| 758/780 [05:03<00:06,  3.46it/s] 97%|| 759/780 [05:03<00:06,  3.37it/s] 97%|| 760/780 [05:04<00:05,  3.40it/s] 98%|| 761/780 [05:04<00:05,  3.42it/s] 98%|| 762/780 [05:04<00:05,  3.43it/s] 98%|| 763/780 [05:04<00:04,  3.44it/s] 98%|| 764/780 [05:05<00:04,  3.45it/s] 98%|| 765/780 [05:05<00:04,  3.45it/s] 98%|| 766/780 [05:05<00:04,  3.45it/s] 98%|| 767/780 [05:06<00:03,  3.46it/s] 98%|| 768/780 [05:06<00:03,  3.46it/s] 99%|| 769/780 [05:06<00:03,  3.46it/s] 99%|| 770/780 [05:07<00:03,  3.32it/s] 99%|| 771/780 [05:07<00:02,  3.36it/s] 99%|| 772/780 [05:07<00:02,  3.39it/s] 99%|| 773/780 [05:07<00:02,  3.41it/s] 99%|| 774/780 [05:08<00:01,  3.43it/s] 99%|| 775/780 [05:08<00:01,  3.44it/s] 99%|| 776/780 [05:08<00:01,  3.44it/s]100%|| 777/780 [05:09<00:00,  3.46it/s]100%|| 778/780 [05:09<00:00,  3.45it/s]100%|| 779/780 [05:09<00:00,  3.46it/s]100%|| 780/780 [05:09<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 04:01:48,851 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:01:48,851 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 04:01:48,851 >>   Batch size = 8
{'eval_loss': 0.9659960865974426, 'eval_runtime': 9.7214, 'eval_samples_per_second': 358.897, 'eval_steps_per_second': 44.952, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 57.04it/s][A
  3%|         | 12/437 [00:00<00:08, 49.30it/s][A
  4%|         | 17/437 [00:00<00:08, 47.34it/s][A
  5%|         | 22/437 [00:00<00:08, 46.41it/s][A
  6%|         | 27/437 [00:00<00:08, 45.92it/s][A
  7%|         | 32/437 [00:00<00:08, 45.59it/s][A
  8%|         | 37/437 [00:00<00:08, 45.45it/s][A
 10%|         | 42/437 [00:00<00:08, 45.34it/s][A
 11%|         | 47/437 [00:01<00:08, 45.32it/s][A
 12%|        | 52/437 [00:01<00:08, 45.48it/s][A
 13%|        | 57/437 [00:01<00:08, 45.29it/s][A
 14%|        | 62/437 [00:01<00:08, 45.30it/s][A
 15%|        | 67/437 [00:01<00:08, 43.60it/s][A
 16%|        | 72/437 [00:01<00:08, 44.06it/s][A
 18%|        | 77/437 [00:01<00:08, 44.34it/s][A
 19%|        | 82/437 [00:01<00:07, 44.60it/s][A
 20%|        | 87/437 [00:01<00:07, 44.73it/s][A
 21%|        | 92/437 [00:02<00:07, 44.90it/s][A
 22%|       | 97/437 [00:02<00:07, 44.91it/s][A
 23%|       | 102/437 [00:02<00:07, 45.19it/s][A
 24%|       | 107/437 [00:02<00:07, 45.02it/s][A
 26%|       | 112/437 [00:02<00:07, 45.07it/s][A
 27%|       | 117/437 [00:02<00:07, 45.09it/s][A
 28%|       | 122/437 [00:02<00:06, 45.09it/s][A
 29%|       | 127/437 [00:02<00:06, 45.11it/s][A
 30%|       | 132/437 [00:02<00:06, 45.09it/s][A
 31%|      | 137/437 [00:03<00:06, 45.14it/s][A
 32%|      | 142/437 [00:03<00:06, 45.04it/s][A
 34%|      | 147/437 [00:03<00:06, 45.20it/s][A
 35%|      | 152/437 [00:03<00:06, 45.19it/s][A
 36%|      | 157/437 [00:03<00:06, 45.20it/s][A
 37%|      | 162/437 [00:03<00:06, 45.11it/s][A
 38%|      | 167/437 [00:03<00:05, 45.15it/s][A
 39%|      | 172/437 [00:03<00:05, 45.12it/s][A
 41%|      | 177/437 [00:03<00:05, 45.14it/s][A
 42%|     | 182/437 [00:04<00:05, 45.12it/s][A
 43%|     | 187/437 [00:04<00:05, 45.12it/s][A
 44%|     | 192/437 [00:04<00:05, 45.24it/s][A
 45%|     | 197/437 [00:04<00:05, 45.24it/s][A
 46%|     | 202/437 [00:04<00:05, 43.86it/s][A
 47%|     | 207/437 [00:04<00:05, 44.40it/s][A
 49%|     | 212/437 [00:04<00:05, 44.64it/s][A
 50%|     | 217/437 [00:04<00:04, 44.84it/s][A
 51%|     | 222/437 [00:04<00:04, 44.90it/s][A
 52%|    | 227/437 [00:05<00:04, 45.02it/s][A
 53%|    | 232/437 [00:05<00:04, 45.03it/s][A
 54%|    | 237/437 [00:05<00:04, 45.13it/s][A
 55%|    | 242/437 [00:05<00:04, 44.83it/s][A
 57%|    | 247/437 [00:05<00:04, 44.97it/s][A
 58%|    | 252/437 [00:05<00:04, 45.14it/s][A
 59%|    | 257/437 [00:05<00:03, 45.16it/s][A
 60%|    | 262/437 [00:05<00:03, 45.17it/s][A
 61%|    | 267/437 [00:05<00:03, 45.11it/s][A
 62%|   | 272/437 [00:06<00:03, 45.12it/s][A
 63%|   | 277/437 [00:06<00:03, 45.14it/s][A
 65%|   | 282/437 [00:06<00:03, 45.11it/s][A
 66%|   | 287/437 [00:06<00:03, 45.06it/s][A
 67%|   | 292/437 [00:06<00:03, 45.11it/s][A
 68%|   | 297/437 [00:06<00:03, 45.21it/s][A
 69%|   | 302/437 [00:06<00:02, 45.23it/s][A
 70%|   | 307/437 [00:06<00:02, 45.06it/s][A
 71%|  | 312/437 [00:06<00:02, 45.19it/s][A
 73%|  | 317/437 [00:07<00:02, 45.21it/s][A
 74%|  | 322/437 [00:07<00:02, 45.18it/s][A
 75%|  | 327/437 [00:07<00:02, 45.15it/s][A
 76%|  | 332/437 [00:07<00:02, 45.13it/s][A
 77%|  | 337/437 [00:07<00:02, 43.76it/s][A
 78%|  | 342/437 [00:07<00:02, 44.35it/s][A
 79%|  | 347/437 [00:07<00:02, 44.61it/s][A
 81%|  | 352/437 [00:07<00:01, 44.77it/s][A
 82%| | 357/437 [00:07<00:01, 44.91it/s][A
 83%| | 362/437 [00:08<00:01, 44.94it/s][A
 84%| | 367/437 [00:08<00:01, 45.04it/s][A
 85%| | 372/437 [00:08<00:01, 45.04it/s][A
 86%| | 377/437 [00:08<00:01, 44.89it/s][A
 87%| | 382/437 [00:08<00:01, 45.00it/s][A
 89%| | 387/437 [00:08<00:01, 45.20it/s][A
 90%| | 392/437 [00:08<00:00, 45.15it/s][A
 91%| | 397/437 [00:08<00:00, 45.24it/s][A
 92%|| 402/437 [00:08<00:00, 45.23it/s][A
 93%|| 407/437 [00:09<00:00, 45.18it/s][A
 94%|| 412/437 [00:09<00:00, 45.21it/s][A
 95%|| 417/437 [00:09<00:00, 45.15it/s][A
 97%|| 422/437 [00:09<00:00, 45.03it/s][A
 98%|| 427/437 [00:09<00:00, 45.06it/s][A
 99%|| 432/437 [00:09<00:00, 45.13it/s][A
100%|| 437/437 [00:09<00:00, 45.24it/s][A                                                 
                                                 [A100%|| 780/780 [05:19<00:00,  3.46it/s]
100%|| 437/437 [00:09<00:00, 45.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:01:58,731 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 04:01:58,894 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:02:01,631 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:02:01,752 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:02:01,828 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:02:08,049 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:02:08,090 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156 (score: 0.9444257020950317).
                                                 100%|| 780/780 [05:43<00:00,  3.46it/s]100%|| 780/780 [05:43<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-29 04:02:22,112 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 04:02:22,208 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:02:25,072 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:02:25,236 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:02:25,354 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:02:25,809 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   train_loss               =     0.6311
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   train_runtime            = 0:05:43.13
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   train_samples_per_second =    145.715
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:25,810 >>   train_steps_per_second   =      2.273
{'eval_loss': 0.9700531363487244, 'eval_runtime': 9.6981, 'eval_samples_per_second': 359.76, 'eval_steps_per_second': 45.06, 'epoch': 5.0}
{'train_runtime': 343.1355, 'train_samples_per_second': 145.715, 'train_steps_per_second': 2.273, 'train_loss': 0.6311025179349459, 'epoch': 5.0}
08/29/2023 04:02:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:02:26,339 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:02:26,339 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 04:02:26,339 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 56.75it/s]  3%|         | 12/437 [00:00<00:08, 50.04it/s]  4%|         | 18/437 [00:00<00:08, 48.21it/s]  5%|         | 23/437 [00:00<00:08, 47.42it/s]  6%|         | 28/437 [00:00<00:08, 46.96it/s]  8%|         | 33/437 [00:00<00:08, 46.69it/s]  9%|         | 38/437 [00:00<00:08, 46.53it/s] 10%|         | 43/437 [00:00<00:08, 46.06it/s] 11%|         | 48/437 [00:01<00:08, 45.43it/s] 12%|        | 53/437 [00:01<00:08, 45.19it/s] 13%|        | 58/437 [00:01<00:08, 45.32it/s] 14%|        | 63/437 [00:01<00:08, 45.44it/s] 16%|        | 68/437 [00:01<00:08, 45.60it/s] 17%|        | 73/437 [00:01<00:07, 45.74it/s] 18%|        | 78/437 [00:01<00:07, 45.80it/s] 19%|        | 83/437 [00:01<00:07, 45.89it/s] 20%|        | 88/437 [00:01<00:07, 45.68it/s] 21%|       | 93/437 [00:02<00:07, 44.14it/s] 22%|       | 98/437 [00:02<00:07, 44.38it/s] 24%|       | 103/437 [00:02<00:07, 44.65it/s] 25%|       | 108/437 [00:02<00:07, 44.98it/s] 26%|       | 113/437 [00:02<00:07, 45.25it/s] 27%|       | 118/437 [00:02<00:07, 45.41it/s] 28%|       | 123/437 [00:02<00:06, 45.64it/s] 29%|       | 128/437 [00:02<00:06, 45.74it/s] 30%|       | 133/437 [00:02<00:06, 45.54it/s] 32%|      | 138/437 [00:03<00:06, 45.29it/s] 33%|      | 143/437 [00:03<00:06, 45.38it/s] 34%|      | 148/437 [00:03<00:06, 45.33it/s] 35%|      | 153/437 [00:03<00:06, 45.34it/s] 36%|      | 158/437 [00:03<00:06, 45.41it/s] 37%|      | 163/437 [00:03<00:06, 45.53it/s] 38%|      | 168/437 [00:03<00:05, 45.64it/s] 40%|      | 173/437 [00:03<00:05, 45.70it/s] 41%|      | 178/437 [00:03<00:05, 45.71it/s] 42%|     | 183/437 [00:03<00:05, 45.68it/s] 43%|     | 188/437 [00:04<00:05, 45.60it/s] 44%|     | 193/437 [00:04<00:05, 45.64it/s] 45%|     | 198/437 [00:04<00:05, 45.59it/s] 46%|     | 203/437 [00:04<00:05, 45.63it/s] 48%|     | 208/437 [00:04<00:05, 45.59it/s] 49%|     | 213/437 [00:04<00:04, 45.78it/s] 50%|     | 218/437 [00:04<00:04, 45.90it/s] 51%|     | 223/437 [00:04<00:04, 45.79it/s] 52%|    | 228/437 [00:04<00:04, 45.73it/s] 53%|    | 233/437 [00:05<00:04, 44.90it/s] 54%|    | 238/437 [00:05<00:04, 45.18it/s] 56%|    | 243/437 [00:05<00:04, 45.32it/s] 57%|    | 248/437 [00:05<00:04, 45.30it/s] 58%|    | 253/437 [00:05<00:04, 45.41it/s] 59%|    | 258/437 [00:05<00:03, 45.48it/s] 60%|    | 263/437 [00:05<00:03, 45.66it/s] 61%|   | 268/437 [00:05<00:03, 45.84it/s] 62%|   | 273/437 [00:05<00:03, 45.70it/s] 64%|   | 278/437 [00:06<00:03, 45.71it/s] 65%|   | 283/437 [00:06<00:03, 45.61it/s] 66%|   | 288/437 [00:06<00:03, 45.57it/s] 67%|   | 293/437 [00:06<00:03, 45.50it/s] 68%|   | 298/437 [00:06<00:03, 45.41it/s] 69%|   | 303/437 [00:06<00:02, 45.50it/s] 70%|   | 308/437 [00:06<00:02, 45.60it/s] 72%|  | 313/437 [00:06<00:02, 45.72it/s] 73%|  | 318/437 [00:06<00:02, 45.72it/s] 74%|  | 323/437 [00:07<00:02, 45.74it/s] 75%|  | 328/437 [00:07<00:02, 45.73it/s] 76%|  | 333/437 [00:07<00:02, 45.69it/s] 77%|  | 338/437 [00:07<00:02, 45.61it/s] 78%|  | 343/437 [00:07<00:02, 45.46it/s] 80%|  | 348/437 [00:07<00:01, 45.41it/s] 81%|  | 353/437 [00:07<00:01, 45.50it/s] 82%| | 358/437 [00:07<00:01, 45.64it/s] 83%| | 363/437 [00:07<00:01, 45.65it/s] 84%| | 368/437 [00:08<00:01, 45.72it/s] 85%| | 373/437 [00:08<00:01, 42.21it/s] 86%| | 378/437 [00:08<00:01, 43.32it/s] 88%| | 383/437 [00:08<00:01, 44.08it/s] 89%| | 388/437 [00:08<00:01, 44.56it/s] 90%| | 393/437 [00:08<00:00, 44.89it/s] 91%| | 398/437 [00:08<00:00, 45.02it/s] 92%|| 403/437 [00:08<00:00, 45.17it/s] 93%|| 408/437 [00:08<00:00, 45.31it/s] 95%|| 413/437 [00:09<00:00, 45.23it/s] 96%|| 418/437 [00:09<00:00, 45.29it/s] 97%|| 423/437 [00:09<00:00, 45.42it/s] 98%|| 428/437 [00:09<00:00, 45.57it/s] 99%|| 433/437 [00:09<00:00, 45.58it/s]100%|| 437/437 [00:09<00:00, 45.53it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:02:35,957 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,957 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   eval_loss               =     0.9444
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   eval_runtime            = 0:00:09.61
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   eval_samples_per_second =    362.753
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   eval_steps_per_second   =     45.435
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:02:35,958 >>   perplexity              =     2.5713
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:45,887 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:45,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:45,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:45,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:45,913 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:02:46,702 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:02:46,703 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:02:47,303 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:02:48,401 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:02:48,401 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:51,361 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:51,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:51,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:51,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:02:51,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:02:52,103 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:02:52,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:02:52,749 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:02:52,965 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:02:52,965 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.57it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:23,  1.56it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.44it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:37,  1.50it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.58it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:43,  1.61it/s]Extractor Predicting: 70it [00:44,  1.64it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.62it/s]Extractor Predicting: 73it [00:46,  1.61it/s]Extractor Predicting: 74it [00:47,  1.63it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:50,  1.62it/s]Extractor Predicting: 80it [00:50,  1.64it/s]Extractor Predicting: 81it [00:51,  1.64it/s]Extractor Predicting: 82it [00:51,  1.66it/s]Extractor Predicting: 83it [00:52,  1.64it/s]Extractor Predicting: 84it [00:53,  1.61it/s]Extractor Predicting: 85it [00:53,  1.64it/s]Extractor Predicting: 86it [00:54,  1.66it/s]Extractor Predicting: 87it [00:55,  1.66it/s]Extractor Predicting: 88it [00:55,  1.65it/s]Extractor Predicting: 89it [00:56,  1.64it/s]Extractor Predicting: 90it [00:56,  1.65it/s]Extractor Predicting: 91it [00:57,  1.64it/s]Extractor Predicting: 92it [00:58,  1.57it/s]Extractor Predicting: 93it [00:58,  1.59it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [01:00,  1.59it/s]Extractor Predicting: 96it [01:00,  1.58it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:01,  1.62it/s]Extractor Predicting: 99it [01:02,  1.60it/s]Extractor Predicting: 100it [01:03,  1.58it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.56it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:06,  1.57it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.56it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:11,  1.56it/s]Extractor Predicting: 114it [01:12,  1.57it/s]Extractor Predicting: 115it [01:12,  1.57it/s]Extractor Predicting: 116it [01:13,  1.59it/s]Extractor Predicting: 117it [01:13,  1.58it/s]Extractor Predicting: 118it [01:14,  1.59it/s]Extractor Predicting: 119it [01:15,  1.63it/s]Extractor Predicting: 120it [01:15,  1.67it/s]Extractor Predicting: 121it [01:16,  1.67it/s]Extractor Predicting: 122it [01:16,  1.67it/s]Extractor Predicting: 123it [01:17,  1.61it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:18,  1.50it/s]Extractor Predicting: 126it [01:19,  1.54it/s]Extractor Predicting: 127it [01:20,  1.56it/s]Extractor Predicting: 128it [01:20,  1.52it/s]Extractor Predicting: 129it [01:21,  1.58it/s]Extractor Predicting: 130it [01:22,  1.56it/s]Extractor Predicting: 131it [01:22,  1.58it/s]Extractor Predicting: 132it [01:23,  1.60it/s]Extractor Predicting: 133it [01:24,  1.55it/s]Extractor Predicting: 134it [01:24,  1.56it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:25,  1.61it/s]Extractor Predicting: 137it [01:26,  1.60it/s]Extractor Predicting: 138it [01:27,  1.58it/s]Extractor Predicting: 139it [01:27,  1.61it/s]Extractor Predicting: 140it [01:28,  1.59it/s]Extractor Predicting: 141it [01:29,  1.59it/s]Extractor Predicting: 142it [01:29,  1.60it/s]Extractor Predicting: 143it [01:30,  1.62it/s]Extractor Predicting: 144it [01:30,  1.62it/s]Extractor Predicting: 145it [01:31,  2.09it/s]Extractor Predicting: 145it [01:31,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:36,729 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:36,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:36,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:36,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:36,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:04:37,744 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:04:37,745 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:04:38,387 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:04:39,510 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:04:39,510 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:42,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:42,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:42,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:42,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:04:42,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:04:43,554 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:04:43,556 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:04:44,160 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:04:44,379 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:04:44,380 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.450920245398773,
  "recall": 0.16852966466036112,
  "score": 0.2453578134779887,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:08,  1.55it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.64it/s]Extractor Predicting: 38it [00:24,  1.64it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:36,  1.64it/s]Extractor Predicting: 59it [00:37,  1.64it/s]Extractor Predicting: 60it [00:37,  1.72it/s]Extractor Predicting: 61it [00:38,  1.74it/s]Extractor Predicting: 62it [00:38,  1.76it/s]Extractor Predicting: 63it [00:39,  1.76it/s]Extractor Predicting: 64it [00:39,  1.78it/s]Extractor Predicting: 65it [00:40,  1.79it/s]Extractor Predicting: 66it [00:40,  1.79it/s]Extractor Predicting: 67it [00:41,  1.82it/s]Extractor Predicting: 68it [00:41,  1.86it/s]Extractor Predicting: 69it [00:42,  1.89it/s]Extractor Predicting: 70it [00:43,  1.85it/s]Extractor Predicting: 71it [00:43,  1.85it/s]Extractor Predicting: 72it [00:44,  1.86it/s]Extractor Predicting: 73it [00:44,  1.86it/s]Extractor Predicting: 74it [00:45,  1.84it/s]Extractor Predicting: 75it [00:45,  1.82it/s]Extractor Predicting: 76it [00:46,  1.84it/s]Extractor Predicting: 77it [00:46,  1.86it/s]Extractor Predicting: 78it [00:47,  1.84it/s]Extractor Predicting: 79it [00:47,  1.86it/s]Extractor Predicting: 80it [00:48,  1.87it/s]Extractor Predicting: 81it [00:49,  1.86it/s]Extractor Predicting: 82it [00:49,  1.78it/s]Extractor Predicting: 83it [00:50,  1.78it/s]Extractor Predicting: 84it [00:50,  1.78it/s]Extractor Predicting: 85it [00:51,  1.80it/s]Extractor Predicting: 86it [00:51,  1.75it/s]Extractor Predicting: 87it [00:52,  1.70it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:53,  1.63it/s]Extractor Predicting: 90it [00:54,  1.58it/s]Extractor Predicting: 91it [00:55,  1.58it/s]Extractor Predicting: 92it [00:55,  1.56it/s]Extractor Predicting: 93it [00:56,  1.58it/s]Extractor Predicting: 94it [00:57,  1.57it/s]Extractor Predicting: 95it [00:57,  1.58it/s]Extractor Predicting: 96it [00:58,  1.57it/s]Extractor Predicting: 97it [00:58,  1.57it/s]Extractor Predicting: 98it [00:59,  1.56it/s]Extractor Predicting: 99it [01:00,  1.58it/s]Extractor Predicting: 100it [01:00,  1.57it/s]Extractor Predicting: 101it [01:01,  1.58it/s]Extractor Predicting: 102it [01:02,  1.56it/s]Extractor Predicting: 103it [01:02,  1.57it/s]Extractor Predicting: 104it [01:03,  1.54it/s]Extractor Predicting: 105it [01:04,  1.55it/s]Extractor Predicting: 106it [01:04,  1.52it/s]Extractor Predicting: 107it [01:05,  1.51it/s]Extractor Predicting: 108it [01:06,  1.51it/s]Extractor Predicting: 109it [01:06,  1.55it/s]Extractor Predicting: 110it [01:07,  1.52it/s]Extractor Predicting: 111it [01:08,  1.52it/s]Extractor Predicting: 112it [01:08,  1.50it/s]Extractor Predicting: 113it [01:09,  1.52it/s]Extractor Predicting: 114it [01:09,  1.56it/s]Extractor Predicting: 115it [01:10,  1.53it/s]Extractor Predicting: 116it [01:11,  1.57it/s]Extractor Predicting: 117it [01:11,  1.60it/s]Extractor Predicting: 118it [01:12,  1.55it/s]Extractor Predicting: 119it [01:13,  1.36it/s]Extractor Predicting: 120it [01:14,  1.40it/s]Extractor Predicting: 121it [01:14,  1.42it/s]Extractor Predicting: 122it [01:15,  1.47it/s]Extractor Predicting: 123it [01:16,  1.50it/s]Extractor Predicting: 124it [01:16,  1.51it/s]Extractor Predicting: 125it [01:17,  1.49it/s]Extractor Predicting: 126it [01:18,  1.48it/s]Extractor Predicting: 127it [01:18,  1.49it/s]Extractor Predicting: 128it [01:19,  1.47it/s]Extractor Predicting: 129it [01:20,  1.48it/s]Extractor Predicting: 130it [01:20,  1.47it/s]Extractor Predicting: 131it [01:21,  1.51it/s]Extractor Predicting: 132it [01:22,  1.52it/s]Extractor Predicting: 133it [01:22,  1.53it/s]Extractor Predicting: 134it [01:23,  1.55it/s]Extractor Predicting: 135it [01:24,  1.52it/s]Extractor Predicting: 136it [01:24,  1.49it/s]Extractor Predicting: 137it [01:25,  1.50it/s]Extractor Predicting: 138it [01:26,  1.50it/s]Extractor Predicting: 139it [01:26,  1.49it/s]Extractor Predicting: 140it [01:27,  1.48it/s]Extractor Predicting: 141it [01:28,  1.50it/s]Extractor Predicting: 142it [01:28,  1.51it/s]Extractor Predicting: 143it [01:29,  1.48it/s]Extractor Predicting: 144it [01:30,  1.49it/s]Extractor Predicting: 145it [01:30,  1.48it/s]Extractor Predicting: 146it [01:31,  1.50it/s]Extractor Predicting: 147it [01:32,  1.53it/s]Extractor Predicting: 148it [01:32,  1.56it/s]Extractor Predicting: 149it [01:33,  1.53it/s]Extractor Predicting: 150it [01:34,  1.51it/s]Extractor Predicting: 151it [01:34,  1.54it/s]Extractor Predicting: 152it [01:35,  1.56it/s]Extractor Predicting: 153it [01:35,  1.57it/s]Extractor Predicting: 154it [01:36,  1.60it/s]Extractor Predicting: 155it [01:37,  1.61it/s]Extractor Predicting: 156it [01:37,  1.62it/s]Extractor Predicting: 157it [01:38,  1.61it/s]Extractor Predicting: 158it [01:38,  1.64it/s]Extractor Predicting: 159it [01:39,  1.69it/s]Extractor Predicting: 160it [01:40,  1.65it/s]Extractor Predicting: 161it [01:40,  1.60it/s]Extractor Predicting: 162it [01:41,  1.58it/s]Extractor Predicting: 163it [01:42,  1.59it/s]Extractor Predicting: 164it [01:42,  1.58it/s]Extractor Predicting: 165it [01:43,  1.58it/s]Extractor Predicting: 166it [01:44,  1.55it/s]Extractor Predicting: 167it [01:44,  1.56it/s]Extractor Predicting: 168it [01:45,  1.56it/s]Extractor Predicting: 169it [01:45,  1.55it/s]Extractor Predicting: 170it [01:46,  1.58it/s]Extractor Predicting: 171it [01:47,  1.56it/s]Extractor Predicting: 172it [01:47,  1.55it/s]Extractor Predicting: 173it [01:48,  1.57it/s]Extractor Predicting: 174it [01:49,  1.57it/s]Extractor Predicting: 175it [01:49,  1.57it/s]Extractor Predicting: 176it [01:50,  1.55it/s]Extractor Predicting: 177it [01:51,  1.60it/s]Extractor Predicting: 178it [01:51,  1.59it/s]Extractor Predicting: 179it [01:52,  1.62it/s]Extractor Predicting: 180it [01:52,  1.63it/s]Extractor Predicting: 181it [01:53,  1.60it/s]Extractor Predicting: 182it [01:54,  1.65it/s]Extractor Predicting: 183it [01:54,  1.66it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:55,  1.67it/s]Extractor Predicting: 186it [01:56,  1.65it/s]Extractor Predicting: 187it [01:57,  1.62it/s]Extractor Predicting: 188it [01:57,  1.66it/s]Extractor Predicting: 189it [01:58,  1.68it/s]Extractor Predicting: 190it [01:58,  1.72it/s]Extractor Predicting: 191it [01:59,  1.66it/s]Extractor Predicting: 192it [02:00,  1.67it/s]Extractor Predicting: 193it [02:00,  1.68it/s]Extractor Predicting: 194it [02:01,  1.73it/s]Extractor Predicting: 195it [02:01,  1.69it/s]Extractor Predicting: 196it [02:02,  1.69it/s]Extractor Predicting: 197it [02:02,  1.70it/s]Extractor Predicting: 198it [02:03,  1.67it/s]Extractor Predicting: 199it [02:04,  1.63it/s]Extractor Predicting: 200it [02:04,  1.65it/s]Extractor Predicting: 201it [02:05,  1.65it/s]Extractor Predicting: 202it [02:06,  1.68it/s]Extractor Predicting: 203it [02:06,  1.72it/s]Extractor Predicting: 204it [02:07,  1.73it/s]Extractor Predicting: 205it [02:07,  1.72it/s]Extractor Predicting: 206it [02:08,  1.78it/s]Extractor Predicting: 207it [02:08,  1.74it/s]Extractor Predicting: 208it [02:09,  1.78it/s]Extractor Predicting: 209it [02:09,  1.76it/s]Extractor Predicting: 210it [02:10,  1.75it/s]Extractor Predicting: 211it [02:11,  1.74it/s]Extractor Predicting: 212it [02:11,  1.79it/s]Extractor Predicting: 213it [02:12,  1.79it/s]Extractor Predicting: 214it [02:12,  1.80it/s]Extractor Predicting: 215it [02:13,  1.84it/s]Extractor Predicting: 216it [02:13,  1.83it/s]Extractor Predicting: 217it [02:14,  1.79it/s]Extractor Predicting: 218it [02:14,  1.77it/s]Extractor Predicting: 219it [02:15,  1.76it/s]Extractor Predicting: 220it [02:16,  1.77it/s]Extractor Predicting: 221it [02:16,  1.79it/s]Extractor Predicting: 222it [02:17,  1.85it/s]Extractor Predicting: 223it [02:17,  1.79it/s]Extractor Predicting: 224it [02:18,  1.82it/s]Extractor Predicting: 225it [02:18,  1.78it/s]Extractor Predicting: 226it [02:19,  1.79it/s]Extractor Predicting: 227it [02:19,  1.82it/s]Extractor Predicting: 228it [02:20,  1.81it/s]Extractor Predicting: 229it [02:21,  1.72it/s]Extractor Predicting: 230it [02:21,  1.67it/s]Extractor Predicting: 231it [02:22,  1.59it/s]Extractor Predicting: 232it [02:23,  1.54it/s]Extractor Predicting: 233it [02:23,  1.55it/s]Extractor Predicting: 234it [02:24,  1.53it/s]Extractor Predicting: 235it [02:25,  1.54it/s]Extractor Predicting: 236it [02:25,  1.53it/s]Extractor Predicting: 237it [02:26,  1.50it/s]Extractor Predicting: 238it [02:27,  1.33it/s]Extractor Predicting: 239it [02:28,  1.38it/s]Extractor Predicting: 240it [02:28,  1.41it/s]Extractor Predicting: 241it [02:29,  1.43it/s]Extractor Predicting: 242it [02:30,  1.47it/s]Extractor Predicting: 243it [02:30,  1.44it/s]Extractor Predicting: 244it [02:31,  1.48it/s]Extractor Predicting: 245it [02:32,  1.50it/s]Extractor Predicting: 246it [02:32,  1.50it/s]Extractor Predicting: 247it [02:33,  1.46it/s]Extractor Predicting: 248it [02:34,  1.46it/s]Extractor Predicting: 249it [02:34,  1.43it/s]Extractor Predicting: 250it [02:35,  1.46it/s]Extractor Predicting: 251it [02:36,  1.47it/s]Extractor Predicting: 252it [02:36,  1.50it/s]Extractor Predicting: 253it [02:37,  1.51it/s]Extractor Predicting: 254it [02:38,  1.50it/s]Extractor Predicting: 255it [02:38,  1.50it/s]Extractor Predicting: 256it [02:39,  1.51it/s]Extractor Predicting: 257it [02:40,  1.57it/s]Extractor Predicting: 258it [02:40,  1.58it/s]Extractor Predicting: 259it [02:41,  1.59it/s]Extractor Predicting: 260it [02:41,  1.63it/s]Extractor Predicting: 261it [02:42,  1.60it/s]Extractor Predicting: 262it [02:43,  1.61it/s]Extractor Predicting: 263it [02:43,  1.63it/s]Extractor Predicting: 264it [02:44,  1.64it/s]Extractor Predicting: 265it [02:45,  1.62it/s]Extractor Predicting: 266it [02:45,  1.62it/s]Extractor Predicting: 267it [02:46,  1.65it/s]Extractor Predicting: 268it [02:46,  1.62it/s]Extractor Predicting: 269it [02:47,  1.64it/s]Extractor Predicting: 270it [02:48,  1.62it/s]Extractor Predicting: 271it [02:48,  1.59it/s]Extractor Predicting: 272it [02:49,  1.59it/s]Extractor Predicting: 273it [02:50,  1.59it/s]Extractor Predicting: 274it [02:50,  1.58it/s]Extractor Predicting: 275it [02:51,  1.62it/s]Extractor Predicting: 276it [02:51,  1.60it/s]Extractor Predicting: 277it [02:52,  1.59it/s]Extractor Predicting: 278it [02:53,  1.56it/s]Extractor Predicting: 279it [02:53,  1.59it/s]Extractor Predicting: 280it [02:54,  1.59it/s]Extractor Predicting: 281it [02:55,  1.61it/s]Extractor Predicting: 282it [02:55,  1.60it/s]Extractor Predicting: 283it [02:56,  1.60it/s]Extractor Predicting: 284it [02:56,  1.62it/s]Extractor Predicting: 285it [02:57,  1.59it/s]Extractor Predicting: 286it [02:58,  1.57it/s]Extractor Predicting: 287it [02:58,  1.60it/s]Extractor Predicting: 288it [02:59,  1.60it/s]Extractor Predicting: 289it [03:00,  1.59it/s]Extractor Predicting: 290it [03:00,  1.60it/s]Extractor Predicting: 291it [03:01,  1.58it/s]Extractor Predicting: 292it [03:01,  1.59it/s]Extractor Predicting: 293it [03:02,  1.59it/s]Extractor Predicting: 294it [03:03,  1.59it/s]Extractor Predicting: 295it [03:03,  1.60it/s]Extractor Predicting: 296it [03:04,  1.59it/s]Extractor Predicting: 297it [03:05,  1.60it/s]Extractor Predicting: 298it [03:05,  1.57it/s]Extractor Predicting: 299it [03:06,  1.62it/s]Extractor Predicting: 300it [03:06,  1.61it/s]Extractor Predicting: 301it [03:07,  1.62it/s]Extractor Predicting: 302it [03:08,  1.60it/s]Extractor Predicting: 303it [03:08,  1.65it/s]Extractor Predicting: 304it [03:09,  1.63it/s]Extractor Predicting: 305it [03:09,  1.66it/s]Extractor Predicting: 306it [03:10,  1.64it/s]Extractor Predicting: 307it [03:11,  1.62it/s]Extractor Predicting: 308it [03:11,  1.61it/s]Extractor Predicting: 309it [03:12,  1.64it/s]Extractor Predicting: 310it [03:13,  1.58it/s]Extractor Predicting: 311it [03:13,  1.57it/s]Extractor Predicting: 312it [03:14,  1.58it/s]Extractor Predicting: 313it [03:14,  1.60it/s]Extractor Predicting: 314it [03:15,  1.60it/s]Extractor Predicting: 315it [03:16,  1.61it/s]Extractor Predicting: 316it [03:16,  1.61it/s]Extractor Predicting: 317it [03:17,  1.60it/s]Extractor Predicting: 318it [03:18,  1.66it/s]Extractor Predicting: 319it [03:18,  1.65it/s]Extractor Predicting: 320it [03:19,  1.67it/s]Extractor Predicting: 321it [03:19,  1.64it/s]Extractor Predicting: 322it [03:20,  1.63it/s]Extractor Predicting: 323it [03:21,  1.62it/s]Extractor Predicting: 324it [03:21,  1.61it/s]Extractor Predicting: 325it [03:22,  1.58it/s]Extractor Predicting: 326it [03:23,  1.59it/s]Extractor Predicting: 327it [03:23,  1.58it/s]Extractor Predicting: 328it [03:24,  1.58it/s]Extractor Predicting: 329it [03:24,  1.54it/s]Extractor Predicting: 330it [03:25,  1.54it/s]Extractor Predicting: 331it [03:26,  1.56it/s]Extractor Predicting: 332it [03:26,  1.60it/s]Extractor Predicting: 333it [03:27,  1.61it/s]Extractor Predicting: 334it [03:28,  1.61it/s]Extractor Predicting: 335it [03:28,  1.60it/s]Extractor Predicting: 336it [03:29,  1.60it/s]Extractor Predicting: 337it [03:29,  1.59it/s]Extractor Predicting: 338it [03:30,  1.60it/s]Extractor Predicting: 339it [03:31,  1.58it/s]Extractor Predicting: 340it [03:31,  1.62it/s]Extractor Predicting: 341it [03:32,  1.61it/s]Extractor Predicting: 342it [03:33,  1.64it/s]Extractor Predicting: 343it [03:33,  1.58it/s]Extractor Predicting: 344it [03:34,  1.60it/s]Extractor Predicting: 345it [03:35,  1.38it/s]Extractor Predicting: 346it [03:35,  1.44it/s]Extractor Predicting: 347it [03:36,  1.48it/s]Extractor Predicting: 348it [03:37,  1.53it/s]Extractor Predicting: 349it [03:37,  1.50it/s]Extractor Predicting: 350it [03:38,  1.50it/s]Extractor Predicting: 351it [03:39,  1.50it/s]Extractor Predicting: 352it [03:39,  1.52it/s]Extractor Predicting: 353it [03:40,  1.52it/s]Extractor Predicting: 354it [03:41,  1.53it/s]Extractor Predicting: 355it [03:41,  1.53it/s]Extractor Predicting: 356it [03:42,  1.57it/s]Extractor Predicting: 357it [03:43,  1.55it/s]Extractor Predicting: 358it [03:43,  1.54it/s]Extractor Predicting: 359it [03:44,  1.51it/s]Extractor Predicting: 360it [03:45,  1.49it/s]Extractor Predicting: 361it [03:45,  1.54it/s]Extractor Predicting: 362it [03:46,  1.57it/s]Extractor Predicting: 363it [03:46,  1.56it/s]Extractor Predicting: 364it [03:47,  1.57it/s]Extractor Predicting: 365it [03:48,  1.58it/s]Extractor Predicting: 366it [03:48,  1.56it/s]Extractor Predicting: 367it [03:49,  1.55it/s]Extractor Predicting: 368it [03:50,  1.55it/s]Extractor Predicting: 369it [03:50,  1.55it/s]Extractor Predicting: 370it [03:51,  1.55it/s]Extractor Predicting: 371it [03:52,  1.58it/s]Extractor Predicting: 372it [03:52,  1.60it/s]Extractor Predicting: 373it [03:53,  1.63it/s]Extractor Predicting: 374it [03:53,  1.58it/s]Extractor Predicting: 375it [03:54,  1.58it/s]Extractor Predicting: 376it [03:55,  1.61it/s]Extractor Predicting: 377it [03:55,  1.62it/s]Extractor Predicting: 378it [03:56,  1.64it/s]Extractor Predicting: 379it [03:56,  1.60it/s]Extractor Predicting: 380it [03:57,  1.61it/s]Extractor Predicting: 381it [03:58,  1.62it/s]Extractor Predicting: 382it [03:58,  1.60it/s]Extractor Predicting: 383it [03:59,  1.63it/s]Extractor Predicting: 384it [04:00,  1.62it/s]Extractor Predicting: 385it [04:00,  1.63it/s]Extractor Predicting: 386it [04:01,  1.67it/s]Extractor Predicting: 387it [04:01,  1.68it/s]Extractor Predicting: 388it [04:02,  1.64it/s]Extractor Predicting: 389it [04:03,  1.64it/s]Extractor Predicting: 390it [04:03,  1.63it/s]Extractor Predicting: 391it [04:04,  1.62it/s]Extractor Predicting: 392it [04:04,  1.61it/s]Extractor Predicting: 393it [04:05,  1.63it/s]Extractor Predicting: 394it [04:06,  1.63it/s]Extractor Predicting: 395it [04:06,  1.59it/s]Extractor Predicting: 396it [04:07,  1.62it/s]Extractor Predicting: 397it [04:08,  1.64it/s]Extractor Predicting: 398it [04:08,  1.61it/s]Extractor Predicting: 399it [04:09,  1.61it/s]Extractor Predicting: 400it [04:09,  1.61it/s]Extractor Predicting: 401it [04:10,  1.63it/s]Extractor Predicting: 402it [04:11,  1.67it/s]Extractor Predicting: 403it [04:11,  1.63it/s]Extractor Predicting: 404it [04:12,  1.65it/s]Extractor Predicting: 405it [04:12,  1.65it/s]Extractor Predicting: 406it [04:13,  1.66it/s]Extractor Predicting: 407it [04:14,  1.64it/s]Extractor Predicting: 408it [04:14,  1.65it/s]Extractor Predicting: 409it [04:15,  1.68it/s]Extractor Predicting: 410it [04:15,  1.67it/s]Extractor Predicting: 411it [04:16,  1.64it/s]Extractor Predicting: 412it [04:17,  1.66it/s]Extractor Predicting: 413it [04:17,  1.67it/s]Extractor Predicting: 414it [04:18,  1.66it/s]Extractor Predicting: 415it [04:18,  1.69it/s]Extractor Predicting: 416it [04:19,  1.70it/s]Extractor Predicting: 417it [04:20,  1.66it/s]Extractor Predicting: 418it [04:20,  1.68it/s]Extractor Predicting: 419it [04:21,  1.66it/s]Extractor Predicting: 420it [04:21,  1.64it/s]Extractor Predicting: 421it [04:22,  1.63it/s]Extractor Predicting: 422it [04:23,  1.63it/s]Extractor Predicting: 423it [04:23,  1.64it/s]Extractor Predicting: 424it [04:24,  1.64it/s]Extractor Predicting: 425it [04:24,  1.85it/s]Extractor Predicting: 425it [04:24,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:22,831 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:22,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:22,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:22,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:22,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:09:23,532 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:09:23,533 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:09:24,154 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:09:25,215 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:09:25,215 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:28,708 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:28,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:28,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:28,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:28,711 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:09:29,361 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:09:29,362 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:09:29,959 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:09:30,128 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:09:30,129 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3093891402714932,
  "recall": 0.10740231690555664,
  "score": 0.15945197493076813,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.77it/s]Extractor Predicting: 7it [00:04,  1.60it/s]
[INFO|configuration_utils.py:515] 2023-08-29 04:09:35,655 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:09:35,656 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:09:35,718 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:09:35,719 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 04:09:35,731 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:09:47,047 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 04:09:47,102 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 04:09:47,340 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:09:47,342 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:09:47,441 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:09:47,523 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.375,
  "recall": 0.03821656050955414,
  "score": 0.06936416184971099,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 04:09:47,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:48,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:48,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:49,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:50,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:50,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:51,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:51,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:52,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:53,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:53,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:54,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:54,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:55,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:56,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:56,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:57,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:57,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:58,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:58,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:09:59,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:12<03:48, 12.00s/it][WARNING|generation_utils.py:914] 2023-08-29 04:09:59,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:00,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:01,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:01,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:02,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:02,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:03,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:04,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:04,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:05,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:05,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:06,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:07,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:08,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:09,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:09,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:10,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:10,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:11,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:11,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:12,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:25<03:48, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-29 04:10:13,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:13,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:14,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:15,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:15,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:16,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:16,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:17,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:17,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:18,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:19,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:19,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:20,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:20,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:21,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:21,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:22,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:23,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:23,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:24,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:24,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:25,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:37<03:36, 12.73s/it][WARNING|generation_utils.py:914] 2023-08-29 04:10:25,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:26,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:26,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:27,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:28,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:28,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:29,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:29,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:30,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:30,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:31,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:31,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:32,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:32,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:33,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:33,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:34,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:34,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:35,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:35,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:36,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:36,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:49<03:14, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-29 04:10:37,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:37,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:38,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:38,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:39,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:40,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:40,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:41,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:41,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:42,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:42,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:43,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:43,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:44,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:45,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:45,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:46,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:46,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:47,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:47,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:48,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [01:01<03:00, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-29 04:10:49,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:49,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:50,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:50,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:51,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:51,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:52,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:53,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:53,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:54,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:54,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:55,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:55,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:56,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:57,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:57,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:58,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:59,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:10:59,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:12<02:44, 11.74s/it][WARNING|generation_utils.py:914] 2023-08-29 04:11:00,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:00,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:01,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:01,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:02,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:02,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:03,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:04,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:04,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:05,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:05,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:06,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:06,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:07,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:07,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:08,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:09,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:09,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:10,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:11,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:11,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:12,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:24<02:36, 12.03s/it][WARNING|generation_utils.py:914] 2023-08-29 04:11:12,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:13,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:13,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:14,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:15,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:15,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:16,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:16,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:17,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:17,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:18,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:18,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:19,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:19,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:20,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:21,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:21,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:22,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:23,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:23,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:24,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:24,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:37<02:25, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-29 04:11:25,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:25,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:26,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:27,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:27,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:28,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:29,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:29,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:30,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:31,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:31,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:32,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:33,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:33,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:34,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:34,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:35,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:36,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:36,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:37,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:37,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:38,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [01:51<02:20, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-29 04:11:39,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:39,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:40,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:41,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:41,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:42,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:42,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:43,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:44,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:44,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:45,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:45,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:46,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:47,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:47,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:48,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:48,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:49,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:50,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:50,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:51,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:03<02:06, 12.69s/it][WARNING|generation_utils.py:914] 2023-08-29 04:11:51,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:52,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:52,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:53,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:54,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:54,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:55,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:55,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:56,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:57,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:57,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:58,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:59,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:11:59,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:00,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:00,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:01,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:02,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:02,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:03,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:16<01:52, 12.52s/it][WARNING|generation_utils.py:914] 2023-08-29 04:12:03,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:04,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:05,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:05,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:06,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:06,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:07,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:08,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:08,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:09,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:09,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:10,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:10,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:11,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:11,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:12,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:13,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:13,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:14,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:14,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:15,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:27<01:38, 12.30s/it][WARNING|generation_utils.py:914] 2023-08-29 04:12:15,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:16,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:16,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:17,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:17,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:18,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:18,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:19,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:19,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:20,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:21,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:21,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:22,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:22,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:23,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:23,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:23,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:24,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:24,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:25,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:25,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:38<01:22, 11.78s/it][WARNING|generation_utils.py:914] 2023-08-29 04:12:26,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:26,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:27,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:28,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:28,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:29,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:30,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:30,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:31,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:32,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:32,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:33,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:34,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:34,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:35,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:35,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:36,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:37,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:37,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:38,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:39,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [02:51<01:13, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-29 04:12:39,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:40,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:41,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:42,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:42,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:43,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:44,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:44,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:45,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:45,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:46,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:47,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:47,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:48,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:48,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:49,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:50,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:50,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:51,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:51,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:52,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [03:05<01:02, 12.58s/it][WARNING|generation_utils.py:914] 2023-08-29 04:12:53,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:53,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:54,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:54,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:55,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:56,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:56,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:57,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:57,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:58,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:12:59,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:00,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:00,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:01,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:01,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:02,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:03,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:03,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:04,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:04,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:05,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:18<00:50, 12.73s/it][WARNING|generation_utils.py:914] 2023-08-29 04:13:06,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:06,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:07,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:07,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:08,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:09,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:09,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:10,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:10,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:11,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:11,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:11,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:12,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:13,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:13,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:14,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:14,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:15,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:16,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:29<00:36, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-29 04:13:17,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:17,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:18,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:18,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:19,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:20,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:20,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:21,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:22,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:22,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:23,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:24,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:25,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:25,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:26,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:26,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:27,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:28,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:41<00:24, 12.23s/it][WARNING|generation_utils.py:914] 2023-08-29 04:13:29,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:29,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:30,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:31,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:31,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:32,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:32,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:33,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:33,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:34,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:34,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:35,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:35,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:36,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:37,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:37,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:38,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:38,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:39,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:39,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:40,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [03:53<00:12, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-29 04:13:41,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:41,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:42,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:43,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:43,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:44,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:44,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:45,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:45,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:46,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:47,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:48,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:48,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:49,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:49,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:50,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:50,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:51,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:13:51,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [04:04<00:00, 11.77s/it]Generating: 100%|| 20/20 [04:04<00:00, 12.21s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:00,438 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:00,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:00,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:00,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:00,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:14:01,081 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:14:01,082 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:14:01,668 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:14:02,724 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:14:02,725 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:05,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:05,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:05,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:05,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:05,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:14:06,697 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:14:06,698 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:14:07,301 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:14:07,475 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:14:07,475 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8764204545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8806818181818182, 'errors': {'', "('1924 Chinese national team', 'competition class', '', 'He was chosen as the 14th overall selection in the 1924 Chinese national team competition , where he led the team to the 1928 and 1931 Olympics .')"}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an additional two years to run until his death . Head Entity : William , Tail Entity : Czech Republic .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an additional two years to run until his death . Head Entity : William , Tail Entity : Czech Republic .\n', 'Relation : country of citizenship . Context : After he was elected to serve as Chancellor in 1999 , he entered Parliament as an independent , then as Prime Minister in 1999 . Head Entity : Prime Minister , Tail Entity : European Union .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : father .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9196428571428571, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9107142857142857, 'errors': {''}}
['Relation : licensed to broadcast to . Context : Later in 2008 , the network announced that Fox would be releasing a series titled " Family Guy " based on George R. R. Martin \'s novel , " The Last Airbender " . Head Entity : The Last Airbender , Tail Entity : HBO .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( 11411149 ) he married Brigadier John B. Nye , son of Frederick II of France , the King of France . Head Entity : Robert B. Nye , Tail Entity : Frederick II of France .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : performer . Context : Later in the year , the band formed with the vocalist and keyboardist of their debut album , " Sweet Home Alabama " . Head Entity : Sweet Home Alabama , Tail Entity : Robert J. Howard .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 11318
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11418, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.29it/s]Extractor Estimating: 2it [00:01,  1.54it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.72it/s]Extractor Estimating: 9it [00:05,  1.72it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:07,  1.62it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.71it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.75it/s]Extractor Estimating: 18it [00:10,  1.74it/s]Extractor Estimating: 19it [00:11,  1.71it/s]Extractor Estimating: 20it [00:12,  1.70it/s]Extractor Estimating: 21it [00:12,  1.70it/s]Extractor Estimating: 22it [00:13,  1.72it/s]Extractor Estimating: 23it [00:13,  1.67it/s]Extractor Estimating: 24it [00:14,  1.72it/s]Extractor Estimating: 25it [00:14,  1.71it/s]Extractor Estimating: 26it [00:15,  1.68it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:16,  1.64it/s]Extractor Estimating: 29it [00:17,  1.65it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:18,  1.68it/s]Extractor Estimating: 32it [00:19,  1.64it/s]Extractor Estimating: 33it [00:19,  1.67it/s]Extractor Estimating: 34it [00:20,  1.70it/s]Extractor Estimating: 35it [00:20,  1.70it/s]Extractor Estimating: 36it [00:21,  1.68it/s]Extractor Estimating: 37it [00:22,  1.68it/s]Extractor Estimating: 38it [00:23,  1.49it/s]Extractor Estimating: 39it [00:23,  1.57it/s]Extractor Estimating: 40it [00:24,  1.57it/s]Extractor Estimating: 41it [00:24,  1.60it/s]Extractor Estimating: 42it [00:25,  1.63it/s]Extractor Estimating: 43it [00:25,  1.65it/s]Extractor Estimating: 44it [00:26,  1.69it/s]Extractor Estimating: 45it [00:27,  1.69it/s]Extractor Estimating: 46it [00:27,  1.69it/s]Extractor Estimating: 47it [00:28,  1.72it/s]Extractor Estimating: 48it [00:28,  1.75it/s]Extractor Estimating: 49it [00:29,  1.68it/s]Extractor Estimating: 50it [00:29,  1.76it/s]Extractor Estimating: 51it [00:30,  1.69it/s]Extractor Estimating: 52it [00:31,  1.64it/s]Extractor Estimating: 53it [00:31,  1.62it/s]Extractor Estimating: 54it [00:32,  1.67it/s]Extractor Estimating: 55it [00:33,  1.61it/s]Extractor Estimating: 56it [00:33,  1.62it/s]Extractor Estimating: 57it [00:34,  1.64it/s]Extractor Estimating: 58it [00:34,  1.67it/s]Extractor Estimating: 59it [00:35,  1.66it/s]Extractor Estimating: 60it [00:36,  1.65it/s]Extractor Estimating: 61it [00:36,  1.66it/s]Extractor Estimating: 62it [00:37,  1.69it/s]Extractor Estimating: 63it [00:37,  1.77it/s]Extractor Estimating: 64it [00:38,  1.83it/s]Extractor Estimating: 65it [00:38,  1.74it/s]Extractor Estimating: 66it [00:39,  1.67it/s]Extractor Estimating: 67it [00:40,  1.74it/s]Extractor Estimating: 68it [00:40,  1.75it/s]Extractor Estimating: 69it [00:41,  1.70it/s]Extractor Estimating: 70it [00:41,  1.67it/s]Extractor Estimating: 71it [00:42,  1.63it/s]Extractor Estimating: 72it [00:43,  1.65it/s]Extractor Estimating: 73it [00:43,  1.67it/s]Extractor Estimating: 74it [00:44,  1.67it/s]Extractor Estimating: 75it [00:44,  1.70it/s]Extractor Estimating: 76it [00:45,  1.74it/s]Extractor Estimating: 77it [00:45,  1.79it/s]Extractor Estimating: 78it [00:46,  1.76it/s]Extractor Estimating: 79it [00:47,  1.77it/s]Extractor Estimating: 80it [00:47,  1.81it/s]Extractor Estimating: 81it [00:48,  1.80it/s]Extractor Estimating: 82it [00:48,  1.78it/s]Extractor Estimating: 83it [00:49,  1.75it/s]Extractor Estimating: 84it [00:49,  1.76it/s]Extractor Estimating: 85it [00:50,  1.81it/s]Extractor Estimating: 86it [00:51,  1.78it/s]Extractor Estimating: 87it [00:51,  1.77it/s]Extractor Estimating: 88it [00:52,  1.78it/s]Extractor Estimating: 89it [00:52,  1.85it/s]Extractor Estimating: 90it [00:53,  1.85it/s]Extractor Estimating: 91it [00:53,  1.85it/s]Extractor Estimating: 92it [00:54,  1.84it/s]Extractor Estimating: 93it [00:54,  1.83it/s]Extractor Estimating: 94it [00:55,  1.84it/s]Extractor Estimating: 95it [00:55,  1.88it/s]Extractor Estimating: 96it [00:56,  1.88it/s]Extractor Estimating: 97it [00:57,  1.84it/s]Extractor Estimating: 98it [00:57,  1.86it/s]Extractor Estimating: 99it [00:58,  1.86it/s]Extractor Estimating: 100it [00:58,  1.81it/s]Extractor Estimating: 101it [00:59,  1.67it/s]Extractor Estimating: 102it [01:00,  1.63it/s]Extractor Estimating: 103it [01:00,  1.64it/s]Extractor Estimating: 104it [01:01,  1.61it/s]Extractor Estimating: 105it [01:01,  1.62it/s]Extractor Estimating: 106it [01:02,  1.68it/s]Extractor Estimating: 107it [01:03,  1.67it/s]Extractor Estimating: 108it [01:03,  1.68it/s]Extractor Estimating: 109it [01:04,  1.63it/s]Extractor Estimating: 110it [01:04,  1.69it/s]Extractor Estimating: 111it [01:05,  1.69it/s]Extractor Estimating: 112it [01:05,  1.74it/s]Extractor Estimating: 113it [01:06,  1.83it/s]Extractor Estimating: 114it [01:06,  1.81it/s]Extractor Estimating: 115it [01:07,  1.73it/s]Extractor Estimating: 116it [01:08,  1.68it/s]Extractor Estimating: 117it [01:08,  1.73it/s]Extractor Estimating: 118it [01:09,  1.75it/s]Extractor Estimating: 119it [01:09,  1.68it/s]Extractor Estimating: 120it [01:10,  1.69it/s]Extractor Estimating: 121it [01:11,  1.74it/s]Extractor Estimating: 122it [01:11,  1.73it/s]Extractor Estimating: 123it [01:12,  1.76it/s]Extractor Estimating: 124it [01:12,  1.78it/s]Extractor Estimating: 125it [01:13,  1.68it/s]Extractor Estimating: 126it [01:14,  1.62it/s]Extractor Estimating: 127it [01:14,  1.58it/s]Extractor Estimating: 128it [01:15,  1.53it/s]Extractor Estimating: 129it [01:16,  1.59it/s]Extractor Estimating: 130it [01:16,  1.63it/s]Extractor Estimating: 131it [01:17,  1.65it/s]Extractor Estimating: 132it [01:17,  1.69it/s]Extractor Estimating: 133it [01:18,  1.66it/s]Extractor Estimating: 134it [01:19,  1.64it/s]Extractor Estimating: 135it [01:19,  1.70it/s]Extractor Estimating: 136it [01:20,  1.64it/s]Extractor Estimating: 137it [01:20,  1.65it/s]Extractor Estimating: 138it [01:21,  1.64it/s]Extractor Estimating: 139it [01:22,  1.67it/s]Extractor Estimating: 140it [01:22,  1.66it/s]Extractor Estimating: 141it [01:23,  1.66it/s]Extractor Estimating: 142it [01:23,  1.61it/s]Extractor Estimating: 143it [01:24,  1.63it/s]Extractor Estimating: 144it [01:25,  1.68it/s]Extractor Estimating: 145it [01:25,  1.67it/s]Extractor Estimating: 146it [01:26,  1.70it/s]Extractor Estimating: 147it [01:26,  1.74it/s]Extractor Estimating: 148it [01:27,  1.71it/s]Extractor Estimating: 149it [01:28,  1.66it/s]Extractor Estimating: 150it [01:28,  1.67it/s]Extractor Estimating: 151it [01:29,  1.73it/s]Extractor Estimating: 152it [01:29,  1.74it/s]Extractor Estimating: 153it [01:30,  1.77it/s]Extractor Estimating: 154it [01:30,  1.77it/s]Extractor Estimating: 155it [01:31,  1.77it/s]Extractor Estimating: 156it [01:31,  1.78it/s]Extractor Estimating: 157it [01:32,  1.81it/s]Extractor Estimating: 158it [01:33,  1.79it/s]Extractor Estimating: 159it [01:33,  1.72it/s]Extractor Estimating: 160it [01:34,  1.75it/s]Extractor Estimating: 161it [01:34,  1.78it/s]Extractor Estimating: 162it [01:35,  1.72it/s]Extractor Estimating: 163it [01:35,  1.74it/s]Extractor Estimating: 164it [01:36,  1.82it/s]Extractor Estimating: 165it [01:37,  1.78it/s]Extractor Estimating: 166it [01:37,  1.80it/s]Extractor Estimating: 167it [01:38,  1.80it/s]Extractor Estimating: 168it [01:38,  1.81it/s]Extractor Estimating: 169it [01:39,  1.54it/s]Extractor Estimating: 170it [01:40,  1.63it/s]Extractor Estimating: 171it [01:40,  1.63it/s]Extractor Estimating: 172it [01:41,  1.65it/s]Extractor Estimating: 173it [01:41,  1.63it/s]Extractor Estimating: 174it [01:42,  1.69it/s]Extractor Estimating: 175it [01:43,  1.69it/s]Extractor Estimating: 176it [01:43,  1.74it/s]Extractor Estimating: 177it [01:44,  1.78it/s]Extractor Estimating: 178it [01:44,  1.78it/s]Extractor Estimating: 179it [01:45,  1.75it/s]Extractor Estimating: 180it [01:45,  1.71it/s]Extractor Estimating: 181it [01:46,  1.76it/s]Extractor Estimating: 182it [01:46,  1.82it/s]Extractor Estimating: 183it [01:47,  1.88it/s]Extractor Estimating: 184it [01:47,  1.90it/s]Extractor Estimating: 185it [01:48,  1.85it/s]Extractor Estimating: 186it [01:49,  1.80it/s]Extractor Estimating: 187it [01:49,  1.72it/s]Extractor Estimating: 188it [01:50,  1.73it/s]Extractor Estimating: 189it [01:50,  1.70it/s]Extractor Estimating: 190it [01:51,  1.80it/s]Extractor Estimating: 191it [01:51,  1.85it/s]Extractor Estimating: 192it [01:52,  1.83it/s]Extractor Estimating: 193it [01:52,  1.85it/s]Extractor Estimating: 194it [01:53,  1.84it/s]Extractor Estimating: 195it [01:54,  1.78it/s]Extractor Estimating: 196it [01:54,  1.81it/s]Extractor Estimating: 197it [01:55,  1.78it/s]Extractor Estimating: 198it [01:55,  1.79it/s]Extractor Estimating: 199it [01:56,  1.83it/s]Extractor Estimating: 200it [01:56,  1.88it/s]Extractor Estimating: 201it [01:57,  1.82it/s]Extractor Estimating: 202it [01:58,  1.76it/s]Extractor Estimating: 203it [01:58,  1.75it/s]Extractor Estimating: 204it [01:59,  1.65it/s]Extractor Estimating: 205it [01:59,  1.70it/s]Extractor Estimating: 206it [02:00,  1.77it/s]Extractor Estimating: 207it [02:00,  1.73it/s]Extractor Estimating: 208it [02:01,  1.71it/s]Extractor Estimating: 209it [02:02,  1.75it/s]Extractor Estimating: 210it [02:02,  1.75it/s]Extractor Estimating: 211it [02:03,  1.77it/s]Extractor Estimating: 212it [02:03,  1.80it/s]Extractor Estimating: 213it [02:04,  1.70it/s]Extractor Estimating: 214it [02:04,  1.73it/s]Extractor Estimating: 215it [02:05,  1.75it/s]Extractor Estimating: 216it [02:06,  1.75it/s]Extractor Estimating: 217it [02:06,  1.75it/s]Extractor Estimating: 218it [02:07,  1.71it/s]Extractor Estimating: 219it [02:07,  1.72it/s]Extractor Estimating: 220it [02:08,  1.73it/s]Extractor Estimating: 221it [02:08,  1.76it/s]Extractor Estimating: 222it [02:09,  1.75it/s]Extractor Estimating: 223it [02:10,  1.74it/s]Extractor Estimating: 224it [02:10,  1.70it/s]Extractor Estimating: 225it [02:11,  1.63it/s]Extractor Estimating: 226it [02:11,  1.67it/s]Extractor Estimating: 227it [02:12,  1.68it/s]Extractor Estimating: 228it [02:13,  1.67it/s]Extractor Estimating: 229it [02:13,  1.70it/s]Extractor Estimating: 230it [02:14,  1.72it/s]Extractor Estimating: 231it [02:14,  1.73it/s]Extractor Estimating: 232it [02:15,  1.70it/s]Extractor Estimating: 233it [02:16,  1.67it/s]Extractor Estimating: 234it [02:16,  1.64it/s]Extractor Estimating: 235it [02:17,  1.58it/s]Extractor Estimating: 236it [02:17,  1.64it/s]Extractor Estimating: 237it [02:18,  1.59it/s]Extractor Estimating: 238it [02:19,  1.67it/s]Extractor Estimating: 239it [02:19,  1.70it/s]Extractor Estimating: 240it [02:20,  1.70it/s]Extractor Estimating: 241it [02:21,  1.62it/s]Extractor Estimating: 242it [02:21,  1.61it/s]Extractor Estimating: 243it [02:22,  1.70it/s]Extractor Estimating: 244it [02:22,  1.69it/s]Extractor Estimating: 245it [02:23,  1.65it/s]Extractor Estimating: 246it [02:24,  1.47it/s]Extractor Estimating: 247it [02:24,  1.55it/s]Extractor Estimating: 248it [02:25,  1.64it/s]Extractor Estimating: 249it [02:25,  1.62it/s]Extractor Estimating: 250it [02:26,  1.65it/s]Extractor Estimating: 251it [02:27,  1.68it/s]Extractor Estimating: 252it [02:27,  1.67it/s]Extractor Estimating: 253it [02:28,  1.71it/s]Extractor Estimating: 254it [02:28,  1.68it/s]Extractor Estimating: 255it [02:29,  1.73it/s]Extractor Estimating: 256it [02:30,  1.72it/s]Extractor Estimating: 257it [02:30,  1.72it/s]Extractor Estimating: 258it [02:31,  1.74it/s]Extractor Estimating: 259it [02:31,  1.78it/s]Extractor Estimating: 260it [02:32,  1.74it/s]Extractor Estimating: 261it [02:32,  1.74it/s]Extractor Estimating: 262it [02:33,  1.69it/s]Extractor Estimating: 263it [02:34,  1.63it/s]Extractor Estimating: 264it [02:34,  1.61it/s]Extractor Estimating: 265it [02:35,  1.63it/s]Extractor Estimating: 266it [02:35,  1.70it/s]Extractor Estimating: 267it [02:36,  1.72it/s]Extractor Estimating: 268it [02:37,  1.75it/s]Extractor Estimating: 269it [02:37,  1.79it/s]Extractor Estimating: 270it [02:38,  1.82it/s]Extractor Estimating: 271it [02:38,  1.85it/s]Extractor Estimating: 272it [02:39,  1.81it/s]Extractor Estimating: 273it [02:39,  1.78it/s]Extractor Estimating: 274it [02:40,  1.78it/s]Extractor Estimating: 275it [02:40,  1.76it/s]Extractor Estimating: 276it [02:41,  1.74it/s]Extractor Estimating: 277it [02:42,  1.71it/s]Extractor Estimating: 278it [02:42,  1.66it/s]Extractor Estimating: 279it [02:43,  1.71it/s]Extractor Estimating: 280it [02:43,  1.70it/s]Extractor Estimating: 281it [02:44,  1.69it/s]Extractor Estimating: 282it [02:45,  1.66it/s]Extractor Estimating: 283it [02:45,  1.69it/s]Extractor Estimating: 284it [02:46,  1.71it/s]Extractor Estimating: 285it [02:46,  1.76it/s]Extractor Estimating: 286it [02:47,  1.68it/s]Extractor Estimating: 287it [02:48,  1.70it/s]Extractor Estimating: 288it [02:48,  1.67it/s]Extractor Estimating: 289it [02:49,  1.74it/s]Extractor Estimating: 290it [02:49,  1.73it/s]Extractor Estimating: 291it [02:50,  1.75it/s]Extractor Estimating: 292it [02:50,  1.76it/s]Extractor Estimating: 293it [02:51,  1.81it/s]Extractor Estimating: 294it [02:52,  1.68it/s]Extractor Estimating: 295it [02:52,  1.67it/s]Extractor Estimating: 296it [02:53,  1.68it/s]Extractor Estimating: 297it [02:53,  1.70it/s]Extractor Estimating: 298it [02:54,  1.71it/s]Extractor Estimating: 299it [02:55,  1.72it/s]Extractor Estimating: 300it [02:55,  1.72it/s]Extractor Estimating: 301it [02:56,  1.79it/s]Extractor Estimating: 302it [02:56,  1.77it/s]Extractor Estimating: 303it [02:57,  1.68it/s]Extractor Estimating: 304it [02:57,  1.71it/s]Extractor Estimating: 305it [02:58,  1.82it/s]Extractor Estimating: 306it [02:58,  1.79it/s]Extractor Estimating: 307it [02:59,  1.73it/s]Extractor Estimating: 308it [03:00,  1.79it/s]Extractor Estimating: 309it [03:00,  1.78it/s]Extractor Estimating: 310it [03:01,  1.83it/s]Extractor Estimating: 311it [03:01,  1.85it/s]Extractor Estimating: 312it [03:02,  1.72it/s]Extractor Estimating: 313it [03:02,  1.75it/s]Extractor Estimating: 314it [03:03,  1.78it/s]Extractor Estimating: 315it [03:04,  1.79it/s]Extractor Estimating: 316it [03:04,  1.83it/s]Extractor Estimating: 317it [03:05,  1.82it/s]Extractor Estimating: 318it [03:05,  1.81it/s]Extractor Estimating: 319it [03:06,  1.81it/s]Extractor Estimating: 320it [03:06,  1.77it/s]Extractor Estimating: 321it [03:07,  1.77it/s]Extractor Estimating: 322it [03:07,  1.77it/s]Extractor Estimating: 323it [03:08,  1.79it/s]Extractor Estimating: 324it [03:09,  1.83it/s]Extractor Estimating: 325it [03:09,  1.82it/s]Extractor Estimating: 326it [03:10,  1.84it/s]Extractor Estimating: 327it [03:10,  1.78it/s]Extractor Estimating: 328it [03:11,  1.80it/s]Extractor Estimating: 329it [03:11,  1.84it/s]Extractor Estimating: 330it [03:12,  1.87it/s]Extractor Estimating: 331it [03:12,  1.85it/s]Extractor Estimating: 332it [03:13,  1.88it/s]Extractor Estimating: 333it [03:13,  1.85it/s]Extractor Estimating: 334it [03:14,  1.81it/s]Extractor Estimating: 335it [03:14,  1.87it/s]Extractor Estimating: 336it [03:15,  1.64it/s]Extractor Estimating: 337it [03:16,  1.72it/s]Extractor Estimating: 338it [03:16,  1.77it/s]Extractor Estimating: 339it [03:17,  1.76it/s]Extractor Estimating: 340it [03:17,  1.73it/s]Extractor Estimating: 341it [03:18,  1.80it/s]Extractor Estimating: 342it [03:18,  1.83it/s]Extractor Estimating: 343it [03:19,  1.84it/s]Extractor Estimating: 344it [03:20,  1.80it/s]Extractor Estimating: 345it [03:20,  1.80it/s]Extractor Estimating: 346it [03:21,  1.75it/s]Extractor Estimating: 347it [03:21,  1.72it/s]Extractor Estimating: 348it [03:22,  1.78it/s]Extractor Estimating: 349it [03:22,  1.80it/s]Extractor Estimating: 350it [03:23,  1.75it/s]Extractor Estimating: 351it [03:24,  1.74it/s]Extractor Estimating: 352it [03:24,  1.76it/s]Extractor Estimating: 353it [03:25,  1.72it/s]Extractor Estimating: 354it [03:25,  1.74it/s]Extractor Estimating: 355it [03:26,  1.70it/s]Extractor Estimating: 356it [03:27,  1.66it/s]Extractor Estimating: 357it [03:27,  1.61it/s]Extractor Estimating: 358it [03:28,  1.61it/s]Extractor Estimating: 359it [03:28,  1.63it/s]Extractor Estimating: 360it [03:29,  1.67it/s]Extractor Estimating: 361it [03:30,  1.62it/s]Extractor Estimating: 362it [03:30,  1.66it/s]Extractor Estimating: 363it [03:31,  1.68it/s]Extractor Estimating: 364it [03:32,  1.64it/s]Extractor Estimating: 365it [03:32,  1.72it/s]Extractor Estimating: 366it [03:33,  1.74it/s]Extractor Estimating: 367it [03:33,  1.76it/s]Extractor Estimating: 368it [03:34,  1.71it/s]Extractor Estimating: 369it [03:34,  1.73it/s]Extractor Estimating: 370it [03:35,  1.74it/s]Extractor Estimating: 371it [03:35,  1.78it/s]Extractor Estimating: 372it [03:36,  1.79it/s]Extractor Estimating: 373it [03:37,  1.82it/s]Extractor Estimating: 374it [03:37,  1.81it/s]Extractor Estimating: 375it [03:38,  1.79it/s]Extractor Estimating: 376it [03:38,  1.69it/s]Extractor Estimating: 377it [03:39,  1.70it/s]Extractor Estimating: 378it [03:40,  1.66it/s]Extractor Estimating: 379it [03:40,  1.65it/s]Extractor Estimating: 380it [03:41,  1.70it/s]Extractor Estimating: 381it [03:41,  1.74it/s]Extractor Estimating: 382it [03:42,  1.69it/s]Extractor Estimating: 383it [03:42,  1.68it/s]Extractor Estimating: 384it [03:43,  1.69it/s]Extractor Estimating: 385it [03:44,  1.75it/s]Extractor Estimating: 386it [03:44,  1.76it/s]Extractor Estimating: 387it [03:45,  1.74it/s]Extractor Estimating: 388it [03:45,  1.73it/s]Extractor Estimating: 389it [03:46,  1.73it/s]Extractor Estimating: 390it [03:46,  1.72it/s]Extractor Estimating: 391it [03:47,  1.70it/s]Extractor Estimating: 392it [03:48,  1.63it/s]Extractor Estimating: 393it [03:48,  1.63it/s]Extractor Estimating: 394it [03:49,  1.62it/s]Extractor Estimating: 395it [03:50,  1.67it/s]Extractor Estimating: 396it [03:50,  1.67it/s]Extractor Estimating: 397it [03:51,  1.73it/s]Extractor Estimating: 398it [03:51,  1.73it/s]Extractor Estimating: 399it [03:52,  1.59it/s]Extractor Estimating: 400it [03:53,  1.66it/s]Extractor Estimating: 401it [03:53,  1.68it/s]Extractor Estimating: 402it [03:54,  1.74it/s]Extractor Estimating: 403it [03:54,  1.71it/s]Extractor Estimating: 404it [03:55,  1.73it/s]Extractor Estimating: 405it [03:55,  1.73it/s]Extractor Estimating: 406it [03:56,  1.71it/s]Extractor Estimating: 407it [03:56,  1.81it/s]Extractor Estimating: 408it [03:57,  1.77it/s]Extractor Estimating: 409it [03:58,  1.75it/s]Extractor Estimating: 410it [03:58,  1.78it/s]Extractor Estimating: 411it [03:59,  1.78it/s]Extractor Estimating: 412it [03:59,  1.83it/s]Extractor Estimating: 413it [04:00,  1.84it/s]Extractor Estimating: 414it [04:00,  1.90it/s]Extractor Estimating: 415it [04:01,  1.87it/s]Extractor Estimating: 416it [04:01,  1.79it/s]Extractor Estimating: 417it [04:02,  1.82it/s]Extractor Estimating: 418it [04:03,  1.83it/s]Extractor Estimating: 419it [04:03,  1.87it/s]Extractor Estimating: 420it [04:04,  1.84it/s]Extractor Estimating: 421it [04:04,  1.84it/s]Extractor Estimating: 422it [04:05,  1.88it/s]Extractor Estimating: 423it [04:05,  1.89it/s]Extractor Estimating: 424it [04:06,  1.95it/s]Extractor Estimating: 425it [04:06,  1.89it/s]Extractor Estimating: 426it [04:07,  1.63it/s]Extractor Estimating: 427it [04:08,  1.64it/s]Extractor Estimating: 428it [04:08,  1.68it/s]Extractor Estimating: 429it [04:09,  1.71it/s]Extractor Estimating: 430it [04:09,  1.74it/s]Extractor Estimating: 431it [04:10,  1.74it/s]Extractor Estimating: 432it [04:10,  1.72it/s]Extractor Estimating: 433it [04:11,  1.72it/s]Extractor Estimating: 434it [04:12,  1.76it/s]Extractor Estimating: 435it [04:12,  1.79it/s]Extractor Estimating: 436it [04:13,  1.72it/s]Extractor Estimating: 437it [04:13,  1.75it/s]Extractor Estimating: 438it [04:14,  1.70it/s]Extractor Estimating: 439it [04:15,  1.69it/s]Extractor Estimating: 440it [04:15,  1.69it/s]Extractor Estimating: 441it [04:16,  1.70it/s]Extractor Estimating: 442it [04:16,  1.67it/s]Extractor Estimating: 443it [04:17,  1.65it/s]Extractor Estimating: 444it [04:17,  1.70it/s]Extractor Estimating: 445it [04:18,  1.68it/s]Extractor Estimating: 446it [04:19,  1.68it/s]Extractor Estimating: 447it [04:19,  1.67it/s]Extractor Estimating: 448it [04:20,  1.68it/s]Extractor Estimating: 449it [04:20,  1.67it/s]Extractor Estimating: 450it [04:21,  1.71it/s]Extractor Estimating: 451it [04:22,  1.76it/s]Extractor Estimating: 452it [04:22,  1.73it/s]Extractor Estimating: 453it [04:23,  1.74it/s]Extractor Estimating: 454it [04:23,  1.77it/s]Extractor Estimating: 455it [04:24,  1.76it/s]Extractor Estimating: 456it [04:24,  1.73it/s]Extractor Estimating: 457it [04:25,  1.77it/s]Extractor Estimating: 458it [04:26,  1.79it/s]Extractor Estimating: 459it [04:26,  1.78it/s]Extractor Estimating: 460it [04:27,  1.78it/s]Extractor Estimating: 461it [04:27,  1.82it/s]Extractor Estimating: 462it [04:28,  1.84it/s]Extractor Estimating: 463it [04:28,  1.90it/s]Extractor Estimating: 464it [04:29,  1.91it/s]Extractor Estimating: 465it [04:29,  1.86it/s]Extractor Estimating: 466it [04:30,  1.83it/s]Extractor Estimating: 467it [04:30,  1.84it/s]Extractor Estimating: 468it [04:31,  1.83it/s]Extractor Estimating: 469it [04:32,  1.79it/s]Extractor Estimating: 470it [04:32,  1.85it/s]Extractor Estimating: 471it [04:33,  1.84it/s]Extractor Estimating: 472it [04:33,  1.84it/s]Extractor Estimating: 473it [04:34,  1.80it/s]Extractor Estimating: 474it [04:34,  1.71it/s]Extractor Estimating: 475it [04:35,  1.69it/s]Extractor Estimating: 476it [04:36,  1.65it/s]Extractor Estimating: 477it [04:36,  1.57it/s]Extractor Estimating: 478it [04:37,  1.59it/s]Extractor Estimating: 479it [04:38,  1.64it/s]Extractor Estimating: 480it [04:38,  1.68it/s]Extractor Estimating: 481it [04:39,  1.73it/s]Extractor Estimating: 482it [04:39,  1.73it/s]Extractor Estimating: 483it [04:40,  1.74it/s]Extractor Estimating: 484it [04:40,  1.72it/s]Extractor Estimating: 485it [04:41,  1.74it/s]Extractor Estimating: 486it [04:41,  1.75it/s]Extractor Estimating: 487it [04:42,  1.72it/s]Extractor Estimating: 488it [04:43,  1.69it/s]Extractor Estimating: 489it [04:43,  1.68it/s]Extractor Estimating: 490it [04:44,  1.71it/s]Extractor Estimating: 491it [04:45,  1.64it/s]Extractor Estimating: 492it [04:45,  1.70it/s]Extractor Estimating: 493it [04:46,  1.73it/s]Extractor Estimating: 494it [04:46,  1.74it/s]Extractor Estimating: 495it [04:47,  1.76it/s]Extractor Estimating: 496it [04:47,  1.76it/s]Extractor Estimating: 497it [04:48,  1.77it/s]Extractor Estimating: 498it [04:48,  1.76it/s]Extractor Estimating: 499it [04:49,  1.81it/s]Extractor Estimating: 500it [04:49,  2.10it/s]Extractor Estimating: 500it [04:49,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:13,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:13,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:13,277 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:13,277 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:13,277 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:19:13,895 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:19:13,896 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:19:14,489 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:19:15,541 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:19:15,541 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:19,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:19,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:19,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:19,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:19:19,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:19:19,681 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:19:19,682 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:19:20,297 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:19:20,464 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:19:20,464 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 07:04:16,376 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 07:04:16,557 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9993 mean pseudo reward: 0.9388431420948546
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 21663
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21763, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21763, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.939, loss:667.5407
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.941, loss:655.7594
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.958, loss:645.1793
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.960, loss:625.9229
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.953, loss:598.1163
>> valid entity prec:0.5601, rec:0.5192, f1:0.5389
>> valid relation prec:0.3370, rec:0.1399, f1:0.1978
>> valid relation with NER prec:0.3370, rec:0.1399, f1:0.1978
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.182, loss:595.9217
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.938, loss:625.7406
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.948, loss:638.0848
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.950, loss:603.9957
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.970, loss:595.5555
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5754, rec:0.5467, f1:0.5607
>> valid relation prec:0.3342, rec:0.1457, f1:0.2029
>> valid relation with NER prec:0.3342, rec:0.1457, f1:0.2029
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.179, loss:631.9003
g_step 1200, step 366, avg_time 0.921, loss:643.9204
g_step 1300, step 49, avg_time 0.944, loss:610.0707
g_step 1400, step 149, avg_time 0.958, loss:577.2561
g_step 1500, step 249, avg_time 0.959, loss:593.9547
>> valid entity prec:0.5924, rec:0.5089, f1:0.5474
>> valid relation prec:0.3470, rec:0.1336, f1:0.1930
>> valid relation with NER prec:0.3470, rec:0.1336, f1:0.1930
g_step 1600, step 349, avg_time 2.158, loss:600.9968
g_step 1700, step 32, avg_time 0.936, loss:596.6261
g_step 1800, step 132, avg_time 0.938, loss:568.1121
g_step 1900, step 232, avg_time 0.955, loss:586.6650
g_step 2000, step 332, avg_time 0.948, loss:571.1187
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6058, rec:0.4985, f1:0.5469
>> valid relation prec:0.3324, rec:0.1408, f1:0.1978
>> valid relation with NER prec:0.3324, rec:0.1408, f1:0.1978
g_step 2100, step 15, avg_time 2.167, loss:572.4916
g_step 2200, step 115, avg_time 0.961, loss:502.2650
g_step 2300, step 215, avg_time 0.965, loss:557.2557
g_step 2400, step 315, avg_time 0.935, loss:568.1049
g_step 2500, step 415, avg_time 0.931, loss:561.2500
>> valid entity prec:0.5675, rec:0.4870, f1:0.5242
>> valid relation prec:0.3396, rec:0.1296, f1:0.1876
>> valid relation with NER prec:0.3396, rec:0.1296, f1:0.1876
g_step 2600, step 98, avg_time 2.163, loss:515.6429
g_step 2700, step 198, avg_time 0.943, loss:506.1206
g_step 2800, step 298, avg_time 0.947, loss:524.8517
g_step 2900, step 398, avg_time 0.953, loss:534.7255
g_step 3000, step 81, avg_time 0.942, loss:492.3552
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5738, rec:0.5182, f1:0.5446
>> valid relation prec:0.3447, rec:0.1445, f1:0.2037
>> valid relation with NER prec:0.3447, rec:0.1445, f1:0.2037
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 181, avg_time 2.169, loss:513.9077
g_step 3200, step 281, avg_time 0.958, loss:504.2082
g_step 3300, step 381, avg_time 0.953, loss:526.7050
g_step 3400, step 64, avg_time 0.946, loss:485.7279
g_step 3500, step 164, avg_time 0.952, loss:476.0195
>> valid entity prec:0.5376, rec:0.4808, f1:0.5076
>> valid relation prec:0.2888, rec:0.1253, f1:0.1748
>> valid relation with NER prec:0.2888, rec:0.1253, f1:0.1748
g_step 3600, step 264, avg_time 2.175, loss:488.9868
g_step 3700, step 364, avg_time 0.951, loss:496.2536
g_step 3800, step 47, avg_time 0.938, loss:491.6893
g_step 3900, step 147, avg_time 0.959, loss:469.4460
g_step 4000, step 247, avg_time 0.944, loss:470.9554
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5531, rec:0.5224, f1:0.5373
>> valid relation prec:0.3146, rec:0.1394, f1:0.1932
>> valid relation with NER prec:0.3146, rec:0.1394, f1:0.1932
g_step 4100, step 347, avg_time 2.170, loss:489.9142
g_step 4200, step 30, avg_time 0.941, loss:451.8169
g_step 4300, step 130, avg_time 0.946, loss:432.3419
g_step 4400, step 230, avg_time 0.958, loss:455.7370
g_step 4500, step 330, avg_time 0.948, loss:474.2341
>> valid entity prec:0.5478, rec:0.4995, f1:0.5225
>> valid relation prec:0.2765, rec:0.1190, f1:0.1664
>> valid relation with NER prec:0.2765, rec:0.1190, f1:0.1664
g_step 4600, step 13, avg_time 2.178, loss:430.0375
g_step 4700, step 113, avg_time 0.947, loss:440.3921
g_step 4800, step 213, avg_time 0.965, loss:421.7985
g_step 4900, step 313, avg_time 0.949, loss:433.8014
g_step 5000, step 413, avg_time 0.944, loss:444.1569
learning rate was adjusted to 0.0008
>> valid entity prec:0.5499, rec:0.5359, f1:0.5428
>> valid relation prec:0.3056, rec:0.1434, f1:0.1952
>> valid relation with NER prec:0.3056, rec:0.1434, f1:0.1952
g_step 5100, step 96, avg_time 2.230, loss:393.9045
g_step 5200, step 196, avg_time 0.939, loss:435.0275
g_step 5300, step 296, avg_time 0.939, loss:436.8603
g_step 5400, step 396, avg_time 0.955, loss:445.4784
g_step 5500, step 79, avg_time 0.944, loss:398.4425
>> valid entity prec:0.5685, rec:0.5086, f1:0.5369
>> valid relation prec:0.3080, rec:0.1417, f1:0.1941
>> valid relation with NER prec:0.3080, rec:0.1417, f1:0.1941
g_step 5600, step 179, avg_time 2.189, loss:405.5810
g_step 5700, step 279, avg_time 0.949, loss:431.3000
g_step 5800, step 379, avg_time 0.957, loss:410.7130
g_step 5900, step 62, avg_time 0.934, loss:403.4503
g_step 6000, step 162, avg_time 0.954, loss:396.3000
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5519, rec:0.4930, f1:0.5208
>> valid relation prec:0.2926, rec:0.1256, f1:0.1758
>> valid relation with NER prec:0.2926, rec:0.1256, f1:0.1758
g_step 6100, step 262, avg_time 2.198, loss:408.8980
g_step 6200, step 362, avg_time 0.955, loss:395.9626
g_step 6300, step 45, avg_time 0.950, loss:383.8245
g_step 6400, step 145, avg_time 0.968, loss:353.2563
g_step 6500, step 245, avg_time 0.952, loss:404.7890
>> valid entity prec:0.5545, rec:0.5041, f1:0.5281
>> valid relation prec:0.2966, rec:0.1282, f1:0.1790
>> valid relation with NER prec:0.2966, rec:0.1282, f1:0.1790
g_step 6600, step 345, avg_time 2.190, loss:396.4794
g_step 6700, step 28, avg_time 0.958, loss:369.5426
g_step 6800, step 128, avg_time 0.950, loss:363.8091
g_step 6900, step 228, avg_time 0.952, loss:376.0343
g_step 7000, step 328, avg_time 0.958, loss:390.8410
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5608, rec:0.5110, f1:0.5347
>> valid relation prec:0.3047, rec:0.1437, f1:0.1953
>> valid relation with NER prec:0.3047, rec:0.1437, f1:0.1953
g_step 7100, step 11, avg_time 2.189, loss:385.0355
g_step 7200, step 111, avg_time 0.957, loss:344.2550
g_step 7300, step 211, avg_time 0.947, loss:351.3776
g_step 7400, step 311, avg_time 0.947, loss:395.1430
g_step 7500, step 411, avg_time 0.962, loss:369.8816
>> valid entity prec:0.5612, rec:0.4767, f1:0.5155
>> valid relation prec:0.2872, rec:0.1202, f1:0.1694
>> valid relation with NER prec:0.2872, rec:0.1202, f1:0.1694
g_step 7600, step 94, avg_time 2.185, loss:331.8943
g_step 7700, step 194, avg_time 0.964, loss:339.1045
g_step 7800, step 294, avg_time 0.937, loss:351.4060
g_step 7900, step 394, avg_time 0.953, loss:359.9590
g_step 8000, step 77, avg_time 0.948, loss:342.8966
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5353, rec:0.5348, f1:0.5350
>> valid relation prec:0.2727, rec:0.1110, f1:0.1578
>> valid relation with NER prec:0.2727, rec:0.1110, f1:0.1578
g_step 8100, step 177, avg_time 2.191, loss:328.9292
g_step 8200, step 277, avg_time 0.946, loss:337.2722
g_step 8300, step 377, avg_time 0.954, loss:372.1524
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:04:16 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:04:16 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-04-16_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:04:17 - WARNING - datasets.builder -   Using custom data configuration default-ac5bae0731b312cd
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ac5bae0731b312cd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:04:19,403 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:04:19,426 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:04:19,426 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:04:19,427 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:04:19,523 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:04:19,559 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:04:19,924 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:04:22,944 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:04:22,967 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ac5bae0731b312cd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.96ba/s] 20%|        | 2/10 [00:00<00:02,  3.89ba/s] 30%|       | 3/10 [00:00<00:01,  4.35ba/s] 40%|      | 4/10 [00:00<00:01,  4.60ba/s] 50%|     | 5/10 [00:01<00:01,  4.68ba/s] 60%|    | 6/10 [00:01<00:00,  4.74ba/s] 70%|   | 7/10 [00:01<00:00,  4.77ba/s] 80%|  | 8/10 [00:01<00:00,  3.97ba/s] 90%| | 9/10 [00:02<00:00,  4.24ba/s]100%|| 10/10 [00:02<00:00,  4.42ba/s]100%|| 10/10 [00:02<00:00,  4.35ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.71ba/s] 50%|     | 2/4 [00:00<00:00,  3.20ba/s] 75%|  | 3/4 [00:00<00:00,  3.70ba/s]100%|| 4/4 [00:01<00:00,  4.58ba/s]100%|| 4/4 [00:01<00:00,  3.99ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  4.94ba/s] 30%|       | 3/10 [00:00<00:00,  8.45ba/s] 50%|     | 5/10 [00:00<00:00,  9.76ba/s] 70%|   | 7/10 [00:00<00:00, 10.38ba/s] 90%| | 9/10 [00:00<00:00, 10.68ba/s]100%|| 10/10 [00:00<00:00, 10.04ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  4.49ba/s] 75%|  | 3/4 [00:00<00:00,  8.08ba/s]100%|| 4/4 [00:00<00:00,  8.97ba/s]
[INFO|trainer.py:414] 2023-08-29 07:04:29,220 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:04:29,344 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:04:29,344 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 07:04:29,344 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:04:29,344 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:04:29,344 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:04:29,344 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:04:29,344 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:53,  3.34it/s]  0%|          | 2/780 [00:00<04:04,  3.18it/s]  0%|          | 3/780 [00:00<03:54,  3.31it/s]  1%|          | 4/780 [00:01<03:51,  3.35it/s]  1%|          | 5/780 [00:01<03:49,  3.37it/s]  1%|          | 6/780 [00:01<03:48,  3.39it/s]  1%|          | 7/780 [00:02<03:47,  3.40it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:45,  3.41it/s]  1%|         | 10/780 [00:02<03:45,  3.41it/s]  1%|         | 11/780 [00:03<03:45,  3.42it/s]  2%|         | 12/780 [00:03<03:44,  3.42it/s]  2%|         | 13/780 [00:03<03:44,  3.42it/s]  2%|         | 14/780 [00:04<03:44,  3.42it/s]  2%|         | 15/780 [00:04<03:43,  3.42it/s]  2%|         | 16/780 [00:04<03:43,  3.42it/s]  2%|         | 17/780 [00:05<03:43,  3.42it/s]  2%|         | 18/780 [00:05<03:42,  3.42it/s]  2%|         | 19/780 [00:05<03:51,  3.29it/s]  3%|         | 20/780 [00:05<03:48,  3.33it/s]  3%|         | 21/780 [00:06<03:46,  3.35it/s]  3%|         | 22/780 [00:06<03:44,  3.37it/s]  3%|         | 23/780 [00:06<03:43,  3.39it/s]  3%|         | 24/780 [00:07<03:42,  3.40it/s]  3%|         | 25/780 [00:07<03:41,  3.40it/s]  3%|         | 26/780 [00:07<03:41,  3.41it/s]  3%|         | 27/780 [00:07<03:40,  3.41it/s]  4%|         | 28/780 [00:08<03:40,  3.41it/s]  4%|         | 29/780 [00:08<03:40,  3.41it/s]  4%|         | 30/780 [00:08<03:40,  3.41it/s]  4%|         | 31/780 [00:09<03:39,  3.41it/s]  4%|         | 32/780 [00:09<03:39,  3.41it/s]  4%|         | 33/780 [00:09<03:39,  3.41it/s]  4%|         | 34/780 [00:10<03:38,  3.41it/s]  4%|         | 35/780 [00:10<03:38,  3.41it/s]  5%|         | 36/780 [00:10<03:37,  3.41it/s]  5%|         | 37/780 [00:10<03:42,  3.34it/s]  5%|         | 38/780 [00:11<03:40,  3.36it/s]  5%|         | 39/780 [00:11<03:39,  3.38it/s]  5%|         | 40/780 [00:11<03:38,  3.39it/s]  5%|         | 41/780 [00:12<03:37,  3.39it/s]  5%|         | 42/780 [00:12<03:37,  3.40it/s]  6%|         | 43/780 [00:12<03:36,  3.40it/s]  6%|         | 44/780 [00:12<03:36,  3.41it/s]  6%|         | 45/780 [00:13<03:35,  3.41it/s]  6%|         | 46/780 [00:13<03:35,  3.41it/s]  6%|         | 47/780 [00:13<03:35,  3.41it/s]  6%|         | 48/780 [00:14<03:34,  3.41it/s]  6%|         | 49/780 [00:14<03:34,  3.41it/s]  6%|         | 50/780 [00:14<03:33,  3.41it/s]  7%|         | 51/780 [00:15<03:33,  3.41it/s]  7%|         | 52/780 [00:15<03:33,  3.41it/s]  7%|         | 53/780 [00:15<03:33,  3.41it/s]  7%|         | 54/780 [00:15<03:37,  3.34it/s]  7%|         | 55/780 [00:16<03:35,  3.36it/s]  7%|         | 56/780 [00:16<03:34,  3.38it/s]  7%|         | 57/780 [00:16<03:33,  3.39it/s]  7%|         | 58/780 [00:17<03:32,  3.39it/s]  8%|         | 59/780 [00:17<03:32,  3.40it/s]  8%|         | 60/780 [00:17<03:31,  3.40it/s]  8%|         | 61/780 [00:17<03:31,  3.40it/s]  8%|         | 62/780 [00:18<03:30,  3.41it/s]  8%|         | 63/780 [00:18<03:30,  3.41it/s]  8%|         | 64/780 [00:18<03:30,  3.41it/s]  8%|         | 65/780 [00:19<03:29,  3.41it/s]  8%|         | 66/780 [00:19<03:29,  3.41it/s]  9%|         | 67/780 [00:19<03:29,  3.41it/s]  9%|         | 68/780 [00:20<03:28,  3.41it/s]  9%|         | 69/780 [00:20<03:28,  3.41it/s]  9%|         | 70/780 [00:20<03:28,  3.41it/s]  9%|         | 71/780 [00:20<03:27,  3.41it/s]  9%|         | 72/780 [00:21<03:32,  3.32it/s]  9%|         | 73/780 [00:21<03:30,  3.35it/s]  9%|         | 74/780 [00:21<03:29,  3.37it/s] 10%|         | 75/780 [00:22<03:28,  3.38it/s] 10%|         | 76/780 [00:22<03:27,  3.39it/s] 10%|         | 77/780 [00:22<03:27,  3.39it/s] 10%|         | 78/780 [00:22<03:26,  3.40it/s] 10%|         | 79/780 [00:23<03:26,  3.40it/s] 10%|         | 80/780 [00:23<03:25,  3.40it/s] 10%|         | 81/780 [00:23<03:25,  3.40it/s] 11%|         | 82/780 [00:24<03:24,  3.41it/s] 11%|         | 83/780 [00:24<03:24,  3.40it/s] 11%|         | 84/780 [00:24<03:24,  3.41it/s] 11%|         | 85/780 [00:25<03:23,  3.41it/s] 11%|         | 86/780 [00:25<03:23,  3.41it/s] 11%|         | 87/780 [00:25<03:23,  3.41it/s] 11%|        | 88/780 [00:25<03:22,  3.41it/s] 11%|        | 89/780 [00:26<03:27,  3.33it/s] 12%|        | 90/780 [00:26<03:25,  3.35it/s] 12%|        | 91/780 [00:26<03:24,  3.37it/s] 12%|        | 92/780 [00:27<03:23,  3.38it/s] 12%|        | 93/780 [00:27<03:22,  3.39it/s] 12%|        | 94/780 [00:27<03:22,  3.39it/s] 12%|        | 95/780 [00:28<03:21,  3.40it/s] 12%|        | 96/780 [00:28<03:21,  3.40it/s] 12%|        | 97/780 [00:28<03:20,  3.40it/s] 13%|        | 98/780 [00:28<03:20,  3.41it/s] 13%|        | 99/780 [00:29<03:19,  3.41it/s] 13%|        | 100/780 [00:29<03:19,  3.41it/s] 13%|        | 101/780 [00:29<03:19,  3.41it/s] 13%|        | 102/780 [00:30<03:19,  3.41it/s] 13%|        | 103/780 [00:30<03:18,  3.41it/s] 13%|        | 104/780 [00:30<03:18,  3.41it/s] 13%|        | 105/780 [00:30<03:18,  3.41it/s] 14%|        | 106/780 [00:31<03:23,  3.32it/s] 14%|        | 107/780 [00:31<03:21,  3.34it/s] 14%|        | 108/780 [00:31<03:19,  3.36it/s] 14%|        | 109/780 [00:32<03:18,  3.38it/s] 14%|        | 110/780 [00:32<03:17,  3.39it/s] 14%|        | 111/780 [00:32<03:17,  3.40it/s] 14%|        | 112/780 [00:33<03:16,  3.40it/s] 14%|        | 113/780 [00:33<03:18,  3.36it/s] 15%|        | 114/780 [00:33<03:17,  3.37it/s] 15%|        | 115/780 [00:33<03:16,  3.38it/s] 15%|        | 116/780 [00:34<03:15,  3.39it/s] 15%|        | 117/780 [00:34<03:15,  3.40it/s] 15%|        | 118/780 [00:34<03:14,  3.40it/s] 15%|        | 119/780 [00:35<03:14,  3.40it/s] 15%|        | 120/780 [00:35<03:13,  3.40it/s] 16%|        | 121/780 [00:35<03:13,  3.40it/s] 16%|        | 122/780 [00:35<03:13,  3.41it/s] 16%|        | 123/780 [00:36<03:12,  3.41it/s] 16%|        | 124/780 [00:36<03:15,  3.35it/s] 16%|        | 125/780 [00:36<03:14,  3.37it/s] 16%|        | 126/780 [00:37<03:13,  3.38it/s] 16%|        | 127/780 [00:37<03:12,  3.39it/s] 16%|        | 128/780 [00:37<03:12,  3.40it/s] 17%|        | 129/780 [00:38<03:11,  3.40it/s] 17%|        | 130/780 [00:38<03:11,  3.40it/s] 17%|        | 131/780 [00:38<03:10,  3.40it/s] 17%|        | 132/780 [00:38<03:10,  3.40it/s] 17%|        | 133/780 [00:39<03:10,  3.40it/s] 17%|        | 134/780 [00:39<03:09,  3.41it/s] 17%|        | 135/780 [00:39<03:09,  3.40it/s] 17%|        | 136/780 [00:40<03:09,  3.41it/s] 18%|        | 137/780 [00:40<03:08,  3.41it/s] 18%|        | 138/780 [00:40<03:08,  3.41it/s] 18%|        | 139/780 [00:40<03:08,  3.41it/s] 18%|        | 140/780 [00:41<03:07,  3.41it/s] 18%|        | 141/780 [00:41<03:11,  3.34it/s] 18%|        | 142/780 [00:41<03:10,  3.36it/s] 18%|        | 143/780 [00:42<03:08,  3.37it/s] 18%|        | 144/780 [00:42<03:07,  3.38it/s] 19%|        | 145/780 [00:42<03:07,  3.39it/s] 19%|        | 146/780 [00:43<03:06,  3.40it/s] 19%|        | 147/780 [00:43<03:06,  3.40it/s] 19%|        | 148/780 [00:43<03:05,  3.40it/s] 19%|        | 149/780 [00:43<03:04,  3.42it/s] 19%|        | 150/780 [00:44<03:03,  3.43it/s] 19%|        | 151/780 [00:44<03:02,  3.44it/s] 19%|        | 152/780 [00:44<03:02,  3.45it/s] 20%|        | 153/780 [00:45<03:01,  3.45it/s] 20%|        | 154/780 [00:45<03:02,  3.43it/s] 20%|        | 155/780 [00:45<03:01,  3.44it/s] 20%|        | 156/780 [00:45<03:01,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 07:05:15,330 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:05:15,330 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:05:15,330 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.71it/s][A
  3%|         | 12/437 [00:00<00:08, 49.45it/s][A
  4%|         | 18/437 [00:00<00:09, 45.29it/s][A
  5%|         | 23/437 [00:00<00:09, 45.40it/s][A
  6%|         | 28/437 [00:00<00:09, 45.29it/s][A
  8%|         | 33/437 [00:00<00:08, 45.23it/s][A
  9%|         | 38/437 [00:00<00:08, 45.16it/s][A
 10%|         | 43/437 [00:00<00:08, 45.25it/s][A
 11%|         | 48/437 [00:01<00:13, 28.67it/s][A
 12%|        | 53/437 [00:01<00:11, 32.72it/s][A
 13%|        | 58/437 [00:01<00:10, 35.86it/s][A
 14%|        | 63/437 [00:01<00:09, 38.36it/s][A
 16%|        | 68/437 [00:01<00:09, 40.27it/s][A
 17%|        | 73/437 [00:01<00:08, 41.77it/s][A
 18%|        | 78/437 [00:01<00:08, 42.95it/s][A
 19%|        | 83/437 [00:02<00:08, 43.83it/s][A
 20%|        | 88/437 [00:02<00:07, 44.14it/s][A
 21%|       | 93/437 [00:02<00:07, 44.06it/s][A
 22%|       | 98/437 [00:02<00:07, 44.12it/s][A
 24%|       | 103/437 [00:02<00:07, 44.34it/s][A
 25%|       | 108/437 [00:02<00:07, 44.56it/s][A
 26%|       | 113/437 [00:02<00:07, 44.94it/s][A
 27%|       | 118/437 [00:02<00:07, 45.15it/s][A
 28%|       | 123/437 [00:02<00:06, 45.35it/s][A
 29%|       | 128/437 [00:03<00:06, 45.53it/s][A
 30%|       | 133/437 [00:03<00:06, 45.56it/s][A
 32%|      | 138/437 [00:03<00:06, 45.29it/s][A
 33%|      | 143/437 [00:03<00:06, 44.99it/s][A
 34%|      | 148/437 [00:03<00:06, 44.77it/s][A
 35%|      | 153/437 [00:03<00:06, 45.01it/s][A
 36%|      | 158/437 [00:03<00:06, 45.14it/s][A
 37%|      | 163/437 [00:03<00:06, 45.33it/s][A
 38%|      | 168/437 [00:03<00:05, 45.39it/s][A
 40%|      | 173/437 [00:04<00:05, 45.57it/s][A
 41%|      | 178/437 [00:04<00:05, 45.59it/s][A
 42%|     | 183/437 [00:04<00:05, 45.42it/s][A
 43%|     | 188/437 [00:04<00:05, 45.20it/s][A
 44%|     | 193/437 [00:04<00:05, 45.02it/s][A
 45%|     | 198/437 [00:04<00:05, 45.07it/s][A
 46%|     | 203/437 [00:04<00:05, 45.14it/s][A
 48%|     | 208/437 [00:04<00:05, 45.31it/s][A
 49%|     | 213/437 [00:04<00:04, 45.31it/s][A
 50%|     | 218/437 [00:04<00:04, 45.52it/s][A
 51%|     | 223/437 [00:05<00:04, 45.46it/s][A
 52%|    | 228/437 [00:05<00:04, 45.58it/s][A
 53%|    | 233/437 [00:05<00:04, 45.36it/s][A
 54%|    | 238/437 [00:05<00:04, 43.95it/s][A
 56%|    | 243/437 [00:05<00:04, 44.43it/s][A
 57%|    | 248/437 [00:05<00:04, 44.63it/s][A
 58%|    | 253/437 [00:05<00:04, 44.97it/s][A
 59%|    | 258/437 [00:05<00:03, 45.07it/s][A
 60%|    | 263/437 [00:06<00:03, 45.27it/s][A
 61%|   | 268/437 [00:06<00:03, 45.35it/s][A
 62%|   | 273/437 [00:06<00:03, 45.38it/s][A
 64%|   | 278/437 [00:06<00:03, 45.19it/s][A
 65%|   | 283/437 [00:06<00:03, 45.09it/s][A
 66%|   | 288/437 [00:06<00:03, 45.18it/s][A
 67%|   | 293/437 [00:06<00:03, 45.23it/s][A
 68%|   | 298/437 [00:06<00:03, 45.38it/s][A
 69%|   | 303/437 [00:06<00:02, 45.46it/s][A
 70%|   | 308/437 [00:06<00:02, 45.45it/s][A
 72%|  | 313/437 [00:07<00:02, 45.52it/s][A
 73%|  | 318/437 [00:07<00:02, 45.35it/s][A
 74%|  | 323/437 [00:07<00:02, 45.29it/s][A
 75%|  | 328/437 [00:07<00:02, 45.25it/s][A
 76%|  | 333/437 [00:07<00:02, 45.22it/s][A
 77%|  | 338/437 [00:07<00:02, 45.27it/s][A
 78%|  | 343/437 [00:07<00:02, 45.32it/s][A
 80%|  | 348/437 [00:07<00:01, 45.39it/s][A
 81%|  | 353/437 [00:07<00:01, 45.42it/s][A
 82%| | 358/437 [00:08<00:01, 45.47it/s][A
 83%| | 363/437 [00:08<00:01, 45.43it/s][A
 84%| | 368/437 [00:08<00:01, 45.34it/s][A
 85%| | 373/437 [00:08<00:01, 45.18it/s][A
 86%| | 378/437 [00:08<00:01, 45.23it/s][A
 88%| | 383/437 [00:08<00:01, 45.17it/s][A
 89%| | 388/437 [00:08<00:01, 45.37it/s][A
 90%| | 393/437 [00:08<00:00, 45.33it/s][A
 91%| | 398/437 [00:08<00:00, 45.36it/s][A
 92%|| 403/437 [00:09<00:00, 45.45it/s][A
 93%|| 408/437 [00:09<00:00, 45.47it/s][A
 95%|| 413/437 [00:09<00:00, 45.38it/s][A
 96%|| 418/437 [00:09<00:00, 45.34it/s][A
 97%|| 423/437 [00:09<00:00, 45.18it/s][A
 98%|| 428/437 [00:09<00:00, 45.18it/s][A
 99%|| 433/437 [00:09<00:00, 45.25it/s][A                                                 
                                                 [A 20%|        | 156/780 [00:55<03:01,  3.44it/s]
100%|| 437/437 [00:09<00:00, 45.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:05:25,379 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 07:05:25,511 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:05:28,867 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:05:29,082 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:05:29,205 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:08<1:13:30,  7.08s/it] 20%|        | 158/780 [01:09<52:25,  5.06s/it]   20%|        | 159/780 [01:09<37:32,  3.63s/it] 21%|        | 160/780 [01:09<27:08,  2.63s/it] 21%|        | 161/780 [01:10<19:52,  1.93s/it] 21%|        | 162/780 [01:10<14:47,  1.44s/it] 21%|        | 163/780 [01:10<11:14,  1.09s/it] 21%|        | 164/780 [01:10<08:45,  1.17it/s] 21%|        | 165/780 [01:11<07:01,  1.46it/s] 21%|       | 166/780 [01:11<05:48,  1.76it/s] 21%|       | 167/780 [01:11<04:57,  2.06it/s] 22%|       | 168/780 [01:12<04:21,  2.34it/s] 22%|       | 169/780 [01:12<04:02,  2.52it/s] 22%|       | 170/780 [01:12<03:43,  2.73it/s] 22%|       | 171/780 [01:13<03:29,  2.91it/s] 22%|       | 172/780 [01:13<03:19,  3.04it/s] 22%|       | 173/780 [01:13<03:12,  3.15it/s] 22%|       | 174/780 [01:13<03:08,  3.22it/s] 22%|       | 175/780 [01:14<03:04,  3.28it/s] 23%|       | 176/780 [01:14<03:01,  3.32it/s] 23%|       | 177/780 [01:14<03:00,  3.34it/s] 23%|       | 178/780 [01:15<02:59,  3.36it/s] 23%|       | 179/780 [01:15<02:57,  3.38it/s] 23%|       | 180/780 [01:15<03:00,  3.32it/s] 23%|       | 181/780 [01:15<02:59,  3.34it/s] 23%|       | 182/780 [01:16<02:57,  3.36it/s] 23%|       | 183/780 [01:16<02:56,  3.38it/s] 24%|       | 184/780 [01:16<02:55,  3.39it/s] 24%|       | 185/780 [01:17<02:55,  3.40it/s] 24%|       | 186/780 [01:17<02:54,  3.40it/s] 24%|       | 187/780 [01:17<02:54,  3.41it/s] 24%|       | 188/780 [01:18<02:53,  3.41it/s] 24%|       | 189/780 [01:18<02:53,  3.41it/s] 24%|       | 190/780 [01:18<02:53,  3.41it/s] 24%|       | 191/780 [01:18<02:59,  3.28it/s] 25%|       | 192/780 [01:19<02:57,  3.31it/s] 25%|       | 193/780 [01:19<02:55,  3.34it/s] 25%|       | 194/780 [01:19<02:53,  3.37it/s] 25%|       | 195/780 [01:20<02:53,  3.38it/s] 25%|       | 196/780 [01:20<02:51,  3.40it/s] 25%|       | 197/780 [01:20<02:50,  3.42it/s] 25%|       | 198/780 [01:21<02:49,  3.43it/s] 26%|       | 199/780 [01:21<02:48,  3.44it/s] 26%|       | 200/780 [01:21<02:48,  3.45it/s] 26%|       | 201/780 [01:21<02:47,  3.46it/s] 26%|       | 202/780 [01:22<02:55,  3.30it/s] 26%|       | 203/780 [01:22<02:52,  3.34it/s] 26%|       | 204/780 [01:22<02:50,  3.38it/s] 26%|       | 205/780 [01:23<02:48,  3.40it/s] 26%|       | 206/780 [01:23<02:47,  3.42it/s] 27%|       | 207/780 [01:23<02:46,  3.44it/s] 27%|       | 208/780 [01:23<02:46,  3.44it/s] 27%|       | 209/780 [01:24<02:45,  3.45it/s] 27%|       | 210/780 [01:24<02:53,  3.29it/s] 27%|       | 211/780 [01:24<02:50,  3.34it/s] 27%|       | 212/780 [01:25<02:48,  3.38it/s] 27%|       | 213/780 [01:25<02:46,  3.40it/s] 27%|       | 214/780 [01:25<02:45,  3.42it/s] 28%|       | 215/780 [01:26<02:44,  3.44it/s] 28%|       | 216/780 [01:26<02:43,  3.44it/s] 28%|       | 217/780 [01:26<02:43,  3.45it/s] 28%|       | 218/780 [01:26<02:42,  3.45it/s] 28%|       | 219/780 [01:27<02:42,  3.45it/s] 28%|       | 220/780 [01:27<02:42,  3.46it/s] 28%|       | 221/780 [01:27<02:45,  3.38it/s] 28%|       | 222/780 [01:28<02:44,  3.40it/s] 29%|       | 223/780 [01:28<02:42,  3.42it/s] 29%|       | 224/780 [01:28<02:41,  3.43it/s] 29%|       | 225/780 [01:28<02:41,  3.44it/s] 29%|       | 226/780 [01:29<02:40,  3.45it/s] 29%|       | 227/780 [01:29<02:40,  3.45it/s] 29%|       | 228/780 [01:29<02:39,  3.46it/s] 29%|       | 229/780 [01:30<02:39,  3.46it/s] 29%|       | 230/780 [01:30<02:39,  3.46it/s] 30%|       | 231/780 [01:30<02:38,  3.46it/s] 30%|       | 232/780 [01:30<02:40,  3.42it/s] 30%|       | 233/780 [01:31<02:39,  3.43it/s] 30%|       | 234/780 [01:31<02:38,  3.44it/s] 30%|       | 235/780 [01:31<02:38,  3.45it/s] 30%|       | 236/780 [01:32<02:37,  3.45it/s] 30%|       | 237/780 [01:32<02:37,  3.46it/s] 31%|       | 238/780 [01:32<02:36,  3.46it/s] 31%|       | 239/780 [01:32<02:36,  3.46it/s] 31%|       | 240/780 [01:33<02:35,  3.46it/s] 31%|       | 241/780 [01:33<02:35,  3.46it/s] 31%|       | 242/780 [01:33<02:35,  3.46it/s] 31%|       | 243/780 [01:34<02:38,  3.39it/s] 31%|      | 244/780 [01:34<02:37,  3.41it/s] 31%|      | 245/780 [01:34<02:36,  3.43it/s] 32%|      | 246/780 [01:35<02:35,  3.44it/s] 32%|      | 247/780 [01:35<02:34,  3.44it/s] 32%|      | 248/780 [01:35<02:34,  3.44it/s] 32%|      | 249/780 [01:35<02:33,  3.45it/s] 32%|      | 250/780 [01:36<02:33,  3.45it/s] 32%|      | 251/780 [01:36<02:32,  3.46it/s] 32%|      | 252/780 [01:36<02:32,  3.46it/s] 32%|      | 253/780 [01:37<02:32,  3.46it/s] 33%|      | 254/780 [01:37<02:34,  3.39it/s] 33%|      | 255/780 [01:37<02:33,  3.41it/s] 33%|      | 256/780 [01:37<02:32,  3.43it/s] 33%|      | 257/780 [01:38<02:32,  3.44it/s] 33%|      | 258/780 [01:38<02:31,  3.45it/s] 33%|      | 259/780 [01:38<02:30,  3.45it/s] 33%|      | 260/780 [01:39<02:30,  3.46it/s] 33%|      | 261/780 [01:39<02:30,  3.46it/s] 34%|      | 262/780 [01:39<02:29,  3.46it/s] 34%|      | 263/780 [01:39<02:29,  3.46it/s] 34%|      | 264/780 [01:40<02:29,  3.46it/s] 34%|      | 265/780 [01:40<02:31,  3.40it/s] 34%|      | 266/780 [01:40<02:30,  3.42it/s] 34%|      | 267/780 [01:41<02:29,  3.43it/s] 34%|      | 268/780 [01:41<02:28,  3.44it/s] 34%|      | 269/780 [01:41<02:28,  3.45it/s] 35%|      | 270/780 [01:41<02:27,  3.45it/s] 35%|      | 271/780 [01:42<02:27,  3.46it/s] 35%|      | 272/780 [01:42<02:26,  3.46it/s] 35%|      | 273/780 [01:42<02:26,  3.46it/s] 35%|      | 274/780 [01:43<02:26,  3.46it/s] 35%|      | 275/780 [01:43<02:25,  3.46it/s] 35%|      | 276/780 [01:43<02:29,  3.37it/s] 36%|      | 277/780 [01:44<02:28,  3.40it/s] 36%|      | 278/780 [01:44<02:26,  3.42it/s] 36%|      | 279/780 [01:44<02:25,  3.44it/s] 36%|      | 280/780 [01:44<02:25,  3.45it/s] 36%|      | 281/780 [01:45<02:24,  3.45it/s] 36%|      | 282/780 [01:45<02:27,  3.38it/s] 36%|      | 283/780 [01:45<02:25,  3.41it/s] 36%|      | 284/780 [01:46<02:24,  3.42it/s] 37%|      | 285/780 [01:46<02:24,  3.44it/s] 37%|      | 286/780 [01:46<02:23,  3.45it/s] 37%|      | 287/780 [01:46<02:27,  3.35it/s] 37%|      | 288/780 [01:47<02:51,  2.87it/s] 37%|      | 289/780 [01:47<02:42,  3.03it/s] 37%|      | 290/780 [01:48<02:35,  3.15it/s] 37%|      | 291/780 [01:48<02:31,  3.24it/s] 37%|      | 292/780 [01:48<02:27,  3.31it/s] 38%|      | 293/780 [01:48<02:25,  3.35it/s] 38%|      | 294/780 [01:49<02:23,  3.39it/s] 38%|      | 295/780 [01:49<02:22,  3.41it/s] 38%|      | 296/780 [01:49<02:21,  3.43it/s] 38%|      | 297/780 [01:50<02:24,  3.35it/s] 38%|      | 298/780 [01:50<02:22,  3.38it/s] 38%|      | 299/780 [01:50<02:21,  3.41it/s] 38%|      | 300/780 [01:50<02:20,  3.42it/s] 39%|      | 301/780 [01:51<02:19,  3.43it/s] 39%|      | 302/780 [01:51<02:18,  3.45it/s] 39%|      | 303/780 [01:51<02:18,  3.45it/s] 39%|      | 304/780 [01:52<02:17,  3.46it/s] 39%|      | 305/780 [01:52<02:17,  3.46it/s] 39%|      | 306/780 [01:52<02:17,  3.46it/s] 39%|      | 307/780 [01:52<02:16,  3.46it/s] 39%|      | 308/780 [01:53<02:19,  3.39it/s] 40%|      | 309/780 [01:53<02:17,  3.42it/s] 40%|      | 310/780 [01:53<02:16,  3.43it/s] 40%|      | 311/780 [01:54<02:16,  3.44it/s] 40%|      | 312/780 [01:54<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 07:06:23,778 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:06:23,778 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:06:23,778 >>   Batch size = 8
{'eval_loss': 0.9601994752883911, 'eval_runtime': 9.883, 'eval_samples_per_second': 353.029, 'eval_steps_per_second': 44.217, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.12it/s][A
  3%|         | 12/437 [00:00<00:08, 48.97it/s][A
  4%|         | 17/437 [00:00<00:08, 47.17it/s][A
  5%|         | 22/437 [00:00<00:08, 46.39it/s][A
  6%|         | 27/437 [00:00<00:09, 44.52it/s][A
  7%|         | 32/437 [00:00<00:09, 44.88it/s][A
  8%|         | 37/437 [00:00<00:08, 45.08it/s][A
 10%|         | 42/437 [00:00<00:08, 45.19it/s][A
 11%|         | 47/437 [00:01<00:08, 45.26it/s][A
 12%|        | 52/437 [00:01<00:08, 45.38it/s][A
 13%|        | 57/437 [00:01<00:08, 45.39it/s][A
 14%|        | 62/437 [00:01<00:08, 45.49it/s][A
 15%|        | 67/437 [00:01<00:08, 42.70it/s][A
 16%|        | 72/437 [00:01<00:08, 44.08it/s][A
 18%|        | 77/437 [00:01<00:08, 44.46it/s][A
 19%|        | 82/437 [00:01<00:07, 44.82it/s][A
 20%|        | 87/437 [00:01<00:07, 44.96it/s][A
 21%|        | 92/437 [00:02<00:07, 45.10it/s][A
 22%|       | 97/437 [00:02<00:07, 45.20it/s][A
 23%|       | 102/437 [00:02<00:07, 45.28it/s][A
 24%|       | 107/437 [00:02<00:07, 45.33it/s][A
 26%|       | 112/437 [00:02<00:07, 45.07it/s][A
 27%|       | 117/437 [00:02<00:07, 45.04it/s][A
 28%|       | 122/437 [00:02<00:06, 45.17it/s][A
 29%|       | 127/437 [00:02<00:06, 45.28it/s][A
 30%|       | 132/437 [00:02<00:06, 45.40it/s][A
 31%|      | 137/437 [00:03<00:06, 45.41it/s][A
 32%|      | 142/437 [00:03<00:06, 45.42it/s][A
 34%|      | 147/437 [00:03<00:06, 45.35it/s][A
 35%|      | 152/437 [00:03<00:06, 45.41it/s][A
 36%|      | 157/437 [00:03<00:06, 45.25it/s][A
 37%|      | 162/437 [00:03<00:06, 40.53it/s][A
 38%|      | 167/437 [00:03<00:06, 42.03it/s][A
 39%|      | 172/437 [00:03<00:06, 43.13it/s][A
 41%|      | 177/437 [00:03<00:05, 43.91it/s][A
 42%|     | 182/437 [00:04<00:05, 44.46it/s][A
 43%|     | 187/437 [00:04<00:05, 44.83it/s][A
 44%|     | 192/437 [00:04<00:05, 45.01it/s][A
 45%|     | 197/437 [00:04<00:05, 45.07it/s][A
 46%|     | 202/437 [00:04<00:05, 44.71it/s][A
 47%|     | 207/437 [00:04<00:05, 44.64it/s][A
 49%|     | 212/437 [00:04<00:05, 44.82it/s][A
 50%|     | 217/437 [00:04<00:04, 45.05it/s][A
 51%|     | 222/437 [00:04<00:04, 45.18it/s][A
 52%|    | 227/437 [00:05<00:04, 45.37it/s][A
 53%|    | 232/437 [00:05<00:04, 45.44it/s][A
 54%|    | 237/437 [00:05<00:04, 45.47it/s][A
 55%|    | 242/437 [00:05<00:04, 45.53it/s][A
 57%|    | 247/437 [00:05<00:04, 45.31it/s][A
 58%|    | 252/437 [00:05<00:04, 45.21it/s][A
 59%|    | 257/437 [00:05<00:03, 45.18it/s][A
 60%|    | 262/437 [00:05<00:03, 45.30it/s][A
 61%|    | 267/437 [00:05<00:03, 45.38it/s][A
 62%|   | 272/437 [00:06<00:03, 45.45it/s][A
 63%|   | 277/437 [00:06<00:03, 45.55it/s][A
 65%|   | 282/437 [00:06<00:03, 45.54it/s][A
 66%|   | 287/437 [00:06<00:03, 45.52it/s][A
 67%|   | 292/437 [00:06<00:03, 45.40it/s][A
 68%|   | 297/437 [00:06<00:03, 40.01it/s][A
 69%|   | 302/437 [00:06<00:03, 41.63it/s][A
 70%|   | 307/437 [00:06<00:03, 42.78it/s][A
 71%|  | 312/437 [00:06<00:02, 43.62it/s][A
 73%|  | 317/437 [00:07<00:02, 44.36it/s][A
 74%|  | 322/437 [00:07<00:02, 44.73it/s][A
 75%|  | 327/437 [00:07<00:02, 44.97it/s][A
 76%|  | 332/437 [00:07<00:02, 45.02it/s][A
 77%|  | 337/437 [00:07<00:02, 44.71it/s][A
 78%|  | 342/437 [00:07<00:02, 44.63it/s][A
 79%|  | 347/437 [00:07<00:02, 44.72it/s][A
 81%|  | 352/437 [00:07<00:01, 44.94it/s][A
 82%| | 357/437 [00:07<00:01, 45.07it/s][A
 83%| | 362/437 [00:08<00:01, 45.26it/s][A
 84%| | 367/437 [00:08<00:01, 45.38it/s][A
 85%| | 372/437 [00:08<00:01, 45.56it/s][A
 86%| | 377/437 [00:08<00:01, 45.49it/s][A
 87%| | 382/437 [00:08<00:01, 45.35it/s][A
 89%| | 387/437 [00:08<00:01, 45.19it/s][A
 90%| | 392/437 [00:08<00:00, 45.05it/s][A
 91%| | 397/437 [00:08<00:00, 45.12it/s][A
 92%|| 402/437 [00:08<00:00, 45.14it/s][A
 93%|| 407/437 [00:09<00:00, 45.31it/s][A
 94%|| 412/437 [00:09<00:00, 45.38it/s][A
 95%|| 417/437 [00:09<00:00, 45.44it/s][A
 97%|| 422/437 [00:09<00:00, 45.47it/s][A
 98%|| 427/437 [00:09<00:00, 45.50it/s][A
 99%|| 432/437 [00:09<00:00, 44.31it/s][A
100%|| 437/437 [00:09<00:00, 44.61it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.61it/s][A 40%|      | 312/780 [02:04<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:06:33,838 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 07:06:34,081 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:06:36,862 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:06:37,100 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:06:37,262 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:15<50:47,  6.53s/it] 40%|      | 314/780 [02:15<36:12,  4.66s/it] 40%|      | 315/780 [02:16<25:58,  3.35s/it] 41%|      | 316/780 [02:16<18:49,  2.43s/it] 41%|      | 317/780 [02:16<13:49,  1.79s/it] 41%|      | 318/780 [02:16<10:19,  1.34s/it] 41%|      | 319/780 [02:17<07:53,  1.03s/it] 41%|      | 320/780 [02:17<06:11,  1.24it/s] 41%|      | 321/780 [02:17<04:59,  1.53it/s] 41%|     | 322/780 [02:18<04:09,  1.84it/s] 41%|     | 323/780 [02:18<03:33,  2.14it/s] 42%|     | 324/780 [02:18<03:08,  2.42it/s] 42%|     | 325/780 [02:19<02:54,  2.61it/s] 42%|     | 326/780 [02:19<02:41,  2.82it/s] 42%|     | 327/780 [02:19<02:31,  2.98it/s] 42%|     | 328/780 [02:19<02:25,  3.11it/s] 42%|     | 329/780 [02:20<02:20,  3.21it/s] 42%|     | 330/780 [02:20<02:17,  3.28it/s] 42%|     | 331/780 [02:20<02:14,  3.34it/s] 43%|     | 332/780 [02:21<02:12,  3.37it/s] 43%|     | 333/780 [02:21<02:11,  3.40it/s] 43%|     | 334/780 [02:21<02:10,  3.42it/s] 43%|     | 335/780 [02:21<02:09,  3.43it/s] 43%|     | 336/780 [02:22<02:11,  3.37it/s] 43%|     | 337/780 [02:22<02:10,  3.39it/s] 43%|     | 338/780 [02:22<02:09,  3.41it/s] 43%|     | 339/780 [02:23<02:08,  3.43it/s] 44%|     | 340/780 [02:23<02:07,  3.44it/s] 44%|     | 341/780 [02:23<02:07,  3.45it/s] 44%|     | 342/780 [02:23<02:06,  3.46it/s] 44%|     | 343/780 [02:24<02:06,  3.46it/s] 44%|     | 344/780 [02:24<02:06,  3.46it/s] 44%|     | 345/780 [02:24<02:05,  3.46it/s] 44%|     | 346/780 [02:25<02:05,  3.46it/s] 44%|     | 347/780 [02:25<02:08,  3.36it/s] 45%|     | 348/780 [02:25<02:07,  3.40it/s] 45%|     | 349/780 [02:25<02:06,  3.41it/s] 45%|     | 350/780 [02:26<02:05,  3.43it/s] 45%|     | 351/780 [02:26<02:04,  3.44it/s] 45%|     | 352/780 [02:26<02:04,  3.45it/s] 45%|     | 353/780 [02:27<02:03,  3.46it/s] 45%|     | 354/780 [02:27<02:03,  3.46it/s] 46%|     | 355/780 [02:27<02:02,  3.46it/s] 46%|     | 356/780 [02:28<02:02,  3.46it/s] 46%|     | 357/780 [02:28<02:02,  3.46it/s] 46%|     | 358/780 [02:28<02:03,  3.41it/s] 46%|     | 359/780 [02:28<02:03,  3.42it/s] 46%|     | 360/780 [02:29<02:02,  3.44it/s] 46%|     | 361/780 [02:29<02:01,  3.44it/s] 46%|     | 362/780 [02:29<02:01,  3.45it/s] 47%|     | 363/780 [02:30<02:00,  3.46it/s] 47%|     | 364/780 [02:30<02:00,  3.46it/s] 47%|     | 365/780 [02:30<01:59,  3.46it/s] 47%|     | 366/780 [02:30<01:59,  3.46it/s] 47%|     | 367/780 [02:31<01:59,  3.46it/s] 47%|     | 368/780 [02:31<01:58,  3.46it/s] 47%|     | 369/780 [02:31<02:00,  3.40it/s] 47%|     | 370/780 [02:32<01:59,  3.42it/s] 48%|     | 371/780 [02:32<01:59,  3.43it/s] 48%|     | 372/780 [02:32<01:58,  3.45it/s] 48%|     | 373/780 [02:32<01:57,  3.45it/s] 48%|     | 374/780 [02:33<01:57,  3.46it/s] 48%|     | 375/780 [02:33<01:57,  3.46it/s] 48%|     | 376/780 [02:33<01:56,  3.45it/s] 48%|     | 377/780 [02:34<01:56,  3.45it/s] 48%|     | 378/780 [02:34<01:56,  3.46it/s] 49%|     | 379/780 [02:34<01:56,  3.46it/s] 49%|     | 380/780 [02:34<01:58,  3.38it/s] 49%|     | 381/780 [02:35<01:57,  3.41it/s] 49%|     | 382/780 [02:35<01:56,  3.42it/s] 49%|     | 383/780 [02:35<01:55,  3.44it/s] 49%|     | 384/780 [02:36<01:54,  3.44it/s] 49%|     | 385/780 [02:36<01:54,  3.45it/s] 49%|     | 386/780 [02:36<01:54,  3.45it/s] 50%|     | 387/780 [02:37<01:53,  3.46it/s] 50%|     | 388/780 [02:37<01:53,  3.46it/s] 50%|     | 389/780 [02:37<01:53,  3.46it/s] 50%|     | 390/780 [02:37<01:52,  3.46it/s] 50%|     | 391/780 [02:38<01:52,  3.46it/s] 50%|     | 392/780 [02:38<01:52,  3.46it/s] 50%|     | 393/780 [02:38<01:51,  3.46it/s] 51%|     | 394/780 [02:39<01:51,  3.46it/s] 51%|     | 395/780 [02:39<01:54,  3.37it/s] 51%|     | 396/780 [02:39<01:52,  3.40it/s] 51%|     | 397/780 [02:39<01:52,  3.42it/s] 51%|     | 398/780 [02:40<01:51,  3.43it/s] 51%|     | 399/780 [02:40<01:50,  3.44it/s] 51%|    | 400/780 [02:40<01:50,  3.45it/s] 51%|    | 401/780 [02:41<01:49,  3.45it/s] 52%|    | 402/780 [02:41<01:49,  3.46it/s] 52%|    | 403/780 [02:41<01:48,  3.46it/s] 52%|    | 404/780 [02:41<01:48,  3.46it/s] 52%|    | 405/780 [02:42<01:48,  3.46it/s] 52%|    | 406/780 [02:42<01:50,  3.39it/s] 52%|    | 407/780 [02:42<01:49,  3.41it/s] 52%|    | 408/780 [02:43<01:48,  3.42it/s] 52%|    | 409/780 [02:43<01:47,  3.44it/s] 53%|    | 410/780 [02:43<01:47,  3.44it/s] 53%|    | 411/780 [02:43<01:47,  3.45it/s] 53%|    | 412/780 [02:44<01:46,  3.45it/s] 53%|    | 413/780 [02:44<01:46,  3.46it/s] 53%|    | 414/780 [02:44<01:45,  3.46it/s] 53%|    | 415/780 [02:45<01:45,  3.46it/s] 53%|    | 416/780 [02:45<01:45,  3.46it/s] 53%|    | 417/780 [02:45<01:51,  3.27it/s] 54%|    | 418/780 [02:46<01:48,  3.32it/s] 54%|    | 419/780 [02:46<01:47,  3.36it/s] 54%|    | 420/780 [02:46<01:46,  3.40it/s] 54%|    | 421/780 [02:46<01:44,  3.42it/s] 54%|    | 422/780 [02:47<01:44,  3.43it/s] 54%|    | 423/780 [02:47<02:00,  2.96it/s] 54%|    | 424/780 [02:47<01:54,  3.10it/s] 54%|    | 425/780 [02:48<01:50,  3.20it/s] 55%|    | 426/780 [02:48<01:48,  3.28it/s] 55%|    | 427/780 [02:48<01:47,  3.28it/s] 55%|    | 428/780 [02:49<01:45,  3.33it/s] 55%|    | 429/780 [02:49<01:44,  3.37it/s] 55%|    | 430/780 [02:49<01:43,  3.40it/s] 55%|    | 431/780 [02:50<01:42,  3.42it/s] 55%|    | 432/780 [02:50<01:41,  3.43it/s] 56%|    | 433/780 [02:50<01:40,  3.44it/s] 56%|    | 434/780 [02:50<01:40,  3.45it/s] 56%|    | 435/780 [02:51<01:39,  3.45it/s] 56%|    | 436/780 [02:51<01:39,  3.45it/s] 56%|    | 437/780 [02:51<01:39,  3.46it/s] 56%|    | 438/780 [02:52<01:40,  3.41it/s] 56%|    | 439/780 [02:52<01:39,  3.42it/s] 56%|    | 440/780 [02:52<01:38,  3.44it/s] 57%|    | 441/780 [02:52<01:38,  3.44it/s] 57%|    | 442/780 [02:53<01:37,  3.45it/s] 57%|    | 443/780 [02:53<01:37,  3.45it/s] 57%|    | 444/780 [02:53<01:37,  3.46it/s] 57%|    | 445/780 [02:54<01:36,  3.46it/s] 57%|    | 446/780 [02:54<01:36,  3.46it/s] 57%|    | 447/780 [02:54<01:36,  3.46it/s] 57%|    | 448/780 [02:54<01:35,  3.46it/s] 58%|    | 449/780 [02:55<01:37,  3.41it/s] 58%|    | 450/780 [02:55<01:36,  3.42it/s] 58%|    | 451/780 [02:55<01:35,  3.43it/s] 58%|    | 452/780 [02:56<01:35,  3.44it/s] 58%|    | 453/780 [02:56<01:34,  3.45it/s] 58%|    | 454/780 [02:56<01:34,  3.45it/s] 58%|    | 455/780 [02:56<01:34,  3.46it/s] 58%|    | 456/780 [02:57<01:33,  3.46it/s] 59%|    | 457/780 [02:57<01:33,  3.46it/s] 59%|    | 458/780 [02:57<01:33,  3.46it/s] 59%|    | 459/780 [02:58<01:32,  3.46it/s] 59%|    | 460/780 [02:58<01:35,  3.37it/s] 59%|    | 461/780 [02:58<01:33,  3.40it/s] 59%|    | 462/780 [02:59<01:33,  3.42it/s] 59%|    | 463/780 [02:59<01:32,  3.43it/s] 59%|    | 464/780 [02:59<01:31,  3.44it/s] 60%|    | 465/780 [02:59<01:31,  3.45it/s] 60%|    | 466/780 [03:00<01:31,  3.45it/s] 60%|    | 467/780 [03:00<01:30,  3.45it/s] 60%|    | 468/780 [03:00<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 07:07:30,126 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:07:30,126 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:07:30,126 >>   Batch size = 8
{'eval_loss': 0.979205846786499, 'eval_runtime': 9.7805, 'eval_samples_per_second': 356.729, 'eval_steps_per_second': 44.681, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.81it/s][A
  3%|         | 12/437 [00:00<00:08, 49.35it/s][A
  4%|         | 17/437 [00:00<00:08, 47.60it/s][A
  5%|         | 22/437 [00:00<00:09, 42.90it/s][A
  6%|         | 27/437 [00:00<00:09, 43.74it/s][A
  7%|         | 32/437 [00:00<00:09, 44.34it/s][A
  8%|         | 37/437 [00:00<00:08, 44.56it/s][A
 10%|         | 42/437 [00:00<00:08, 44.74it/s][A
 11%|         | 47/437 [00:01<00:08, 44.92it/s][A
 12%|        | 52/437 [00:01<00:08, 45.24it/s][A
 13%|        | 57/437 [00:01<00:08, 45.32it/s][A
 14%|        | 62/437 [00:01<00:08, 45.11it/s][A
 15%|        | 67/437 [00:01<00:08, 45.09it/s][A
 16%|        | 72/437 [00:01<00:08, 45.17it/s][A
 18%|        | 77/437 [00:01<00:07, 45.26it/s][A
 19%|        | 82/437 [00:01<00:07, 45.30it/s][A
 20%|        | 87/437 [00:01<00:07, 45.31it/s][A
 21%|        | 92/437 [00:02<00:07, 45.37it/s][A
 22%|       | 97/437 [00:02<00:07, 45.48it/s][A
 23%|       | 102/437 [00:02<00:07, 45.48it/s][A
 24%|       | 107/437 [00:02<00:07, 45.42it/s][A
 26%|       | 112/437 [00:02<00:07, 45.28it/s][A
 27%|       | 117/437 [00:02<00:07, 45.29it/s][A
 28%|       | 122/437 [00:02<00:06, 45.23it/s][A
 29%|       | 127/437 [00:02<00:06, 45.29it/s][A
 30%|       | 132/437 [00:02<00:06, 45.34it/s][A
 31%|      | 137/437 [00:03<00:06, 45.35it/s][A
 32%|      | 142/437 [00:03<00:06, 45.38it/s][A
 34%|      | 147/437 [00:03<00:06, 45.44it/s][A
 35%|      | 152/437 [00:03<00:06, 45.44it/s][A
 36%|      | 157/437 [00:03<00:06, 41.98it/s][A
 37%|      | 162/437 [00:03<00:06, 43.04it/s][A
 38%|      | 167/437 [00:03<00:06, 43.76it/s][A
 39%|      | 172/437 [00:03<00:05, 44.28it/s][A
 41%|      | 177/437 [00:03<00:05, 44.61it/s][A
 42%|     | 182/437 [00:04<00:05, 44.78it/s][A
 43%|     | 187/437 [00:04<00:05, 45.02it/s][A
 44%|     | 192/437 [00:04<00:05, 45.18it/s][A
 45%|     | 197/437 [00:04<00:05, 44.87it/s][A
 46%|     | 202/437 [00:04<00:05, 44.89it/s][A
 47%|     | 207/437 [00:04<00:05, 45.05it/s][A
 49%|     | 212/437 [00:04<00:04, 45.23it/s][A
 50%|     | 217/437 [00:04<00:04, 45.33it/s][A
 51%|     | 222/437 [00:04<00:04, 45.36it/s][A
 52%|    | 227/437 [00:05<00:04, 45.30it/s][A
 53%|    | 232/437 [00:05<00:04, 45.34it/s][A
 54%|    | 237/437 [00:05<00:04, 45.40it/s][A
 55%|    | 242/437 [00:05<00:04, 45.24it/s][A
 57%|    | 247/437 [00:05<00:04, 45.10it/s][A
 58%|    | 252/437 [00:05<00:04, 45.13it/s][A
 59%|    | 257/437 [00:05<00:03, 45.22it/s][A
 60%|    | 262/437 [00:05<00:03, 45.40it/s][A
 61%|    | 267/437 [00:05<00:04, 42.39it/s][A
 62%|   | 272/437 [00:06<00:03, 43.34it/s][A
 63%|   | 277/437 [00:06<00:03, 44.05it/s][A
 65%|   | 282/437 [00:06<00:03, 44.50it/s][A
 66%|   | 287/437 [00:06<00:03, 44.65it/s][A
 67%|   | 292/437 [00:06<00:03, 44.76it/s][A
 68%|   | 297/437 [00:06<00:03, 45.03it/s][A
 69%|   | 302/437 [00:06<00:02, 45.10it/s][A
 70%|   | 307/437 [00:06<00:02, 44.88it/s][A
 71%|  | 312/437 [00:06<00:02, 44.94it/s][A
 73%|  | 317/437 [00:07<00:02, 45.19it/s][A
 74%|  | 322/437 [00:07<00:02, 45.38it/s][A
 75%|  | 327/437 [00:07<00:02, 45.46it/s][A
 76%|  | 332/437 [00:07<00:02, 45.35it/s][A
 77%|  | 337/437 [00:07<00:02, 45.38it/s][A
 78%|  | 342/437 [00:07<00:02, 45.41it/s][A
 79%|  | 347/437 [00:07<00:01, 45.39it/s][A
 81%|  | 352/437 [00:07<00:01, 45.18it/s][A
 82%| | 357/437 [00:07<00:01, 45.02it/s][A
 83%| | 362/437 [00:08<00:01, 45.25it/s][A
 84%| | 367/437 [00:08<00:01, 45.33it/s][A
 85%| | 372/437 [00:08<00:01, 45.47it/s][A
 86%| | 377/437 [00:08<00:01, 45.42it/s][A
 87%| | 382/437 [00:08<00:01, 45.45it/s][A
 89%| | 387/437 [00:08<00:01, 45.54it/s][A
 90%| | 392/437 [00:08<00:00, 45.48it/s][A
 91%| | 397/437 [00:08<00:00, 45.31it/s][A
 92%|| 402/437 [00:08<00:00, 41.29it/s][A
 93%|| 407/437 [00:09<00:00, 42.48it/s][A
 94%|| 412/437 [00:09<00:00, 43.35it/s][A
 95%|| 417/437 [00:09<00:00, 43.91it/s][A
 97%|| 422/437 [00:09<00:00, 44.48it/s][A
 98%|| 427/437 [00:09<00:00, 44.96it/s][A
 99%|| 432/437 [00:09<00:00, 45.23it/s][A
100%|| 437/437 [00:09<00:00, 45.37it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.37it/s][A 60%|    | 468/780 [03:10<01:30,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:07:40,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 07:07:40,146 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:07:42,738 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:07:42,843 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:07:42,914 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:19<30:36,  5.91s/it] 60%|    | 470/780 [03:20<21:50,  4.23s/it] 60%|    | 471/780 [03:20<15:41,  3.05s/it] 61%|    | 472/780 [03:20<11:23,  2.22s/it] 61%|    | 473/780 [03:20<08:24,  1.64s/it] 61%|    | 474/780 [03:21<06:18,  1.24s/it] 61%|    | 475/780 [03:21<04:51,  1.05it/s] 61%|    | 476/780 [03:21<03:49,  1.32it/s] 61%|    | 477/780 [03:22<03:06,  1.62it/s] 61%|   | 478/780 [03:22<02:36,  1.92it/s] 61%|   | 479/780 [03:22<02:16,  2.21it/s] 62%|   | 480/780 [03:22<02:01,  2.47it/s] 62%|   | 481/780 [03:23<01:53,  2.64it/s] 62%|   | 482/780 [03:23<01:45,  2.83it/s] 62%|   | 483/780 [03:23<01:39,  2.98it/s] 62%|   | 484/780 [03:24<01:35,  3.10it/s] 62%|   | 485/780 [03:24<01:32,  3.19it/s] 62%|   | 486/780 [03:24<01:30,  3.25it/s] 62%|   | 487/780 [03:25<01:28,  3.30it/s] 63%|   | 488/780 [03:25<01:27,  3.35it/s] 63%|   | 489/780 [03:25<01:26,  3.38it/s] 63%|   | 490/780 [03:25<01:25,  3.41it/s] 63%|   | 491/780 [03:26<01:24,  3.43it/s] 63%|   | 492/780 [03:26<01:26,  3.33it/s] 63%|   | 493/780 [03:26<01:25,  3.35it/s] 63%|   | 494/780 [03:27<01:24,  3.37it/s] 63%|   | 495/780 [03:27<01:24,  3.38it/s] 64%|   | 496/780 [03:27<01:23,  3.39it/s] 64%|   | 497/780 [03:28<01:23,  3.40it/s] 64%|   | 498/780 [03:28<01:22,  3.40it/s] 64%|   | 499/780 [03:28<01:22,  3.41it/s] 64%|   | 500/780 [03:28<01:22,  3.41it/s]                                                  64%|   | 500/780 [03:28<01:22,  3.41it/s] 64%|   | 501/780 [03:29<01:21,  3.41it/s] 64%|   | 502/780 [03:29<01:21,  3.41it/s] 64%|   | 503/780 [03:29<01:22,  3.37it/s] 65%|   | 504/780 [03:30<01:21,  3.38it/s] 65%|   | 505/780 [03:30<01:21,  3.39it/s] 65%|   | 506/780 [03:30<01:20,  3.40it/s] 65%|   | 507/780 [03:30<01:20,  3.40it/s] 65%|   | 508/780 [03:31<01:19,  3.40it/s] 65%|   | 509/780 [03:31<01:19,  3.41it/s] 65%|   | 510/780 [03:31<01:19,  3.41it/s] 66%|   | 511/780 [03:32<01:18,  3.41it/s] 66%|   | 512/780 [03:32<01:18,  3.41it/s] 66%|   | 513/780 [03:32<01:18,  3.41it/s] 66%|   | 514/780 [03:33<01:19,  3.36it/s] 66%|   | 515/780 [03:33<01:18,  3.38it/s] 66%|   | 516/780 [03:33<01:17,  3.39it/s] 66%|   | 517/780 [03:33<01:17,  3.39it/s] 66%|   | 518/780 [03:34<01:17,  3.40it/s] 67%|   | 519/780 [03:34<01:16,  3.40it/s] 67%|   | 520/780 [03:34<01:16,  3.41it/s] 67%|   | 521/780 [03:35<01:16,  3.40it/s] 67%|   | 522/780 [03:35<01:15,  3.41it/s] 67%|   | 523/780 [03:35<01:15,  3.41it/s] 67%|   | 524/780 [03:35<01:15,  3.41it/s] 67%|   | 525/780 [03:36<01:16,  3.33it/s] 67%|   | 526/780 [03:36<01:15,  3.36it/s] 68%|   | 527/780 [03:36<01:15,  3.37it/s] 68%|   | 528/780 [03:37<01:14,  3.39it/s] 68%|   | 529/780 [03:37<01:13,  3.40it/s] 68%|   | 530/780 [03:37<01:13,  3.40it/s] 68%|   | 531/780 [03:38<01:13,  3.40it/s] 68%|   | 532/780 [03:38<01:12,  3.41it/s] 68%|   | 533/780 [03:38<01:11,  3.44it/s] 68%|   | 534/780 [03:38<01:11,  3.44it/s] 69%|   | 535/780 [03:39<01:11,  3.45it/s] 69%|   | 536/780 [03:39<01:10,  3.45it/s] 69%|   | 537/780 [03:39<01:10,  3.46it/s] 69%|   | 538/780 [03:40<01:10,  3.45it/s] 69%|   | 539/780 [03:40<01:09,  3.46it/s] 69%|   | 540/780 [03:40<01:11,  3.37it/s] 69%|   | 541/780 [03:40<01:10,  3.40it/s] 69%|   | 542/780 [03:41<01:09,  3.41it/s] 70%|   | 543/780 [03:41<01:09,  3.43it/s] 70%|   | 544/780 [03:41<01:08,  3.44it/s] 70%|   | 545/780 [03:42<01:08,  3.45it/s] 70%|   | 546/780 [03:42<01:07,  3.45it/s] 70%|   | 547/780 [03:42<01:07,  3.45it/s] 70%|   | 548/780 [03:42<01:07,  3.45it/s] 70%|   | 549/780 [03:43<01:06,  3.46it/s] 71%|   | 550/780 [03:43<01:06,  3.46it/s] 71%|   | 551/780 [03:43<01:07,  3.40it/s] 71%|   | 552/780 [03:44<01:06,  3.42it/s] 71%|   | 553/780 [03:44<01:06,  3.43it/s] 71%|   | 554/780 [03:44<01:05,  3.44it/s] 71%|   | 555/780 [03:45<01:05,  3.45it/s] 71%|  | 556/780 [03:45<01:04,  3.45it/s] 71%|  | 557/780 [03:45<01:04,  3.46it/s] 72%|  | 558/780 [03:45<01:05,  3.41it/s] 72%|  | 559/780 [03:46<01:04,  3.42it/s] 72%|  | 560/780 [03:46<01:04,  3.44it/s] 72%|  | 561/780 [03:46<01:03,  3.44it/s] 72%|  | 562/780 [03:47<01:04,  3.36it/s] 72%|  | 563/780 [03:47<01:04,  3.39it/s] 72%|  | 564/780 [03:47<01:13,  2.94it/s] 72%|  | 565/780 [03:48<01:09,  3.08it/s] 73%|  | 566/780 [03:48<01:07,  3.19it/s] 73%|  | 567/780 [03:48<01:05,  3.26it/s] 73%|  | 568/780 [03:48<01:03,  3.32it/s] 73%|  | 569/780 [03:49<01:02,  3.36it/s] 73%|  | 570/780 [03:49<01:01,  3.39it/s] 73%|  | 571/780 [03:49<01:01,  3.42it/s] 73%|  | 572/780 [03:50<01:01,  3.36it/s] 73%|  | 573/780 [03:50<01:00,  3.40it/s] 74%|  | 574/780 [03:50<01:00,  3.42it/s] 74%|  | 575/780 [03:50<00:59,  3.43it/s] 74%|  | 576/780 [03:51<00:59,  3.44it/s] 74%|  | 577/780 [03:51<00:58,  3.45it/s] 74%|  | 578/780 [03:51<00:58,  3.45it/s] 74%|  | 579/780 [03:52<00:58,  3.45it/s] 74%|  | 580/780 [03:52<00:57,  3.46it/s] 74%|  | 581/780 [03:52<00:57,  3.46it/s] 75%|  | 582/780 [03:53<00:57,  3.47it/s] 75%|  | 583/780 [03:53<00:57,  3.40it/s] 75%|  | 584/780 [03:53<00:57,  3.42it/s] 75%|  | 585/780 [03:53<00:56,  3.44it/s] 75%|  | 586/780 [03:54<00:56,  3.44it/s] 75%|  | 587/780 [03:54<00:55,  3.45it/s] 75%|  | 588/780 [03:54<00:55,  3.46it/s] 76%|  | 589/780 [03:55<00:55,  3.46it/s] 76%|  | 590/780 [03:55<00:54,  3.46it/s] 76%|  | 591/780 [03:55<00:54,  3.46it/s] 76%|  | 592/780 [03:55<00:54,  3.46it/s] 76%|  | 593/780 [03:56<00:54,  3.46it/s] 76%|  | 594/780 [03:56<00:55,  3.36it/s] 76%|  | 595/780 [03:56<00:54,  3.40it/s] 76%|  | 596/780 [03:57<00:53,  3.42it/s] 77%|  | 597/780 [03:57<00:53,  3.43it/s] 77%|  | 598/780 [03:57<00:52,  3.44it/s] 77%|  | 599/780 [03:57<00:52,  3.45it/s] 77%|  | 600/780 [03:58<00:52,  3.45it/s] 77%|  | 601/780 [03:58<00:51,  3.45it/s] 77%|  | 602/780 [03:58<00:51,  3.46it/s] 77%|  | 603/780 [03:59<00:51,  3.46it/s] 77%|  | 604/780 [03:59<00:50,  3.46it/s] 78%|  | 605/780 [03:59<00:51,  3.39it/s] 78%|  | 606/780 [04:00<00:51,  3.41it/s] 78%|  | 607/780 [04:00<00:50,  3.43it/s] 78%|  | 608/780 [04:00<00:50,  3.43it/s] 78%|  | 609/780 [04:00<00:49,  3.45it/s] 78%|  | 610/780 [04:01<00:49,  3.45it/s] 78%|  | 611/780 [04:01<00:48,  3.45it/s] 78%|  | 612/780 [04:01<00:48,  3.46it/s] 79%|  | 613/780 [04:02<00:48,  3.46it/s] 79%|  | 614/780 [04:02<00:47,  3.46it/s] 79%|  | 615/780 [04:02<00:47,  3.46it/s] 79%|  | 616/780 [04:02<00:49,  3.31it/s] 79%|  | 617/780 [04:03<00:48,  3.35it/s] 79%|  | 618/780 [04:03<00:47,  3.38it/s] 79%|  | 619/780 [04:03<00:47,  3.41it/s] 79%|  | 620/780 [04:04<00:46,  3.42it/s] 80%|  | 621/780 [04:04<00:46,  3.43it/s] 80%|  | 622/780 [04:04<00:45,  3.45it/s] 80%|  | 623/780 [04:04<00:45,  3.45it/s] 80%|  | 624/780 [04:05<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 07:08:34,634 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:08:34,634 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:08:34,634 >>   Batch size = 8
{'eval_loss': 0.9836779236793518, 'eval_runtime': 9.7649, 'eval_samples_per_second': 357.299, 'eval_steps_per_second': 44.752, 'epoch': 3.0}
{'loss': 0.5276, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.50it/s][A
  3%|         | 12/437 [00:00<00:08, 49.44it/s][A
  4%|         | 18/437 [00:00<00:08, 47.23it/s][A
  5%|         | 23/437 [00:00<00:09, 42.69it/s][A
  6%|         | 28/437 [00:00<00:09, 43.50it/s][A
  8%|         | 33/437 [00:00<00:09, 44.13it/s][A
  9%|         | 38/437 [00:00<00:08, 44.46it/s][A
 10%|         | 43/437 [00:00<00:08, 44.67it/s][A
 11%|         | 48/437 [00:01<00:08, 44.99it/s][A
 12%|        | 53/437 [00:01<00:08, 45.28it/s][A
 13%|        | 58/437 [00:01<00:08, 45.29it/s][A
 14%|        | 63/437 [00:01<00:08, 45.02it/s][A
 16%|        | 68/437 [00:01<00:08, 44.89it/s][A
 17%|        | 73/437 [00:01<00:08, 45.14it/s][A
 18%|        | 78/437 [00:01<00:07, 45.17it/s][A
 19%|        | 83/437 [00:01<00:07, 45.24it/s][A
 20%|        | 88/437 [00:01<00:07, 45.24it/s][A
 21%|       | 93/437 [00:02<00:07, 45.35it/s][A
 22%|       | 98/437 [00:02<00:07, 45.47it/s][A
 24%|       | 103/437 [00:02<00:07, 45.52it/s][A
 25%|       | 108/437 [00:02<00:07, 45.33it/s][A
 26%|       | 113/437 [00:02<00:07, 45.22it/s][A
 27%|       | 118/437 [00:02<00:07, 45.30it/s][A
 28%|       | 123/437 [00:02<00:06, 45.30it/s][A
 29%|       | 128/437 [00:02<00:07, 38.67it/s][A
 30%|       | 133/437 [00:03<00:07, 40.51it/s][A
 32%|      | 138/437 [00:03<00:07, 41.88it/s][A
 33%|      | 143/437 [00:03<00:06, 43.00it/s][A
 34%|      | 148/437 [00:03<00:06, 43.76it/s][A
 35%|      | 153/437 [00:03<00:06, 44.22it/s][A
 36%|      | 158/437 [00:03<00:06, 44.59it/s][A
 37%|      | 163/437 [00:03<00:06, 44.91it/s][A
 38%|      | 168/437 [00:03<00:06, 44.66it/s][A
 40%|      | 173/437 [00:03<00:05, 44.63it/s][A
 41%|      | 178/437 [00:03<00:05, 44.79it/s][A
 42%|     | 183/437 [00:04<00:05, 44.93it/s][A
 43%|     | 188/437 [00:04<00:05, 45.20it/s][A
 44%|     | 193/437 [00:04<00:05, 45.31it/s][A
 45%|     | 198/437 [00:04<00:05, 45.37it/s][A
 46%|     | 203/437 [00:04<00:05, 45.24it/s][A
 48%|     | 208/437 [00:04<00:05, 45.36it/s][A
 49%|     | 213/437 [00:04<00:04, 45.23it/s][A
 50%|     | 218/437 [00:04<00:04, 45.19it/s][A
 51%|     | 223/437 [00:04<00:04, 45.03it/s][A
 52%|    | 228/437 [00:05<00:04, 45.11it/s][A
 53%|    | 233/437 [00:05<00:04, 45.30it/s][A
 54%|    | 238/437 [00:05<00:04, 45.43it/s][A
 56%|    | 243/437 [00:05<00:04, 45.46it/s][A
 57%|    | 248/437 [00:05<00:04, 45.45it/s][A
 58%|    | 253/437 [00:05<00:04, 45.41it/s][A
 59%|    | 258/437 [00:05<00:03, 45.33it/s][A
 60%|    | 263/437 [00:05<00:04, 41.48it/s][A
 61%|   | 268/437 [00:06<00:03, 42.62it/s][A
 62%|   | 273/437 [00:06<00:03, 43.49it/s][A
 64%|   | 278/437 [00:06<00:03, 44.13it/s][A
 65%|   | 283/437 [00:06<00:03, 44.58it/s][A
 66%|   | 288/437 [00:06<00:03, 44.85it/s][A
 67%|   | 293/437 [00:06<00:03, 45.00it/s][A
 68%|   | 298/437 [00:06<00:03, 45.19it/s][A
 69%|   | 303/437 [00:06<00:02, 44.92it/s][A
 70%|   | 308/437 [00:06<00:02, 44.79it/s][A
 72%|  | 313/437 [00:07<00:02, 44.90it/s][A
 73%|  | 318/437 [00:07<00:02, 45.11it/s][A
 74%|  | 323/437 [00:07<00:02, 45.26it/s][A
 75%|  | 328/437 [00:07<00:02, 45.35it/s][A
 76%|  | 333/437 [00:07<00:02, 45.44it/s][A
 77%|  | 338/437 [00:07<00:02, 45.37it/s][A
 78%|  | 343/437 [00:07<00:02, 45.42it/s][A
 80%|  | 348/437 [00:07<00:01, 45.26it/s][A
 81%|  | 353/437 [00:07<00:01, 45.06it/s][A
 82%| | 358/437 [00:08<00:01, 44.97it/s][A
 83%| | 363/437 [00:08<00:01, 45.05it/s][A
 84%| | 368/437 [00:08<00:01, 45.18it/s][A
 85%| | 373/437 [00:08<00:01, 45.28it/s][A
 86%| | 378/437 [00:08<00:01, 45.43it/s][A
 88%| | 383/437 [00:08<00:01, 45.42it/s][A
 89%| | 388/437 [00:08<00:01, 45.48it/s][A
 90%| | 393/437 [00:08<00:00, 45.35it/s][A
 91%| | 398/437 [00:08<00:00, 44.65it/s][A
 92%|| 403/437 [00:08<00:00, 44.66it/s][A
 93%|| 408/437 [00:09<00:00, 44.80it/s][A
 95%|| 413/437 [00:09<00:00, 44.97it/s][A
 96%|| 418/437 [00:09<00:00, 45.17it/s][A
 97%|| 423/437 [00:09<00:00, 45.24it/s][A
 98%|| 428/437 [00:09<00:00, 45.37it/s][A
 99%|| 433/437 [00:09<00:00, 45.44it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.44it/s][A 80%|  | 624/780 [04:15<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:08:44,600 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 07:08:44,711 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:08:47,856 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:08:48,046 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:08:48,136 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:25<16:05,  6.23s/it] 80%|  | 626/780 [04:25<11:26,  4.46s/it] 80%|  | 627/780 [04:25<08:11,  3.21s/it] 81%|  | 628/780 [04:26<05:54,  2.33s/it] 81%|  | 629/780 [04:26<04:19,  1.72s/it] 81%|  | 630/780 [04:26<03:14,  1.29s/it] 81%|  | 631/780 [04:27<02:27,  1.01it/s] 81%|  | 632/780 [04:27<01:55,  1.28it/s] 81%|  | 633/780 [04:27<01:33,  1.57it/s] 81%| | 634/780 [04:28<01:17,  1.88it/s] 81%| | 635/780 [04:28<01:06,  2.17it/s] 82%| | 636/780 [04:28<00:59,  2.43it/s] 82%| | 637/780 [04:28<00:54,  2.64it/s] 82%| | 638/780 [04:29<00:50,  2.84it/s] 82%| | 639/780 [04:29<00:47,  2.99it/s] 82%| | 640/780 [04:29<00:45,  3.10it/s] 82%| | 641/780 [04:30<00:43,  3.19it/s] 82%| | 642/780 [04:30<00:42,  3.25it/s] 82%| | 643/780 [04:30<00:41,  3.30it/s] 83%| | 644/780 [04:30<00:40,  3.33it/s] 83%| | 645/780 [04:31<00:40,  3.36it/s] 83%| | 646/780 [04:31<00:39,  3.37it/s] 83%| | 647/780 [04:31<00:39,  3.38it/s] 83%| | 648/780 [04:32<00:39,  3.34it/s] 83%| | 649/780 [04:32<00:38,  3.36it/s] 83%| | 650/780 [04:32<00:38,  3.38it/s] 83%| | 651/780 [04:33<00:38,  3.39it/s] 84%| | 652/780 [04:33<00:37,  3.40it/s] 84%| | 653/780 [04:33<00:37,  3.40it/s] 84%| | 654/780 [04:33<00:37,  3.40it/s] 84%| | 655/780 [04:34<00:36,  3.41it/s] 84%| | 656/780 [04:34<00:36,  3.41it/s] 84%| | 657/780 [04:34<00:36,  3.41it/s] 84%| | 658/780 [04:35<00:35,  3.41it/s] 84%| | 659/780 [04:35<00:36,  3.35it/s] 85%| | 660/780 [04:35<00:35,  3.36it/s] 85%| | 661/780 [04:35<00:35,  3.38it/s] 85%| | 662/780 [04:36<00:34,  3.39it/s] 85%| | 663/780 [04:36<00:34,  3.40it/s] 85%| | 664/780 [04:36<00:34,  3.40it/s] 85%| | 665/780 [04:37<00:33,  3.41it/s] 85%| | 666/780 [04:37<00:33,  3.41it/s] 86%| | 667/780 [04:37<00:33,  3.41it/s] 86%| | 668/780 [04:38<00:32,  3.41it/s] 86%| | 669/780 [04:38<00:32,  3.42it/s] 86%| | 670/780 [04:38<00:33,  3.33it/s] 86%| | 671/780 [04:38<00:32,  3.36it/s] 86%| | 672/780 [04:39<00:32,  3.37it/s] 86%| | 673/780 [04:39<00:31,  3.39it/s] 86%| | 674/780 [04:39<00:31,  3.39it/s] 87%| | 675/780 [04:40<00:30,  3.40it/s] 87%| | 676/780 [04:40<00:30,  3.41it/s] 87%| | 677/780 [04:40<00:30,  3.40it/s] 87%| | 678/780 [04:40<00:29,  3.41it/s] 87%| | 679/780 [04:41<00:29,  3.41it/s] 87%| | 680/780 [04:41<00:29,  3.41it/s] 87%| | 681/780 [04:41<00:29,  3.41it/s] 87%| | 682/780 [04:42<00:29,  3.35it/s] 88%| | 683/780 [04:42<00:28,  3.37it/s] 88%| | 684/780 [04:42<00:28,  3.38it/s] 88%| | 685/780 [04:43<00:28,  3.39it/s] 88%| | 686/780 [04:43<00:27,  3.40it/s] 88%| | 687/780 [04:43<00:27,  3.40it/s] 88%| | 688/780 [04:43<00:27,  3.40it/s] 88%| | 689/780 [04:44<00:26,  3.40it/s] 88%| | 690/780 [04:44<00:26,  3.41it/s] 89%| | 691/780 [04:44<00:26,  3.41it/s] 89%| | 692/780 [04:45<00:25,  3.41it/s] 89%| | 693/780 [04:45<00:26,  3.34it/s] 89%| | 694/780 [04:45<00:25,  3.36it/s] 89%| | 695/780 [04:46<00:25,  3.28it/s] 89%| | 696/780 [04:46<00:25,  3.32it/s] 89%| | 697/780 [04:46<00:24,  3.34it/s] 89%| | 698/780 [04:46<00:24,  3.36it/s] 90%| | 699/780 [04:47<00:23,  3.38it/s] 90%| | 700/780 [04:47<00:23,  3.39it/s] 90%| | 701/780 [04:47<00:28,  2.81it/s] 90%| | 702/780 [04:48<00:26,  2.97it/s] 90%| | 703/780 [04:48<00:25,  3.02it/s] 90%| | 704/780 [04:48<00:24,  3.13it/s] 90%| | 705/780 [04:49<00:23,  3.21it/s] 91%| | 706/780 [04:49<00:22,  3.26it/s] 91%| | 707/780 [04:49<00:22,  3.31it/s] 91%| | 708/780 [04:50<00:21,  3.36it/s] 91%| | 709/780 [04:50<00:20,  3.39it/s] 91%| | 710/780 [04:50<00:20,  3.41it/s] 91%| | 711/780 [04:50<00:20,  3.43it/s] 91%|| 712/780 [04:51<00:19,  3.44it/s] 91%|| 713/780 [04:51<00:19,  3.45it/s] 92%|| 714/780 [04:51<00:19,  3.39it/s] 92%|| 715/780 [04:52<00:19,  3.41it/s] 92%|| 716/780 [04:52<00:18,  3.43it/s] 92%|| 717/780 [04:52<00:18,  3.44it/s] 92%|| 718/780 [04:52<00:17,  3.45it/s] 92%|| 719/780 [04:53<00:17,  3.45it/s] 92%|| 720/780 [04:53<00:17,  3.46it/s] 92%|| 721/780 [04:53<00:17,  3.46it/s] 93%|| 722/780 [04:54<00:16,  3.46it/s] 93%|| 723/780 [04:54<00:16,  3.46it/s] 93%|| 724/780 [04:54<00:16,  3.46it/s] 93%|| 725/780 [04:55<00:16,  3.37it/s] 93%|| 726/780 [04:55<00:15,  3.40it/s] 93%|| 727/780 [04:55<00:15,  3.42it/s] 93%|| 728/780 [04:55<00:15,  3.43it/s] 93%|| 729/780 [04:56<00:14,  3.45it/s] 94%|| 730/780 [04:56<00:14,  3.45it/s] 94%|| 731/780 [04:56<00:14,  3.45it/s] 94%|| 732/780 [04:57<00:13,  3.46it/s] 94%|| 733/780 [04:57<00:13,  3.46it/s] 94%|| 734/780 [04:57<00:13,  3.46it/s] 94%|| 735/780 [04:57<00:12,  3.47it/s] 94%|| 736/780 [04:58<00:12,  3.43it/s] 94%|| 737/780 [04:58<00:12,  3.44it/s] 95%|| 738/780 [04:58<00:12,  3.45it/s] 95%|| 739/780 [04:59<00:11,  3.45it/s] 95%|| 740/780 [04:59<00:11,  3.46it/s] 95%|| 741/780 [04:59<00:11,  3.46it/s] 95%|| 742/780 [04:59<00:10,  3.46it/s] 95%|| 743/780 [05:00<00:10,  3.46it/s] 95%|| 744/780 [05:00<00:10,  3.46it/s] 96%|| 745/780 [05:00<00:10,  3.47it/s] 96%|| 746/780 [05:01<00:09,  3.47it/s] 96%|| 747/780 [05:01<00:09,  3.41it/s] 96%|| 748/780 [05:01<00:09,  3.42it/s] 96%|| 749/780 [05:01<00:09,  3.44it/s] 96%|| 750/780 [05:02<00:08,  3.45it/s] 96%|| 751/780 [05:02<00:08,  3.45it/s] 96%|| 752/780 [05:02<00:08,  3.46it/s] 97%|| 753/780 [05:03<00:07,  3.46it/s] 97%|| 754/780 [05:03<00:07,  3.46it/s] 97%|| 755/780 [05:03<00:07,  3.46it/s] 97%|| 756/780 [05:03<00:06,  3.46it/s] 97%|| 757/780 [05:04<00:06,  3.46it/s] 97%|| 758/780 [05:04<00:06,  3.30it/s] 97%|| 759/780 [05:04<00:06,  3.34it/s] 97%|| 760/780 [05:05<00:05,  3.38it/s] 98%|| 761/780 [05:05<00:05,  3.40it/s] 98%|| 762/780 [05:05<00:05,  3.42it/s] 98%|| 763/780 [05:06<00:04,  3.44it/s] 98%|| 764/780 [05:06<00:04,  3.44it/s] 98%|| 765/780 [05:06<00:04,  3.45it/s] 98%|| 766/780 [05:06<00:04,  3.45it/s] 98%|| 767/780 [05:07<00:03,  3.46it/s] 98%|| 768/780 [05:07<00:03,  3.46it/s] 99%|| 769/780 [05:07<00:03,  3.34it/s] 99%|| 770/780 [05:08<00:02,  3.38it/s] 99%|| 771/780 [05:08<00:02,  3.40it/s] 99%|| 772/780 [05:08<00:02,  3.42it/s] 99%|| 773/780 [05:08<00:02,  3.43it/s] 99%|| 774/780 [05:09<00:01,  3.44it/s] 99%|| 775/780 [05:09<00:01,  3.44it/s] 99%|| 776/780 [05:09<00:01,  3.36it/s]100%|| 777/780 [05:10<00:00,  3.39it/s]100%|| 778/780 [05:10<00:00,  3.41it/s]100%|| 779/780 [05:10<00:00,  3.43it/s]100%|| 780/780 [05:11<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 07:09:40,363 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:09:40,364 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:09:40,364 >>   Batch size = 8
{'eval_loss': 0.9905542135238647, 'eval_runtime': 9.791, 'eval_samples_per_second': 356.349, 'eval_steps_per_second': 44.633, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.74it/s][A
  3%|         | 12/437 [00:00<00:08, 49.47it/s][A
  4%|         | 18/437 [00:00<00:08, 47.51it/s][A
  5%|         | 23/437 [00:00<00:08, 46.59it/s][A
  6%|         | 28/437 [00:00<00:08, 46.14it/s][A
  8%|         | 33/437 [00:00<00:08, 45.67it/s][A
  9%|         | 38/437 [00:00<00:08, 45.53it/s][A
 10%|         | 43/437 [00:00<00:08, 45.41it/s][A
 11%|         | 48/437 [00:01<00:08, 45.46it/s][A
 12%|        | 53/437 [00:01<00:08, 45.48it/s][A
 13%|        | 58/437 [00:01<00:08, 45.58it/s][A
 14%|        | 63/437 [00:01<00:08, 45.58it/s][A
 16%|        | 68/437 [00:01<00:08, 45.48it/s][A
 17%|        | 73/437 [00:01<00:08, 41.60it/s][A
 18%|        | 78/437 [00:01<00:08, 42.69it/s][A
 19%|        | 83/437 [00:01<00:08, 43.57it/s][A
 20%|        | 88/437 [00:01<00:07, 44.06it/s][A
 21%|       | 93/437 [00:02<00:07, 44.55it/s][A
 22%|       | 98/437 [00:02<00:07, 44.76it/s][A
 24%|       | 103/437 [00:02<00:07, 45.03it/s][A
 25%|       | 108/437 [00:02<00:07, 45.16it/s][A
 26%|       | 113/437 [00:02<00:07, 44.95it/s][A
 27%|       | 118/437 [00:02<00:07, 44.82it/s][A
 28%|       | 123/437 [00:02<00:06, 45.02it/s][A
 29%|       | 128/437 [00:02<00:06, 45.14it/s][A
 30%|       | 133/437 [00:02<00:06, 45.30it/s][A
 32%|      | 138/437 [00:03<00:06, 45.37it/s][A
 33%|      | 143/437 [00:03<00:06, 45.49it/s][A
 34%|      | 148/437 [00:03<00:06, 45.45it/s][A
 35%|      | 153/437 [00:03<00:06, 45.49it/s][A
 36%|      | 158/437 [00:03<00:06, 45.36it/s][A
 37%|      | 163/437 [00:03<00:06, 45.17it/s][A
 38%|      | 168/437 [00:03<00:05, 45.14it/s][A
 40%|      | 173/437 [00:03<00:05, 45.25it/s][A
 41%|      | 178/437 [00:03<00:05, 45.32it/s][A
 42%|     | 183/437 [00:04<00:05, 45.35it/s][A
 43%|     | 188/437 [00:04<00:05, 45.49it/s][A
 44%|     | 193/437 [00:04<00:05, 45.46it/s][A
 45%|     | 198/437 [00:04<00:05, 45.43it/s][A
 46%|     | 203/437 [00:04<00:05, 45.33it/s][A
 48%|     | 208/437 [00:04<00:05, 44.82it/s][A
 49%|     | 213/437 [00:04<00:04, 44.84it/s][A
 50%|     | 218/437 [00:04<00:04, 45.03it/s][A
 51%|     | 223/437 [00:04<00:04, 45.08it/s][A
 52%|    | 228/437 [00:05<00:04, 45.25it/s][A
 53%|    | 233/437 [00:05<00:04, 45.38it/s][A
 54%|    | 238/437 [00:05<00:04, 45.39it/s][A
 56%|    | 243/437 [00:05<00:04, 45.41it/s][A
 57%|    | 248/437 [00:05<00:04, 45.31it/s][A
 58%|    | 253/437 [00:05<00:04, 45.23it/s][A
 59%|    | 258/437 [00:05<00:03, 45.16it/s][A
 60%|    | 263/437 [00:05<00:03, 45.22it/s][A
 61%|   | 268/437 [00:05<00:03, 45.23it/s][A
 62%|   | 273/437 [00:06<00:03, 45.29it/s][A
 64%|   | 278/437 [00:06<00:03, 45.37it/s][A
 65%|   | 283/437 [00:06<00:03, 45.46it/s][A
 66%|   | 288/437 [00:06<00:03, 45.43it/s][A
 67%|   | 293/437 [00:06<00:03, 45.42it/s][A
 68%|   | 298/437 [00:06<00:03, 45.40it/s][A
 69%|   | 303/437 [00:06<00:02, 45.37it/s][A
 70%|   | 308/437 [00:06<00:02, 45.27it/s][A
 72%|  | 313/437 [00:06<00:02, 45.22it/s][A
 73%|  | 318/437 [00:07<00:02, 45.30it/s][A
 74%|  | 323/437 [00:07<00:02, 45.40it/s][A
 75%|  | 328/437 [00:07<00:02, 45.37it/s][A
 76%|  | 333/437 [00:07<00:02, 45.45it/s][A
 77%|  | 338/437 [00:07<00:02, 45.43it/s][A
 78%|  | 343/437 [00:07<00:02, 45.39it/s][A
 80%|  | 348/437 [00:07<00:02, 43.82it/s][A
 81%|  | 353/437 [00:07<00:01, 44.28it/s][A
 82%| | 358/437 [00:07<00:01, 44.56it/s][A
 83%| | 363/437 [00:08<00:01, 44.81it/s][A
 84%| | 368/437 [00:08<00:01, 44.97it/s][A
 85%| | 373/437 [00:08<00:01, 45.07it/s][A
 86%| | 378/437 [00:08<00:01, 45.20it/s][A
 88%| | 383/437 [00:08<00:01, 45.24it/s][A
 89%| | 388/437 [00:08<00:01, 45.07it/s][A
 90%| | 393/437 [00:08<00:00, 45.15it/s][A
 91%| | 398/437 [00:08<00:00, 45.20it/s][A
 92%|| 403/437 [00:08<00:00, 45.24it/s][A
 93%|| 408/437 [00:09<00:00, 45.38it/s][A
 95%|| 413/437 [00:09<00:00, 45.37it/s][A
 96%|| 418/437 [00:09<00:00, 45.39it/s][A
 97%|| 423/437 [00:09<00:00, 45.45it/s][A
 98%|| 428/437 [00:09<00:00, 45.27it/s][A
 99%|| 433/437 [00:09<00:00, 45.29it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.29it/s][A100%|| 780/780 [05:20<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:09:50,172 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 07:09:50,300 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:09:53,140 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:09:53,281 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:09:53,344 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:10:02,403 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:10:02,431 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156 (score: 0.9601994752883911).
                                                 100%|| 780/780 [05:40<00:00,  3.44it/s]100%|| 780/780 [05:40<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-29 07:10:10,283 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 07:10:10,465 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:10:13,345 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:10:13,480 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:10:13,550 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:10:14,007 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,007 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,008 >>   train_loss               =      0.518
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,008 >>   train_runtime            = 0:05:40.91
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,008 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,008 >>   train_samples_per_second =    146.648
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:14,008 >>   train_steps_per_second   =      2.288
{'eval_loss': 0.9948396682739258, 'eval_runtime': 9.6746, 'eval_samples_per_second': 360.636, 'eval_steps_per_second': 45.17, 'epoch': 5.0}
{'train_runtime': 340.9194, 'train_samples_per_second': 146.648, 'train_steps_per_second': 2.288, 'train_loss': 0.5179712148813101, 'epoch': 5.0}
08/29/2023 07:10:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:10:14,252 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:10:14,252 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 07:10:14,252 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 56.20it/s]  3%|         | 12/437 [00:00<00:08, 49.83it/s]  4%|         | 18/437 [00:00<00:08, 47.97it/s]  5%|         | 23/437 [00:00<00:08, 47.25it/s]  6%|         | 28/437 [00:00<00:08, 46.74it/s]  8%|         | 33/437 [00:00<00:08, 46.46it/s]  9%|         | 38/437 [00:00<00:08, 46.28it/s] 10%|         | 43/437 [00:00<00:08, 45.95it/s] 11%|         | 48/437 [00:01<00:08, 45.47it/s] 12%|        | 53/437 [00:01<00:08, 44.04it/s] 13%|        | 58/437 [00:01<00:08, 44.59it/s] 14%|        | 63/437 [00:01<00:08, 45.06it/s] 16%|        | 68/437 [00:01<00:08, 45.37it/s] 17%|        | 73/437 [00:01<00:08, 45.43it/s] 18%|        | 78/437 [00:01<00:07, 45.55it/s] 19%|        | 83/437 [00:01<00:07, 45.53it/s] 20%|        | 88/437 [00:01<00:07, 45.37it/s] 21%|       | 93/437 [00:02<00:07, 45.11it/s] 22%|       | 98/437 [00:02<00:07, 44.94it/s] 24%|       | 103/437 [00:02<00:07, 44.15it/s] 25%|       | 108/437 [00:02<00:07, 44.72it/s] 26%|       | 113/437 [00:02<00:07, 45.21it/s] 27%|       | 118/437 [00:02<00:07, 45.41it/s] 28%|       | 123/437 [00:02<00:06, 45.63it/s] 29%|       | 128/437 [00:02<00:06, 45.59it/s] 30%|       | 133/437 [00:03<00:10, 28.55it/s] 32%|      | 138/437 [00:03<00:09, 32.30it/s] 33%|      | 143/437 [00:03<00:08, 35.39it/s] 34%|      | 148/437 [00:03<00:07, 38.04it/s] 35%|      | 153/437 [00:03<00:07, 40.07it/s] 36%|      | 158/437 [00:03<00:06, 41.64it/s] 37%|      | 163/437 [00:03<00:06, 42.78it/s] 38%|      | 168/437 [00:03<00:06, 43.70it/s] 40%|      | 173/437 [00:04<00:06, 43.96it/s] 41%|      | 178/437 [00:04<00:05, 44.12it/s] 42%|     | 183/437 [00:04<00:05, 44.44it/s] 43%|     | 188/437 [00:04<00:05, 44.79it/s] 44%|     | 193/437 [00:04<00:05, 45.06it/s] 45%|     | 198/437 [00:04<00:05, 45.23it/s] 46%|     | 203/437 [00:04<00:05, 45.43it/s] 48%|     | 208/437 [00:04<00:05, 45.63it/s] 49%|     | 213/437 [00:04<00:04, 45.57it/s] 50%|     | 218/437 [00:04<00:04, 45.33it/s] 51%|     | 223/437 [00:05<00:04, 45.17it/s] 52%|    | 228/437 [00:05<00:04, 45.13it/s] 53%|    | 233/437 [00:05<00:04, 45.13it/s] 54%|    | 238/437 [00:05<00:04, 45.28it/s] 56%|    | 243/437 [00:05<00:04, 45.40it/s] 57%|    | 248/437 [00:05<00:04, 45.59it/s] 58%|    | 253/437 [00:05<00:04, 45.67it/s] 59%|    | 258/437 [00:05<00:03, 45.65it/s] 60%|    | 263/437 [00:05<00:03, 45.39it/s] 61%|   | 268/437 [00:06<00:03, 45.28it/s] 62%|   | 273/437 [00:06<00:03, 45.17it/s] 64%|   | 278/437 [00:06<00:03, 45.08it/s] 65%|   | 283/437 [00:06<00:03, 45.30it/s] 66%|   | 288/437 [00:06<00:03, 45.51it/s] 67%|   | 293/437 [00:06<00:03, 45.65it/s] 68%|   | 298/437 [00:06<00:03, 45.70it/s] 69%|   | 303/437 [00:06<00:02, 45.61it/s] 70%|   | 308/437 [00:06<00:02, 45.41it/s] 72%|  | 313/437 [00:07<00:02, 45.32it/s] 73%|  | 318/437 [00:07<00:02, 45.15it/s] 74%|  | 323/437 [00:07<00:02, 45.14it/s] 75%|  | 328/437 [00:07<00:02, 45.22it/s] 76%|  | 333/437 [00:07<00:02, 45.45it/s] 77%|  | 338/437 [00:07<00:02, 45.53it/s] 78%|  | 343/437 [00:07<00:02, 45.58it/s] 80%|  | 348/437 [00:07<00:01, 45.55it/s] 81%|  | 353/437 [00:07<00:01, 45.46it/s] 82%| | 358/437 [00:08<00:01, 45.34it/s] 83%| | 363/437 [00:08<00:01, 45.17it/s] 84%| | 368/437 [00:08<00:01, 45.15it/s] 85%| | 373/437 [00:08<00:01, 43.65it/s] 86%| | 378/437 [00:08<00:01, 44.31it/s] 88%| | 383/437 [00:08<00:01, 44.74it/s] 89%| | 388/437 [00:08<00:01, 45.14it/s] 90%| | 393/437 [00:08<00:00, 45.30it/s] 91%| | 398/437 [00:08<00:00, 45.29it/s] 92%|| 403/437 [00:09<00:00, 45.11it/s] 93%|| 408/437 [00:09<00:00, 45.04it/s] 95%|| 413/437 [00:09<00:00, 44.95it/s] 96%|| 418/437 [00:09<00:00, 45.05it/s] 97%|| 423/437 [00:09<00:00, 45.20it/s] 98%|| 428/437 [00:09<00:00, 45.27it/s] 99%|| 433/437 [00:09<00:00, 45.43it/s]100%|| 437/437 [00:09<00:00, 44.45it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:10:24,102 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   eval_loss               =     0.9602
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   eval_runtime            = 0:00:09.84
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   eval_samples_per_second =    354.245
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   eval_steps_per_second   =      44.37
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:10:24,102 >>   perplexity              =     2.6122
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:33,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:33,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:33,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:33,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:33,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:10:34,863 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:10:34,864 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:10:35,501 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:10:36,619 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:10:36,619 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:39,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:39,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:39,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:39,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:39,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:10:40,585 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:10:40,587 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:10:41,231 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:10:41,481 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:10:41,481 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:32,  1.55it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:35,  1.54it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:38,  1.54it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.59it/s]Extractor Predicting: 66it [00:41,  1.59it/s]Extractor Predicting: 67it [00:42,  1.61it/s]Extractor Predicting: 68it [00:43,  1.62it/s]Extractor Predicting: 69it [00:43,  1.65it/s]Extractor Predicting: 70it [00:44,  1.66it/s]Extractor Predicting: 71it [00:44,  1.66it/s]Extractor Predicting: 72it [00:45,  1.51it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:46,  1.59it/s]Extractor Predicting: 75it [00:47,  1.55it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:50,  1.63it/s]Extractor Predicting: 80it [00:50,  1.60it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:51,  1.63it/s]Extractor Predicting: 83it [00:52,  1.63it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:53,  1.63it/s]Extractor Predicting: 86it [00:54,  1.64it/s]Extractor Predicting: 87it [00:54,  1.65it/s]Extractor Predicting: 88it [00:55,  1.64it/s]Extractor Predicting: 89it [00:56,  1.61it/s]Extractor Predicting: 90it [00:56,  1.62it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:58,  1.57it/s]Extractor Predicting: 93it [00:58,  1.59it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [00:59,  1.58it/s]Extractor Predicting: 96it [01:00,  1.57it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:01,  1.61it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.56it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:06,  1.56it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:08,  1.56it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:10,  1.56it/s]Extractor Predicting: 113it [01:11,  1.56it/s]Extractor Predicting: 114it [01:12,  1.58it/s]Extractor Predicting: 115it [01:12,  1.57it/s]Extractor Predicting: 116it [01:13,  1.60it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.60it/s]Extractor Predicting: 119it [01:15,  1.64it/s]Extractor Predicting: 120it [01:15,  1.68it/s]Extractor Predicting: 121it [01:16,  1.65it/s]Extractor Predicting: 122it [01:16,  1.66it/s]Extractor Predicting: 123it [01:17,  1.63it/s]Extractor Predicting: 124it [01:18,  1.64it/s]Extractor Predicting: 125it [01:18,  1.65it/s]Extractor Predicting: 126it [01:19,  1.65it/s]Extractor Predicting: 127it [01:19,  1.64it/s]Extractor Predicting: 128it [01:20,  1.61it/s]Extractor Predicting: 129it [01:21,  1.64it/s]Extractor Predicting: 130it [01:21,  1.60it/s]Extractor Predicting: 131it [01:22,  1.62it/s]Extractor Predicting: 132it [01:23,  1.63it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:24,  1.60it/s]Extractor Predicting: 135it [01:24,  1.62it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.61it/s]Extractor Predicting: 138it [01:26,  1.59it/s]Extractor Predicting: 139it [01:27,  1.62it/s]Extractor Predicting: 140it [01:28,  1.61it/s]Extractor Predicting: 141it [01:28,  1.62it/s]Extractor Predicting: 142it [01:29,  1.62it/s]Extractor Predicting: 143it [01:29,  1.63it/s]Extractor Predicting: 144it [01:30,  1.63it/s]Extractor Predicting: 145it [01:30,  2.15it/s]Extractor Predicting: 145it [01:30,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:24,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:24,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:24,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:24,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:24,487 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:12:25,122 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:12:25,123 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:12:25,706 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:12:26,751 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:12:26,751 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:29,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:29,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:29,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:29,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:29,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:12:30,421 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:12:30,422 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:12:31,018 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:12:31,182 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:12:31,182 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4784172661870504,
  "recall": 0.1524792204069934,
  "score": 0.23125407520104324,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.61it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:27,  1.61it/s]Extractor Predicting: 44it [00:27,  1.59it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:36,  1.65it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.73it/s]Extractor Predicting: 61it [00:38,  1.74it/s]Extractor Predicting: 62it [00:38,  1.75it/s]Extractor Predicting: 63it [00:39,  1.76it/s]Extractor Predicting: 64it [00:39,  1.79it/s]Extractor Predicting: 65it [00:40,  1.80it/s]Extractor Predicting: 66it [00:40,  1.79it/s]Extractor Predicting: 67it [00:41,  1.81it/s]Extractor Predicting: 68it [00:41,  1.85it/s]Extractor Predicting: 69it [00:42,  1.67it/s]Extractor Predicting: 70it [00:43,  1.68it/s]Extractor Predicting: 71it [00:43,  1.72it/s]Extractor Predicting: 72it [00:44,  1.76it/s]Extractor Predicting: 73it [00:44,  1.79it/s]Extractor Predicting: 74it [00:45,  1.78it/s]Extractor Predicting: 75it [00:45,  1.78it/s]Extractor Predicting: 76it [00:46,  1.82it/s]Extractor Predicting: 77it [00:46,  1.85it/s]Extractor Predicting: 78it [00:47,  1.83it/s]Extractor Predicting: 79it [00:48,  1.84it/s]Extractor Predicting: 80it [00:48,  1.84it/s]Extractor Predicting: 81it [00:49,  1.82it/s]Extractor Predicting: 82it [00:49,  1.80it/s]Extractor Predicting: 83it [00:50,  1.79it/s]Extractor Predicting: 84it [00:50,  1.78it/s]Extractor Predicting: 85it [00:51,  1.80it/s]Extractor Predicting: 86it [00:52,  1.75it/s]Extractor Predicting: 87it [00:52,  1.70it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:53,  1.62it/s]Extractor Predicting: 90it [00:54,  1.58it/s]Extractor Predicting: 91it [00:55,  1.58it/s]Extractor Predicting: 92it [00:55,  1.56it/s]Extractor Predicting: 93it [00:56,  1.60it/s]Extractor Predicting: 94it [00:57,  1.58it/s]Extractor Predicting: 95it [00:57,  1.59it/s]Extractor Predicting: 96it [00:58,  1.56it/s]Extractor Predicting: 97it [00:59,  1.57it/s]Extractor Predicting: 98it [00:59,  1.56it/s]Extractor Predicting: 99it [01:00,  1.58it/s]Extractor Predicting: 100it [01:00,  1.57it/s]Extractor Predicting: 101it [01:01,  1.57it/s]Extractor Predicting: 102it [01:02,  1.56it/s]Extractor Predicting: 103it [01:02,  1.58it/s]Extractor Predicting: 104it [01:03,  1.51it/s]Extractor Predicting: 105it [01:04,  1.52it/s]Extractor Predicting: 106it [01:04,  1.50it/s]Extractor Predicting: 107it [01:05,  1.49it/s]Extractor Predicting: 108it [01:06,  1.50it/s]Extractor Predicting: 109it [01:06,  1.54it/s]Extractor Predicting: 110it [01:07,  1.54it/s]Extractor Predicting: 111it [01:08,  1.53it/s]Extractor Predicting: 112it [01:08,  1.47it/s]Extractor Predicting: 113it [01:09,  1.50it/s]Extractor Predicting: 114it [01:10,  1.55it/s]Extractor Predicting: 115it [01:10,  1.54it/s]Extractor Predicting: 116it [01:11,  1.57it/s]Extractor Predicting: 117it [01:12,  1.60it/s]Extractor Predicting: 118it [01:12,  1.56it/s]Extractor Predicting: 119it [01:13,  1.52it/s]Extractor Predicting: 120it [01:14,  1.50it/s]Extractor Predicting: 121it [01:14,  1.50it/s]Extractor Predicting: 122it [01:15,  1.53it/s]Extractor Predicting: 123it [01:16,  1.55it/s]Extractor Predicting: 124it [01:16,  1.55it/s]Extractor Predicting: 125it [01:17,  1.53it/s]Extractor Predicting: 126it [01:18,  1.51it/s]Extractor Predicting: 127it [01:18,  1.51it/s]Extractor Predicting: 128it [01:19,  1.48it/s]Extractor Predicting: 129it [01:20,  1.49it/s]Extractor Predicting: 130it [01:20,  1.49it/s]Extractor Predicting: 131it [01:21,  1.53it/s]Extractor Predicting: 132it [01:21,  1.53it/s]Extractor Predicting: 133it [01:22,  1.54it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:23,  1.55it/s]Extractor Predicting: 136it [01:24,  1.50it/s]Extractor Predicting: 137it [01:25,  1.52it/s]Extractor Predicting: 138it [01:25,  1.51it/s]Extractor Predicting: 139it [01:26,  1.51it/s]Extractor Predicting: 140it [01:27,  1.53it/s]Extractor Predicting: 141it [01:27,  1.53it/s]Extractor Predicting: 142it [01:28,  1.54it/s]Extractor Predicting: 143it [01:29,  1.48it/s]Extractor Predicting: 144it [01:29,  1.49it/s]Extractor Predicting: 145it [01:30,  1.50it/s]Extractor Predicting: 146it [01:31,  1.51it/s]Extractor Predicting: 147it [01:31,  1.55it/s]Extractor Predicting: 148it [01:32,  1.57it/s]Extractor Predicting: 149it [01:33,  1.54it/s]Extractor Predicting: 150it [01:33,  1.54it/s]Extractor Predicting: 151it [01:34,  1.56it/s]Extractor Predicting: 152it [01:35,  1.58it/s]Extractor Predicting: 153it [01:35,  1.60it/s]Extractor Predicting: 154it [01:36,  1.61it/s]Extractor Predicting: 155it [01:36,  1.63it/s]Extractor Predicting: 156it [01:37,  1.65it/s]Extractor Predicting: 157it [01:38,  1.64it/s]Extractor Predicting: 158it [01:38,  1.66it/s]Extractor Predicting: 159it [01:39,  1.70it/s]Extractor Predicting: 160it [01:39,  1.64it/s]Extractor Predicting: 161it [01:40,  1.62it/s]Extractor Predicting: 162it [01:41,  1.59it/s]Extractor Predicting: 163it [01:41,  1.60it/s]Extractor Predicting: 164it [01:42,  1.59it/s]Extractor Predicting: 165it [01:43,  1.59it/s]Extractor Predicting: 166it [01:43,  1.59it/s]Extractor Predicting: 167it [01:44,  1.58it/s]Extractor Predicting: 168it [01:44,  1.57it/s]Extractor Predicting: 169it [01:45,  1.39it/s]Extractor Predicting: 170it [01:46,  1.46it/s]Extractor Predicting: 171it [01:47,  1.49it/s]Extractor Predicting: 172it [01:47,  1.49it/s]Extractor Predicting: 173it [01:48,  1.53it/s]Extractor Predicting: 174it [01:49,  1.54it/s]Extractor Predicting: 175it [01:49,  1.52it/s]Extractor Predicting: 176it [01:50,  1.53it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:51,  1.58it/s]Extractor Predicting: 179it [01:52,  1.61it/s]Extractor Predicting: 180it [01:52,  1.63it/s]Extractor Predicting: 181it [01:53,  1.61it/s]Extractor Predicting: 182it [01:53,  1.66it/s]Extractor Predicting: 183it [01:54,  1.66it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:55,  1.66it/s]Extractor Predicting: 186it [01:56,  1.66it/s]Extractor Predicting: 187it [01:57,  1.62it/s]Extractor Predicting: 188it [01:57,  1.66it/s]Extractor Predicting: 189it [01:58,  1.68it/s]Extractor Predicting: 190it [01:58,  1.72it/s]Extractor Predicting: 191it [01:59,  1.64it/s]Extractor Predicting: 192it [01:59,  1.65it/s]Extractor Predicting: 193it [02:00,  1.67it/s]Extractor Predicting: 194it [02:01,  1.71it/s]Extractor Predicting: 195it [02:01,  1.67it/s]Extractor Predicting: 196it [02:02,  1.67it/s]Extractor Predicting: 197it [02:02,  1.69it/s]Extractor Predicting: 198it [02:03,  1.67it/s]Extractor Predicting: 199it [02:04,  1.63it/s]Extractor Predicting: 200it [02:04,  1.64it/s]Extractor Predicting: 201it [02:05,  1.61it/s]Extractor Predicting: 202it [02:05,  1.65it/s]Extractor Predicting: 203it [02:06,  1.69it/s]Extractor Predicting: 204it [02:07,  1.70it/s]Extractor Predicting: 205it [02:07,  1.70it/s]Extractor Predicting: 206it [02:08,  1.76it/s]Extractor Predicting: 207it [02:08,  1.73it/s]Extractor Predicting: 208it [02:09,  1.78it/s]Extractor Predicting: 209it [02:09,  1.72it/s]Extractor Predicting: 210it [02:10,  1.73it/s]Extractor Predicting: 211it [02:11,  1.72it/s]Extractor Predicting: 212it [02:11,  1.77it/s]Extractor Predicting: 213it [02:12,  1.79it/s]Extractor Predicting: 214it [02:12,  1.80it/s]Extractor Predicting: 215it [02:13,  1.84it/s]Extractor Predicting: 216it [02:13,  1.83it/s]Extractor Predicting: 217it [02:14,  1.78it/s]Extractor Predicting: 218it [02:15,  1.76it/s]Extractor Predicting: 219it [02:15,  1.76it/s]Extractor Predicting: 220it [02:16,  1.78it/s]Extractor Predicting: 221it [02:16,  1.79it/s]Extractor Predicting: 222it [02:17,  1.84it/s]Extractor Predicting: 223it [02:17,  1.79it/s]Extractor Predicting: 224it [02:18,  1.82it/s]Extractor Predicting: 225it [02:18,  1.80it/s]Extractor Predicting: 226it [02:19,  1.81it/s]Extractor Predicting: 227it [02:19,  1.83it/s]Extractor Predicting: 228it [02:20,  1.80it/s]Extractor Predicting: 229it [02:21,  1.72it/s]Extractor Predicting: 230it [02:21,  1.67it/s]Extractor Predicting: 231it [02:22,  1.60it/s]Extractor Predicting: 232it [02:23,  1.56it/s]Extractor Predicting: 233it [02:23,  1.57it/s]Extractor Predicting: 234it [02:24,  1.54it/s]Extractor Predicting: 235it [02:25,  1.55it/s]Extractor Predicting: 236it [02:25,  1.54it/s]Extractor Predicting: 237it [02:26,  1.51it/s]Extractor Predicting: 238it [02:27,  1.49it/s]Extractor Predicting: 239it [02:27,  1.50it/s]Extractor Predicting: 240it [02:28,  1.51it/s]Extractor Predicting: 241it [02:29,  1.53it/s]Extractor Predicting: 242it [02:29,  1.55it/s]Extractor Predicting: 243it [02:30,  1.50it/s]Extractor Predicting: 244it [02:31,  1.52it/s]Extractor Predicting: 245it [02:31,  1.52it/s]Extractor Predicting: 246it [02:32,  1.54it/s]Extractor Predicting: 247it [02:33,  1.49it/s]Extractor Predicting: 248it [02:33,  1.49it/s]Extractor Predicting: 249it [02:34,  1.45it/s]Extractor Predicting: 250it [02:35,  1.46it/s]Extractor Predicting: 251it [02:35,  1.47it/s]Extractor Predicting: 252it [02:36,  1.50it/s]Extractor Predicting: 253it [02:37,  1.51it/s]Extractor Predicting: 254it [02:37,  1.50it/s]Extractor Predicting: 255it [02:38,  1.50it/s]Extractor Predicting: 256it [02:39,  1.52it/s]Extractor Predicting: 257it [02:39,  1.58it/s]Extractor Predicting: 258it [02:40,  1.59it/s]Extractor Predicting: 259it [02:40,  1.60it/s]Extractor Predicting: 260it [02:41,  1.63it/s]Extractor Predicting: 261it [02:42,  1.62it/s]Extractor Predicting: 262it [02:42,  1.62it/s]Extractor Predicting: 263it [02:43,  1.64it/s]Extractor Predicting: 264it [02:43,  1.65it/s]Extractor Predicting: 265it [02:44,  1.61it/s]Extractor Predicting: 266it [02:45,  1.64it/s]Extractor Predicting: 267it [02:45,  1.67it/s]Extractor Predicting: 268it [02:46,  1.63it/s]Extractor Predicting: 269it [02:47,  1.64it/s]Extractor Predicting: 270it [02:47,  1.62it/s]Extractor Predicting: 271it [02:48,  1.61it/s]Extractor Predicting: 272it [02:48,  1.60it/s]Extractor Predicting: 273it [02:49,  1.60it/s]Extractor Predicting: 274it [02:50,  1.59it/s]Extractor Predicting: 275it [02:50,  1.62it/s]Extractor Predicting: 276it [02:51,  1.61it/s]Extractor Predicting: 277it [02:52,  1.59it/s]Extractor Predicting: 278it [02:52,  1.56it/s]Extractor Predicting: 279it [02:53,  1.60it/s]Extractor Predicting: 280it [02:53,  1.59it/s]Extractor Predicting: 281it [02:54,  1.62it/s]Extractor Predicting: 282it [02:55,  1.61it/s]Extractor Predicting: 283it [02:55,  1.61it/s]Extractor Predicting: 284it [02:56,  1.62it/s]Extractor Predicting: 285it [02:57,  1.59it/s]Extractor Predicting: 286it [02:57,  1.59it/s]Extractor Predicting: 287it [02:58,  1.62it/s]Extractor Predicting: 288it [02:59,  1.42it/s]Extractor Predicting: 289it [02:59,  1.46it/s]Extractor Predicting: 290it [03:00,  1.50it/s]Extractor Predicting: 291it [03:01,  1.52it/s]Extractor Predicting: 292it [03:01,  1.53it/s]Extractor Predicting: 293it [03:02,  1.54it/s]Extractor Predicting: 294it [03:02,  1.56it/s]Extractor Predicting: 295it [03:03,  1.57it/s]Extractor Predicting: 296it [03:04,  1.58it/s]Extractor Predicting: 297it [03:04,  1.59it/s]Extractor Predicting: 298it [03:05,  1.55it/s]Extractor Predicting: 299it [03:06,  1.61it/s]Extractor Predicting: 300it [03:06,  1.58it/s]Extractor Predicting: 301it [03:07,  1.59it/s]Extractor Predicting: 302it [03:07,  1.59it/s]Extractor Predicting: 303it [03:08,  1.64it/s]Extractor Predicting: 304it [03:09,  1.63it/s]Extractor Predicting: 305it [03:09,  1.67it/s]Extractor Predicting: 306it [03:10,  1.64it/s]Extractor Predicting: 307it [03:10,  1.62it/s]Extractor Predicting: 308it [03:11,  1.59it/s]Extractor Predicting: 309it [03:12,  1.62it/s]Extractor Predicting: 310it [03:12,  1.58it/s]Extractor Predicting: 311it [03:13,  1.53it/s]Extractor Predicting: 312it [03:14,  1.55it/s]Extractor Predicting: 313it [03:14,  1.58it/s]Extractor Predicting: 314it [03:15,  1.59it/s]Extractor Predicting: 315it [03:16,  1.62it/s]Extractor Predicting: 316it [03:16,  1.59it/s]Extractor Predicting: 317it [03:17,  1.59it/s]Extractor Predicting: 318it [03:17,  1.65it/s]Extractor Predicting: 319it [03:18,  1.65it/s]Extractor Predicting: 320it [03:19,  1.69it/s]Extractor Predicting: 321it [03:19,  1.65it/s]Extractor Predicting: 322it [03:20,  1.64it/s]Extractor Predicting: 323it [03:20,  1.64it/s]Extractor Predicting: 324it [03:21,  1.62it/s]Extractor Predicting: 325it [03:22,  1.60it/s]Extractor Predicting: 326it [03:22,  1.60it/s]Extractor Predicting: 327it [03:23,  1.58it/s]Extractor Predicting: 328it [03:24,  1.58it/s]Extractor Predicting: 329it [03:24,  1.54it/s]Extractor Predicting: 330it [03:25,  1.56it/s]Extractor Predicting: 331it [03:26,  1.57it/s]Extractor Predicting: 332it [03:26,  1.61it/s]Extractor Predicting: 333it [03:27,  1.61it/s]Extractor Predicting: 334it [03:27,  1.61it/s]Extractor Predicting: 335it [03:28,  1.61it/s]Extractor Predicting: 336it [03:29,  1.61it/s]Extractor Predicting: 337it [03:29,  1.60it/s]Extractor Predicting: 338it [03:30,  1.60it/s]Extractor Predicting: 339it [03:31,  1.59it/s]Extractor Predicting: 340it [03:31,  1.64it/s]Extractor Predicting: 341it [03:32,  1.60it/s]Extractor Predicting: 342it [03:32,  1.63it/s]Extractor Predicting: 343it [03:33,  1.58it/s]Extractor Predicting: 344it [03:34,  1.60it/s]Extractor Predicting: 345it [03:34,  1.57it/s]Extractor Predicting: 346it [03:35,  1.59it/s]Extractor Predicting: 347it [03:36,  1.60it/s]Extractor Predicting: 348it [03:36,  1.63it/s]Extractor Predicting: 349it [03:37,  1.56it/s]Extractor Predicting: 350it [03:37,  1.55it/s]Extractor Predicting: 351it [03:38,  1.54it/s]Extractor Predicting: 352it [03:39,  1.56it/s]Extractor Predicting: 353it [03:39,  1.55it/s]Extractor Predicting: 354it [03:40,  1.57it/s]Extractor Predicting: 355it [03:41,  1.56it/s]Extractor Predicting: 356it [03:41,  1.60it/s]Extractor Predicting: 357it [03:42,  1.57it/s]Extractor Predicting: 358it [03:43,  1.56it/s]Extractor Predicting: 359it [03:43,  1.55it/s]Extractor Predicting: 360it [03:44,  1.52it/s]Extractor Predicting: 361it [03:44,  1.57it/s]Extractor Predicting: 362it [03:45,  1.60it/s]Extractor Predicting: 363it [03:46,  1.59it/s]Extractor Predicting: 364it [03:46,  1.62it/s]Extractor Predicting: 365it [03:47,  1.61it/s]Extractor Predicting: 366it [03:48,  1.59it/s]Extractor Predicting: 367it [03:48,  1.59it/s]Extractor Predicting: 368it [03:49,  1.57it/s]Extractor Predicting: 369it [03:50,  1.58it/s]Extractor Predicting: 370it [03:50,  1.57it/s]Extractor Predicting: 371it [03:51,  1.59it/s]Extractor Predicting: 372it [03:51,  1.60it/s]Extractor Predicting: 373it [03:52,  1.61it/s]Extractor Predicting: 374it [03:53,  1.61it/s]Extractor Predicting: 375it [03:53,  1.60it/s]Extractor Predicting: 376it [03:54,  1.62it/s]Extractor Predicting: 377it [03:54,  1.62it/s]Extractor Predicting: 378it [03:55,  1.63it/s]Extractor Predicting: 379it [03:56,  1.65it/s]Extractor Predicting: 380it [03:56,  1.64it/s]Extractor Predicting: 381it [03:57,  1.64it/s]Extractor Predicting: 382it [03:58,  1.60it/s]Extractor Predicting: 383it [03:58,  1.63it/s]Extractor Predicting: 384it [03:59,  1.65it/s]Extractor Predicting: 385it [03:59,  1.65it/s]Extractor Predicting: 386it [04:00,  1.68it/s]Extractor Predicting: 387it [04:00,  1.69it/s]Extractor Predicting: 388it [04:01,  1.65it/s]Extractor Predicting: 389it [04:02,  1.66it/s]Extractor Predicting: 390it [04:02,  1.64it/s]Extractor Predicting: 391it [04:03,  1.64it/s]Extractor Predicting: 392it [04:04,  1.45it/s]Extractor Predicting: 393it [04:04,  1.50it/s]Extractor Predicting: 394it [04:05,  1.55it/s]Extractor Predicting: 395it [04:06,  1.53it/s]Extractor Predicting: 396it [04:06,  1.57it/s]Extractor Predicting: 397it [04:07,  1.60it/s]Extractor Predicting: 398it [04:08,  1.56it/s]Extractor Predicting: 399it [04:08,  1.58it/s]Extractor Predicting: 400it [04:09,  1.58it/s]Extractor Predicting: 401it [04:09,  1.61it/s]Extractor Predicting: 402it [04:10,  1.65it/s]Extractor Predicting: 403it [04:11,  1.63it/s]Extractor Predicting: 404it [04:11,  1.65it/s]Extractor Predicting: 405it [04:12,  1.65it/s]Extractor Predicting: 406it [04:12,  1.62it/s]Extractor Predicting: 407it [04:13,  1.61it/s]Extractor Predicting: 408it [04:14,  1.64it/s]Extractor Predicting: 409it [04:14,  1.67it/s]Extractor Predicting: 410it [04:15,  1.66it/s]Extractor Predicting: 411it [04:16,  1.62it/s]Extractor Predicting: 412it [04:16,  1.65it/s]Extractor Predicting: 413it [04:17,  1.65it/s]Extractor Predicting: 414it [04:17,  1.66it/s]Extractor Predicting: 415it [04:18,  1.65it/s]Extractor Predicting: 416it [04:19,  1.66it/s]Extractor Predicting: 417it [04:19,  1.63it/s]Extractor Predicting: 418it [04:20,  1.66it/s]Extractor Predicting: 419it [04:20,  1.64it/s]Extractor Predicting: 420it [04:21,  1.64it/s]Extractor Predicting: 421it [04:22,  1.63it/s]Extractor Predicting: 422it [04:22,  1.62it/s]Extractor Predicting: 423it [04:23,  1.61it/s]Extractor Predicting: 424it [04:23,  1.62it/s]Extractor Predicting: 425it [04:24,  1.84it/s]Extractor Predicting: 425it [04:24,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:06,944 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:17:07,897 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:17:07,898 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:17:08,540 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:17:09,670 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:17:09,670 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:12,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:12,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:12,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:12,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:12,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:17:13,457 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:17:13,459 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:17:14,055 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:17:14,265 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:17:14,265 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.29310872894333845,
  "recall": 0.0939524838012959,
  "score": 0.1422942532153743,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.76it/s]Extractor Predicting: 7it [00:04,  1.59it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:17:19,867 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:17:19,868 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:17:19,893 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:17:19,893 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:17:19,910 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:17:29,218 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:17:29,248 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:17:29,387 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:17:29,388 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:17:29,460 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,485 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,486 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:17:29,486 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.028662420382165606,
  "score": 0.05421686746987952,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:17:29,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:30,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:30,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:31,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:31,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:32,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:33,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:33,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:34,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:34,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:35,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:36,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:36,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:37,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:37,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:38,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:38,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:39,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:40,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|         | 1/20 [00:11<03:40, 11.62s/it][WARNING|generation_utils.py:914] 2023-08-29 07:17:41,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:41,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:42,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:43,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:43,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:44,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:45,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:45,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:46,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:46,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:47,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:47,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:48,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:49,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:49,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:50,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:51,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:51,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:52,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:52,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:53,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:53,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|         | 2/20 [00:24<03:46, 12.58s/it][WARNING|generation_utils.py:914] 2023-08-29 07:17:54,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:55,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:55,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:56,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:56,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:57,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:58,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:58,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:59,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:17:59,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:00,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:00,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:01,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:02,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:03,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:03,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:04,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:04,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:05,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:06,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|        | 3/20 [00:36<03:29, 12.32s/it][WARNING|generation_utils.py:914] 2023-08-29 07:18:06,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:07,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:07,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:08,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:08,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:09,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:09,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:10,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:10,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:11,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:11,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:12,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:13,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:13,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:14,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:14,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:15,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:16,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:17,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:17,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|        | 4/20 [00:48<03:11, 11.99s/it][WARNING|generation_utils.py:914] 2023-08-29 07:18:18,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:18,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:19,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:19,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:20,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:20,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:21,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:22,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:22,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:23,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:23,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:24,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:24,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:25,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:25,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:26,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:26,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:27,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:28,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:28,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:29,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|       | 5/20 [00:59<02:57, 11.85s/it][WARNING|generation_utils.py:914] 2023-08-29 07:18:29,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:30,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:30,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:31,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:31,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:32,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:32,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:33,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:34,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:34,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:35,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:35,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:36,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:36,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:37,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:37,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:38,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:39,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:39,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:40,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|       | 6/20 [01:11<02:44, 11.72s/it][WARNING|generation_utils.py:914] 2023-08-29 07:18:41,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:41,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:42,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:42,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:43,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:43,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:44,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:44,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:45,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:46,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:46,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:47,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:47,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:48,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:48,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:49,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:49,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:50,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:51,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:51,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:52,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|      | 7/20 [01:22<02:31, 11.63s/it][WARNING|generation_utils.py:914] 2023-08-29 07:18:52,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:53,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:53,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:54,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:54,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:55,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:55,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:56,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:57,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:57,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:58,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:58,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:59,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:18:59,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:00,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:00,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:01,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:02,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:02,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:03,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:03,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:04,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|      | 8/20 [01:35<02:21, 11.83s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:04,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:05,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:06,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:06,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:07,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:08,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:09,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:09,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:10,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:11,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:11,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:12,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:13,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:13,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:14,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:14,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:15,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:16,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:16,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:17,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|     | 9/20 [01:48<02:14, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:17,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:18,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:19,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:19,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:20,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:21,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:21,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:22,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:22,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:23,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:24,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:24,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:25,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:25,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:26,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:27,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:27,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:28,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:29,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:30,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|     | 10/20 [02:01<02:04, 12.41s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:30,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:31,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:32,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:33,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:33,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:34,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:34,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:35,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:35,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:36,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:36,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:37,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:38,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:38,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:39,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:39,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:40,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:40,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:41,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:42,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|    | 11/20 [02:12<01:50, 12.25s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:42,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:43,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:43,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:44,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:44,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:46,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:46,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:47,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:47,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:48,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:48,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:49,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:50,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:50,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:51,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:51,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:52,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:52,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:53,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:53,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|    | 12/20 [02:24<01:36, 12.10s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:54,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:54,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:55,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:56,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:56,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:57,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:57,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:58,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:58,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:59,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:59,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:00,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:00,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:01,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:01,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:02,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:03,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:03,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:04,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:04,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|   | 13/20 [02:35<01:21, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:05,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:05,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:06,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:06,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:07,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:08,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:08,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:09,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:09,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:10,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:10,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:11,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:12,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:12,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:13,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:14,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:14,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:15,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:16,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:16,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|   | 14/20 [02:47<01:10, 11.82s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:17,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:17,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:18,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:18,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:19,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:20,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:20,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:21,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:21,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:22,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:23,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:23,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:24,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:24,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:25,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:25,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:26,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:26,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:27,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:28,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:28,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|  | 15/20 [02:59<00:59, 11.86s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:29,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:29,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:30,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:31,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:32,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:33,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:34,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:34,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:35,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:35,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:36,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:36,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:37,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:38,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:38,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:39,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:39,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:40,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:41,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:41,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|  | 16/20 [03:12<00:48, 12.24s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:42,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:42,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:43,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:43,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:44,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:45,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:45,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:46,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:46,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:47,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:47,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:48,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:48,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:49,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:49,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:50,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:50,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:51,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:51,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:52,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%| | 17/20 [03:23<00:35, 11.70s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:52,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:53,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:54,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:54,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:55,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:55,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:56,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:57,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:57,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:58,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:59,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:59,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:00,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:00,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:01,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:01,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:02,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:02,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:03,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:04,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%| | 18/20 [03:34<00:23, 11.74s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:04,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:05,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:05,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:06,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:06,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:07,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:07,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:08,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:09,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:09,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:10,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:10,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:11,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:11,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:12,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:12,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:13,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:13,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:14,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:14,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:15,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|| 19/20 [03:46<00:11, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:16,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:16,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:17,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:18,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:18,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:19,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:19,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:20,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:20,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:21,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:21,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:22,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:22,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:23,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:23,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:24,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:25,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:25,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:26,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:26,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|| 20/20 [03:57<00:00, 11.58s/it]Generating: 100%|| 20/20 [03:57<00:00, 11.89s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:37,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:37,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:37,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:37,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:37,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:21:38,155 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:21:38,156 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:21:38,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:21:39,932 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:21:39,932 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:42,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:42,997 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:42,997 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:42,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:21:42,998 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:21:43,787 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:21:43,788 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:21:44,413 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:21:44,626 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:21:44,626 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9578125, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : location . Context : Later in the year , the baroque opera " The Three Musketeers at the Circus " had been performed here at a concert in the neighborhood , and a number of older patrons had been invited . Head Entity : The Three Musketeers at the Circus , Tail Entity : Tokyo .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : member of political party . Context : Following his leadership in the 2010 elections , the party 's leader , Manuel Valls , was appointed Prime Minister . Head Entity : Manuel Valls , Tail Entity : Democratic Party .\n"]
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in the year , the leader of the party , the Social Democratic Party of Germany ( SPD ) , said that he was " now in the dark about the party \'s future " . Head Entity : Social Democrat Party of Germany , Tail Entity : Social Democratic Party of Germany .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9734375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8958333333333334, 'errors': {''}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an additional two years to run until his death . Head Entity : William , Tail Entity : Czech Republic .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : father .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : heritage designation .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9345238095238095, 'errors': {''}}
['Relation : licensed to broadcast to . Context : Later in the year , the network aired Fox \'s " All in the Family at 8 p.m. ET on Fox Television One Network . Head Entity : All in the Family at 8 p.m., Tail Entity : Fox Television One Network .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : located in the administrative territorial entity .', 'success_rate': 0.9421875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9151785714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', "('Austrian National Assembly', 'occupation', '', 'He was appointed as an officer later at the end of 20th century , although he was not elected to the Austrian National Assembly until 1918 .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 9937
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10037, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.64it/s]Extractor Estimating: 3it [00:01,  1.67it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:02,  1.68it/s]Extractor Estimating: 6it [00:03,  1.67it/s]Extractor Estimating: 7it [00:04,  1.70it/s]Extractor Estimating: 8it [00:04,  1.58it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.51it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:09,  1.60it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:11,  1.65it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.58it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.70it/s]Extractor Estimating: 24it [00:14,  1.51it/s]Extractor Estimating: 25it [00:15,  1.53it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.65it/s]Extractor Estimating: 29it [00:17,  1.60it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.66it/s]Extractor Estimating: 32it [00:19,  1.62it/s]Extractor Estimating: 33it [00:20,  1.65it/s]Extractor Estimating: 34it [00:20,  1.70it/s]Extractor Estimating: 35it [00:21,  1.71it/s]Extractor Estimating: 36it [00:22,  1.74it/s]Extractor Estimating: 37it [00:22,  1.70it/s]Extractor Estimating: 38it [00:23,  1.67it/s]Extractor Estimating: 39it [00:23,  1.68it/s]Extractor Estimating: 40it [00:24,  1.66it/s]Extractor Estimating: 41it [00:25,  1.72it/s]Extractor Estimating: 42it [00:25,  1.78it/s]Extractor Estimating: 43it [00:26,  1.61it/s]Extractor Estimating: 44it [00:26,  1.61it/s]Extractor Estimating: 45it [00:27,  1.60it/s]Extractor Estimating: 46it [00:28,  1.63it/s]Extractor Estimating: 47it [00:28,  1.67it/s]Extractor Estimating: 48it [00:29,  1.67it/s]Extractor Estimating: 49it [00:29,  1.63it/s]Extractor Estimating: 50it [00:30,  1.61it/s]Extractor Estimating: 51it [00:31,  1.66it/s]Extractor Estimating: 52it [00:31,  1.61it/s]Extractor Estimating: 53it [00:32,  1.62it/s]Extractor Estimating: 54it [00:33,  1.60it/s]Extractor Estimating: 55it [00:33,  1.64it/s]Extractor Estimating: 56it [00:34,  1.57it/s]Extractor Estimating: 57it [00:34,  1.65it/s]Extractor Estimating: 58it [00:35,  1.65it/s]Extractor Estimating: 59it [00:36,  1.68it/s]Extractor Estimating: 60it [00:36,  1.67it/s]Extractor Estimating: 61it [00:37,  1.70it/s]Extractor Estimating: 62it [00:37,  1.67it/s]Extractor Estimating: 63it [00:38,  1.70it/s]Extractor Estimating: 64it [00:38,  1.71it/s]Extractor Estimating: 65it [00:39,  1.62it/s]Extractor Estimating: 66it [00:40,  1.63it/s]Extractor Estimating: 67it [00:40,  1.64it/s]Extractor Estimating: 68it [00:41,  1.67it/s]Extractor Estimating: 69it [00:42,  1.64it/s]Extractor Estimating: 70it [00:42,  1.61it/s]Extractor Estimating: 71it [00:43,  1.69it/s]Extractor Estimating: 72it [00:43,  1.72it/s]Extractor Estimating: 73it [00:44,  1.71it/s]Extractor Estimating: 74it [00:45,  1.66it/s]Extractor Estimating: 75it [00:45,  1.70it/s]Extractor Estimating: 76it [00:46,  1.72it/s]Extractor Estimating: 77it [00:46,  1.68it/s]Extractor Estimating: 78it [00:47,  1.66it/s]Extractor Estimating: 79it [00:47,  1.67it/s]Extractor Estimating: 80it [00:48,  1.71it/s]Extractor Estimating: 81it [00:49,  1.72it/s]Extractor Estimating: 82it [00:49,  1.71it/s]Extractor Estimating: 83it [00:50,  1.77it/s]Extractor Estimating: 84it [00:50,  1.73it/s]Extractor Estimating: 85it [00:51,  1.71it/s]Extractor Estimating: 86it [00:52,  1.66it/s]Extractor Estimating: 87it [00:52,  1.63it/s]Extractor Estimating: 88it [00:53,  1.58it/s]Extractor Estimating: 89it [00:54,  1.49it/s]Extractor Estimating: 90it [00:54,  1.52it/s]Extractor Estimating: 91it [00:55,  1.51it/s]Extractor Estimating: 92it [00:56,  1.57it/s]Extractor Estimating: 93it [00:56,  1.60it/s]Extractor Estimating: 94it [00:57,  1.65it/s]Extractor Estimating: 95it [00:57,  1.66it/s]Extractor Estimating: 96it [00:58,  1.66it/s]Extractor Estimating: 97it [00:58,  1.70it/s]Extractor Estimating: 98it [00:59,  1.68it/s]Extractor Estimating: 99it [01:00,  1.73it/s]Extractor Estimating: 100it [01:00,  1.71it/s]Extractor Estimating: 101it [01:01,  1.78it/s]Extractor Estimating: 102it [01:01,  1.75it/s]Extractor Estimating: 103it [01:02,  1.76it/s]Extractor Estimating: 104it [01:02,  1.81it/s]Extractor Estimating: 105it [01:03,  1.76it/s]Extractor Estimating: 106it [01:04,  1.75it/s]Extractor Estimating: 107it [01:04,  1.76it/s]Extractor Estimating: 108it [01:05,  1.69it/s]Extractor Estimating: 109it [01:05,  1.75it/s]Extractor Estimating: 110it [01:06,  1.76it/s]Extractor Estimating: 111it [01:06,  1.82it/s]Extractor Estimating: 112it [01:07,  1.81it/s]Extractor Estimating: 113it [01:07,  1.80it/s]Extractor Estimating: 114it [01:08,  1.81it/s]Extractor Estimating: 115it [01:09,  1.77it/s]Extractor Estimating: 116it [01:09,  1.81it/s]Extractor Estimating: 117it [01:10,  1.82it/s]Extractor Estimating: 118it [01:10,  1.79it/s]Extractor Estimating: 119it [01:11,  1.70it/s]Extractor Estimating: 120it [01:11,  1.72it/s]Extractor Estimating: 121it [01:12,  1.71it/s]Extractor Estimating: 122it [01:13,  1.75it/s]Extractor Estimating: 123it [01:13,  1.77it/s]Extractor Estimating: 124it [01:14,  1.78it/s]Extractor Estimating: 125it [01:14,  1.83it/s]Extractor Estimating: 126it [01:15,  1.78it/s]Extractor Estimating: 127it [01:15,  1.70it/s]Extractor Estimating: 128it [01:16,  1.70it/s]Extractor Estimating: 129it [01:17,  1.69it/s]Extractor Estimating: 130it [01:17,  1.69it/s]Extractor Estimating: 131it [01:18,  1.64it/s]Extractor Estimating: 132it [01:18,  1.69it/s]Extractor Estimating: 133it [01:19,  1.74it/s]Extractor Estimating: 134it [01:20,  1.67it/s]Extractor Estimating: 135it [01:20,  1.68it/s]Extractor Estimating: 136it [01:21,  1.68it/s]Extractor Estimating: 137it [01:21,  1.70it/s]Extractor Estimating: 138it [01:22,  1.69it/s]Extractor Estimating: 139it [01:23,  1.59it/s]Extractor Estimating: 140it [01:23,  1.65it/s]Extractor Estimating: 141it [01:24,  1.71it/s]Extractor Estimating: 142it [01:25,  1.54it/s]Extractor Estimating: 143it [01:25,  1.50it/s]Extractor Estimating: 144it [01:26,  1.53it/s]Extractor Estimating: 145it [01:26,  1.61it/s]Extractor Estimating: 146it [01:27,  1.60it/s]Extractor Estimating: 147it [01:28,  1.54it/s]Extractor Estimating: 148it [01:28,  1.54it/s]Extractor Estimating: 149it [01:29,  1.59it/s]Extractor Estimating: 150it [01:30,  1.57it/s]Extractor Estimating: 151it [01:30,  1.65it/s]Extractor Estimating: 152it [01:31,  1.69it/s]Extractor Estimating: 153it [01:31,  1.70it/s]Extractor Estimating: 154it [01:32,  1.69it/s]Extractor Estimating: 155it [01:33,  1.71it/s]Extractor Estimating: 156it [01:33,  1.76it/s]Extractor Estimating: 157it [01:34,  1.77it/s]Extractor Estimating: 158it [01:34,  1.77it/s]Extractor Estimating: 159it [01:35,  1.72it/s]Extractor Estimating: 160it [01:35,  1.67it/s]Extractor Estimating: 161it [01:36,  1.76it/s]Extractor Estimating: 162it [01:36,  1.80it/s]Extractor Estimating: 163it [01:37,  1.78it/s]Extractor Estimating: 164it [01:38,  1.80it/s]Extractor Estimating: 165it [01:38,  1.78it/s]Extractor Estimating: 166it [01:39,  1.76it/s]Extractor Estimating: 167it [01:39,  1.76it/s]Extractor Estimating: 168it [01:40,  1.79it/s]Extractor Estimating: 169it [01:40,  1.82it/s]Extractor Estimating: 170it [01:41,  1.78it/s]Extractor Estimating: 171it [01:42,  1.78it/s]Extractor Estimating: 172it [01:42,  1.81it/s]Extractor Estimating: 173it [01:43,  1.79it/s]Extractor Estimating: 174it [01:43,  1.82it/s]Extractor Estimating: 175it [01:44,  1.83it/s]Extractor Estimating: 176it [01:44,  1.79it/s]Extractor Estimating: 177it [01:45,  1.77it/s]Extractor Estimating: 178it [01:45,  1.79it/s]Extractor Estimating: 179it [01:46,  1.82it/s]Extractor Estimating: 180it [01:47,  1.78it/s]Extractor Estimating: 181it [01:47,  1.83it/s]Extractor Estimating: 182it [01:48,  1.87it/s]Extractor Estimating: 183it [01:48,  1.78it/s]Extractor Estimating: 184it [01:49,  1.78it/s]Extractor Estimating: 185it [01:49,  1.83it/s]Extractor Estimating: 186it [01:50,  1.82it/s]Extractor Estimating: 187it [01:50,  1.80it/s]Extractor Estimating: 188it [01:51,  1.77it/s]Extractor Estimating: 189it [01:52,  1.78it/s]Extractor Estimating: 190it [01:52,  1.78it/s]Extractor Estimating: 191it [01:53,  1.85it/s]Extractor Estimating: 192it [01:53,  1.87it/s]Extractor Estimating: 193it [01:54,  1.82it/s]Extractor Estimating: 194it [01:54,  1.81it/s]Extractor Estimating: 195it [01:55,  1.85it/s]Extractor Estimating: 196it [01:55,  1.89it/s]Extractor Estimating: 197it [01:56,  1.91it/s]Extractor Estimating: 198it [01:56,  1.90it/s]Extractor Estimating: 199it [01:57,  1.88it/s]Extractor Estimating: 200it [01:57,  1.87it/s]Extractor Estimating: 201it [01:58,  1.83it/s]Extractor Estimating: 202it [01:59,  1.80it/s]Extractor Estimating: 203it [01:59,  1.72it/s]Extractor Estimating: 204it [02:00,  1.71it/s]Extractor Estimating: 205it [02:00,  1.76it/s]Extractor Estimating: 206it [02:01,  1.81it/s]Extractor Estimating: 207it [02:01,  1.77it/s]Extractor Estimating: 208it [02:02,  1.70it/s]Extractor Estimating: 209it [02:03,  1.72it/s]Extractor Estimating: 210it [02:03,  1.76it/s]Extractor Estimating: 211it [02:04,  1.77it/s]Extractor Estimating: 212it [02:04,  1.74it/s]Extractor Estimating: 213it [02:05,  1.72it/s]Extractor Estimating: 214it [02:05,  1.75it/s]Extractor Estimating: 215it [02:06,  1.75it/s]Extractor Estimating: 216it [02:07,  1.69it/s]Extractor Estimating: 217it [02:07,  1.77it/s]Extractor Estimating: 218it [02:08,  1.71it/s]Extractor Estimating: 219it [02:08,  1.62it/s]Extractor Estimating: 220it [02:09,  1.58it/s]Extractor Estimating: 221it [02:10,  1.66it/s]Extractor Estimating: 222it [02:10,  1.65it/s]Extractor Estimating: 223it [02:11,  1.67it/s]Extractor Estimating: 224it [02:11,  1.69it/s]Extractor Estimating: 225it [02:12,  1.67it/s]Extractor Estimating: 226it [02:13,  1.54it/s]Extractor Estimating: 227it [02:13,  1.59it/s]Extractor Estimating: 228it [02:14,  1.60it/s]Extractor Estimating: 229it [02:15,  1.63it/s]Extractor Estimating: 230it [02:15,  1.59it/s]Extractor Estimating: 231it [02:16,  1.64it/s]Extractor Estimating: 232it [02:16,  1.67it/s]Extractor Estimating: 233it [02:17,  1.68it/s]Extractor Estimating: 234it [02:18,  1.67it/s]Extractor Estimating: 235it [02:18,  1.73it/s]Extractor Estimating: 236it [02:19,  1.67it/s]Extractor Estimating: 237it [02:19,  1.63it/s]Extractor Estimating: 238it [02:20,  1.64it/s]Extractor Estimating: 239it [02:21,  1.62it/s]Extractor Estimating: 240it [02:21,  1.64it/s]Extractor Estimating: 241it [02:22,  1.73it/s]Extractor Estimating: 242it [02:22,  1.68it/s]Extractor Estimating: 243it [02:23,  1.64it/s]Extractor Estimating: 244it [02:24,  1.65it/s]Extractor Estimating: 245it [02:24,  1.65it/s]Extractor Estimating: 246it [02:25,  1.63it/s]Extractor Estimating: 247it [02:26,  1.63it/s]Extractor Estimating: 248it [02:26,  1.63it/s]Extractor Estimating: 249it [02:27,  1.66it/s]Extractor Estimating: 250it [02:27,  1.68it/s]Extractor Estimating: 251it [02:28,  1.69it/s]Extractor Estimating: 252it [02:28,  1.66it/s]Extractor Estimating: 253it [02:29,  1.73it/s]Extractor Estimating: 254it [02:30,  1.74it/s]Extractor Estimating: 255it [02:30,  1.70it/s]Extractor Estimating: 256it [02:31,  1.65it/s]Extractor Estimating: 257it [02:31,  1.66it/s]Extractor Estimating: 258it [02:32,  1.71it/s]Extractor Estimating: 259it [02:33,  1.70it/s]Extractor Estimating: 260it [02:33,  1.73it/s]Extractor Estimating: 261it [02:34,  1.75it/s]Extractor Estimating: 262it [02:34,  1.71it/s]Extractor Estimating: 263it [02:35,  1.75it/s]Extractor Estimating: 264it [02:35,  1.79it/s]Extractor Estimating: 265it [02:36,  1.78it/s]Extractor Estimating: 266it [02:37,  1.79it/s]Extractor Estimating: 267it [02:37,  1.77it/s]Extractor Estimating: 268it [02:38,  1.73it/s]Extractor Estimating: 269it [02:38,  1.76it/s]Extractor Estimating: 270it [02:39,  1.75it/s]Extractor Estimating: 271it [02:39,  1.74it/s]Extractor Estimating: 272it [02:40,  1.75it/s]Extractor Estimating: 273it [02:41,  1.72it/s]Extractor Estimating: 274it [02:41,  1.74it/s]Extractor Estimating: 275it [02:42,  1.72it/s]Extractor Estimating: 276it [02:42,  1.74it/s]Extractor Estimating: 277it [02:43,  1.73it/s]Extractor Estimating: 278it [02:43,  1.72it/s]Extractor Estimating: 279it [02:44,  1.67it/s]Extractor Estimating: 280it [02:45,  1.61it/s]Extractor Estimating: 281it [02:45,  1.58it/s]Extractor Estimating: 282it [02:46,  1.61it/s]Extractor Estimating: 283it [02:47,  1.65it/s]Extractor Estimating: 284it [02:47,  1.64it/s]Extractor Estimating: 285it [02:48,  1.64it/s]Extractor Estimating: 286it [02:48,  1.62it/s]Extractor Estimating: 287it [02:49,  1.63it/s]Extractor Estimating: 288it [02:50,  1.58it/s]Extractor Estimating: 289it [02:50,  1.62it/s]Extractor Estimating: 290it [02:51,  1.63it/s]Extractor Estimating: 291it [02:52,  1.62it/s]Extractor Estimating: 292it [02:52,  1.66it/s]Extractor Estimating: 293it [02:53,  1.70it/s]Extractor Estimating: 294it [02:53,  1.66it/s]Extractor Estimating: 295it [02:54,  1.69it/s]Extractor Estimating: 296it [02:55,  1.63it/s]Extractor Estimating: 297it [02:55,  1.67it/s]Extractor Estimating: 298it [02:56,  1.66it/s]Extractor Estimating: 299it [02:56,  1.68it/s]Extractor Estimating: 300it [02:57,  1.63it/s]Extractor Estimating: 301it [02:58,  1.65it/s]Extractor Estimating: 302it [02:58,  1.67it/s]Extractor Estimating: 303it [02:59,  1.65it/s]Extractor Estimating: 304it [02:59,  1.70it/s]Extractor Estimating: 305it [03:00,  1.70it/s]Extractor Estimating: 306it [03:00,  1.67it/s]Extractor Estimating: 307it [03:01,  1.66it/s]Extractor Estimating: 308it [03:02,  1.71it/s]Extractor Estimating: 309it [03:02,  1.77it/s]Extractor Estimating: 310it [03:03,  1.82it/s]Extractor Estimating: 311it [03:03,  1.76it/s]Extractor Estimating: 312it [03:04,  1.77it/s]Extractor Estimating: 313it [03:05,  1.52it/s]Extractor Estimating: 314it [03:05,  1.57it/s]Extractor Estimating: 315it [03:06,  1.61it/s]Extractor Estimating: 316it [03:06,  1.66it/s]Extractor Estimating: 317it [03:07,  1.70it/s]Extractor Estimating: 318it [03:08,  1.72it/s]Extractor Estimating: 319it [03:08,  1.66it/s]Extractor Estimating: 320it [03:09,  1.66it/s]Extractor Estimating: 321it [03:09,  1.73it/s]Extractor Estimating: 322it [03:10,  1.75it/s]Extractor Estimating: 323it [03:10,  1.83it/s]Extractor Estimating: 324it [03:11,  1.79it/s]Extractor Estimating: 325it [03:12,  1.77it/s]Extractor Estimating: 326it [03:12,  1.79it/s]Extractor Estimating: 327it [03:13,  1.82it/s]Extractor Estimating: 328it [03:13,  1.70it/s]Extractor Estimating: 329it [03:14,  1.73it/s]Extractor Estimating: 330it [03:14,  1.74it/s]Extractor Estimating: 331it [03:15,  1.77it/s]Extractor Estimating: 332it [03:16,  1.72it/s]Extractor Estimating: 333it [03:16,  1.71it/s]Extractor Estimating: 334it [03:17,  1.76it/s]Extractor Estimating: 335it [03:17,  1.77it/s]Extractor Estimating: 336it [03:18,  1.82it/s]Extractor Estimating: 337it [03:18,  1.83it/s]Extractor Estimating: 338it [03:19,  1.81it/s]Extractor Estimating: 339it [03:20,  1.72it/s]Extractor Estimating: 340it [03:20,  1.77it/s]Extractor Estimating: 341it [03:21,  1.75it/s]Extractor Estimating: 342it [03:21,  1.71it/s]Extractor Estimating: 343it [03:22,  1.69it/s]Extractor Estimating: 344it [03:22,  1.70it/s]Extractor Estimating: 345it [03:23,  1.69it/s]Extractor Estimating: 346it [03:24,  1.76it/s]Extractor Estimating: 347it [03:24,  1.73it/s]Extractor Estimating: 348it [03:25,  1.76it/s]Extractor Estimating: 349it [03:25,  1.73it/s]Extractor Estimating: 350it [03:26,  1.72it/s]Extractor Estimating: 351it [03:27,  1.69it/s]Extractor Estimating: 352it [03:27,  1.74it/s]Extractor Estimating: 353it [03:28,  1.74it/s]Extractor Estimating: 354it [03:28,  1.76it/s]Extractor Estimating: 355it [03:29,  1.79it/s]Extractor Estimating: 356it [03:29,  1.77it/s]Extractor Estimating: 357it [03:30,  1.81it/s]Extractor Estimating: 358it [03:30,  1.85it/s]Extractor Estimating: 359it [03:31,  1.87it/s]Extractor Estimating: 360it [03:31,  1.89it/s]Extractor Estimating: 361it [03:32,  1.84it/s]Extractor Estimating: 362it [03:33,  1.73it/s]Extractor Estimating: 363it [03:33,  1.75it/s]Extractor Estimating: 364it [03:34,  1.80it/s]Extractor Estimating: 365it [03:34,  1.81it/s]Extractor Estimating: 366it [03:35,  1.80it/s]Extractor Estimating: 367it [03:35,  1.73it/s]Extractor Estimating: 368it [03:36,  1.73it/s]Extractor Estimating: 369it [03:37,  1.77it/s]Extractor Estimating: 370it [03:37,  1.76it/s]Extractor Estimating: 371it [03:38,  1.78it/s]Extractor Estimating: 372it [03:38,  1.79it/s]Extractor Estimating: 373it [03:39,  1.83it/s]Extractor Estimating: 374it [03:39,  1.75it/s]Extractor Estimating: 375it [03:40,  1.79it/s]Extractor Estimating: 376it [03:40,  1.74it/s]Extractor Estimating: 377it [03:41,  1.67it/s]Extractor Estimating: 378it [03:42,  1.68it/s]Extractor Estimating: 379it [03:42,  1.65it/s]Extractor Estimating: 380it [03:43,  1.66it/s]Extractor Estimating: 381it [03:44,  1.68it/s]Extractor Estimating: 382it [03:44,  1.73it/s]Extractor Estimating: 383it [03:45,  1.76it/s]Extractor Estimating: 384it [03:45,  1.71it/s]Extractor Estimating: 385it [03:46,  1.71it/s]Extractor Estimating: 386it [03:46,  1.73it/s]Extractor Estimating: 387it [03:47,  1.76it/s]Extractor Estimating: 388it [03:48,  1.75it/s]Extractor Estimating: 389it [03:48,  1.72it/s]Extractor Estimating: 390it [03:49,  1.71it/s]Extractor Estimating: 391it [03:49,  1.68it/s]Extractor Estimating: 392it [03:50,  1.75it/s]Extractor Estimating: 393it [03:50,  1.77it/s]Extractor Estimating: 394it [03:51,  1.71it/s]Extractor Estimating: 395it [03:52,  1.54it/s]Extractor Estimating: 396it [03:53,  1.50it/s]Extractor Estimating: 397it [03:53,  1.58it/s]Extractor Estimating: 398it [03:54,  1.58it/s]Extractor Estimating: 399it [03:54,  1.64it/s]Extractor Estimating: 400it [03:55,  1.68it/s]Extractor Estimating: 401it [03:55,  1.70it/s]Extractor Estimating: 402it [03:56,  1.72it/s]Extractor Estimating: 403it [03:57,  1.75it/s]Extractor Estimating: 404it [03:57,  1.74it/s]Extractor Estimating: 405it [03:58,  1.78it/s]Extractor Estimating: 406it [03:58,  1.81it/s]Extractor Estimating: 407it [03:59,  1.77it/s]Extractor Estimating: 408it [03:59,  1.80it/s]Extractor Estimating: 409it [04:00,  1.66it/s]Extractor Estimating: 410it [04:01,  1.72it/s]Extractor Estimating: 411it [04:01,  1.74it/s]Extractor Estimating: 412it [04:02,  1.70it/s]Extractor Estimating: 413it [04:02,  1.70it/s]Extractor Estimating: 414it [04:03,  1.74it/s]Extractor Estimating: 415it [04:03,  1.81it/s]Extractor Estimating: 416it [04:04,  1.69it/s]Extractor Estimating: 417it [04:05,  1.73it/s]Extractor Estimating: 418it [04:05,  1.75it/s]Extractor Estimating: 419it [04:06,  1.74it/s]Extractor Estimating: 420it [04:06,  1.80it/s]Extractor Estimating: 421it [04:07,  1.76it/s]Extractor Estimating: 422it [04:07,  1.73it/s]Extractor Estimating: 423it [04:08,  1.78it/s]Extractor Estimating: 424it [04:09,  1.79it/s]Extractor Estimating: 425it [04:09,  1.80it/s]Extractor Estimating: 426it [04:10,  1.76it/s]Extractor Estimating: 427it [04:10,  1.71it/s]Extractor Estimating: 428it [04:11,  1.69it/s]Extractor Estimating: 429it [04:11,  1.70it/s]Extractor Estimating: 430it [04:12,  1.76it/s]Extractor Estimating: 431it [04:13,  1.75it/s]Extractor Estimating: 432it [04:13,  1.67it/s]Extractor Estimating: 433it [04:14,  1.67it/s]Extractor Estimating: 434it [04:14,  1.66it/s]Extractor Estimating: 435it [04:15,  1.65it/s]Extractor Estimating: 436it [04:16,  1.68it/s]Extractor Estimating: 437it [04:16,  1.60it/s]Extractor Estimating: 438it [04:17,  1.62it/s]Extractor Estimating: 439it [04:18,  1.62it/s]Extractor Estimating: 440it [04:18,  1.64it/s]Extractor Estimating: 441it [04:19,  1.68it/s]Extractor Estimating: 442it [04:19,  1.64it/s]Extractor Estimating: 443it [04:20,  1.63it/s]Extractor Estimating: 444it [04:21,  1.62it/s]Extractor Estimating: 445it [04:21,  1.67it/s]Extractor Estimating: 446it [04:22,  1.64it/s]Extractor Estimating: 447it [04:22,  1.65it/s]Extractor Estimating: 448it [04:23,  1.64it/s]Extractor Estimating: 449it [04:24,  1.67it/s]Extractor Estimating: 450it [04:24,  1.68it/s]Extractor Estimating: 451it [04:25,  1.76it/s]Extractor Estimating: 452it [04:25,  1.82it/s]Extractor Estimating: 453it [04:26,  1.84it/s]Extractor Estimating: 454it [04:26,  1.82it/s]Extractor Estimating: 455it [04:27,  1.80it/s]Extractor Estimating: 456it [04:27,  1.81it/s]Extractor Estimating: 457it [04:28,  1.86it/s]Extractor Estimating: 458it [04:28,  1.85it/s]Extractor Estimating: 459it [04:29,  1.83it/s]Extractor Estimating: 460it [04:30,  1.86it/s]Extractor Estimating: 461it [04:30,  1.91it/s]Extractor Estimating: 462it [04:31,  1.86it/s]Extractor Estimating: 463it [04:31,  1.86it/s]Extractor Estimating: 464it [04:32,  1.89it/s]Extractor Estimating: 465it [04:32,  1.87it/s]Extractor Estimating: 466it [04:33,  1.89it/s]Extractor Estimating: 467it [04:33,  1.91it/s]Extractor Estimating: 468it [04:34,  1.91it/s]Extractor Estimating: 469it [04:34,  1.89it/s]Extractor Estimating: 470it [04:35,  1.88it/s]Extractor Estimating: 471it [04:35,  1.83it/s]Extractor Estimating: 472it [04:36,  1.84it/s]Extractor Estimating: 473it [04:36,  1.86it/s]Extractor Estimating: 474it [04:37,  1.82it/s]Extractor Estimating: 475it [04:38,  1.80it/s]Extractor Estimating: 476it [04:38,  1.79it/s]Extractor Estimating: 477it [04:39,  1.82it/s]Extractor Estimating: 478it [04:39,  1.77it/s]Extractor Estimating: 479it [04:40,  1.78it/s]Extractor Estimating: 480it [04:40,  1.77it/s]Extractor Estimating: 481it [04:41,  1.54it/s]Extractor Estimating: 482it [04:42,  1.54it/s]Extractor Estimating: 483it [04:42,  1.61it/s]Extractor Estimating: 484it [04:43,  1.61it/s]Extractor Estimating: 485it [04:44,  1.64it/s]Extractor Estimating: 486it [04:44,  1.68it/s]Extractor Estimating: 487it [04:45,  1.70it/s]Extractor Estimating: 488it [04:45,  1.75it/s]Extractor Estimating: 489it [04:46,  1.73it/s]Extractor Estimating: 490it [04:47,  1.65it/s]Extractor Estimating: 491it [04:47,  1.68it/s]Extractor Estimating: 492it [04:48,  1.70it/s]Extractor Estimating: 493it [04:49,  1.56it/s]Extractor Estimating: 494it [04:49,  1.56it/s]Extractor Estimating: 495it [04:50,  1.56it/s]Extractor Estimating: 496it [04:50,  1.58it/s]Extractor Estimating: 497it [04:51,  1.61it/s]Extractor Estimating: 498it [04:52,  1.63it/s]Extractor Estimating: 499it [04:52,  1.67it/s]Extractor Estimating: 500it [04:53,  1.86it/s]Extractor Estimating: 500it [04:53,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:53,320 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:53,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:53,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:53,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:53,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:26:54,001 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:26:54,002 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:26:54,582 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:26:55,647 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:26:55,647 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:58,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:58,769 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:58,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:58,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:26:58,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:26:59,448 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:26:59,449 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:27:00,042 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:27:00,209 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:27:00,209 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:20:38,481 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:20:38,719 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9993 mean pseudo reward: 0.9225574666431104
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl'}
train vocab size: 17479
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17579, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17579, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.983, loss:645.5691
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.956, loss:636.7633
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.968, loss:670.1434
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.970, loss:673.0821
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.949, loss:599.1749
>> valid entity prec:0.5799, rec:0.5313, f1:0.5546
>> valid relation prec:0.3675, rec:0.1746, f1:0.2368
>> valid relation with NER prec:0.3675, rec:0.1746, f1:0.2368
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.217, loss:622.1433
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.969, loss:595.5013
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.976, loss:637.2906
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.969, loss:600.4155
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.983, loss:590.2912
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5411, rec:0.5719, f1:0.5561
>> valid relation prec:0.2951, rec:0.1712, f1:0.2167
>> valid relation with NER prec:0.2951, rec:0.1712, f1:0.2167
new max entity f1 on valid!
g_step 1100, step 266, avg_time 2.200, loss:621.0049
g_step 1200, step 366, avg_time 0.983, loss:601.4431
g_step 1300, step 49, avg_time 0.985, loss:581.1072
g_step 1400, step 149, avg_time 0.977, loss:553.3857
g_step 1500, step 249, avg_time 1.000, loss:575.0306
>> valid entity prec:0.5307, rec:0.5460, f1:0.5383
>> valid relation prec:0.2703, rec:0.1402, f1:0.1847
>> valid relation with NER prec:0.2703, rec:0.1402, f1:0.1847
g_step 1600, step 349, avg_time 2.314, loss:577.8427
g_step 1700, step 32, avg_time 1.011, loss:579.4735
g_step 1800, step 132, avg_time 1.009, loss:528.2740
g_step 1900, step 232, avg_time 1.014, loss:553.1213
g_step 2000, step 332, avg_time 1.008, loss:540.6470
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5693, rec:0.5176, f1:0.5422
>> valid relation prec:0.3162, rec:0.1583, f1:0.2110
>> valid relation with NER prec:0.3162, rec:0.1583, f1:0.2110
g_step 2100, step 15, avg_time 2.269, loss:561.1746
g_step 2200, step 115, avg_time 1.008, loss:511.6093
g_step 2300, step 215, avg_time 1.014, loss:519.8099
g_step 2400, step 315, avg_time 1.022, loss:522.1137
g_step 2500, step 415, avg_time 0.990, loss:539.0782
>> valid entity prec:0.5148, rec:0.5456, f1:0.5297
>> valid relation prec:0.3279, rec:0.1612, f1:0.2161
>> valid relation with NER prec:0.3279, rec:0.1612, f1:0.2161
g_step 2600, step 98, avg_time 2.277, loss:503.6279
g_step 2700, step 198, avg_time 1.021, loss:478.9155
g_step 2800, step 298, avg_time 1.002, loss:484.3431
g_step 2900, step 398, avg_time 1.018, loss:521.3497
g_step 3000, step 81, avg_time 0.998, loss:467.3604
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5695, rec:0.5106, f1:0.5385
>> valid relation prec:0.3121, rec:0.1643, f1:0.2153
>> valid relation with NER prec:0.3121, rec:0.1643, f1:0.2153
g_step 3100, step 181, avg_time 2.282, loss:490.2225
g_step 3200, step 281, avg_time 1.028, loss:481.2041
g_step 3300, step 381, avg_time 1.010, loss:485.7026
g_step 3400, step 64, avg_time 1.015, loss:459.6067
g_step 3500, step 164, avg_time 1.003, loss:460.6869
>> valid entity prec:0.5872, rec:0.4831, f1:0.5301
>> valid relation prec:0.3192, rec:0.1534, f1:0.2072
>> valid relation with NER prec:0.3192, rec:0.1534, f1:0.2072
g_step 3600, step 264, avg_time 2.292, loss:450.8557
g_step 3700, step 364, avg_time 1.027, loss:477.5954
g_step 3800, step 47, avg_time 0.993, loss:457.4570
g_step 3900, step 147, avg_time 1.004, loss:432.1593
g_step 4000, step 247, avg_time 1.012, loss:446.7808
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5500, rec:0.5413, f1:0.5456
>> valid relation prec:0.3040, rec:0.1804, f1:0.2264
>> valid relation with NER prec:0.3040, rec:0.1804, f1:0.2264
g_step 4100, step 347, avg_time 2.281, loss:466.5172
g_step 4200, step 30, avg_time 1.027, loss:448.9512
g_step 4300, step 130, avg_time 1.021, loss:399.3138
g_step 4400, step 230, avg_time 1.001, loss:424.9340
g_step 4500, step 330, avg_time 1.020, loss:445.6223
>> valid entity prec:0.5535, rec:0.5109, f1:0.5313
>> valid relation prec:0.3173, rec:0.1632, f1:0.2155
>> valid relation with NER prec:0.3173, rec:0.1632, f1:0.2155
g_step 4600, step 13, avg_time 2.286, loss:443.2155
g_step 4700, step 113, avg_time 1.004, loss:406.6069
g_step 4800, step 213, avg_time 1.022, loss:430.7484
g_step 4900, step 313, avg_time 1.010, loss:418.7615
g_step 5000, step 413, avg_time 1.007, loss:411.3536
learning rate was adjusted to 0.0008
>> valid entity prec:0.5416, rec:0.5243, f1:0.5328
>> valid relation prec:0.2730, rec:0.1551, f1:0.1978
>> valid relation with NER prec:0.2730, rec:0.1551, f1:0.1978
g_step 5100, step 96, avg_time 2.278, loss:381.9032
g_step 5200, step 196, avg_time 1.003, loss:402.5053
g_step 5300, step 296, avg_time 1.024, loss:410.3675
g_step 5400, step 396, avg_time 1.020, loss:418.6682
g_step 5500, step 79, avg_time 1.031, loss:395.1377
>> valid entity prec:0.5626, rec:0.5280, f1:0.5448
>> valid relation prec:0.2979, rec:0.1726, f1:0.2186
>> valid relation with NER prec:0.2979, rec:0.1726, f1:0.2186
g_step 5600, step 179, avg_time 2.278, loss:391.7230
g_step 5700, step 279, avg_time 1.020, loss:399.2151
g_step 5800, step 379, avg_time 1.010, loss:390.5684
g_step 5900, step 62, avg_time 1.004, loss:384.9293
g_step 6000, step 162, avg_time 1.016, loss:385.1884
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5497, rec:0.5156, f1:0.5321
>> valid relation prec:0.2600, rec:0.1563, f1:0.1952
>> valid relation with NER prec:0.2600, rec:0.1563, f1:0.1952
g_step 6100, step 262, avg_time 2.279, loss:387.0258
g_step 6200, step 362, avg_time 1.021, loss:382.7945
g_step 6300, step 45, avg_time 1.013, loss:370.9466
g_step 6400, step 145, avg_time 1.005, loss:369.4323
g_step 6500, step 245, avg_time 1.011, loss:372.1832
>> valid entity prec:0.5341, rec:0.5215, f1:0.5277
>> valid relation prec:0.2452, rec:0.1391, f1:0.1775
>> valid relation with NER prec:0.2452, rec:0.1391, f1:0.1775
g_step 6600, step 345, avg_time 2.293, loss:377.6395
g_step 6700, step 28, avg_time 0.997, loss:363.6339
g_step 6800, step 128, avg_time 1.011, loss:367.8364
g_step 6900, step 228, avg_time 1.010, loss:353.7888
g_step 7000, step 328, avg_time 1.016, loss:370.3280
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5560, rec:0.5044, f1:0.5290
>> valid relation prec:0.2942, rec:0.1537, f1:0.2019
>> valid relation with NER prec:0.2942, rec:0.1537, f1:0.2019
g_step 7100, step 11, avg_time 2.287, loss:363.0022
g_step 7200, step 111, avg_time 0.994, loss:324.6982
g_step 7300, step 211, avg_time 1.020, loss:345.8545
g_step 7400, step 311, avg_time 1.017, loss:342.8703
g_step 7500, step 411, avg_time 1.015, loss:374.8099
>> valid entity prec:0.5655, rec:0.5233, f1:0.5436
>> valid relation prec:0.3067, rec:0.1772, f1:0.2246
>> valid relation with NER prec:0.3067, rec:0.1772, f1:0.2246
g_step 7600, step 94, avg_time 2.293, loss:342.1751
g_step 7700, step 194, avg_time 0.998, loss:350.1863
g_step 7800, step 294, avg_time 1.015, loss:359.2455
g_step 7900, step 394, avg_time 1.005, loss:347.3531
g_step 8000, step 77, avg_time 0.999, loss:329.5416
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5531, rec:0.5076, f1:0.5294
>> valid relation prec:0.2864, rec:0.1569, f1:0.2027
>> valid relation with NER prec:0.2864, rec:0.1569, f1:0.2027
g_step 8100, step 177, avg_time 2.275, loss:312.6189
g_step 8200, step 277, avg_time 1.023, loss:338.4631
g_step 8300, step 377, avg_time 1.020, loss:339.1046
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:20:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:20:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-20-38_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:20:39 - WARNING - datasets.builder -   Using custom data configuration default-f9c7bc0226893540
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f9c7bc0226893540/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:20:41,806 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:20:41,848 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:20:41,848 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:20:41,849 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:20:41,967 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:20:42,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:20:42,431 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:20:45,604 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:20:45,604 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f9c7bc0226893540/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:03,  2.62ba/s] 20%|        | 2/10 [00:00<00:02,  3.61ba/s] 30%|       | 3/10 [00:00<00:01,  4.11ba/s] 40%|      | 4/10 [00:00<00:01,  4.40ba/s] 50%|     | 5/10 [00:01<00:01,  4.60ba/s] 60%|    | 6/10 [00:01<00:00,  4.73ba/s] 70%|   | 7/10 [00:01<00:00,  4.78ba/s] 80%|  | 8/10 [00:01<00:00,  4.82ba/s] 90%| | 9/10 [00:02<00:00,  4.87ba/s]100%|| 10/10 [00:02<00:00,  4.56ba/s]100%|| 10/10 [00:02<00:00,  4.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:01,  2.87ba/s] 50%|     | 2/4 [00:00<00:00,  3.61ba/s] 75%|  | 3/4 [00:00<00:00,  3.94ba/s]100%|| 4/4 [00:00<00:00,  5.03ba/s]100%|| 4/4 [00:00<00:00,  4.37ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|         | 1/10 [00:00<00:01,  4.77ba/s] 30%|       | 3/10 [00:00<00:00,  8.17ba/s] 50%|     | 5/10 [00:00<00:00,  9.43ba/s] 70%|   | 7/10 [00:00<00:00, 10.04ba/s] 90%| | 9/10 [00:00<00:00, 10.42ba/s]100%|| 10/10 [00:01<00:00,  9.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|       | 1/4 [00:00<00:00,  4.78ba/s] 75%|  | 3/4 [00:00<00:00,  8.11ba/s]100%|| 4/4 [00:00<00:00,  9.05ba/s]
[INFO|trainer.py:414] 2023-08-29 10:20:51,950 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:20:52,084 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:20:52,084 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 10:20:52,085 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:20:52,085 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:20:52,085 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:20:52,085 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:20:52,085 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:56,  3.30it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<04:01,  3.22it/s]  1%|          | 4/780 [00:01<03:54,  3.31it/s]  1%|          | 5/780 [00:01<03:50,  3.37it/s]  1%|          | 6/780 [00:01<03:47,  3.40it/s]  1%|          | 7/780 [00:02<03:46,  3.42it/s]  1%|          | 8/780 [00:02<03:45,  3.43it/s]  1%|          | 9/780 [00:02<03:44,  3.43it/s]  1%|         | 10/780 [00:02<03:43,  3.44it/s]  1%|         | 11/780 [00:03<03:42,  3.45it/s]  2%|         | 12/780 [00:03<03:42,  3.45it/s]  2%|         | 13/780 [00:03<03:42,  3.45it/s]  2%|         | 14/780 [00:04<03:41,  3.45it/s]  2%|         | 15/780 [00:04<03:41,  3.46it/s]  2%|         | 16/780 [00:04<03:40,  3.46it/s]  2%|         | 17/780 [00:04<03:40,  3.46it/s]  2%|         | 18/780 [00:05<03:40,  3.46it/s]  2%|         | 19/780 [00:05<03:39,  3.46it/s]  3%|         | 20/780 [00:05<03:39,  3.46it/s]  3%|         | 21/780 [00:06<03:44,  3.38it/s]  3%|         | 22/780 [00:06<03:42,  3.40it/s]  3%|         | 23/780 [00:06<03:41,  3.42it/s]  3%|         | 24/780 [00:07<03:40,  3.43it/s]  3%|         | 25/780 [00:07<03:39,  3.44it/s]  3%|         | 26/780 [00:07<03:38,  3.45it/s]  3%|         | 27/780 [00:07<03:38,  3.45it/s]  4%|         | 28/780 [00:08<03:43,  3.37it/s]  4%|         | 29/780 [00:08<03:41,  3.40it/s]  4%|         | 30/780 [00:08<03:39,  3.42it/s]  4%|         | 31/780 [00:09<03:38,  3.43it/s]  4%|         | 32/780 [00:09<03:37,  3.44it/s]  4%|         | 33/780 [00:09<03:36,  3.45it/s]  4%|         | 34/780 [00:09<03:36,  3.45it/s]  4%|         | 35/780 [00:10<03:35,  3.45it/s]  5%|         | 36/780 [00:10<03:35,  3.45it/s]  5%|         | 37/780 [00:10<03:35,  3.45it/s]  5%|         | 38/780 [00:11<03:34,  3.45it/s]  5%|         | 39/780 [00:11<03:50,  3.21it/s]  5%|         | 40/780 [00:11<03:45,  3.28it/s]  5%|         | 41/780 [00:12<03:41,  3.33it/s]  5%|         | 42/780 [00:12<03:38,  3.37it/s]  6%|         | 43/780 [00:12<03:37,  3.39it/s]  6%|         | 44/780 [00:12<03:35,  3.41it/s]  6%|         | 45/780 [00:13<03:34,  3.42it/s]  6%|         | 46/780 [00:13<03:33,  3.43it/s]  6%|         | 47/780 [00:13<03:33,  3.44it/s]  6%|         | 48/780 [00:14<03:32,  3.45it/s]  6%|         | 49/780 [00:14<03:32,  3.45it/s]  6%|         | 50/780 [00:14<03:31,  3.45it/s]  7%|         | 51/780 [00:14<03:31,  3.45it/s]  7%|         | 52/780 [00:15<03:30,  3.46it/s]  7%|         | 53/780 [00:15<03:30,  3.45it/s]  7%|         | 54/780 [00:15<03:30,  3.45it/s]  7%|         | 55/780 [00:16<03:29,  3.46it/s]  7%|         | 56/780 [00:16<03:41,  3.27it/s]  7%|         | 57/780 [00:16<03:37,  3.33it/s]  7%|         | 58/780 [00:16<03:34,  3.36it/s]  8%|         | 59/780 [00:17<03:32,  3.39it/s]  8%|         | 60/780 [00:17<03:31,  3.41it/s]  8%|         | 61/780 [00:17<03:30,  3.42it/s]  8%|         | 62/780 [00:18<03:29,  3.43it/s]  8%|         | 63/780 [00:18<03:28,  3.44it/s]  8%|         | 64/780 [00:18<03:27,  3.45it/s]  8%|         | 65/780 [00:19<03:27,  3.45it/s]  8%|         | 66/780 [00:19<03:26,  3.45it/s]  9%|         | 67/780 [00:19<03:26,  3.45it/s]  9%|         | 68/780 [00:19<03:26,  3.45it/s]  9%|         | 69/780 [00:20<03:26,  3.45it/s]  9%|         | 70/780 [00:20<03:25,  3.45it/s]  9%|         | 71/780 [00:20<03:25,  3.45it/s]  9%|         | 72/780 [00:21<03:24,  3.45it/s]  9%|         | 73/780 [00:21<03:24,  3.45it/s]  9%|         | 74/780 [00:21<03:37,  3.25it/s] 10%|         | 75/780 [00:21<03:33,  3.31it/s] 10%|         | 76/780 [00:22<03:30,  3.35it/s] 10%|         | 77/780 [00:22<03:28,  3.38it/s] 10%|         | 78/780 [00:22<03:26,  3.40it/s] 10%|         | 79/780 [00:23<03:25,  3.41it/s] 10%|         | 80/780 [00:23<03:24,  3.43it/s] 10%|         | 81/780 [00:23<03:23,  3.43it/s] 11%|         | 82/780 [00:24<03:22,  3.44it/s] 11%|         | 83/780 [00:24<03:22,  3.44it/s] 11%|         | 84/780 [00:24<03:21,  3.45it/s] 11%|         | 85/780 [00:24<03:21,  3.45it/s] 11%|         | 86/780 [00:25<03:21,  3.45it/s] 11%|         | 87/780 [00:25<03:20,  3.45it/s] 11%|        | 88/780 [00:25<03:20,  3.45it/s] 11%|        | 89/780 [00:26<03:20,  3.45it/s] 12%|        | 90/780 [00:26<03:19,  3.45it/s] 12%|        | 91/780 [00:26<03:25,  3.35it/s] 12%|        | 92/780 [00:26<03:23,  3.38it/s] 12%|        | 93/780 [00:27<03:21,  3.40it/s] 12%|        | 94/780 [00:27<03:20,  3.41it/s] 12%|        | 95/780 [00:27<03:19,  3.43it/s] 12%|        | 96/780 [00:28<03:19,  3.43it/s] 12%|        | 97/780 [00:28<03:18,  3.44it/s] 13%|        | 98/780 [00:28<03:17,  3.45it/s] 13%|        | 99/780 [00:28<03:17,  3.45it/s] 13%|        | 100/780 [00:29<03:17,  3.45it/s] 13%|        | 101/780 [00:29<03:16,  3.45it/s] 13%|        | 102/780 [00:29<03:16,  3.45it/s] 13%|        | 103/780 [00:30<03:16,  3.45it/s] 13%|        | 104/780 [00:30<03:15,  3.45it/s] 13%|        | 105/780 [00:30<03:15,  3.45it/s] 14%|        | 106/780 [00:30<03:15,  3.45it/s] 14%|        | 107/780 [00:31<03:14,  3.45it/s] 14%|        | 108/780 [00:31<03:14,  3.45it/s] 14%|        | 109/780 [00:31<03:19,  3.37it/s] 14%|        | 110/780 [00:32<03:17,  3.39it/s] 14%|        | 111/780 [00:32<03:16,  3.40it/s] 14%|        | 112/780 [00:32<03:15,  3.41it/s] 14%|        | 113/780 [00:33<03:14,  3.43it/s] 15%|        | 114/780 [00:33<03:14,  3.43it/s] 15%|        | 115/780 [00:33<03:13,  3.44it/s] 15%|        | 116/780 [00:33<03:12,  3.44it/s] 15%|        | 117/780 [00:34<03:12,  3.45it/s] 15%|        | 118/780 [00:34<03:11,  3.45it/s] 15%|        | 119/780 [00:34<03:11,  3.45it/s] 15%|        | 120/780 [00:35<03:11,  3.45it/s] 16%|        | 121/780 [00:35<03:11,  3.45it/s] 16%|        | 122/780 [00:35<03:10,  3.45it/s] 16%|        | 123/780 [00:35<03:10,  3.45it/s] 16%|        | 124/780 [00:36<03:10,  3.45it/s] 16%|        | 125/780 [00:36<03:09,  3.45it/s] 16%|        | 126/780 [00:36<03:13,  3.39it/s] 16%|        | 127/780 [00:37<03:16,  3.32it/s] 16%|        | 128/780 [00:37<03:14,  3.36it/s] 17%|        | 129/780 [00:37<03:12,  3.39it/s] 17%|        | 130/780 [00:38<03:10,  3.40it/s] 17%|        | 131/780 [00:38<03:09,  3.42it/s] 17%|        | 132/780 [00:38<03:09,  3.43it/s] 17%|        | 133/780 [00:38<03:08,  3.44it/s] 17%|        | 134/780 [00:39<03:07,  3.44it/s] 17%|        | 135/780 [00:39<03:07,  3.45it/s] 17%|        | 136/780 [00:39<03:06,  3.45it/s] 18%|        | 137/780 [00:40<03:06,  3.45it/s] 18%|        | 138/780 [00:40<03:06,  3.45it/s] 18%|        | 139/780 [00:40<03:05,  3.45it/s] 18%|        | 140/780 [00:40<03:05,  3.45it/s] 18%|        | 141/780 [00:41<03:05,  3.45it/s] 18%|        | 142/780 [00:41<03:05,  3.45it/s] 18%|        | 143/780 [00:41<03:04,  3.45it/s] 18%|        | 144/780 [00:42<03:07,  3.40it/s] 19%|        | 145/780 [00:42<03:06,  3.41it/s] 19%|        | 146/780 [00:42<03:05,  3.42it/s] 19%|        | 147/780 [00:42<03:04,  3.43it/s] 19%|        | 148/780 [00:43<03:03,  3.44it/s] 19%|        | 149/780 [00:43<03:03,  3.44it/s] 19%|        | 150/780 [00:43<03:02,  3.45it/s] 19%|        | 151/780 [00:44<03:02,  3.45it/s] 19%|        | 152/780 [00:44<03:02,  3.45it/s] 20%|        | 153/780 [00:44<03:01,  3.45it/s] 20%|        | 154/780 [00:44<03:01,  3.45it/s] 20%|        | 155/780 [00:45<03:01,  3.45it/s] 20%|        | 156/780 [00:45<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 10:21:37,690 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:21:37,690 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:21:37,690 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.83it/s][A
  3%|         | 12/437 [00:00<00:08, 49.27it/s][A
  4%|         | 17/437 [00:00<00:08, 47.45it/s][A
  5%|         | 22/437 [00:00<00:08, 46.67it/s][A
  6%|         | 27/437 [00:00<00:08, 46.03it/s][A
  7%|         | 32/437 [00:00<00:08, 45.69it/s][A
  8%|         | 37/437 [00:00<00:13, 29.01it/s][A
 10%|         | 42/437 [00:01<00:11, 33.27it/s][A
 11%|         | 47/437 [00:01<00:11, 34.59it/s][A
 12%|        | 52/437 [00:01<00:10, 37.42it/s][A
 13%|        | 57/437 [00:01<00:09, 39.53it/s][A
 14%|        | 62/437 [00:01<00:09, 41.31it/s][A
 15%|        | 67/437 [00:01<00:08, 42.48it/s][A
 16%|        | 72/437 [00:01<00:08, 43.35it/s][A
 18%|        | 77/437 [00:01<00:08, 43.97it/s][A
 19%|        | 82/437 [00:01<00:08, 44.12it/s][A
 20%|        | 87/437 [00:02<00:07, 44.08it/s][A
 21%|        | 92/437 [00:02<00:07, 44.21it/s][A
 22%|       | 97/437 [00:02<00:07, 44.41it/s][A
 23%|       | 102/437 [00:02<00:07, 44.78it/s][A
 24%|       | 107/437 [00:02<00:07, 44.87it/s][A
 26%|       | 112/437 [00:02<00:07, 45.16it/s][A
 27%|       | 117/437 [00:02<00:07, 45.25it/s][A
 28%|       | 122/437 [00:02<00:06, 45.33it/s][A
 29%|       | 127/437 [00:02<00:06, 45.34it/s][A
 30%|       | 132/437 [00:03<00:06, 45.33it/s][A
 31%|      | 137/437 [00:03<00:06, 45.28it/s][A
 32%|      | 142/437 [00:03<00:06, 45.15it/s][A
 34%|      | 147/437 [00:03<00:06, 45.13it/s][A
 35%|      | 152/437 [00:03<00:06, 45.17it/s][A
 36%|      | 157/437 [00:03<00:06, 45.14it/s][A
 37%|      | 162/437 [00:03<00:06, 45.34it/s][A
 38%|      | 167/437 [00:03<00:05, 45.31it/s][A
 39%|      | 172/437 [00:03<00:05, 45.38it/s][A
 41%|      | 177/437 [00:04<00:05, 45.40it/s][A
 42%|     | 182/437 [00:04<00:05, 45.26it/s][A
 43%|     | 187/437 [00:04<00:05, 45.21it/s][A
 44%|     | 192/437 [00:04<00:05, 45.36it/s][A
 45%|     | 197/437 [00:04<00:05, 45.29it/s][A
 46%|     | 202/437 [00:04<00:05, 45.32it/s][A
 47%|     | 207/437 [00:04<00:05, 45.29it/s][A
 49%|     | 212/437 [00:04<00:04, 45.26it/s][A
 50%|     | 217/437 [00:04<00:04, 45.35it/s][A
 51%|     | 222/437 [00:05<00:04, 45.37it/s][A
 52%|    | 227/437 [00:05<00:04, 45.41it/s][A
 53%|    | 232/437 [00:05<00:04, 45.37it/s][A
 54%|    | 237/437 [00:05<00:04, 45.37it/s][A
 55%|    | 242/437 [00:05<00:04, 45.32it/s][A
 57%|    | 247/437 [00:05<00:04, 45.40it/s][A
 58%|    | 252/437 [00:05<00:04, 45.29it/s][A
 59%|    | 257/437 [00:05<00:03, 45.31it/s][A
 60%|    | 262/437 [00:05<00:03, 45.35it/s][A
 61%|    | 267/437 [00:06<00:03, 45.34it/s][A
 62%|   | 272/437 [00:06<00:03, 45.40it/s][A
 63%|   | 277/437 [00:06<00:03, 43.82it/s][A
 65%|   | 282/437 [00:06<00:03, 44.42it/s][A
 66%|   | 287/437 [00:06<00:03, 44.58it/s][A
 67%|   | 292/437 [00:06<00:03, 44.80it/s][A
 68%|   | 297/437 [00:06<00:03, 44.87it/s][A
 69%|   | 302/437 [00:06<00:02, 45.12it/s][A
 70%|   | 307/437 [00:06<00:02, 45.17it/s][A
 71%|  | 312/437 [00:07<00:02, 45.23it/s][A
 73%|  | 317/437 [00:07<00:02, 45.15it/s][A
 74%|  | 322/437 [00:07<00:02, 45.19it/s][A
 75%|  | 327/437 [00:07<00:02, 45.20it/s][A
 76%|  | 332/437 [00:07<00:02, 45.24it/s][A
 77%|  | 337/437 [00:07<00:02, 45.27it/s][A
 78%|  | 342/437 [00:07<00:02, 45.22it/s][A
 79%|  | 347/437 [00:07<00:01, 45.32it/s][A
 81%|  | 352/437 [00:07<00:01, 45.31it/s][A
 82%| | 357/437 [00:08<00:01, 45.35it/s][A
 83%| | 362/437 [00:08<00:01, 45.33it/s][A
 84%| | 367/437 [00:08<00:01, 45.44it/s][A
 85%| | 372/437 [00:08<00:01, 41.72it/s][A
 86%| | 377/437 [00:08<00:01, 42.82it/s][A
 87%| | 382/437 [00:08<00:01, 43.54it/s][A
 89%| | 387/437 [00:08<00:01, 44.16it/s][A
 90%| | 392/437 [00:08<00:01, 44.61it/s][A
 91%| | 397/437 [00:08<00:00, 44.95it/s][A
 92%|| 402/437 [00:09<00:00, 45.01it/s][A
 93%|| 407/437 [00:09<00:00, 45.17it/s][A
 94%|| 412/437 [00:09<00:00, 44.82it/s][A
 95%|| 417/437 [00:09<00:00, 44.84it/s][A
 97%|| 422/437 [00:09<00:00, 45.05it/s][A
 98%|| 427/437 [00:09<00:00, 45.23it/s][A
 99%|| 432/437 [00:09<00:00, 45.30it/s][A
100%|| 437/437 [00:09<00:00, 45.42it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 45.42it/s][A 20%|        | 156/780 [00:55<03:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:21:48,005 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 10:21:48,261 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:21:51,811 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:21:51,985 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:21:52,072 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|        | 157/780 [01:08<1:12:32,  6.99s/it] 20%|        | 158/780 [01:08<51:43,  4.99s/it]   20%|        | 159/780 [01:08<37:03,  3.58s/it] 21%|        | 160/780 [01:09<26:48,  2.59s/it] 21%|        | 161/780 [01:09<19:38,  1.90s/it] 21%|        | 162/780 [01:09<14:38,  1.42s/it] 21%|        | 163/780 [01:09<11:08,  1.08s/it] 21%|        | 164/780 [01:10<08:41,  1.18it/s] 21%|        | 165/780 [01:10<06:58,  1.47it/s] 21%|       | 166/780 [01:10<05:46,  1.77it/s] 21%|       | 167/780 [01:11<04:56,  2.07it/s] 22%|       | 168/780 [01:11<04:20,  2.35it/s] 22%|       | 169/780 [01:11<04:06,  2.47it/s] 22%|       | 170/780 [01:12<03:46,  2.70it/s] 22%|       | 171/780 [01:12<03:30,  2.89it/s] 22%|       | 172/780 [01:12<03:20,  3.04it/s] 22%|       | 173/780 [01:12<03:12,  3.15it/s] 22%|       | 174/780 [01:13<03:07,  3.24it/s] 22%|       | 175/780 [01:13<03:03,  3.30it/s] 23%|       | 176/780 [01:13<03:00,  3.34it/s] 23%|       | 177/780 [01:14<02:58,  3.37it/s] 23%|       | 178/780 [01:14<02:57,  3.40it/s] 23%|       | 179/780 [01:14<02:55,  3.42it/s] 23%|       | 180/780 [01:15<03:04,  3.25it/s] 23%|       | 181/780 [01:15<03:01,  3.31it/s] 23%|       | 182/780 [01:15<02:58,  3.35it/s] 23%|       | 183/780 [01:15<02:56,  3.38it/s] 24%|       | 184/780 [01:16<02:55,  3.40it/s] 24%|       | 185/780 [01:16<02:54,  3.42it/s] 24%|       | 186/780 [01:16<02:53,  3.43it/s] 24%|       | 187/780 [01:17<02:52,  3.43it/s] 24%|       | 188/780 [01:17<02:51,  3.44it/s] 24%|       | 189/780 [01:17<02:51,  3.44it/s] 24%|       | 190/780 [01:17<02:51,  3.45it/s] 24%|       | 191/780 [01:18<02:57,  3.33it/s] 25%|       | 192/780 [01:18<02:54,  3.36it/s] 25%|       | 193/780 [01:18<02:53,  3.39it/s] 25%|       | 194/780 [01:19<02:51,  3.41it/s] 25%|       | 195/780 [01:19<02:50,  3.42it/s] 25%|       | 196/780 [01:19<02:50,  3.43it/s] 25%|       | 197/780 [01:19<02:49,  3.44it/s] 25%|       | 198/780 [01:20<02:49,  3.44it/s] 26%|       | 199/780 [01:20<02:48,  3.45it/s] 26%|       | 200/780 [01:20<02:48,  3.45it/s] 26%|       | 201/780 [01:21<02:47,  3.45it/s] 26%|       | 202/780 [01:21<02:52,  3.35it/s] 26%|       | 203/780 [01:21<02:50,  3.38it/s] 26%|       | 204/780 [01:22<02:49,  3.40it/s] 26%|       | 205/780 [01:22<02:48,  3.41it/s] 26%|       | 206/780 [01:22<02:47,  3.43it/s] 27%|       | 207/780 [01:22<02:46,  3.43it/s] 27%|       | 208/780 [01:23<02:46,  3.44it/s] 27%|       | 209/780 [01:23<02:46,  3.44it/s] 27%|       | 210/780 [01:23<02:45,  3.44it/s] 27%|       | 211/780 [01:24<02:45,  3.45it/s] 27%|       | 212/780 [01:24<02:44,  3.45it/s] 27%|       | 213/780 [01:24<02:49,  3.35it/s] 27%|       | 214/780 [01:24<02:47,  3.38it/s] 28%|       | 215/780 [01:25<02:46,  3.40it/s] 28%|       | 216/780 [01:25<02:44,  3.42it/s] 28%|       | 217/780 [01:25<02:44,  3.43it/s] 28%|       | 218/780 [01:26<02:43,  3.44it/s] 28%|       | 219/780 [01:26<02:43,  3.44it/s] 28%|       | 220/780 [01:26<02:42,  3.45it/s] 28%|       | 221/780 [01:27<02:42,  3.45it/s] 28%|       | 222/780 [01:27<02:41,  3.45it/s] 29%|       | 223/780 [01:27<02:41,  3.45it/s] 29%|       | 224/780 [01:27<02:41,  3.45it/s] 29%|       | 225/780 [01:28<02:48,  3.30it/s] 29%|       | 226/780 [01:28<02:45,  3.34it/s] 29%|       | 227/780 [01:28<02:43,  3.38it/s] 29%|       | 228/780 [01:29<02:42,  3.40it/s] 29%|       | 229/780 [01:29<02:41,  3.42it/s] 29%|       | 230/780 [01:29<02:40,  3.43it/s] 30%|       | 231/780 [01:29<02:39,  3.44it/s] 30%|       | 232/780 [01:30<02:39,  3.44it/s] 30%|       | 233/780 [01:30<02:38,  3.44it/s] 30%|       | 234/780 [01:30<02:38,  3.45it/s] 30%|       | 235/780 [01:31<02:38,  3.45it/s] 30%|       | 236/780 [01:31<02:41,  3.37it/s] 30%|       | 237/780 [01:31<02:39,  3.40it/s] 31%|       | 238/780 [01:31<02:38,  3.41it/s] 31%|       | 239/780 [01:32<02:38,  3.42it/s] 31%|       | 240/780 [01:32<02:37,  3.43it/s] 31%|       | 241/780 [01:32<02:36,  3.44it/s] 31%|       | 242/780 [01:33<02:36,  3.44it/s] 31%|       | 243/780 [01:33<02:35,  3.44it/s] 31%|      | 244/780 [01:33<02:35,  3.45it/s] 31%|      | 245/780 [01:34<02:35,  3.45it/s] 32%|      | 246/780 [01:34<02:34,  3.45it/s] 32%|      | 247/780 [01:34<02:40,  3.32it/s] 32%|      | 248/780 [01:34<02:38,  3.36it/s] 32%|      | 249/780 [01:35<02:37,  3.37it/s] 32%|      | 250/780 [01:35<02:36,  3.40it/s] 32%|      | 251/780 [01:35<02:35,  3.41it/s] 32%|      | 252/780 [01:36<02:34,  3.42it/s] 32%|      | 253/780 [01:36<02:33,  3.43it/s] 33%|      | 254/780 [01:36<02:33,  3.44it/s] 33%|      | 255/780 [01:36<02:32,  3.44it/s] 33%|      | 256/780 [01:37<02:37,  3.33it/s] 33%|      | 257/780 [01:37<02:35,  3.36it/s] 33%|      | 258/780 [01:37<02:33,  3.39it/s] 33%|      | 259/780 [01:38<02:36,  3.34it/s] 33%|      | 260/780 [01:38<02:34,  3.37it/s] 33%|      | 261/780 [01:38<02:32,  3.39it/s] 34%|      | 262/780 [01:39<02:31,  3.41it/s] 34%|      | 263/780 [01:39<02:31,  3.42it/s] 34%|      | 264/780 [01:39<02:30,  3.43it/s] 34%|      | 265/780 [01:39<02:29,  3.44it/s] 34%|      | 266/780 [01:40<02:29,  3.44it/s] 34%|      | 267/780 [01:40<02:29,  3.44it/s] 34%|      | 268/780 [01:40<02:28,  3.45it/s] 34%|      | 269/780 [01:41<02:28,  3.45it/s] 35%|      | 270/780 [01:41<02:27,  3.45it/s] 35%|      | 271/780 [01:41<02:27,  3.45it/s] 35%|      | 272/780 [01:41<02:27,  3.45it/s] 35%|      | 273/780 [01:42<02:26,  3.45it/s] 35%|      | 274/780 [01:42<02:26,  3.45it/s] 35%|      | 275/780 [01:42<02:26,  3.45it/s] 35%|      | 276/780 [01:43<02:26,  3.45it/s] 36%|      | 277/780 [01:43<02:28,  3.38it/s] 36%|      | 278/780 [01:43<02:27,  3.40it/s] 36%|      | 279/780 [01:43<02:26,  3.42it/s] 36%|      | 280/780 [01:44<02:25,  3.42it/s] 36%|      | 281/780 [01:44<02:25,  3.43it/s] 36%|      | 282/780 [01:44<02:24,  3.44it/s] 36%|      | 283/780 [01:45<02:24,  3.45it/s] 36%|      | 284/780 [01:45<02:27,  3.37it/s] 37%|      | 285/780 [01:45<02:25,  3.40it/s] 37%|      | 286/780 [01:46<02:24,  3.41it/s] 37%|      | 287/780 [01:46<02:24,  3.42it/s] 37%|      | 288/780 [01:46<03:13,  2.54it/s] 37%|      | 289/780 [01:47<02:58,  2.76it/s] 37%|      | 290/780 [01:47<02:47,  2.93it/s] 37%|      | 291/780 [01:47<02:39,  3.07it/s] 37%|      | 292/780 [01:48<02:33,  3.17it/s] 38%|      | 293/780 [01:48<02:35,  3.14it/s] 38%|      | 294/780 [01:48<02:30,  3.23it/s] 38%|      | 295/780 [01:49<02:27,  3.29it/s] 38%|      | 296/780 [01:49<02:25,  3.34it/s] 38%|      | 297/780 [01:49<02:23,  3.37it/s] 38%|      | 298/780 [01:49<02:22,  3.39it/s] 38%|      | 299/780 [01:50<02:21,  3.41it/s] 38%|      | 300/780 [01:50<02:20,  3.42it/s] 39%|      | 301/780 [01:50<02:19,  3.43it/s] 39%|      | 302/780 [01:51<02:19,  3.43it/s] 39%|      | 303/780 [01:51<02:18,  3.44it/s] 39%|      | 304/780 [01:51<02:18,  3.44it/s] 39%|      | 305/780 [01:51<02:17,  3.45it/s] 39%|      | 306/780 [01:52<02:17,  3.45it/s] 39%|      | 307/780 [01:52<02:17,  3.45it/s] 39%|      | 308/780 [01:52<02:16,  3.45it/s] 40%|      | 309/780 [01:53<02:16,  3.45it/s] 40%|      | 310/780 [01:53<02:16,  3.45it/s] 40%|      | 311/780 [01:53<02:18,  3.38it/s] 40%|      | 312/780 [01:53<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 10:22:46,097 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:22:46,098 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:22:46,098 >>   Batch size = 8
{'eval_loss': 0.9992029666900635, 'eval_runtime': 9.9179, 'eval_samples_per_second': 351.787, 'eval_steps_per_second': 44.062, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.15it/s][A
  3%|         | 12/437 [00:00<00:08, 49.40it/s][A
  4%|         | 17/437 [00:00<00:08, 47.49it/s][A
  5%|         | 22/437 [00:00<00:08, 46.70it/s][A
  6%|         | 27/437 [00:00<00:08, 46.00it/s][A
  7%|         | 32/437 [00:00<00:08, 45.60it/s][A
  8%|         | 37/437 [00:00<00:08, 45.36it/s][A
 10%|         | 42/437 [00:00<00:08, 45.05it/s][A
 11%|         | 47/437 [00:01<00:08, 45.17it/s][A
 12%|        | 52/437 [00:01<00:08, 45.22it/s][A
 13%|        | 57/437 [00:01<00:08, 45.21it/s][A
 14%|        | 62/437 [00:01<00:08, 45.37it/s][A
 15%|        | 67/437 [00:01<00:08, 45.32it/s][A
 16%|        | 72/437 [00:01<00:08, 45.37it/s][A
 18%|        | 77/437 [00:01<00:07, 45.42it/s][A
 19%|        | 82/437 [00:01<00:07, 45.32it/s][A
 20%|        | 87/437 [00:01<00:07, 45.25it/s][A
 21%|        | 92/437 [00:02<00:07, 45.18it/s][A
 22%|       | 97/437 [00:02<00:07, 45.07it/s][A
 23%|       | 102/437 [00:02<00:07, 45.23it/s][A
 24%|       | 107/437 [00:02<00:07, 45.27it/s][A
 26%|       | 112/437 [00:02<00:07, 45.41it/s][A
 27%|       | 117/437 [00:02<00:07, 45.29it/s][A
 28%|       | 122/437 [00:02<00:06, 45.38it/s][A
 29%|       | 127/437 [00:02<00:06, 45.29it/s][A
 30%|       | 132/437 [00:02<00:06, 45.20it/s][A
 31%|      | 137/437 [00:03<00:06, 45.14it/s][A
 32%|      | 142/437 [00:03<00:06, 45.09it/s][A
 34%|      | 147/437 [00:03<00:06, 45.07it/s][A
 35%|      | 152/437 [00:03<00:06, 45.25it/s][A
 36%|      | 157/437 [00:03<00:06, 45.38it/s][A
 37%|      | 162/437 [00:03<00:06, 45.33it/s][A
 38%|      | 167/437 [00:03<00:05, 45.40it/s][A
 39%|      | 172/437 [00:03<00:05, 45.27it/s][A
 41%|      | 177/437 [00:03<00:05, 45.24it/s][A
 42%|     | 182/437 [00:04<00:05, 45.12it/s][A
 43%|     | 187/437 [00:04<00:05, 45.17it/s][A
 44%|     | 192/437 [00:04<00:05, 45.14it/s][A
 45%|     | 197/437 [00:04<00:05, 45.19it/s][A
 46%|     | 202/437 [00:04<00:05, 44.57it/s][A
 47%|     | 207/437 [00:04<00:05, 44.83it/s][A
 49%|     | 212/437 [00:04<00:04, 45.08it/s][A
 50%|     | 217/437 [00:04<00:04, 45.26it/s][A
 51%|     | 222/437 [00:04<00:04, 45.28it/s][A
 52%|    | 227/437 [00:04<00:04, 45.22it/s][A
 53%|    | 232/437 [00:05<00:04, 45.24it/s][A
 54%|    | 237/437 [00:05<00:04, 45.18it/s][A
 55%|    | 242/437 [00:05<00:04, 45.07it/s][A
 57%|    | 247/437 [00:05<00:04, 45.24it/s][A
 58%|    | 252/437 [00:05<00:04, 45.23it/s][A
 59%|    | 257/437 [00:05<00:03, 45.28it/s][A
 60%|    | 262/437 [00:05<00:03, 45.34it/s][A
 61%|    | 267/437 [00:05<00:03, 45.42it/s][A
 62%|   | 272/437 [00:05<00:03, 45.39it/s][A
 63%|   | 277/437 [00:06<00:03, 45.17it/s][A
 65%|   | 282/437 [00:06<00:03, 45.23it/s][A
 66%|   | 287/437 [00:06<00:03, 45.17it/s][A
 67%|   | 292/437 [00:06<00:03, 45.21it/s][A
 68%|   | 297/437 [00:06<00:03, 40.49it/s][A
 69%|   | 303/437 [00:06<00:03, 43.22it/s][A
 70%|   | 308/437 [00:06<00:02, 43.89it/s][A
 72%|  | 313/437 [00:06<00:02, 44.41it/s][A
 73%|  | 318/437 [00:07<00:02, 44.69it/s][A
 74%|  | 323/437 [00:07<00:02, 44.83it/s][A
 75%|  | 328/437 [00:07<00:02, 44.90it/s][A
 76%|  | 333/437 [00:07<00:02, 44.66it/s][A
 77%|  | 338/437 [00:07<00:02, 44.71it/s][A
 78%|  | 343/437 [00:07<00:02, 44.79it/s][A
 80%|  | 348/437 [00:07<00:01, 44.92it/s][A
 81%|  | 353/437 [00:07<00:01, 45.02it/s][A
 82%| | 358/437 [00:07<00:01, 45.25it/s][A
 83%| | 363/437 [00:08<00:01, 45.16it/s][A
 84%| | 368/437 [00:08<00:01, 45.45it/s][A
 85%| | 373/437 [00:08<00:01, 45.30it/s][A
 86%| | 378/437 [00:08<00:01, 45.17it/s][A
 88%| | 383/437 [00:08<00:01, 44.97it/s][A
 89%| | 388/437 [00:08<00:01, 44.99it/s][A
 90%| | 393/437 [00:08<00:00, 44.87it/s][A
 91%| | 398/437 [00:08<00:00, 45.15it/s][A
 92%|| 403/437 [00:08<00:00, 45.26it/s][A
 93%|| 408/437 [00:09<00:00, 45.33it/s][A
 95%|| 413/437 [00:09<00:00, 45.43it/s][A
 96%|| 418/437 [00:09<00:00, 45.44it/s][A
 97%|| 423/437 [00:09<00:00, 45.41it/s][A
 98%|| 428/437 [00:09<00:00, 45.40it/s][A
 99%|| 433/437 [00:09<00:00, 44.23it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.23it/s][A 40%|      | 312/780 [02:03<02:17,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:22:55,977 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 10:22:56,132 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:22:59,366 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:22:59,445 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:22:59,506 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|      | 313/780 [02:17<56:11,  7.22s/it] 40%|      | 314/780 [02:17<40:04,  5.16s/it] 40%|      | 315/780 [02:17<28:40,  3.70s/it] 41%|      | 316/780 [02:18<20:42,  2.68s/it] 41%|      | 317/780 [02:18<15:08,  1.96s/it] 41%|      | 318/780 [02:18<11:15,  1.46s/it] 41%|      | 319/780 [02:19<08:32,  1.11s/it] 41%|      | 320/780 [02:19<06:38,  1.15it/s] 41%|      | 321/780 [02:19<05:18,  1.44it/s] 41%|     | 322/780 [02:20<04:22,  1.74it/s] 41%|     | 323/780 [02:20<03:43,  2.04it/s] 42%|     | 324/780 [02:20<03:16,  2.32it/s] 42%|     | 325/780 [02:20<03:02,  2.49it/s] 42%|     | 326/780 [02:21<02:47,  2.71it/s] 42%|     | 327/780 [02:21<02:37,  2.89it/s] 42%|     | 328/780 [02:21<02:29,  3.02it/s] 42%|     | 329/780 [02:22<02:24,  3.13it/s] 42%|     | 330/780 [02:22<02:20,  3.21it/s] 42%|     | 331/780 [02:22<02:17,  3.27it/s] 43%|     | 332/780 [02:23<02:15,  3.31it/s] 43%|     | 333/780 [02:23<02:13,  3.34it/s] 43%|     | 334/780 [02:23<02:12,  3.36it/s] 43%|     | 335/780 [02:23<02:12,  3.37it/s] 43%|     | 336/780 [02:24<02:18,  3.21it/s] 43%|     | 337/780 [02:24<02:15,  3.27it/s] 43%|     | 338/780 [02:24<02:13,  3.31it/s] 43%|     | 339/780 [02:25<02:12,  3.34it/s] 44%|     | 340/780 [02:25<02:11,  3.36it/s] 44%|     | 341/780 [02:25<02:10,  3.37it/s] 44%|     | 342/780 [02:26<02:09,  3.38it/s] 44%|     | 343/780 [02:26<02:08,  3.39it/s] 44%|     | 344/780 [02:26<02:08,  3.39it/s] 44%|     | 345/780 [02:26<02:07,  3.40it/s] 44%|     | 346/780 [02:27<02:07,  3.40it/s] 44%|     | 347/780 [02:27<02:10,  3.31it/s] 45%|     | 348/780 [02:27<02:09,  3.34it/s] 45%|     | 349/780 [02:28<02:08,  3.36it/s] 45%|     | 350/780 [02:28<02:07,  3.37it/s] 45%|     | 351/780 [02:28<02:06,  3.38it/s] 45%|     | 352/780 [02:28<02:06,  3.39it/s] 45%|     | 353/780 [02:29<02:05,  3.39it/s] 45%|     | 354/780 [02:29<02:05,  3.39it/s] 46%|     | 355/780 [02:29<02:05,  3.39it/s] 46%|     | 356/780 [02:30<02:04,  3.40it/s] 46%|     | 357/780 [02:30<02:04,  3.40it/s] 46%|     | 358/780 [02:30<02:07,  3.31it/s] 46%|     | 359/780 [02:31<02:06,  3.34it/s] 46%|     | 360/780 [02:31<02:05,  3.36it/s] 46%|     | 361/780 [02:31<02:04,  3.37it/s] 46%|     | 362/780 [02:31<02:03,  3.38it/s] 47%|     | 363/780 [02:32<02:03,  3.39it/s] 47%|     | 364/780 [02:32<02:02,  3.39it/s] 47%|     | 365/780 [02:32<02:02,  3.39it/s] 47%|     | 366/780 [02:33<02:01,  3.39it/s] 47%|     | 367/780 [02:33<02:01,  3.40it/s] 47%|     | 368/780 [02:33<02:01,  3.40it/s] 47%|     | 369/780 [02:34<02:00,  3.40it/s] 47%|     | 370/780 [02:34<02:00,  3.40it/s] 48%|     | 371/780 [02:34<02:03,  3.30it/s] 48%|     | 372/780 [02:34<02:02,  3.33it/s] 48%|     | 373/780 [02:35<02:01,  3.35it/s] 48%|     | 374/780 [02:35<02:00,  3.37it/s] 48%|     | 375/780 [02:35<01:59,  3.38it/s] 48%|     | 376/780 [02:36<01:59,  3.39it/s] 48%|     | 377/780 [02:36<01:58,  3.39it/s] 48%|     | 378/780 [02:36<01:58,  3.39it/s] 49%|     | 379/780 [02:36<01:58,  3.40it/s] 49%|     | 380/780 [02:37<02:01,  3.29it/s] 49%|     | 381/780 [02:37<02:05,  3.18it/s] 49%|     | 382/780 [02:37<02:02,  3.25it/s] 49%|     | 383/780 [02:38<02:00,  3.29it/s] 49%|     | 384/780 [02:38<01:59,  3.32it/s] 49%|     | 385/780 [02:38<01:58,  3.35it/s] 49%|     | 386/780 [02:39<01:57,  3.36it/s] 50%|     | 387/780 [02:39<01:56,  3.37it/s] 50%|     | 388/780 [02:39<01:55,  3.38it/s] 50%|     | 389/780 [02:39<01:55,  3.39it/s] 50%|     | 390/780 [02:40<01:54,  3.39it/s] 50%|     | 391/780 [02:40<01:54,  3.39it/s] 50%|     | 392/780 [02:40<02:00,  3.23it/s] 50%|     | 393/780 [02:41<01:57,  3.28it/s] 51%|     | 394/780 [02:41<01:56,  3.32it/s] 51%|     | 395/780 [02:41<01:55,  3.33it/s] 51%|     | 396/780 [02:42<01:54,  3.35it/s] 51%|     | 397/780 [02:42<01:53,  3.37it/s] 51%|     | 398/780 [02:42<01:53,  3.38it/s] 51%|     | 399/780 [02:42<01:52,  3.38it/s] 51%|    | 400/780 [02:43<01:52,  3.39it/s] 51%|    | 401/780 [02:43<01:51,  3.39it/s] 52%|    | 402/780 [02:43<01:51,  3.40it/s] 52%|    | 403/780 [02:44<01:50,  3.40it/s] 52%|    | 404/780 [02:44<01:50,  3.40it/s] 52%|    | 405/780 [02:44<01:52,  3.33it/s] 52%|    | 406/780 [02:45<01:51,  3.35it/s] 52%|    | 407/780 [02:45<01:50,  3.36it/s] 52%|    | 408/780 [02:45<01:50,  3.38it/s] 52%|    | 409/780 [02:45<01:49,  3.38it/s] 53%|    | 410/780 [02:46<01:49,  3.39it/s] 53%|    | 411/780 [02:46<01:48,  3.39it/s] 53%|    | 412/780 [02:47<02:12,  2.78it/s] 53%|    | 413/780 [02:47<02:04,  2.94it/s] 53%|    | 414/780 [02:47<01:59,  3.07it/s] 53%|    | 415/780 [02:47<01:55,  3.16it/s] 53%|    | 416/780 [02:48<01:52,  3.23it/s] 53%|    | 417/780 [02:48<01:50,  3.28it/s] 54%|    | 418/780 [02:48<01:49,  3.32it/s] 54%|    | 419/780 [02:49<01:48,  3.34it/s] 54%|    | 420/780 [02:49<01:47,  3.36it/s] 54%|    | 421/780 [02:49<01:46,  3.37it/s] 54%|    | 422/780 [02:50<01:48,  3.30it/s] 54%|    | 423/780 [02:50<01:47,  3.33it/s] 54%|    | 424/780 [02:50<01:46,  3.35it/s] 54%|    | 425/780 [02:50<01:45,  3.36it/s] 55%|    | 426/780 [02:51<01:44,  3.38it/s] 55%|    | 427/780 [02:51<01:44,  3.38it/s] 55%|    | 428/780 [02:51<01:43,  3.39it/s] 55%|    | 429/780 [02:52<01:43,  3.39it/s] 55%|    | 430/780 [02:52<01:43,  3.40it/s] 55%|    | 431/780 [02:52<01:42,  3.40it/s] 55%|    | 432/780 [02:52<01:42,  3.40it/s] 56%|    | 433/780 [02:53<01:42,  3.40it/s] 56%|    | 434/780 [02:53<01:41,  3.40it/s] 56%|    | 435/780 [02:53<01:41,  3.40it/s] 56%|    | 436/780 [02:54<01:41,  3.40it/s] 56%|    | 437/780 [02:54<01:40,  3.40it/s] 56%|    | 438/780 [02:54<01:40,  3.40it/s] 56%|    | 439/780 [02:55<01:44,  3.27it/s] 56%|    | 440/780 [02:55<01:42,  3.31it/s] 57%|    | 441/780 [02:55<01:41,  3.33it/s] 57%|    | 442/780 [02:55<01:40,  3.36it/s] 57%|    | 443/780 [02:56<01:40,  3.37it/s] 57%|    | 444/780 [02:56<01:39,  3.37it/s] 57%|    | 445/780 [02:56<01:39,  3.38it/s] 57%|    | 446/780 [02:57<01:40,  3.34it/s] 57%|    | 447/780 [02:57<01:39,  3.35it/s] 57%|    | 448/780 [02:57<01:38,  3.37it/s] 58%|    | 449/780 [02:57<01:37,  3.38it/s] 58%|    | 450/780 [02:58<01:37,  3.38it/s] 58%|    | 451/780 [02:58<01:37,  3.38it/s] 58%|    | 452/780 [02:58<01:36,  3.39it/s] 58%|    | 453/780 [02:59<01:36,  3.39it/s] 58%|    | 454/780 [02:59<01:36,  3.39it/s] 58%|    | 455/780 [02:59<01:35,  3.40it/s] 58%|    | 456/780 [03:00<01:35,  3.40it/s] 59%|    | 457/780 [03:00<01:36,  3.34it/s] 59%|    | 458/780 [03:00<01:39,  3.22it/s] 59%|    | 459/780 [03:00<01:37,  3.28it/s] 59%|    | 460/780 [03:01<01:36,  3.31it/s] 59%|    | 461/780 [03:01<01:35,  3.34it/s] 59%|    | 462/780 [03:01<01:34,  3.36it/s] 59%|    | 463/780 [03:02<01:34,  3.37it/s] 59%|    | 464/780 [03:02<01:33,  3.37it/s] 60%|    | 465/780 [03:02<01:33,  3.38it/s] 60%|    | 466/780 [03:03<01:32,  3.39it/s] 60%|    | 467/780 [03:03<01:32,  3.39it/s] 60%|    | 468/780 [03:03<01:32,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 10:23:55,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:23:55,782 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:23:55,782 >>   Batch size = 8
{'eval_loss': 1.0219917297363281, 'eval_runtime': 9.7268, 'eval_samples_per_second': 358.699, 'eval_steps_per_second': 44.927, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.44it/s][A
  3%|         | 12/437 [00:00<00:08, 49.29it/s][A
  4%|         | 17/437 [00:00<00:08, 47.72it/s][A
  5%|         | 22/437 [00:00<00:08, 46.71it/s][A
  6%|         | 27/437 [00:00<00:08, 46.14it/s][A
  7%|         | 32/437 [00:00<00:08, 45.82it/s][A
  8%|         | 37/437 [00:00<00:08, 45.50it/s][A
 10%|         | 42/437 [00:00<00:08, 45.14it/s][A
 11%|         | 47/437 [00:01<00:08, 45.13it/s][A
 12%|        | 52/437 [00:01<00:08, 45.31it/s][A
 13%|        | 57/437 [00:01<00:08, 45.39it/s][A
 14%|        | 62/437 [00:01<00:08, 44.08it/s][A
 15%|        | 67/437 [00:01<00:08, 44.49it/s][A
 16%|        | 72/437 [00:01<00:08, 44.85it/s][A
 18%|        | 77/437 [00:01<00:08, 44.95it/s][A
 19%|        | 82/437 [00:01<00:07, 44.98it/s][A
 20%|        | 87/437 [00:01<00:07, 44.84it/s][A
 21%|        | 92/437 [00:02<00:07, 44.89it/s][A
 22%|       | 97/437 [00:02<00:07, 44.93it/s][A
 23%|       | 102/437 [00:02<00:07, 45.08it/s][A
 24%|       | 107/437 [00:02<00:07, 45.22it/s][A
 26%|       | 112/437 [00:02<00:07, 45.22it/s][A
 27%|       | 117/437 [00:02<00:07, 45.36it/s][A
 28%|       | 122/437 [00:02<00:06, 45.32it/s][A
 29%|       | 127/437 [00:02<00:06, 45.28it/s][A
 30%|       | 132/437 [00:02<00:06, 45.10it/s][A
 31%|      | 137/437 [00:03<00:06, 45.14it/s][A
 32%|      | 142/437 [00:03<00:06, 45.04it/s][A
 34%|      | 147/437 [00:03<00:06, 45.13it/s][A
 35%|      | 152/437 [00:03<00:06, 45.26it/s][A
 36%|      | 157/437 [00:03<00:06, 45.28it/s][A
 37%|      | 162/437 [00:03<00:06, 45.28it/s][A
 38%|      | 167/437 [00:03<00:05, 45.41it/s][A
 39%|      | 172/437 [00:03<00:05, 45.30it/s][A
 41%|      | 177/437 [00:03<00:05, 45.29it/s][A
 42%|     | 182/437 [00:04<00:05, 45.17it/s][A
 43%|     | 187/437 [00:04<00:05, 45.04it/s][A
 44%|     | 192/437 [00:04<00:05, 45.09it/s][A
 45%|     | 197/437 [00:04<00:05, 45.17it/s][A
 46%|     | 202/437 [00:04<00:05, 45.00it/s][A
 47%|     | 207/437 [00:04<00:05, 45.28it/s][A
 49%|     | 212/437 [00:04<00:04, 45.37it/s][A
 50%|     | 217/437 [00:04<00:04, 45.30it/s][A
 51%|     | 222/437 [00:04<00:04, 45.41it/s][A
 52%|    | 227/437 [00:05<00:04, 45.25it/s][A
 53%|    | 232/437 [00:05<00:04, 45.12it/s][A
 54%|    | 237/437 [00:05<00:04, 45.20it/s][A
 55%|    | 242/437 [00:05<00:04, 45.15it/s][A
 57%|    | 247/437 [00:05<00:04, 45.22it/s][A
 58%|    | 252/437 [00:05<00:04, 45.29it/s][A
 59%|    | 257/437 [00:05<00:03, 45.37it/s][A
 60%|    | 262/437 [00:05<00:03, 45.43it/s][A
 61%|    | 267/437 [00:05<00:03, 45.39it/s][A
 62%|   | 272/437 [00:05<00:03, 45.34it/s][A
 63%|   | 277/437 [00:06<00:03, 45.28it/s][A
 65%|   | 282/437 [00:06<00:03, 45.17it/s][A
 66%|   | 287/437 [00:06<00:03, 45.17it/s][A
 67%|   | 292/437 [00:06<00:03, 44.22it/s][A
 68%|   | 297/437 [00:06<00:03, 44.60it/s][A
 69%|   | 302/437 [00:06<00:03, 44.85it/s][A
 70%|   | 307/437 [00:06<00:02, 45.03it/s][A
 71%|  | 312/437 [00:06<00:02, 45.16it/s][A
 73%|  | 317/437 [00:06<00:02, 45.13it/s][A
 74%|  | 322/437 [00:07<00:02, 45.07it/s][A
 75%|  | 327/437 [00:07<00:02, 45.07it/s][A
 76%|  | 332/437 [00:07<00:02, 45.04it/s][A
 77%|  | 337/437 [00:07<00:02, 45.09it/s][A
 78%|  | 342/437 [00:07<00:02, 45.21it/s][A
 79%|  | 347/437 [00:07<00:01, 45.29it/s][A
 81%|  | 352/437 [00:07<00:01, 45.36it/s][A
 82%| | 357/437 [00:07<00:01, 45.41it/s][A
 83%| | 362/437 [00:07<00:01, 45.44it/s][A
 84%| | 367/437 [00:08<00:01, 45.36it/s][A
 85%| | 372/437 [00:08<00:01, 45.20it/s][A
 86%| | 377/437 [00:08<00:01, 45.21it/s][A
 87%| | 382/437 [00:08<00:01, 45.16it/s][A
 89%| | 387/437 [00:08<00:01, 40.99it/s][A
 90%| | 392/437 [00:08<00:01, 42.27it/s][A
 91%| | 397/437 [00:08<00:00, 43.30it/s][A
 92%|| 402/437 [00:08<00:00, 44.00it/s][A
 93%|| 407/437 [00:09<00:00, 44.49it/s][A
 94%|| 412/437 [00:09<00:00, 44.77it/s][A
 95%|| 417/437 [00:09<00:00, 44.81it/s][A
 97%|| 422/437 [00:09<00:00, 44.74it/s][A
 98%|| 427/437 [00:09<00:00, 44.62it/s][A
 99%|| 432/437 [00:09<00:00, 44.62it/s][A
100%|| 437/437 [00:09<00:00, 44.85it/s][A
                                                 [A                                                 
100%|| 437/437 [00:09<00:00, 44.85it/s][A 60%|    | 468/780 [03:13<01:32,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:24:05,998 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 10:24:06,408 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:24:11,716 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:24:12,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:24:12,283 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|    | 469/780 [03:32<45:18,  8.74s/it] 60%|    | 470/780 [03:32<32:11,  6.23s/it] 60%|    | 471/780 [03:32<22:54,  4.45s/it] 61%|    | 472/780 [03:33<16:26,  3.20s/it] 61%|    | 473/780 [03:33<11:55,  2.33s/it] 61%|    | 474/780 [03:33<08:46,  1.72s/it] 61%|    | 475/780 [03:33<06:33,  1.29s/it] 61%|    | 476/780 [03:34<05:01,  1.01it/s] 61%|    | 477/780 [03:34<03:57,  1.28it/s] 61%|   | 478/780 [03:34<03:11,  1.57it/s] 61%|   | 479/780 [03:35<02:40,  1.88it/s] 62%|   | 480/780 [03:35<02:22,  2.11it/s] 62%|   | 481/780 [03:35<02:05,  2.38it/s] 62%|   | 482/780 [03:36<01:53,  2.62it/s] 62%|   | 483/780 [03:36<01:45,  2.81it/s] 62%|   | 484/780 [03:36<01:39,  2.97it/s] 62%|   | 485/780 [03:36<01:35,  3.09it/s] 62%|   | 486/780 [03:37<01:32,  3.18it/s] 62%|   | 487/780 [03:37<01:33,  3.12it/s] 63%|   | 488/780 [03:37<01:31,  3.20it/s] 63%|   | 489/780 [03:38<01:29,  3.26it/s] 63%|   | 490/780 [03:38<01:35,  3.03it/s] 63%|   | 491/780 [03:38<01:32,  3.14it/s] 63%|   | 492/780 [03:39<01:29,  3.21it/s] 63%|   | 493/780 [03:39<01:27,  3.27it/s] 63%|   | 494/780 [03:39<01:26,  3.31it/s] 63%|   | 495/780 [03:39<01:25,  3.34it/s] 64%|   | 496/780 [03:40<01:24,  3.36it/s] 64%|   | 497/780 [03:40<01:23,  3.37it/s] 64%|   | 498/780 [03:40<01:23,  3.38it/s] 64%|   | 499/780 [03:41<01:22,  3.39it/s] 64%|   | 500/780 [03:41<01:26,  3.25it/s]                                                  64%|   | 500/780 [03:41<01:26,  3.25it/s] 64%|   | 501/780 [03:41<01:24,  3.29it/s] 64%|   | 502/780 [03:42<01:23,  3.33it/s] 64%|   | 503/780 [03:42<01:22,  3.35it/s] 65%|   | 504/780 [03:42<01:21,  3.37it/s] 65%|   | 505/780 [03:42<01:21,  3.38it/s] 65%|   | 506/780 [03:43<01:20,  3.39it/s] 65%|   | 507/780 [03:43<01:20,  3.39it/s] 65%|   | 508/780 [03:43<01:20,  3.40it/s] 65%|   | 509/780 [03:44<01:19,  3.40it/s] 65%|   | 510/780 [03:44<01:19,  3.40it/s] 66%|   | 511/780 [03:44<01:19,  3.40it/s] 66%|   | 512/780 [03:45<01:18,  3.40it/s] 66%|   | 513/780 [03:45<01:18,  3.40it/s] 66%|   | 514/780 [03:45<01:18,  3.40it/s] 66%|   | 515/780 [03:45<01:17,  3.40it/s] 66%|   | 516/780 [03:46<01:19,  3.30it/s] 66%|   | 517/780 [03:46<01:18,  3.33it/s] 66%|   | 518/780 [03:46<01:18,  3.35it/s] 67%|   | 519/780 [03:47<01:42,  2.54it/s] 67%|   | 520/780 [03:47<01:34,  2.75it/s] 67%|   | 521/780 [03:48<01:28,  2.92it/s] 67%|   | 522/780 [03:48<01:24,  3.04it/s] 67%|   | 523/780 [03:48<01:21,  3.14it/s] 67%|   | 524/780 [03:48<01:19,  3.22it/s] 67%|   | 525/780 [03:49<01:20,  3.18it/s] 67%|   | 526/780 [03:49<01:18,  3.24it/s] 68%|   | 527/780 [03:49<01:16,  3.29it/s] 68%|   | 528/780 [03:50<01:15,  3.32it/s] 68%|   | 529/780 [03:50<01:14,  3.35it/s] 68%|   | 530/780 [03:50<01:14,  3.36it/s] 68%|   | 531/780 [03:50<01:13,  3.37it/s] 68%|   | 532/780 [03:51<01:13,  3.39it/s] 68%|   | 533/780 [03:51<01:12,  3.39it/s] 68%|   | 534/780 [03:51<01:12,  3.40it/s] 69%|   | 535/780 [03:52<01:12,  3.40it/s] 69%|   | 536/780 [03:52<01:13,  3.32it/s] 69%|   | 537/780 [03:52<01:12,  3.35it/s] 69%|   | 538/780 [03:53<01:11,  3.37it/s] 69%|   | 539/780 [03:53<01:11,  3.38it/s] 69%|   | 540/780 [03:53<01:10,  3.38it/s] 69%|   | 541/780 [03:53<01:10,  3.39it/s] 69%|   | 542/780 [03:54<01:10,  3.40it/s] 70%|   | 543/780 [03:54<01:09,  3.40it/s] 70%|   | 544/780 [03:54<01:09,  3.40it/s] 70%|   | 545/780 [03:55<01:09,  3.40it/s] 70%|   | 546/780 [03:55<01:08,  3.40it/s] 70%|   | 547/780 [03:55<01:10,  3.31it/s] 70%|   | 548/780 [03:56<01:09,  3.34it/s] 70%|   | 549/780 [03:56<01:08,  3.36it/s] 71%|   | 550/780 [03:56<01:08,  3.37it/s] 71%|   | 551/780 [03:56<01:07,  3.38it/s] 71%|   | 552/780 [03:57<01:07,  3.39it/s] 71%|   | 553/780 [03:57<01:06,  3.40it/s] 71%|   | 554/780 [03:57<01:06,  3.39it/s] 71%|   | 555/780 [03:58<01:06,  3.40it/s] 71%|  | 556/780 [03:58<01:05,  3.40it/s] 71%|  | 557/780 [03:58<01:05,  3.40it/s] 72%|  | 558/780 [03:58<01:05,  3.40it/s] 72%|  | 559/780 [03:59<01:04,  3.40it/s] 72%|  | 560/780 [03:59<01:04,  3.40it/s] 72%|  | 561/780 [03:59<01:04,  3.40it/s] 72%|  | 562/780 [04:00<01:04,  3.40it/s] 72%|  | 563/780 [04:00<01:03,  3.40it/s] 72%|  | 564/780 [04:00<01:07,  3.18it/s] 72%|  | 565/780 [04:01<01:06,  3.25it/s] 73%|  | 566/780 [04:01<01:05,  3.29it/s] 73%|  | 567/780 [04:01<01:05,  3.24it/s] 73%|  | 568/780 [04:02<01:04,  3.29it/s] 73%|  | 569/780 [04:02<01:03,  3.32it/s] 73%|  | 570/780 [04:02<01:02,  3.34it/s] 73%|  | 571/780 [04:02<01:02,  3.36it/s] 73%|  | 572/780 [04:03<01:01,  3.37it/s] 73%|  | 573/780 [04:03<01:01,  3.38it/s] 74%|  | 574/780 [04:03<01:00,  3.39it/s] 74%|  | 575/780 [04:04<01:00,  3.40it/s] 74%|  | 576/780 [04:04<01:00,  3.40it/s] 74%|  | 577/780 [04:04<00:59,  3.40it/s] 74%|  | 578/780 [04:04<00:59,  3.40it/s] 74%|  | 579/780 [04:05<00:59,  3.40it/s] 74%|  | 580/780 [04:05<00:58,  3.40it/s] 74%|  | 581/780 [04:05<00:58,  3.40it/s] 75%|  | 582/780 [04:06<00:58,  3.40it/s] 75%|  | 583/780 [04:06<00:57,  3.40it/s] 75%|  | 584/780 [04:06<01:00,  3.24it/s] 75%|  | 585/780 [04:07<00:59,  3.29it/s] 75%|  | 586/780 [04:07<00:58,  3.32it/s] 75%|  | 587/780 [04:07<00:57,  3.35it/s] 75%|  | 588/780 [04:07<00:57,  3.36it/s] 76%|  | 589/780 [04:08<00:56,  3.37it/s] 76%|  | 590/780 [04:08<00:56,  3.38it/s] 76%|  | 591/780 [04:08<00:55,  3.39it/s] 76%|  | 592/780 [04:09<00:55,  3.39it/s] 76%|  | 593/780 [04:09<00:55,  3.40it/s] 76%|  | 594/780 [04:09<00:54,  3.40it/s] 76%|  | 595/780 [04:09<00:54,  3.40it/s] 76%|  | 596/780 [04:10<00:54,  3.40it/s] 77%|  | 597/780 [04:10<00:53,  3.40it/s] 77%|  | 598/780 [04:10<00:53,  3.40it/s] 77%|  | 599/780 [04:11<00:53,  3.39it/s] 77%|  | 600/780 [04:11<00:52,  3.40it/s] 77%|  | 601/780 [04:11<00:55,  3.25it/s] 77%|  | 602/780 [04:12<00:54,  3.29it/s] 77%|  | 603/780 [04:12<00:53,  3.32it/s] 77%|  | 604/780 [04:12<00:52,  3.35it/s] 78%|  | 605/780 [04:12<00:52,  3.36it/s] 78%|  | 606/780 [04:13<00:51,  3.37it/s] 78%|  | 607/780 [04:13<00:51,  3.38it/s] 78%|  | 608/780 [04:13<00:50,  3.39it/s] 78%|  | 609/780 [04:14<00:50,  3.39it/s] 78%|  | 610/780 [04:14<00:50,  3.39it/s] 78%|  | 611/780 [04:14<00:49,  3.40it/s] 78%|  | 612/780 [04:15<00:49,  3.40it/s] 79%|  | 613/780 [04:15<00:49,  3.40it/s] 79%|  | 614/780 [04:15<00:48,  3.40it/s] 79%|  | 615/780 [04:15<00:48,  3.40it/s] 79%|  | 616/780 [04:16<00:48,  3.40it/s] 79%|  | 617/780 [04:16<00:47,  3.40it/s] 79%|  | 618/780 [04:16<00:47,  3.40it/s] 79%|  | 619/780 [04:17<00:50,  3.18it/s] 79%|  | 620/780 [04:17<00:49,  3.24it/s] 80%|  | 621/780 [04:17<00:48,  3.29it/s] 80%|  | 622/780 [04:18<00:47,  3.32it/s] 80%|  | 623/780 [04:18<00:46,  3.34it/s] 80%|  | 624/780 [04:18<00:46,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 10:25:10,763 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:25:10,764 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:25:10,764 >>   Batch size = 8
{'eval_loss': 1.0316153764724731, 'eval_runtime': 9.7345, 'eval_samples_per_second': 358.417, 'eval_steps_per_second': 44.892, 'epoch': 3.0}
{'loss': 0.4155, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.40it/s][A
  3%|         | 12/437 [00:00<00:08, 49.19it/s][A
  4%|         | 17/437 [00:00<00:08, 47.54it/s][A
  5%|         | 22/437 [00:00<00:08, 46.73it/s][A
  6%|         | 27/437 [00:00<00:08, 45.93it/s][A
  7%|         | 32/437 [00:00<00:08, 45.82it/s][A
  8%|         | 37/437 [00:00<00:08, 45.60it/s][A
 10%|         | 42/437 [00:00<00:08, 45.37it/s][A
 11%|         | 47/437 [00:01<00:08, 45.29it/s][A
 12%|        | 52/437 [00:01<00:08, 45.38it/s][A
 13%|        | 57/437 [00:01<00:09, 39.31it/s][A
 14%|        | 62/437 [00:01<00:09, 41.08it/s][A
 15%|        | 67/437 [00:01<00:08, 42.25it/s][A
 16%|        | 72/437 [00:01<00:08, 43.21it/s][A
 18%|        | 77/437 [00:01<00:08, 43.95it/s][A
 19%|        | 82/437 [00:01<00:08, 44.36it/s][A
 20%|        | 87/437 [00:01<00:07, 44.72it/s][A
 21%|        | 92/437 [00:02<00:07, 44.90it/s][A
 22%|       | 97/437 [00:02<00:07, 44.75it/s][A
 23%|       | 102/437 [00:02<00:07, 44.84it/s][A
 24%|       | 107/437 [00:02<00:07, 44.86it/s][A
 26%|       | 112/437 [00:02<00:07, 45.03it/s][A
 27%|       | 117/437 [00:02<00:07, 45.18it/s][A
 28%|       | 122/437 [00:02<00:06, 45.29it/s][A
 29%|       | 127/437 [00:02<00:06, 45.36it/s][A
 30%|       | 132/437 [00:02<00:06, 45.43it/s][A
 31%|      | 137/437 [00:03<00:06, 45.40it/s][A
 32%|      | 142/437 [00:03<00:06, 45.31it/s][A
 34%|      | 147/437 [00:03<00:06, 45.16it/s][A
 35%|      | 152/437 [00:03<00:06, 45.14it/s][A
 36%|      | 157/437 [00:03<00:06, 45.16it/s][A
 37%|      | 162/437 [00:03<00:06, 45.22it/s][A
 38%|      | 167/437 [00:03<00:05, 45.35it/s][A
 39%|      | 172/437 [00:03<00:05, 45.41it/s][A
 41%|      | 177/437 [00:03<00:05, 45.35it/s][A
 42%|     | 182/437 [00:04<00:05, 45.30it/s][A
 43%|     | 187/437 [00:04<00:05, 45.25it/s][A
 44%|     | 192/437 [00:04<00:06, 39.97it/s][A
 45%|     | 197/437 [00:04<00:05, 41.61it/s][A
 46%|     | 202/437 [00:04<00:05, 42.83it/s][A
 47%|     | 207/437 [00:04<00:05, 43.69it/s][A
 49%|     | 212/437 [00:04<00:05, 44.31it/s][A
 50%|     | 217/437 [00:04<00:04, 44.62it/s][A
 51%|     | 222/437 [00:04<00:04, 44.87it/s][A
 52%|    | 227/437 [00:05<00:04, 44.96it/s][A
 53%|    | 232/437 [00:05<00:04, 44.55it/s][A
 54%|    | 237/437 [00:05<00:04, 44.50it/s][A
 55%|    | 242/437 [00:05<00:04, 44.72it/s][A
 57%|    | 247/437 [00:05<00:04, 44.93it/s][A
 58%|    | 252/437 [00:05<00:04, 45.12it/s][A
 59%|    | 257/437 [00:05<00:03, 45.25it/s][A
 60%|    | 262/437 [00:05<00:03, 45.31it/s][A
 61%|    | 267/437 [00:05<00:03, 45.43it/s][A
 62%|   | 272/437 [00:06<00:03, 45.37it/s][A
 63%|   | 277/437 [00:06<00:03, 45.28it/s][A
 65%|   | 282/437 [00:06<00:03, 45.06it/s][A
 66%|   | 287/437 [00:06<00:03, 44.91it/s][A
 67%|   | 292/437 [00:06<00:03, 45.03it/s][A
 68%|   | 297/437 [00:06<00:03, 45.04it/s][A
 69%|   | 302/437 [00:06<00:02, 45.20it/s][A
 70%|   | 307/437 [00:06<00:02, 45.25it/s][A
 71%|  | 312/437 [00:06<00:02, 45.41it/s][A
 73%|  | 317/437 [00:07<00:02, 45.38it/s][A
 74%|  | 322/437 [00:07<00:02, 45.40it/s][A
 75%|  | 327/437 [00:07<00:02, 39.79it/s][A
 76%|  | 332/437 [00:07<00:02, 41.44it/s][A
 77%|  | 337/437 [00:07<00:02, 42.57it/s][A
 78%|  | 342/437 [00:07<00:02, 43.46it/s][A
 79%|  | 347/437 [00:07<00:02, 44.09it/s][A
 81%|  | 352/437 [00:07<00:01, 44.51it/s][A
 82%| | 357/437 [00:08<00:01, 44.87it/s][A
 83%| | 362/437 [00:08<00:01, 44.99it/s][A
 84%| | 367/437 [00:08<00:01, 44.76it/s][A
 85%| | 372/437 [00:08<00:01, 44.65it/s][A
 86%| | 377/437 [00:08<00:01, 44.62it/s][A
 87%| | 382/437 [00:08<00:01, 44.82it/s][A
 89%| | 387/437 [00:08<00:01, 45.02it/s][A
 90%| | 392/437 [00:08<00:00, 45.15it/s][A
 91%| | 397/437 [00:08<00:00, 45.25it/s][A
 92%|| 402/437 [00:09<00:00, 45.37it/s][A
 93%|| 407/437 [00:09<00:00, 45.42it/s][A
 94%|| 412/437 [00:09<00:00, 45.28it/s][A
 95%|| 417/437 [00:09<00:00, 45.09it/s][A
 97%|| 422/437 [00:09<00:00, 44.97it/s][A
 98%|| 427/437 [00:09<00:00, 45.00it/s][A
 99%|| 432/437 [00:09<00:00, 45.03it/s][A
100%|| 437/437 [00:09<00:00, 45.24it/s][A                                                 
                                                 [A 80%|  | 624/780 [04:28<00:46,  3.36it/s]
100%|| 437/437 [00:09<00:00, 45.24it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:25:20,779 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 10:25:20,986 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:25:24,840 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:25:24,981 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:25:25,057 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|  | 625/780 [04:43<19:29,  7.54s/it] 80%|  | 626/780 [04:43<13:48,  5.38s/it] 80%|  | 627/780 [04:43<09:49,  3.85s/it] 81%|  | 628/780 [04:44<07:03,  2.79s/it] 81%|  | 629/780 [04:44<05:07,  2.04s/it] 81%|  | 630/780 [04:44<03:47,  1.51s/it] 81%|  | 631/780 [04:44<02:51,  1.15s/it] 81%|  | 632/780 [04:45<02:12,  1.12it/s] 81%|  | 633/780 [04:45<01:44,  1.40it/s] 81%| | 634/780 [04:45<01:25,  1.70it/s] 81%| | 635/780 [04:46<01:12,  2.00it/s] 82%| | 636/780 [04:46<01:02,  2.29it/s] 82%| | 637/780 [04:46<00:58,  2.46it/s] 82%| | 638/780 [04:46<00:52,  2.68it/s] 82%| | 639/780 [04:47<00:59,  2.38it/s] 82%| | 640/780 [04:47<00:53,  2.62it/s] 82%| | 641/780 [04:48<00:49,  2.83it/s] 82%| | 642/780 [04:48<00:46,  2.99it/s] 82%| | 643/780 [04:48<00:43,  3.12it/s] 83%| | 644/780 [04:48<00:42,  3.21it/s] 83%| | 645/780 [04:49<00:41,  3.27it/s] 83%| | 646/780 [04:49<00:40,  3.31it/s] 83%| | 647/780 [04:49<00:41,  3.19it/s] 83%| | 648/780 [04:50<00:40,  3.25it/s] 83%| | 649/780 [04:50<00:39,  3.29it/s] 83%| | 650/780 [04:50<00:39,  3.33it/s] 83%| | 651/780 [04:51<00:38,  3.35it/s] 84%| | 652/780 [04:51<00:38,  3.36it/s] 84%| | 653/780 [04:51<00:37,  3.38it/s] 84%| | 654/780 [04:51<00:37,  3.38it/s] 84%| | 655/780 [04:52<00:36,  3.38it/s] 84%| | 656/780 [04:52<00:36,  3.39it/s] 84%| | 657/780 [04:52<00:36,  3.40it/s] 84%| | 658/780 [04:53<00:37,  3.30it/s] 84%| | 659/780 [04:53<00:36,  3.33it/s] 85%| | 660/780 [04:53<00:35,  3.35it/s] 85%| | 661/780 [04:54<00:35,  3.37it/s] 85%| | 662/780 [04:54<00:34,  3.38it/s] 85%| | 663/780 [04:54<00:34,  3.38it/s] 85%| | 664/780 [04:54<00:34,  3.39it/s] 85%| | 665/780 [04:55<00:33,  3.40it/s] 85%| | 666/780 [04:55<00:33,  3.40it/s] 86%| | 667/780 [04:55<00:33,  3.40it/s] 86%| | 668/780 [04:56<00:32,  3.40it/s] 86%| | 669/780 [04:56<00:33,  3.31it/s] 86%| | 670/780 [04:56<00:32,  3.34it/s] 86%| | 671/780 [04:56<00:32,  3.36it/s] 86%| | 672/780 [04:57<00:31,  3.38it/s] 86%| | 673/780 [04:57<00:31,  3.38it/s] 86%| | 674/780 [04:57<00:31,  3.39it/s] 87%| | 675/780 [04:58<00:30,  3.39it/s] 87%| | 676/780 [04:58<00:30,  3.40it/s] 87%| | 677/780 [04:58<00:30,  3.40it/s] 87%| | 678/780 [04:59<00:30,  3.40it/s] 87%| | 679/780 [04:59<00:29,  3.40it/s] 87%| | 680/780 [04:59<00:30,  3.29it/s] 87%| | 681/780 [05:00<00:31,  3.19it/s] 87%| | 682/780 [05:00<00:30,  3.25it/s] 88%| | 683/780 [05:00<00:29,  3.29it/s] 88%| | 684/780 [05:00<00:30,  3.15it/s] 88%| | 685/780 [05:01<00:29,  3.22it/s] 88%| | 686/780 [05:01<00:28,  3.27it/s] 88%| | 687/780 [05:01<00:28,  3.31it/s] 88%| | 688/780 [05:02<00:27,  3.34it/s] 88%| | 689/780 [05:02<00:27,  3.35it/s] 88%| | 690/780 [05:02<00:26,  3.36it/s] 89%| | 691/780 [05:03<00:27,  3.24it/s] 89%| | 692/780 [05:03<00:26,  3.29it/s] 89%| | 693/780 [05:03<00:26,  3.32it/s] 89%| | 694/780 [05:03<00:25,  3.35it/s] 89%| | 695/780 [05:04<00:25,  3.36it/s] 89%| | 696/780 [05:04<00:24,  3.37it/s] 89%| | 697/780 [05:04<00:24,  3.38it/s] 89%| | 698/780 [05:05<00:24,  3.39it/s] 90%| | 699/780 [05:05<00:23,  3.39it/s] 90%| | 700/780 [05:05<00:23,  3.39it/s] 90%| | 701/780 [05:05<00:23,  3.40it/s] 90%| | 702/780 [05:06<00:23,  3.27it/s] 90%| | 703/780 [05:06<00:23,  3.31it/s] 90%| | 704/780 [05:06<00:22,  3.34it/s] 90%| | 705/780 [05:07<00:22,  3.36it/s] 91%| | 706/780 [05:07<00:21,  3.37it/s] 91%| | 707/780 [05:07<00:21,  3.38it/s] 91%| | 708/780 [05:08<00:21,  3.39it/s] 91%| | 709/780 [05:08<00:20,  3.39it/s] 91%| | 710/780 [05:08<00:20,  3.39it/s] 91%| | 711/780 [05:08<00:20,  3.40it/s] 91%|| 712/780 [05:09<00:20,  3.40it/s] 91%|| 713/780 [05:09<00:20,  3.33it/s] 92%|| 714/780 [05:09<00:19,  3.36it/s] 92%|| 715/780 [05:10<00:19,  3.37it/s] 92%|| 716/780 [05:10<00:18,  3.38it/s] 92%|| 717/780 [05:10<00:18,  3.39it/s] 92%|| 718/780 [05:11<00:18,  3.39it/s] 92%|| 719/780 [05:11<00:17,  3.39it/s] 92%|| 720/780 [05:11<00:17,  3.39it/s] 92%|| 721/780 [05:11<00:17,  3.40it/s] 93%|| 722/780 [05:12<00:17,  3.40it/s] 93%|| 723/780 [05:12<00:16,  3.40it/s] 93%|| 724/780 [05:12<00:16,  3.29it/s] 93%|| 725/780 [05:13<00:16,  3.32it/s] 93%|| 726/780 [05:13<00:16,  3.35it/s] 93%|| 727/780 [05:13<00:15,  3.36it/s] 93%|| 728/780 [05:14<00:15,  3.37it/s] 93%|| 729/780 [05:14<00:15,  3.38it/s] 94%|| 730/780 [05:14<00:14,  3.38it/s] 94%|| 731/780 [05:14<00:14,  3.39it/s] 94%|| 732/780 [05:15<00:14,  3.39it/s] 94%|| 733/780 [05:15<00:13,  3.39it/s] 94%|| 734/780 [05:15<00:13,  3.40it/s] 94%|| 735/780 [05:16<00:14,  3.19it/s] 94%|| 736/780 [05:16<00:13,  3.25it/s] 94%|| 737/780 [05:16<00:13,  3.29it/s] 95%|| 738/780 [05:17<00:12,  3.33it/s] 95%|| 739/780 [05:17<00:12,  3.35it/s] 95%|| 740/780 [05:17<00:11,  3.36it/s] 95%|| 741/780 [05:17<00:11,  3.38it/s] 95%|| 742/780 [05:18<00:11,  3.40it/s] 95%|| 743/780 [05:18<00:10,  3.42it/s] 95%|| 744/780 [05:18<00:10,  3.43it/s] 96%|| 745/780 [05:19<00:10,  3.43it/s] 96%|| 746/780 [05:19<00:10,  3.14it/s] 96%|| 747/780 [05:19<00:10,  3.23it/s] 96%|| 748/780 [05:20<00:09,  3.29it/s] 96%|| 749/780 [05:20<00:09,  3.34it/s] 96%|| 750/780 [05:20<00:08,  3.37it/s] 96%|| 751/780 [05:20<00:08,  3.39it/s] 96%|| 752/780 [05:21<00:08,  3.41it/s] 97%|| 753/780 [05:21<00:07,  3.42it/s] 97%|| 754/780 [05:21<00:07,  3.43it/s] 97%|| 755/780 [05:22<00:07,  3.44it/s] 97%|| 756/780 [05:22<00:06,  3.44it/s] 97%|| 757/780 [05:22<00:07,  3.18it/s] 97%|| 758/780 [05:22<00:06,  3.26it/s] 97%|| 759/780 [05:23<00:06,  3.31it/s] 97%|| 760/780 [05:23<00:05,  3.35it/s] 98%|| 761/780 [05:23<00:05,  3.38it/s] 98%|| 762/780 [05:24<00:05,  3.40it/s] 98%|| 763/780 [05:24<00:04,  3.42it/s] 98%|| 764/780 [05:24<00:04,  3.43it/s] 98%|| 765/780 [05:25<00:04,  3.43it/s] 98%|| 766/780 [05:25<00:04,  3.44it/s] 98%|| 767/780 [05:25<00:03,  3.44it/s] 98%|| 768/780 [05:25<00:03,  3.26it/s] 99%|| 769/780 [05:26<00:03,  3.32it/s] 99%|| 770/780 [05:26<00:02,  3.36it/s] 99%|| 771/780 [05:26<00:02,  3.38it/s] 99%|| 772/780 [05:27<00:02,  3.40it/s] 99%|| 773/780 [05:27<00:02,  3.42it/s] 99%|| 774/780 [05:27<00:01,  3.43it/s] 99%|| 775/780 [05:27<00:01,  3.43it/s] 99%|| 776/780 [05:28<00:01,  3.44it/s]100%|| 777/780 [05:28<00:00,  3.44it/s]100%|| 778/780 [05:28<00:00,  3.44it/s]100%|| 779/780 [05:29<00:00,  3.01it/s]100%|| 780/780 [05:29<00:00,  3.13it/s][INFO|trainer.py:2140] 2023-08-29 10:26:21,658 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:26:21,658 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:26:21,658 >>   Batch size = 8
{'eval_loss': 1.043361783027649, 'eval_runtime': 9.8247, 'eval_samples_per_second': 355.126, 'eval_steps_per_second': 44.48, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|         | 6/437 [00:00<00:07, 56.72it/s][A
  3%|         | 12/437 [00:00<00:08, 49.66it/s][A
  4%|         | 18/437 [00:00<00:08, 47.12it/s][A
  5%|         | 23/437 [00:00<00:08, 46.37it/s][A
  6%|         | 28/437 [00:00<00:08, 45.91it/s][A
  8%|         | 33/437 [00:00<00:08, 45.68it/s][A
  9%|         | 38/437 [00:00<00:08, 45.45it/s][A
 10%|         | 43/437 [00:00<00:10, 39.25it/s][A
 11%|         | 48/437 [00:01<00:09, 40.98it/s][A
 12%|        | 53/437 [00:01<00:09, 42.42it/s][A
 13%|        | 58/437 [00:01<00:08, 43.13it/s][A
 14%|        | 63/437 [00:01<00:08, 43.98it/s][A
 16%|        | 68/437 [00:01<00:08, 44.37it/s][A
 17%|        | 73/437 [00:01<00:08, 44.82it/s][A
 18%|        | 78/437 [00:01<00:08, 44.79it/s][A
 19%|        | 83/437 [00:01<00:07, 44.64it/s][A
 20%|        | 88/437 [00:01<00:07, 44.45it/s][A
 21%|       | 93/437 [00:02<00:07, 44.57it/s][A
 22%|       | 98/437 [00:02<00:07, 44.88it/s][A
 24%|       | 103/437 [00:02<00:07, 45.03it/s][A
 25%|       | 108/437 [00:02<00:07, 45.15it/s][A
 26%|       | 113/437 [00:02<00:07, 45.15it/s][A
 27%|       | 118/437 [00:02<00:07, 45.15it/s][A
 28%|       | 123/437 [00:02<00:06, 45.28it/s][A
 29%|       | 128/437 [00:02<00:06, 45.17it/s][A
 30%|       | 133/437 [00:02<00:06, 44.98it/s][A
 32%|      | 138/437 [00:03<00:06, 44.92it/s][A
 33%|      | 143/437 [00:03<00:06, 44.96it/s][A
 34%|      | 148/437 [00:03<00:06, 45.11it/s][A
 35%|      | 153/437 [00:03<00:06, 45.28it/s][A
 36%|      | 158/437 [00:03<00:06, 45.32it/s][A
 37%|      | 163/437 [00:03<00:06, 45.37it/s][A
 38%|      | 168/437 [00:03<00:05, 45.40it/s][A
 40%|      | 173/437 [00:03<00:05, 45.28it/s][A
 41%|      | 178/437 [00:03<00:06, 41.72it/s][A
 42%|     | 183/437 [00:04<00:05, 42.76it/s][A
 43%|     | 188/437 [00:04<00:05, 43.57it/s][A
 44%|     | 193/437 [00:04<00:05, 44.12it/s][A
 45%|     | 198/437 [00:04<00:05, 44.47it/s][A
 46%|     | 203/437 [00:04<00:05, 44.83it/s][A
 48%|     | 208/437 [00:04<00:05, 44.88it/s][A
 49%|     | 213/437 [00:04<00:04, 44.92it/s][A
 50%|     | 218/437 [00:04<00:04, 44.78it/s][A
 51%|     | 223/437 [00:04<00:04, 44.70it/s][A
 52%|    | 228/437 [00:05<00:04, 44.90it/s][A
 53%|    | 233/437 [00:05<00:04, 45.08it/s][A
 54%|    | 238/437 [00:05<00:04, 45.08it/s][A
 56%|    | 243/437 [00:05<00:04, 45.31it/s][A
 57%|    | 248/437 [00:05<00:04, 45.35it/s][A
 58%|    | 253/437 [00:05<00:04, 45.34it/s][A
 59%|    | 258/437 [00:05<00:03, 45.19it/s][A
 60%|    | 263/437 [00:05<00:03, 45.01it/s][A
 61%|   | 268/437 [00:05<00:03, 44.80it/s][A
 62%|   | 273/437 [00:06<00:03, 44.95it/s][A
 64%|   | 278/437 [00:06<00:03, 45.12it/s][A
 65%|   | 283/437 [00:06<00:03, 45.15it/s][A
 66%|   | 288/437 [00:06<00:03, 45.30it/s][A
 67%|   | 293/437 [00:06<00:03, 45.32it/s][A
 68%|   | 298/437 [00:06<00:03, 45.27it/s][A
 69%|   | 303/437 [00:06<00:02, 45.26it/s][A
 70%|   | 308/437 [00:06<00:02, 45.04it/s][A
 72%|  | 313/437 [00:07<00:02, 42.93it/s][A
 73%|  | 318/437 [00:07<00:02, 43.70it/s][A
 74%|  | 323/437 [00:07<00:02, 44.13it/s][A
 75%|  | 328/437 [00:07<00:02, 44.52it/s][A
 76%|  | 333/437 [00:07<00:02, 44.81it/s][A
 77%|  | 338/437 [00:07<00:02, 45.00it/s][A
 78%|  | 343/437 [00:07<00:02, 45.16it/s][A
 80%|  | 348/437 [00:07<00:01, 45.12it/s][A
 81%|  | 353/437 [00:07<00:01, 44.78it/s][A
 82%| | 358/437 [00:08<00:01, 44.79it/s][A
 83%| | 363/437 [00:08<00:01, 44.86it/s][A
 84%| | 368/437 [00:08<00:01, 45.01it/s][A
 85%| | 373/437 [00:08<00:01, 44.06it/s][A
 86%| | 378/437 [00:08<00:01, 44.54it/s][A
 88%| | 383/437 [00:08<00:01, 44.87it/s][A
 89%| | 388/437 [00:08<00:01, 45.03it/s][A
 90%| | 393/437 [00:08<00:00, 45.19it/s][A
 91%| | 398/437 [00:08<00:00, 45.03it/s][A
 92%|| 403/437 [00:09<00:00, 45.08it/s][A
 93%|| 408/437 [00:09<00:00, 45.01it/s][A
 95%|| 413/437 [00:09<00:00, 45.08it/s][A
 96%|| 418/437 [00:09<00:00, 45.14it/s][A
 97%|| 423/437 [00:09<00:00, 45.26it/s][A
 98%|| 428/437 [00:09<00:00, 45.20it/s][A
 99%|| 433/437 [00:09<00:00, 45.28it/s][A                                                 
                                                 [A100%|| 780/780 [05:39<00:00,  3.13it/s]
100%|| 437/437 [00:09<00:00, 45.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:26:31,711 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 10:26:32,149 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:26:37,077 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:26:37,323 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:26:37,442 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:26:46,761 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:26:46,793 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156 (score: 0.9992029666900635).
                                                 100%|| 780/780 [06:08<00:00,  3.13it/s]100%|| 780/780 [06:08<00:00,  2.12it/s]
[INFO|trainer.py:1894] 2023-08-29 10:27:00,453 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 10:27:00,672 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:27:04,534 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:27:04,705 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:27:04,830 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:27:05,522 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   train_loss               =     0.4078
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   train_runtime            = 0:06:08.19
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   train_samples_per_second =    135.798
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:05,523 >>   train_steps_per_second   =      2.118
{'eval_loss': 1.046210765838623, 'eval_runtime': 9.7694, 'eval_samples_per_second': 357.134, 'eval_steps_per_second': 44.731, 'epoch': 5.0}
{'train_runtime': 368.1927, 'train_samples_per_second': 135.798, 'train_steps_per_second': 2.118, 'train_loss': 0.40780297303811097, 'epoch': 5.0}
08/29/2023 10:27:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:27:05,906 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:27:05,906 >>   Num examples = 3489
[INFO|trainer.py:2145] 2023-08-29 10:27:05,906 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|         | 6/437 [00:00<00:07, 55.59it/s]  3%|         | 12/437 [00:00<00:08, 49.44it/s]  4%|         | 17/437 [00:00<00:08, 47.92it/s]  5%|         | 22/437 [00:00<00:08, 47.14it/s]  6%|         | 27/437 [00:00<00:08, 46.69it/s]  7%|         | 32/437 [00:00<00:09, 43.38it/s]  8%|         | 37/437 [00:00<00:09, 44.11it/s] 10%|         | 42/437 [00:00<00:08, 44.47it/s] 11%|         | 47/437 [00:01<00:08, 44.54it/s] 12%|        | 52/437 [00:01<00:08, 44.77it/s] 13%|        | 57/437 [00:01<00:08, 45.09it/s] 14%|        | 62/437 [00:01<00:08, 45.24it/s] 15%|        | 67/437 [00:01<00:08, 45.33it/s] 16%|        | 72/437 [00:01<00:08, 45.26it/s] 18%|        | 77/437 [00:01<00:07, 45.42it/s] 19%|        | 82/437 [00:01<00:07, 45.60it/s] 20%|        | 87/437 [00:01<00:07, 45.50it/s] 21%|        | 92/437 [00:02<00:07, 45.33it/s] 22%|       | 97/437 [00:02<00:07, 45.19it/s] 23%|       | 102/437 [00:02<00:07, 45.12it/s] 24%|       | 107/437 [00:02<00:07, 45.32it/s] 26%|       | 112/437 [00:02<00:07, 45.50it/s] 27%|       | 117/437 [00:02<00:07, 45.50it/s] 28%|       | 122/437 [00:02<00:06, 45.47it/s] 29%|       | 127/437 [00:02<00:06, 45.52it/s] 30%|       | 132/437 [00:02<00:06, 45.37it/s] 31%|      | 137/437 [00:03<00:06, 45.29it/s] 32%|      | 142/437 [00:03<00:06, 45.23it/s] 34%|      | 147/437 [00:03<00:06, 45.12it/s] 35%|      | 152/437 [00:03<00:06, 45.18it/s] 36%|      | 157/437 [00:03<00:06, 45.38it/s] 37%|      | 162/437 [00:03<00:06, 45.41it/s] 38%|      | 167/437 [00:03<00:05, 45.44it/s] 39%|      | 172/437 [00:03<00:06, 39.01it/s] 41%|      | 177/437 [00:03<00:06, 40.91it/s] 42%|     | 182/437 [00:04<00:06, 42.37it/s] 43%|     | 187/437 [00:04<00:05, 43.38it/s] 44%|     | 192/437 [00:04<00:05, 44.11it/s] 45%|     | 197/437 [00:04<00:05, 44.67it/s] 46%|     | 202/437 [00:04<00:05, 45.03it/s] 47%|     | 207/437 [00:04<00:05, 45.20it/s] 49%|     | 212/437 [00:04<00:05, 44.90it/s] 50%|     | 217/437 [00:04<00:04, 44.69it/s] 51%|     | 222/437 [00:04<00:04, 44.68it/s] 52%|    | 227/437 [00:05<00:04, 44.88it/s] 53%|    | 232/437 [00:05<00:04, 45.14it/s] 54%|    | 237/437 [00:05<00:04, 45.36it/s] 55%|    | 242/437 [00:05<00:04, 45.57it/s] 57%|    | 247/437 [00:05<00:04, 45.65it/s] 58%|    | 252/437 [00:05<00:04, 45.63it/s] 59%|    | 257/437 [00:05<00:03, 45.42it/s] 60%|    | 262/437 [00:05<00:03, 45.06it/s] 61%|    | 267/437 [00:05<00:03, 44.96it/s] 62%|   | 272/437 [00:06<00:03, 45.02it/s] 63%|   | 277/437 [00:06<00:03, 45.26it/s] 65%|   | 282/437 [00:06<00:03, 45.28it/s] 66%|   | 287/437 [00:06<00:03, 45.56it/s] 67%|   | 292/437 [00:06<00:03, 45.59it/s] 68%|   | 297/437 [00:06<00:03, 45.75it/s] 69%|   | 302/437 [00:06<00:02, 45.52it/s] 70%|   | 307/437 [00:06<00:03, 37.75it/s] 71%|  | 312/437 [00:06<00:03, 39.84it/s] 73%|  | 317/437 [00:07<00:02, 41.47it/s] 74%|  | 322/437 [00:07<00:02, 42.59it/s] 75%|  | 327/437 [00:07<00:02, 43.43it/s] 76%|  | 332/437 [00:07<00:02, 44.17it/s] 77%|  | 337/437 [00:07<00:02, 44.61it/s] 78%|  | 342/437 [00:07<00:02, 44.92it/s] 79%|  | 347/437 [00:07<00:02, 44.70it/s] 81%|  | 352/437 [00:07<00:01, 44.81it/s] 82%| | 357/437 [00:07<00:01, 45.00it/s] 83%| | 362/437 [00:08<00:01, 45.27it/s] 84%| | 367/437 [00:08<00:01, 45.39it/s] 85%| | 372/437 [00:08<00:01, 45.48it/s] 86%| | 377/437 [00:08<00:01, 45.42it/s] 87%| | 382/437 [00:08<00:01, 45.36it/s] 89%| | 387/437 [00:08<00:01, 45.30it/s] 90%| | 392/437 [00:08<00:00, 45.19it/s] 91%| | 397/437 [00:08<00:00, 45.07it/s] 92%|| 402/437 [00:08<00:00, 45.06it/s] 93%|| 407/437 [00:09<00:00, 45.19it/s] 94%|| 412/437 [00:09<00:00, 45.35it/s] 95%|| 417/437 [00:09<00:00, 45.51it/s] 97%|| 422/437 [00:09<00:00, 45.54it/s] 98%|| 427/437 [00:09<00:00, 45.51it/s] 99%|| 432/437 [00:09<00:00, 45.40it/s]100%|| 437/437 [00:09<00:00, 45.30it/s]100%|| 437/437 [00:09<00:00, 44.42it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:27:15,760 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   eval_loss               =     0.9992
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   eval_runtime            = 0:00:09.85
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   eval_samples            =       3489
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   eval_samples_per_second =    354.054
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   eval_steps_per_second   =     44.346
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:27:15,761 >>   perplexity              =     2.7161
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:29,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:29,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:29,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:29,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:29,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:27:29,417 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:27:29,418 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:27:29,710 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:27:30,771 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:27:30,771 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:32,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:32,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:32,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:32,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:27:32,342 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:27:32,692 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:27:32,693 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:27:32,976 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:27:33,120 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:27:33,120 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/dev.jsonl', 'labels': ['has part', 'location', 'member of political party', 'platform', 'position held'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13258
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13358, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.60it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.52it/s]Extractor Predicting: 50it [00:32,  1.42it/s]Extractor Predicting: 51it [00:33,  1.44it/s]Extractor Predicting: 52it [00:34,  1.45it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.44it/s]Extractor Predicting: 58it [00:38,  1.46it/s]Extractor Predicting: 59it [00:38,  1.45it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:44,  1.54it/s]Extractor Predicting: 68it [00:44,  1.55it/s]Extractor Predicting: 69it [00:45,  1.58it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:46,  1.59it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:47,  1.57it/s]Extractor Predicting: 74it [00:48,  1.59it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:51,  1.59it/s]Extractor Predicting: 80it [00:52,  1.60it/s]Extractor Predicting: 81it [00:52,  1.60it/s]Extractor Predicting: 82it [00:53,  1.59it/s]Extractor Predicting: 83it [00:54,  1.59it/s]Extractor Predicting: 84it [00:54,  1.57it/s]Extractor Predicting: 85it [00:55,  1.60it/s]Extractor Predicting: 86it [00:55,  1.61it/s]Extractor Predicting: 87it [00:56,  1.58it/s]Extractor Predicting: 88it [00:57,  1.59it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:58,  1.60it/s]Extractor Predicting: 91it [00:59,  1.59it/s]Extractor Predicting: 92it [00:59,  1.55it/s]Extractor Predicting: 93it [01:00,  1.56it/s]Extractor Predicting: 94it [01:01,  1.55it/s]Extractor Predicting: 95it [01:01,  1.54it/s]Extractor Predicting: 96it [01:02,  1.54it/s]Extractor Predicting: 97it [01:03,  1.55it/s]Extractor Predicting: 98it [01:03,  1.58it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:06,  1.55it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.51it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:08,  1.52it/s]Extractor Predicting: 107it [01:09,  1.55it/s]Extractor Predicting: 108it [01:10,  1.56it/s]Extractor Predicting: 109it [01:10,  1.49it/s]Extractor Predicting: 110it [01:11,  1.50it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:12,  1.51it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.51it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:15,  1.41it/s]Extractor Predicting: 117it [01:16,  1.45it/s]Extractor Predicting: 118it [01:16,  1.48it/s]Extractor Predicting: 119it [01:17,  1.51it/s]Extractor Predicting: 120it [01:18,  1.56it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:19,  1.59it/s]Extractor Predicting: 123it [01:20,  1.57it/s]Extractor Predicting: 124it [01:20,  1.53it/s]Extractor Predicting: 125it [01:21,  1.56it/s]Extractor Predicting: 126it [01:22,  1.57it/s]Extractor Predicting: 127it [01:22,  1.57it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:23,  1.56it/s]Extractor Predicting: 130it [01:24,  1.53it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:25,  1.56it/s]Extractor Predicting: 133it [01:26,  1.55it/s]Extractor Predicting: 134it [01:27,  1.53it/s]Extractor Predicting: 135it [01:27,  1.56it/s]Extractor Predicting: 136it [01:28,  1.57it/s]Extractor Predicting: 137it [01:29,  1.55it/s]Extractor Predicting: 138it [01:29,  1.54it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.56it/s]Extractor Predicting: 141it [01:31,  1.54it/s]Extractor Predicting: 142it [01:32,  1.55it/s]Extractor Predicting: 143it [01:32,  1.57it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:33,  2.06it/s]Extractor Predicting: 145it [01:33,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:30,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:31,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:31,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:31,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:31,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:29:32,092 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:29:32,093 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:29:32,449 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:29:33,603 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:29:33,603 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:35,170 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:35,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:35,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:35,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:29:35,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:29:35,821 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:29:35,822 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:29:36,186 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:29:36,447 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:29:36,448 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4381084840055633,
  "recall": 0.18056749785038692,
  "score": 0.25573371219809216,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 25753
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25853, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:21,  1.46it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.58it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.58it/s]Extractor Predicting: 59it [00:38,  1.59it/s]Extractor Predicting: 60it [00:39,  1.66it/s]Extractor Predicting: 61it [00:39,  1.68it/s]Extractor Predicting: 62it [00:40,  1.70it/s]Extractor Predicting: 63it [00:40,  1.71it/s]Extractor Predicting: 64it [00:41,  1.74it/s]Extractor Predicting: 65it [00:41,  1.75it/s]Extractor Predicting: 66it [00:42,  1.70it/s]Extractor Predicting: 67it [00:43,  1.73it/s]Extractor Predicting: 68it [00:43,  1.78it/s]Extractor Predicting: 69it [00:44,  1.82it/s]Extractor Predicting: 70it [00:44,  1.80it/s]Extractor Predicting: 71it [00:45,  1.80it/s]Extractor Predicting: 72it [00:45,  1.81it/s]Extractor Predicting: 73it [00:46,  1.81it/s]Extractor Predicting: 74it [00:46,  1.79it/s]Extractor Predicting: 75it [00:47,  1.73it/s]Extractor Predicting: 76it [00:48,  1.78it/s]Extractor Predicting: 77it [00:48,  1.81it/s]Extractor Predicting: 78it [00:49,  1.79it/s]Extractor Predicting: 79it [00:49,  1.81it/s]Extractor Predicting: 80it [00:50,  1.82it/s]Extractor Predicting: 81it [00:50,  1.80it/s]Extractor Predicting: 82it [00:51,  1.79it/s]Extractor Predicting: 83it [00:51,  1.77it/s]Extractor Predicting: 84it [00:52,  1.74it/s]Extractor Predicting: 85it [00:53,  1.76it/s]Extractor Predicting: 86it [00:53,  1.72it/s]Extractor Predicting: 87it [00:54,  1.67it/s]Extractor Predicting: 88it [00:54,  1.64it/s]Extractor Predicting: 89it [00:55,  1.61it/s]Extractor Predicting: 90it [00:56,  1.56it/s]Extractor Predicting: 91it [00:56,  1.56it/s]Extractor Predicting: 92it [00:57,  1.51it/s]Extractor Predicting: 93it [00:58,  1.56it/s]Extractor Predicting: 94it [00:58,  1.55it/s]Extractor Predicting: 95it [00:59,  1.55it/s]Extractor Predicting: 96it [01:00,  1.54it/s]Extractor Predicting: 97it [01:00,  1.54it/s]Extractor Predicting: 98it [01:01,  1.54it/s]Extractor Predicting: 99it [01:02,  1.55it/s]Extractor Predicting: 100it [01:02,  1.53it/s]Extractor Predicting: 101it [01:03,  1.54it/s]Extractor Predicting: 102it [01:04,  1.53it/s]Extractor Predicting: 103it [01:04,  1.56it/s]Extractor Predicting: 104it [01:05,  1.52it/s]Extractor Predicting: 105it [01:06,  1.53it/s]Extractor Predicting: 106it [01:06,  1.50it/s]Extractor Predicting: 107it [01:07,  1.49it/s]Extractor Predicting: 108it [01:08,  1.48it/s]Extractor Predicting: 109it [01:08,  1.51it/s]Extractor Predicting: 110it [01:09,  1.51it/s]Extractor Predicting: 111it [01:10,  1.50it/s]Extractor Predicting: 112it [01:10,  1.47it/s]Extractor Predicting: 113it [01:11,  1.48it/s]Extractor Predicting: 114it [01:12,  1.53it/s]Extractor Predicting: 115it [01:12,  1.50it/s]Extractor Predicting: 116it [01:13,  1.53it/s]Extractor Predicting: 117it [01:14,  1.56it/s]Extractor Predicting: 118it [01:14,  1.52it/s]Extractor Predicting: 119it [01:15,  1.48it/s]Extractor Predicting: 120it [01:16,  1.50it/s]Extractor Predicting: 121it [01:16,  1.49it/s]Extractor Predicting: 122it [01:17,  1.52it/s]Extractor Predicting: 123it [01:18,  1.51it/s]Extractor Predicting: 124it [01:18,  1.51it/s]Extractor Predicting: 125it [01:19,  1.50it/s]Extractor Predicting: 126it [01:20,  1.48it/s]Extractor Predicting: 127it [01:20,  1.48it/s]Extractor Predicting: 128it [01:21,  1.46it/s]Extractor Predicting: 129it [01:22,  1.47it/s]Extractor Predicting: 130it [01:22,  1.46it/s]Extractor Predicting: 131it [01:23,  1.46it/s]Extractor Predicting: 132it [01:24,  1.48it/s]Extractor Predicting: 133it [01:24,  1.49it/s]Extractor Predicting: 134it [01:25,  1.52it/s]Extractor Predicting: 135it [01:26,  1.51it/s]Extractor Predicting: 136it [01:26,  1.48it/s]Extractor Predicting: 137it [01:27,  1.49it/s]Extractor Predicting: 138it [01:28,  1.45it/s]Extractor Predicting: 139it [01:29,  1.30it/s]Extractor Predicting: 140it [01:29,  1.36it/s]Extractor Predicting: 141it [01:30,  1.39it/s]Extractor Predicting: 142it [01:31,  1.42it/s]Extractor Predicting: 143it [01:31,  1.40it/s]Extractor Predicting: 144it [01:32,  1.42it/s]Extractor Predicting: 145it [01:33,  1.41it/s]Extractor Predicting: 146it [01:34,  1.43it/s]Extractor Predicting: 147it [01:34,  1.47it/s]Extractor Predicting: 148it [01:35,  1.50it/s]Extractor Predicting: 149it [01:35,  1.47it/s]Extractor Predicting: 150it [01:36,  1.47it/s]Extractor Predicting: 151it [01:37,  1.50it/s]Extractor Predicting: 152it [01:37,  1.53it/s]Extractor Predicting: 153it [01:38,  1.50it/s]Extractor Predicting: 154it [01:39,  1.53it/s]Extractor Predicting: 155it [01:39,  1.55it/s]Extractor Predicting: 156it [01:40,  1.52it/s]Extractor Predicting: 157it [01:41,  1.53it/s]Extractor Predicting: 158it [01:41,  1.57it/s]Extractor Predicting: 159it [01:42,  1.62it/s]Extractor Predicting: 160it [01:43,  1.59it/s]Extractor Predicting: 161it [01:43,  1.54it/s]Extractor Predicting: 162it [01:44,  1.52it/s]Extractor Predicting: 163it [01:45,  1.54it/s]Extractor Predicting: 164it [01:45,  1.53it/s]Extractor Predicting: 165it [01:46,  1.53it/s]Extractor Predicting: 166it [01:47,  1.53it/s]Extractor Predicting: 167it [01:47,  1.53it/s]Extractor Predicting: 168it [01:48,  1.53it/s]Extractor Predicting: 169it [01:49,  1.50it/s]Extractor Predicting: 170it [01:49,  1.54it/s]Extractor Predicting: 171it [01:50,  1.53it/s]Extractor Predicting: 172it [01:50,  1.52it/s]Extractor Predicting: 173it [01:51,  1.54it/s]Extractor Predicting: 174it [01:52,  1.54it/s]Extractor Predicting: 175it [01:52,  1.54it/s]Extractor Predicting: 176it [01:53,  1.54it/s]Extractor Predicting: 177it [01:54,  1.56it/s]Extractor Predicting: 178it [01:54,  1.56it/s]Extractor Predicting: 179it [01:55,  1.58it/s]Extractor Predicting: 180it [01:56,  1.60it/s]Extractor Predicting: 181it [01:56,  1.58it/s]Extractor Predicting: 182it [01:57,  1.63it/s]Extractor Predicting: 183it [01:57,  1.63it/s]Extractor Predicting: 184it [01:58,  1.60it/s]Extractor Predicting: 185it [01:59,  1.60it/s]Extractor Predicting: 186it [01:59,  1.61it/s]Extractor Predicting: 187it [02:00,  1.58it/s]Extractor Predicting: 188it [02:00,  1.62it/s]Extractor Predicting: 189it [02:01,  1.64it/s]Extractor Predicting: 190it [02:02,  1.68it/s]Extractor Predicting: 191it [02:02,  1.61it/s]Extractor Predicting: 192it [02:03,  1.64it/s]Extractor Predicting: 193it [02:04,  1.64it/s]Extractor Predicting: 194it [02:04,  1.68it/s]Extractor Predicting: 195it [02:05,  1.64it/s]Extractor Predicting: 196it [02:05,  1.64it/s]Extractor Predicting: 197it [02:06,  1.66it/s]Extractor Predicting: 198it [02:07,  1.64it/s]Extractor Predicting: 199it [02:07,  1.60it/s]Extractor Predicting: 200it [02:08,  1.61it/s]Extractor Predicting: 201it [02:08,  1.61it/s]Extractor Predicting: 202it [02:09,  1.63it/s]Extractor Predicting: 203it [02:10,  1.66it/s]Extractor Predicting: 204it [02:10,  1.68it/s]Extractor Predicting: 205it [02:11,  1.68it/s]Extractor Predicting: 206it [02:11,  1.73it/s]Extractor Predicting: 207it [02:12,  1.70it/s]Extractor Predicting: 208it [02:12,  1.74it/s]Extractor Predicting: 209it [02:13,  1.72it/s]Extractor Predicting: 210it [02:14,  1.68it/s]Extractor Predicting: 211it [02:14,  1.68it/s]Extractor Predicting: 212it [02:15,  1.72it/s]Extractor Predicting: 213it [02:15,  1.75it/s]Extractor Predicting: 214it [02:16,  1.76it/s]Extractor Predicting: 215it [02:16,  1.80it/s]Extractor Predicting: 216it [02:17,  1.79it/s]Extractor Predicting: 217it [02:18,  1.74it/s]Extractor Predicting: 218it [02:18,  1.72it/s]Extractor Predicting: 219it [02:19,  1.70it/s]Extractor Predicting: 220it [02:19,  1.72it/s]Extractor Predicting: 221it [02:20,  1.74it/s]Extractor Predicting: 222it [02:20,  1.79it/s]Extractor Predicting: 223it [02:21,  1.74it/s]Extractor Predicting: 224it [02:22,  1.78it/s]Extractor Predicting: 225it [02:22,  1.76it/s]Extractor Predicting: 226it [02:23,  1.77it/s]Extractor Predicting: 227it [02:23,  1.79it/s]Extractor Predicting: 228it [02:24,  1.76it/s]Extractor Predicting: 229it [02:25,  1.67it/s]Extractor Predicting: 230it [02:25,  1.63it/s]Extractor Predicting: 231it [02:26,  1.56it/s]Extractor Predicting: 232it [02:27,  1.53it/s]Extractor Predicting: 233it [02:27,  1.54it/s]Extractor Predicting: 234it [02:28,  1.51it/s]Extractor Predicting: 235it [02:29,  1.52it/s]Extractor Predicting: 236it [02:29,  1.50it/s]Extractor Predicting: 237it [02:30,  1.47it/s]Extractor Predicting: 238it [02:31,  1.46it/s]Extractor Predicting: 239it [02:31,  1.47it/s]Extractor Predicting: 240it [02:32,  1.48it/s]Extractor Predicting: 241it [02:33,  1.49it/s]Extractor Predicting: 242it [02:33,  1.52it/s]Extractor Predicting: 243it [02:34,  1.47it/s]Extractor Predicting: 244it [02:35,  1.46it/s]Extractor Predicting: 245it [02:36,  1.31it/s]Extractor Predicting: 246it [02:36,  1.36it/s]Extractor Predicting: 247it [02:37,  1.36it/s]Extractor Predicting: 248it [02:38,  1.38it/s]Extractor Predicting: 249it [02:39,  1.37it/s]Extractor Predicting: 250it [02:39,  1.40it/s]Extractor Predicting: 251it [02:40,  1.39it/s]Extractor Predicting: 252it [02:41,  1.42it/s]Extractor Predicting: 253it [02:41,  1.44it/s]Extractor Predicting: 254it [02:42,  1.44it/s]Extractor Predicting: 255it [02:43,  1.44it/s]Extractor Predicting: 256it [02:43,  1.46it/s]Extractor Predicting: 257it [02:44,  1.52it/s]Extractor Predicting: 258it [02:45,  1.51it/s]Extractor Predicting: 259it [02:45,  1.53it/s]Extractor Predicting: 260it [02:46,  1.56it/s]Extractor Predicting: 261it [02:47,  1.53it/s]Extractor Predicting: 262it [02:47,  1.54it/s]Extractor Predicting: 263it [02:48,  1.56it/s]Extractor Predicting: 264it [02:48,  1.57it/s]Extractor Predicting: 265it [02:49,  1.55it/s]Extractor Predicting: 266it [02:50,  1.56it/s]Extractor Predicting: 267it [02:50,  1.59it/s]Extractor Predicting: 268it [02:51,  1.57it/s]Extractor Predicting: 269it [02:52,  1.57it/s]Extractor Predicting: 270it [02:52,  1.56it/s]Extractor Predicting: 271it [02:53,  1.55it/s]Extractor Predicting: 272it [02:54,  1.55it/s]Extractor Predicting: 273it [02:54,  1.54it/s]Extractor Predicting: 274it [02:55,  1.52it/s]Extractor Predicting: 275it [02:56,  1.56it/s]Extractor Predicting: 276it [02:56,  1.56it/s]Extractor Predicting: 277it [02:57,  1.55it/s]Extractor Predicting: 278it [02:57,  1.52it/s]Extractor Predicting: 279it [02:58,  1.55it/s]Extractor Predicting: 280it [02:59,  1.55it/s]Extractor Predicting: 281it [02:59,  1.58it/s]Extractor Predicting: 282it [03:00,  1.54it/s]Extractor Predicting: 283it [03:01,  1.54it/s]Extractor Predicting: 284it [03:01,  1.57it/s]Extractor Predicting: 285it [03:02,  1.54it/s]Extractor Predicting: 286it [03:03,  1.54it/s]Extractor Predicting: 287it [03:03,  1.57it/s]Extractor Predicting: 288it [03:04,  1.56it/s]Extractor Predicting: 289it [03:05,  1.56it/s]Extractor Predicting: 290it [03:05,  1.55it/s]Extractor Predicting: 291it [03:06,  1.55it/s]Extractor Predicting: 292it [03:06,  1.56it/s]Extractor Predicting: 293it [03:07,  1.55it/s]Extractor Predicting: 294it [03:08,  1.56it/s]Extractor Predicting: 295it [03:08,  1.56it/s]Extractor Predicting: 296it [03:09,  1.56it/s]Extractor Predicting: 297it [03:10,  1.58it/s]Extractor Predicting: 298it [03:10,  1.51it/s]Extractor Predicting: 299it [03:11,  1.56it/s]Extractor Predicting: 300it [03:12,  1.56it/s]Extractor Predicting: 301it [03:12,  1.57it/s]Extractor Predicting: 302it [03:13,  1.57it/s]Extractor Predicting: 303it [03:13,  1.61it/s]Extractor Predicting: 304it [03:14,  1.60it/s]Extractor Predicting: 305it [03:15,  1.64it/s]Extractor Predicting: 306it [03:15,  1.59it/s]Extractor Predicting: 307it [03:16,  1.57it/s]Extractor Predicting: 308it [03:17,  1.57it/s]Extractor Predicting: 309it [03:17,  1.59it/s]Extractor Predicting: 310it [03:18,  1.55it/s]Extractor Predicting: 311it [03:19,  1.55it/s]Extractor Predicting: 312it [03:19,  1.55it/s]Extractor Predicting: 313it [03:20,  1.58it/s]Extractor Predicting: 314it [03:20,  1.56it/s]Extractor Predicting: 315it [03:21,  1.59it/s]Extractor Predicting: 316it [03:22,  1.58it/s]Extractor Predicting: 317it [03:22,  1.56it/s]Extractor Predicting: 318it [03:23,  1.61it/s]Extractor Predicting: 319it [03:24,  1.61it/s]Extractor Predicting: 320it [03:24,  1.65it/s]Extractor Predicting: 321it [03:25,  1.62it/s]Extractor Predicting: 322it [03:25,  1.58it/s]Extractor Predicting: 323it [03:26,  1.58it/s]Extractor Predicting: 324it [03:27,  1.57it/s]Extractor Predicting: 325it [03:27,  1.56it/s]Extractor Predicting: 326it [03:28,  1.57it/s]Extractor Predicting: 327it [03:29,  1.55it/s]Extractor Predicting: 328it [03:29,  1.55it/s]Extractor Predicting: 329it [03:30,  1.51it/s]Extractor Predicting: 330it [03:31,  1.49it/s]Extractor Predicting: 331it [03:31,  1.51it/s]Extractor Predicting: 332it [03:32,  1.56it/s]Extractor Predicting: 333it [03:33,  1.54it/s]Extractor Predicting: 334it [03:33,  1.56it/s]Extractor Predicting: 335it [03:34,  1.56it/s]Extractor Predicting: 336it [03:35,  1.56it/s]Extractor Predicting: 337it [03:35,  1.56it/s]Extractor Predicting: 338it [03:36,  1.53it/s]Extractor Predicting: 339it [03:37,  1.52it/s]Extractor Predicting: 340it [03:37,  1.38it/s]Extractor Predicting: 341it [03:38,  1.42it/s]Extractor Predicting: 342it [03:39,  1.48it/s]Extractor Predicting: 343it [03:39,  1.45it/s]Extractor Predicting: 344it [03:40,  1.49it/s]Extractor Predicting: 345it [03:41,  1.48it/s]Extractor Predicting: 346it [03:41,  1.48it/s]Extractor Predicting: 347it [03:42,  1.50it/s]Extractor Predicting: 348it [03:43,  1.53it/s]Extractor Predicting: 349it [03:43,  1.48it/s]Extractor Predicting: 350it [03:44,  1.47it/s]Extractor Predicting: 351it [03:45,  1.47it/s]Extractor Predicting: 352it [03:45,  1.49it/s]Extractor Predicting: 353it [03:46,  1.47it/s]Extractor Predicting: 354it [03:47,  1.49it/s]Extractor Predicting: 355it [03:47,  1.49it/s]Extractor Predicting: 356it [03:48,  1.53it/s]Extractor Predicting: 357it [03:49,  1.51it/s]Extractor Predicting: 358it [03:49,  1.50it/s]Extractor Predicting: 359it [03:50,  1.49it/s]Extractor Predicting: 360it [03:51,  1.46it/s]Extractor Predicting: 361it [03:51,  1.49it/s]Extractor Predicting: 362it [03:52,  1.52it/s]Extractor Predicting: 363it [03:53,  1.52it/s]Extractor Predicting: 364it [03:53,  1.55it/s]Extractor Predicting: 365it [03:54,  1.55it/s]Extractor Predicting: 366it [03:55,  1.53it/s]Extractor Predicting: 367it [03:55,  1.53it/s]Extractor Predicting: 368it [03:56,  1.52it/s]Extractor Predicting: 369it [03:57,  1.52it/s]Extractor Predicting: 370it [03:57,  1.51it/s]Extractor Predicting: 371it [03:58,  1.54it/s]Extractor Predicting: 372it [03:59,  1.55it/s]Extractor Predicting: 373it [03:59,  1.57it/s]Extractor Predicting: 374it [04:00,  1.55it/s]Extractor Predicting: 375it [04:00,  1.54it/s]Extractor Predicting: 376it [04:01,  1.56it/s]Extractor Predicting: 377it [04:02,  1.57it/s]Extractor Predicting: 378it [04:02,  1.59it/s]Extractor Predicting: 379it [04:03,  1.58it/s]Extractor Predicting: 380it [04:04,  1.58it/s]Extractor Predicting: 381it [04:04,  1.58it/s]Extractor Predicting: 382it [04:05,  1.56it/s]Extractor Predicting: 383it [04:06,  1.58it/s]Extractor Predicting: 384it [04:06,  1.57it/s]Extractor Predicting: 385it [04:07,  1.58it/s]Extractor Predicting: 386it [04:07,  1.61it/s]Extractor Predicting: 387it [04:08,  1.63it/s]Extractor Predicting: 388it [04:09,  1.60it/s]Extractor Predicting: 389it [04:09,  1.60it/s]Extractor Predicting: 390it [04:10,  1.59it/s]Extractor Predicting: 391it [04:11,  1.60it/s]Extractor Predicting: 392it [04:11,  1.60it/s]Extractor Predicting: 393it [04:12,  1.59it/s]Extractor Predicting: 394it [04:12,  1.61it/s]Extractor Predicting: 395it [04:13,  1.56it/s]Extractor Predicting: 396it [04:14,  1.59it/s]Extractor Predicting: 397it [04:14,  1.60it/s]Extractor Predicting: 398it [04:15,  1.57it/s]Extractor Predicting: 399it [04:16,  1.58it/s]Extractor Predicting: 400it [04:16,  1.58it/s]Extractor Predicting: 401it [04:17,  1.58it/s]Extractor Predicting: 402it [04:17,  1.62it/s]Extractor Predicting: 403it [04:18,  1.61it/s]Extractor Predicting: 404it [04:19,  1.63it/s]Extractor Predicting: 405it [04:19,  1.62it/s]Extractor Predicting: 406it [04:20,  1.63it/s]Extractor Predicting: 407it [04:21,  1.61it/s]Extractor Predicting: 408it [04:21,  1.64it/s]Extractor Predicting: 409it [04:22,  1.64it/s]Extractor Predicting: 410it [04:22,  1.64it/s]Extractor Predicting: 411it [04:23,  1.60it/s]Extractor Predicting: 412it [04:24,  1.63it/s]Extractor Predicting: 413it [04:24,  1.62it/s]Extractor Predicting: 414it [04:25,  1.63it/s]Extractor Predicting: 415it [04:25,  1.66it/s]Extractor Predicting: 416it [04:26,  1.66it/s]Extractor Predicting: 417it [04:27,  1.63it/s]Extractor Predicting: 418it [04:27,  1.64it/s]Extractor Predicting: 419it [04:28,  1.62it/s]Extractor Predicting: 420it [04:28,  1.62it/s]Extractor Predicting: 421it [04:29,  1.61it/s]Extractor Predicting: 422it [04:30,  1.60it/s]Extractor Predicting: 423it [04:30,  1.61it/s]Extractor Predicting: 424it [04:31,  1.62it/s]Extractor Predicting: 425it [04:31,  1.82it/s]Extractor Predicting: 425it [04:31,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:28,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:28,847 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:28,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:28,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:28,848 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:34:30,051 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:34:30,052 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:34:30,809 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:34:32,031 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:34:32,096 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:35,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:35,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:35,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:35,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:34:35,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:34:36,137 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:34:36,138 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:34:36,765 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:34:36,993 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:34:37,023 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27054327054327054,
  "recall": 0.09729039858629492,
  "score": 0.14311502635569356,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 7it [00:04,  1.55it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.03821656050955414,
  "score": 0.07100591715976332,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/', 'labels': ['characters', 'competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'instrument', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'original broadcaster', 'performer', 'position played on team / speciality', 'record label'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
