Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:20<04:45, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:41<04:28, 20.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:00<04:00, 20.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:19<03:33, 19.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:39<03:16, 19.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:57<02:53, 19.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:19<02:42, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:40<02:21, 20.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:59<02:00, 20.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:18<01:38, 19.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:39<01:19, 19.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:58<00:59, 19.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:18<00:39, 19.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:38<00:19, 19.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:57<00:00, 19.65s/it]Generating: 100%|██████████| 15/15 [04:57<00:00, 19.85s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.8072916666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.77125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 558, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.75625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : owned by . Context : Later in the year , the department built a high - speed commuter railway crossing the Bordeaux River into New York City from Union Station in the Empire State Building . Head Entity : Union Station , Tail Entity : Department of Transportation .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78515625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 473, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 520, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 587, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7025462962962963, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 494, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 593, 'raw': 800}
{'target': 600, 'success': 618, 'raw': 832}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7427884615384616, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n']
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n', "Relation : location of formation . Context : After the death of Emperor Nefertiti ( 9 January 1789 - 7 February 1819 ) , St Peter was succeeded as Archbishop by the emperor , St Peter 's son , Emperor Frederick II ( 18 November 1861 - 12 December 1955 ) . Head Entity : Emperor Frederick II , Tail Entity : Emperor St Peter .\n"]
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8152173913043478, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8551136363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 600, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.78125, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : place served by transport hub . Context : The city of Stuttgart is a medieval city located in the eastern part of Germany . Head Entity : Struttgart , Tail Entity : Struttgart .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Way I Am " . Head Entity : The Way I Am , Tail Entity : The Bizarre Bizarre .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 399, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 470, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7211538461538461, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.78515625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 14638
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14738, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.80s/it]Extractor Estimating: 2it [00:19,  8.12s/it]Extractor Estimating: 3it [00:19,  4.73s/it]Extractor Estimating: 4it [00:20,  3.12s/it]Extractor Estimating: 5it [00:21,  2.23s/it]Extractor Estimating: 6it [00:21,  1.70s/it]Extractor Estimating: 7it [00:22,  1.36s/it]Extractor Estimating: 8it [00:23,  1.14s/it]Extractor Estimating: 9it [00:23,  1.03it/s]Extractor Estimating: 10it [00:24,  1.10it/s]Extractor Estimating: 11it [00:25,  1.16it/s]Extractor Estimating: 12it [00:26,  1.01it/s]Extractor Estimating: 13it [00:27,  1.14it/s]Extractor Estimating: 14it [00:27,  1.22it/s]Extractor Estimating: 15it [00:28,  1.30it/s]Extractor Estimating: 16it [00:29,  1.36it/s]Extractor Estimating: 17it [00:29,  1.38it/s]Extractor Estimating: 18it [00:30,  1.41it/s]Extractor Estimating: 19it [00:31,  1.43it/s]Extractor Estimating: 20it [00:31,  1.41it/s]Extractor Estimating: 21it [00:32,  1.48it/s]Extractor Estimating: 22it [00:36,  1.57s/it]Extractor Estimating: 23it [00:36,  1.31s/it]Extractor Estimating: 24it [00:37,  1.12s/it]Extractor Estimating: 25it [00:38,  1.02s/it]Extractor Estimating: 26it [00:39,  1.09it/s]Extractor Estimating: 27it [00:39,  1.17it/s]Extractor Estimating: 28it [00:40,  1.22it/s]Extractor Estimating: 29it [00:41,  1.26it/s]Extractor Estimating: 30it [00:41,  1.32it/s]Extractor Estimating: 31it [00:42,  1.34it/s]Extractor Estimating: 32it [00:43,  1.36it/s]Extractor Estimating: 33it [00:43,  1.39it/s]Extractor Estimating: 34it [00:44,  1.39it/s]Extractor Estimating: 35it [00:45,  1.41it/s]Extractor Estimating: 36it [00:46,  1.39it/s]Extractor Estimating: 37it [00:46,  1.36it/s]Extractor Estimating: 38it [00:47,  1.35it/s]Extractor Estimating: 39it [00:48,  1.38it/s]Extractor Estimating: 40it [00:49,  1.38it/s]Extractor Estimating: 41it [00:49,  1.42it/s]Extractor Estimating: 42it [00:50,  1.37it/s]Extractor Estimating: 43it [00:51,  1.42it/s]Extractor Estimating: 44it [00:51,  1.39it/s]Extractor Estimating: 45it [00:52,  1.42it/s]Extractor Estimating: 46it [00:53,  1.42it/s]Extractor Estimating: 47it [00:53,  1.45it/s]Extractor Estimating: 48it [00:54,  1.44it/s]Extractor Estimating: 49it [00:55,  1.40it/s]Extractor Estimating: 50it [00:56,  1.43it/s]Extractor Estimating: 51it [00:56,  1.48it/s]Extractor Estimating: 52it [00:57,  1.51it/s]Extractor Estimating: 53it [00:58,  1.49it/s]Extractor Estimating: 54it [00:58,  1.51it/s]Extractor Estimating: 55it [00:59,  1.51it/s]Extractor Estimating: 56it [00:59,  1.52it/s]Extractor Estimating: 57it [01:00,  1.49it/s]Extractor Estimating: 58it [01:01,  1.47it/s]Extractor Estimating: 59it [01:02,  1.50it/s]Extractor Estimating: 60it [01:02,  1.50it/s]Extractor Estimating: 61it [01:03,  1.50it/s]Extractor Estimating: 62it [01:04,  1.51it/s]Extractor Estimating: 63it [01:04,  1.49it/s]Extractor Estimating: 64it [01:05,  1.49it/s]Extractor Estimating: 65it [01:06,  1.50it/s]Extractor Estimating: 66it [01:06,  1.51it/s]Extractor Estimating: 67it [01:07,  1.55it/s]Extractor Estimating: 68it [01:07,  1.54it/s]Extractor Estimating: 69it [01:08,  1.59it/s]Extractor Estimating: 70it [01:09,  1.51it/s]Extractor Estimating: 71it [01:09,  1.52it/s]Extractor Estimating: 72it [01:10,  1.52it/s]Extractor Estimating: 73it [01:11,  1.52it/s]Extractor Estimating: 74it [01:11,  1.52it/s]Extractor Estimating: 75it [01:12,  1.56it/s]Extractor Estimating: 76it [01:13,  1.54it/s]Extractor Estimating: 77it [01:13,  1.56it/s]Extractor Estimating: 78it [01:14,  1.57it/s]Extractor Estimating: 79it [01:14,  1.61it/s]Extractor Estimating: 80it [01:15,  1.61it/s]Extractor Estimating: 81it [01:16,  1.61it/s]Extractor Estimating: 82it [01:16,  1.62it/s]Extractor Estimating: 83it [01:17,  1.58it/s]Extractor Estimating: 84it [01:18,  1.56it/s]Extractor Estimating: 85it [01:18,  1.50it/s]Extractor Estimating: 86it [01:19,  1.49it/s]Extractor Estimating: 87it [01:20,  1.42it/s]Extractor Estimating: 88it [01:20,  1.46it/s]Extractor Estimating: 89it [01:21,  1.51it/s]Extractor Estimating: 90it [01:22,  1.51it/s]Extractor Estimating: 91it [01:22,  1.52it/s]Extractor Estimating: 92it [01:23,  1.54it/s]Extractor Estimating: 93it [01:24,  1.61it/s]Extractor Estimating: 94it [01:24,  1.63it/s]Extractor Estimating: 95it [01:25,  1.61it/s]Extractor Estimating: 96it [01:25,  1.61it/s]Extractor Estimating: 97it [01:26,  1.60it/s]Extractor Estimating: 98it [01:27,  1.53it/s]Extractor Estimating: 99it [01:27,  1.51it/s]Extractor Estimating: 100it [01:28,  1.58it/s]Extractor Estimating: 101it [01:29,  1.58it/s]Extractor Estimating: 102it [01:29,  1.60it/s]Extractor Estimating: 103it [01:30,  1.57it/s]Extractor Estimating: 104it [01:31,  1.41it/s]Extractor Estimating: 105it [01:32,  1.37it/s]Extractor Estimating: 106it [01:32,  1.37it/s]Extractor Estimating: 107it [01:33,  1.41it/s]Extractor Estimating: 108it [01:34,  1.44it/s]Extractor Estimating: 109it [01:34,  1.46it/s]Extractor Estimating: 110it [01:35,  1.43it/s]Extractor Estimating: 111it [01:36,  1.42it/s]Extractor Estimating: 112it [01:36,  1.41it/s]Extractor Estimating: 113it [01:37,  1.48it/s]Extractor Estimating: 114it [01:38,  1.51it/s]Extractor Estimating: 115it [01:38,  1.48it/s]Extractor Estimating: 116it [01:39,  1.53it/s]Extractor Estimating: 117it [01:40,  1.52it/s]Extractor Estimating: 118it [01:40,  1.53it/s]Extractor Estimating: 119it [01:41,  1.48it/s]Extractor Estimating: 120it [01:42,  1.49it/s]Extractor Estimating: 121it [01:42,  1.51it/s]Extractor Estimating: 122it [01:43,  1.50it/s]Extractor Estimating: 123it [01:44,  1.47it/s]Extractor Estimating: 124it [01:44,  1.48it/s]Extractor Estimating: 125it [01:45,  1.52it/s]Extractor Estimating: 126it [01:46,  1.50it/s]Extractor Estimating: 127it [01:46,  1.44it/s]Extractor Estimating: 128it [01:47,  1.47it/s]Extractor Estimating: 129it [01:48,  1.41it/s]Extractor Estimating: 130it [01:49,  1.48it/s]Extractor Estimating: 131it [01:49,  1.50it/s]Extractor Estimating: 132it [01:50,  1.46it/s]Extractor Estimating: 133it [01:51,  1.47it/s]Extractor Estimating: 134it [01:51,  1.49it/s]Extractor Estimating: 135it [01:52,  1.49it/s]Extractor Estimating: 136it [01:53,  1.50it/s]Extractor Estimating: 137it [01:53,  1.51it/s]Extractor Estimating: 138it [01:54,  1.55it/s]Extractor Estimating: 139it [01:54,  1.54it/s]Extractor Estimating: 140it [01:55,  1.57it/s]Extractor Estimating: 141it [01:56,  1.55it/s]Extractor Estimating: 142it [01:56,  1.47it/s]Extractor Estimating: 143it [01:57,  1.44it/s]Extractor Estimating: 144it [01:58,  1.44it/s]Extractor Estimating: 145it [01:59,  1.45it/s]Extractor Estimating: 146it [01:59,  1.46it/s]Extractor Estimating: 147it [02:00,  1.41it/s]Extractor Estimating: 148it [02:01,  1.37it/s]Extractor Estimating: 149it [02:02,  1.36it/s]Extractor Estimating: 150it [02:02,  1.38it/s]Extractor Estimating: 151it [02:03,  1.40it/s]Extractor Estimating: 152it [02:04,  1.39it/s]Extractor Estimating: 153it [02:04,  1.41it/s]Extractor Estimating: 154it [02:05,  1.41it/s]Extractor Estimating: 155it [02:06,  1.42it/s]Extractor Estimating: 156it [02:06,  1.41it/s]Extractor Estimating: 157it [02:07,  1.38it/s]Extractor Estimating: 158it [02:08,  1.37it/s]Extractor Estimating: 159it [02:09,  1.38it/s]Extractor Estimating: 160it [02:10,  1.33it/s]Extractor Estimating: 161it [02:10,  1.37it/s]Extractor Estimating: 162it [02:11,  1.35it/s]Extractor Estimating: 163it [02:12,  1.31it/s]Extractor Estimating: 164it [02:13,  1.31it/s]Extractor Estimating: 165it [02:13,  1.32it/s]Extractor Estimating: 166it [02:14,  1.35it/s]Extractor Estimating: 167it [02:15,  1.34it/s]Extractor Estimating: 168it [02:15,  1.38it/s]Extractor Estimating: 169it [02:16,  1.35it/s]Extractor Estimating: 170it [02:17,  1.43it/s]Extractor Estimating: 171it [02:17,  1.50it/s]Extractor Estimating: 172it [02:18,  1.42it/s]Extractor Estimating: 173it [02:19,  1.40it/s]Extractor Estimating: 174it [02:20,  1.45it/s]Extractor Estimating: 175it [02:20,  1.44it/s]Extractor Estimating: 176it [02:21,  1.48it/s]Extractor Estimating: 177it [02:22,  1.42it/s]Extractor Estimating: 178it [02:22,  1.48it/s]Extractor Estimating: 179it [02:23,  1.37it/s]Extractor Estimating: 180it [02:24,  1.45it/s]Extractor Estimating: 181it [02:24,  1.52it/s]Extractor Estimating: 182it [02:25,  1.47it/s]Extractor Estimating: 183it [02:26,  1.48it/s]Extractor Estimating: 184it [02:26,  1.50it/s]Extractor Estimating: 185it [02:27,  1.50it/s]Extractor Estimating: 186it [02:28,  1.48it/s]Extractor Estimating: 187it [02:28,  1.43it/s]Extractor Estimating: 188it [02:29,  1.46it/s]Extractor Estimating: 189it [02:30,  1.48it/s]Extractor Estimating: 190it [02:30,  1.50it/s]Extractor Estimating: 191it [02:31,  1.51it/s]Extractor Estimating: 192it [02:32,  1.52it/s]Extractor Estimating: 193it [02:32,  1.50it/s]Extractor Estimating: 194it [02:33,  1.46it/s]Extractor Estimating: 195it [02:34,  1.46it/s]Extractor Estimating: 196it [02:35,  1.45it/s]Extractor Estimating: 197it [02:35,  1.44it/s]Extractor Estimating: 198it [02:36,  1.53it/s]Extractor Estimating: 199it [02:37,  1.49it/s]Extractor Estimating: 200it [02:37,  1.39it/s]Extractor Estimating: 201it [02:38,  1.40it/s]Extractor Estimating: 202it [02:39,  1.45it/s]Extractor Estimating: 203it [02:39,  1.46it/s]Extractor Estimating: 204it [02:40,  1.43it/s]Extractor Estimating: 205it [02:41,  1.40it/s]Extractor Estimating: 206it [02:41,  1.46it/s]Extractor Estimating: 207it [02:42,  1.44it/s]Extractor Estimating: 208it [02:43,  1.41it/s]Extractor Estimating: 209it [02:44,  1.40it/s]Extractor Estimating: 210it [02:44,  1.45it/s]Extractor Estimating: 211it [02:45,  1.46it/s]Extractor Estimating: 212it [02:46,  1.43it/s]Extractor Estimating: 213it [02:46,  1.42it/s]Extractor Estimating: 214it [02:47,  1.47it/s]Extractor Estimating: 215it [02:48,  1.44it/s]Extractor Estimating: 216it [02:48,  1.48it/s]Extractor Estimating: 217it [02:49,  1.44it/s]Extractor Estimating: 218it [02:50,  1.46it/s]Extractor Estimating: 219it [02:51,  1.43it/s]Extractor Estimating: 220it [02:51,  1.40it/s]Extractor Estimating: 221it [02:52,  1.37it/s]Extractor Estimating: 222it [02:53,  1.37it/s]Extractor Estimating: 223it [02:53,  1.39it/s]Extractor Estimating: 224it [02:54,  1.37it/s]Extractor Estimating: 225it [02:55,  1.39it/s]Extractor Estimating: 226it [02:56,  1.45it/s]Extractor Estimating: 227it [02:56,  1.49it/s]Extractor Estimating: 228it [02:57,  1.48it/s]Extractor Estimating: 229it [02:57,  1.49it/s]Extractor Estimating: 230it [02:58,  1.51it/s]Extractor Estimating: 231it [02:59,  1.45it/s]Extractor Estimating: 232it [03:00,  1.49it/s]Extractor Estimating: 233it [03:00,  1.51it/s]Extractor Estimating: 234it [03:01,  1.51it/s]Extractor Estimating: 235it [03:01,  1.51it/s]Extractor Estimating: 236it [03:02,  1.50it/s]Extractor Estimating: 237it [03:03,  1.50it/s]Extractor Estimating: 238it [03:04,  1.42it/s]Extractor Estimating: 239it [03:04,  1.46it/s]Extractor Estimating: 240it [03:05,  1.53it/s]Extractor Estimating: 241it [03:06,  1.50it/s]Extractor Estimating: 242it [03:06,  1.52it/s]Extractor Estimating: 243it [03:07,  1.55it/s]Extractor Estimating: 244it [03:07,  1.52it/s]Extractor Estimating: 245it [03:08,  1.53it/s]Extractor Estimating: 246it [03:09,  1.50it/s]Extractor Estimating: 247it [03:10,  1.46it/s]Extractor Estimating: 248it [03:10,  1.51it/s]Extractor Estimating: 249it [03:11,  1.47it/s]Extractor Estimating: 250it [03:12,  1.50it/s]Extractor Estimating: 251it [03:12,  1.54it/s]Extractor Estimating: 252it [03:13,  1.41it/s]Extractor Estimating: 253it [03:14,  1.40it/s]Extractor Estimating: 254it [03:15,  1.31it/s]Extractor Estimating: 255it [03:15,  1.27it/s]Extractor Estimating: 256it [03:16,  1.38it/s]Extractor Estimating: 257it [03:17,  1.44it/s]Extractor Estimating: 258it [03:17,  1.46it/s]Extractor Estimating: 259it [03:18,  1.47it/s]Extractor Estimating: 260it [03:19,  1.47it/s]Extractor Estimating: 261it [03:19,  1.39it/s]Extractor Estimating: 262it [03:20,  1.38it/s]Extractor Estimating: 263it [03:21,  1.45it/s]Extractor Estimating: 264it [03:21,  1.49it/s]Extractor Estimating: 265it [03:22,  1.46it/s]Extractor Estimating: 266it [03:23,  1.45it/s]Extractor Estimating: 267it [03:24,  1.45it/s]Extractor Estimating: 268it [03:24,  1.43it/s]Extractor Estimating: 269it [03:25,  1.45it/s]Extractor Estimating: 270it [03:26,  1.49it/s]Extractor Estimating: 271it [03:26,  1.49it/s]Extractor Estimating: 272it [03:27,  1.53it/s]Extractor Estimating: 273it [03:27,  1.52it/s]Extractor Estimating: 274it [03:28,  1.53it/s]Extractor Estimating: 275it [03:29,  1.49it/s]Extractor Estimating: 276it [03:29,  1.51it/s]Extractor Estimating: 277it [03:30,  1.50it/s]Extractor Estimating: 278it [03:31,  1.54it/s]Extractor Estimating: 279it [03:31,  1.55it/s]Extractor Estimating: 280it [03:32,  1.55it/s]Extractor Estimating: 281it [03:33,  1.59it/s]Extractor Estimating: 282it [03:33,  1.56it/s]Extractor Estimating: 283it [03:34,  1.50it/s]Extractor Estimating: 284it [03:35,  1.52it/s]Extractor Estimating: 285it [03:35,  1.52it/s]Extractor Estimating: 286it [03:36,  1.49it/s]Extractor Estimating: 287it [03:37,  1.52it/s]Extractor Estimating: 288it [03:37,  1.45it/s]Extractor Estimating: 289it [03:38,  1.49it/s]Extractor Estimating: 290it [03:39,  1.53it/s]Extractor Estimating: 291it [03:39,  1.59it/s]Extractor Estimating: 292it [03:40,  1.62it/s]Extractor Estimating: 293it [03:40,  1.59it/s]Extractor Estimating: 294it [03:41,  1.58it/s]Extractor Estimating: 295it [03:42,  1.58it/s]Extractor Estimating: 296it [03:42,  1.55it/s]Extractor Estimating: 297it [03:43,  1.54it/s]Extractor Estimating: 298it [03:44,  1.55it/s]Extractor Estimating: 299it [03:44,  1.56it/s]Extractor Estimating: 300it [03:45,  1.59it/s]Extractor Estimating: 301it [03:46,  1.54it/s]Extractor Estimating: 302it [03:46,  1.48it/s]Extractor Estimating: 303it [03:47,  1.46it/s]Extractor Estimating: 304it [03:48,  1.43it/s]Extractor Estimating: 305it [03:48,  1.46it/s]Extractor Estimating: 306it [03:49,  1.48it/s]Extractor Estimating: 307it [03:50,  1.53it/s]Extractor Estimating: 308it [03:50,  1.54it/s]Extractor Estimating: 309it [03:51,  1.57it/s]Extractor Estimating: 310it [03:52,  1.54it/s]Extractor Estimating: 311it [03:52,  1.56it/s]Extractor Estimating: 312it [03:53,  1.54it/s]Extractor Estimating: 313it [03:54,  1.53it/s]Extractor Estimating: 314it [03:54,  1.52it/s]Extractor Estimating: 315it [03:55,  1.49it/s]Extractor Estimating: 316it [03:56,  1.54it/s]Extractor Estimating: 317it [03:56,  1.50it/s]Extractor Estimating: 318it [03:57,  1.45it/s]Extractor Estimating: 319it [03:58,  1.43it/s]Extractor Estimating: 320it [03:58,  1.45it/s]Extractor Estimating: 321it [03:59,  1.41it/s]Extractor Estimating: 322it [04:00,  1.45it/s]Extractor Estimating: 323it [04:00,  1.46it/s]Extractor Estimating: 324it [04:01,  1.48it/s]Extractor Estimating: 325it [04:02,  1.48it/s]Extractor Estimating: 326it [04:02,  1.49it/s]Extractor Estimating: 327it [04:03,  1.48it/s]Extractor Estimating: 328it [04:04,  1.46it/s]Extractor Estimating: 329it [04:05,  1.36it/s]Extractor Estimating: 330it [04:05,  1.39it/s]Extractor Estimating: 331it [04:06,  1.44it/s]Extractor Estimating: 332it [04:07,  1.45it/s]Extractor Estimating: 333it [04:07,  1.53it/s]Extractor Estimating: 334it [04:08,  1.54it/s]Extractor Estimating: 335it [04:09,  1.55it/s]Extractor Estimating: 336it [04:09,  1.52it/s]Extractor Estimating: 337it [04:10,  1.47it/s]Extractor Estimating: 338it [04:11,  1.45it/s]Extractor Estimating: 339it [04:11,  1.48it/s]Extractor Estimating: 340it [04:12,  1.55it/s]Extractor Estimating: 341it [04:13,  1.52it/s]Extractor Estimating: 342it [04:13,  1.54it/s]Extractor Estimating: 343it [04:14,  1.54it/s]Extractor Estimating: 344it [04:15,  1.53it/s]Extractor Estimating: 345it [04:15,  1.49it/s]Extractor Estimating: 346it [04:16,  1.50it/s]Extractor Estimating: 347it [04:17,  1.47it/s]Extractor Estimating: 348it [04:17,  1.51it/s]Extractor Estimating: 349it [04:18,  1.49it/s]Extractor Estimating: 350it [04:19,  1.52it/s]Extractor Estimating: 351it [04:19,  1.50it/s]Extractor Estimating: 352it [04:20,  1.48it/s]Extractor Estimating: 353it [04:21,  1.44it/s]Extractor Estimating: 354it [04:21,  1.45it/s]Extractor Estimating: 355it [04:22,  1.53it/s]Extractor Estimating: 356it [04:23,  1.46it/s]Extractor Estimating: 357it [04:23,  1.43it/s]Extractor Estimating: 358it [04:24,  1.48it/s]Extractor Estimating: 359it [04:25,  1.49it/s]Extractor Estimating: 360it [04:25,  1.47it/s]Extractor Estimating: 361it [04:26,  1.45it/s]Extractor Estimating: 362it [04:27,  1.41it/s]Extractor Estimating: 363it [04:28,  1.38it/s]Extractor Estimating: 364it [04:28,  1.40it/s]Extractor Estimating: 365it [04:29,  1.40it/s]Extractor Estimating: 366it [04:30,  1.42it/s]Extractor Estimating: 367it [04:30,  1.44it/s]Extractor Estimating: 368it [04:31,  1.43it/s]Extractor Estimating: 369it [04:32,  1.43it/s]Extractor Estimating: 370it [04:32,  1.45it/s]Extractor Estimating: 371it [04:33,  1.44it/s]Extractor Estimating: 372it [04:34,  1.42it/s]Extractor Estimating: 373it [04:35,  1.43it/s]Extractor Estimating: 374it [04:35,  1.45it/s]Extractor Estimating: 375it [04:36,  1.56it/s]Extractor Estimating: 375it [04:36,  1.36it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7527 mean pseudo reward: 0.9298647667242588
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 27292
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27392, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27392, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.351, loss:2927.3204
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.054, loss:2181.3747
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.062, loss:1914.8985
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 86, avg_time 1.070, loss:1778.4212
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 186, avg_time 1.067, loss:1646.7427
>> valid entity prec:0.5478, rec:0.5953, f1:0.5706
>> valid relation prec:0.5692, rec:0.0624, f1:0.1125
>> valid relation with NER prec:0.5692, rec:0.0624, f1:0.1125
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 286, avg_time 2.334, loss:1502.6849
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 72, avg_time 1.062, loss:1379.9483
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 172, avg_time 1.064, loss:1326.5741
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 272, avg_time 1.062, loss:1320.2117
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 58, avg_time 1.070, loss:1171.3358
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5724, rec:0.5813, f1:0.5768
>> valid relation prec:0.4740, rec:0.0705, f1:0.1227
>> valid relation with NER prec:0.4740, rec:0.0705, f1:0.1227
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 158, avg_time 2.335, loss:1195.3573
g_step 1200, step 258, avg_time 1.063, loss:1185.6651
g_step 1300, step 44, avg_time 1.066, loss:1069.5422
g_step 1400, step 144, avg_time 1.058, loss:1056.8220
g_step 1500, step 244, avg_time 1.063, loss:1085.5698
>> valid entity prec:0.6297, rec:0.4645, f1:0.5347
>> valid relation prec:0.2774, rec:0.0312, f1:0.0561
>> valid relation with NER prec:0.2774, rec:0.0312, f1:0.0561
g_step 1600, step 30, avg_time 2.333, loss:1015.1499
g_step 1700, step 130, avg_time 1.069, loss:1014.8857
g_step 1800, step 230, avg_time 1.070, loss:965.6778
g_step 1900, step 16, avg_time 1.055, loss:983.2554
g_step 2000, step 116, avg_time 1.067, loss:917.8282
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6139, rec:0.5630, f1:0.5873
>> valid relation prec:0.3161, rec:0.0753, f1:0.1217
>> valid relation with NER prec:0.3161, rec:0.0753, f1:0.1217
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 216, avg_time 2.348, loss:924.1396
g_step 2200, step 2, avg_time 1.054, loss:943.8700
g_step 2300, step 102, avg_time 1.071, loss:849.9255
g_step 2400, step 202, avg_time 1.055, loss:873.0033
g_step 2500, step 302, avg_time 1.067, loss:892.2300
>> valid entity prec:0.5829, rec:0.5048, f1:0.5410
>> valid relation prec:0.3708, rec:0.0851, f1:0.1384
>> valid relation with NER prec:0.3708, rec:0.0851, f1:0.1384
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 88, avg_time 2.338, loss:830.6538
g_step 2700, step 188, avg_time 1.065, loss:840.9645
g_step 2800, step 288, avg_time 1.072, loss:816.5173
g_step 2900, step 74, avg_time 1.054, loss:806.6082
g_step 3000, step 174, avg_time 1.069, loss:784.6642
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6176, rec:0.5141, f1:0.5611
>> valid relation prec:0.2641, rec:0.0630, f1:0.1018
>> valid relation with NER prec:0.2641, rec:0.0630, f1:0.1018
g_step 3100, step 274, avg_time 2.336, loss:803.9538
g_step 3200, step 60, avg_time 1.070, loss:778.0675
g_step 3300, step 160, avg_time 1.050, loss:722.5862
g_step 3400, step 260, avg_time 1.062, loss:771.8923
g_step 3500, step 46, avg_time 1.071, loss:721.5679
>> valid entity prec:0.6291, rec:0.4835, f1:0.5468
>> valid relation prec:0.3538, rec:0.1005, f1:0.1566
>> valid relation with NER prec:0.3538, rec:0.1005, f1:0.1566
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 146, avg_time 2.329, loss:718.1096
g_step 3700, step 246, avg_time 1.071, loss:726.4155
g_step 3800, step 32, avg_time 1.066, loss:716.9086
g_step 3900, step 132, avg_time 1.063, loss:690.0102
g_step 4000, step 232, avg_time 1.065, loss:688.7221
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5605, rec:0.5645, f1:0.5625
>> valid relation prec:0.2321, rec:0.0613, f1:0.0970
>> valid relation with NER prec:0.2321, rec:0.0613, f1:0.0970
g_step 4100, step 18, avg_time 2.338, loss:668.3739
g_step 4200, step 118, avg_time 1.058, loss:640.8460
g_step 4300, step 218, avg_time 1.064, loss:650.0426
g_step 4400, step 4, avg_time 1.068, loss:671.5462
g_step 4500, step 104, avg_time 1.070, loss:604.7155
>> valid entity prec:0.6223, rec:0.4469, f1:0.5202
>> valid relation prec:0.3344, rec:0.0862, f1:0.1371
>> valid relation with NER prec:0.3344, rec:0.0862, f1:0.1371
g_step 4600, step 204, avg_time 2.342, loss:642.2890
g_step 4700, step 304, avg_time 1.059, loss:625.6037
g_step 4800, step 90, avg_time 1.074, loss:574.3689
g_step 4900, step 190, avg_time 1.071, loss:598.5436
g_step 5000, step 290, avg_time 1.058, loss:636.9190
learning rate was adjusted to 0.0008
>> valid entity prec:0.5571, rec:0.5724, f1:0.5647
>> valid relation prec:0.2758, rec:0.0879, f1:0.1334
>> valid relation with NER prec:0.2758, rec:0.0879, f1:0.1334
g_step 5100, step 76, avg_time 2.338, loss:556.1703
g_step 5200, step 176, avg_time 1.060, loss:581.0320
g_step 5300, step 276, avg_time 1.066, loss:570.2331
g_step 5400, step 62, avg_time 1.069, loss:552.4175
g_step 5500, step 162, avg_time 1.063, loss:541.8523
>> valid entity prec:0.5848, rec:0.4736, f1:0.5234
>> valid relation prec:0.2784, rec:0.0856, f1:0.1310
>> valid relation with NER prec:0.2784, rec:0.0856, f1:0.1310
g_step 5600, step 262, avg_time 2.331, loss:576.7217
g_step 5700, step 48, avg_time 1.064, loss:528.4840
g_step 5800, step 148, avg_time 1.061, loss:509.5319
g_step 5900, step 248, avg_time 1.065, loss:541.8365
g_step 6000, step 34, avg_time 1.071, loss:503.0740
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5481, rec:0.5848, f1:0.5659
>> valid relation prec:0.2200, rec:0.0693, f1:0.1054
>> valid relation with NER prec:0.2200, rec:0.0693, f1:0.1054
g_step 6100, step 134, avg_time 2.340, loss:491.6902
g_step 6200, step 234, avg_time 1.064, loss:522.3052
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:16:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:16:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-16-50_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:16:51 - WARNING - datasets.builder -   Using custom data configuration default-4ef4ba9916268f72
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4ef4ba9916268f72/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:16:53,656 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:16:53,680 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:16:53,680 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:16:53,681 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:16:53,770 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,774 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,774 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,775 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,775 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,775 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:16:53,775 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:16:53,904 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:16:57,009 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:16:57,009 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4ef4ba9916268f72/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 13:16:57 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14c56f5e3ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.22ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.94ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.19ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.31ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.39ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.41ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.44ba/s]100%|██████████| 8/8 [00:01<00:00,  5.17ba/s]100%|██████████| 8/8 [00:01<00:00,  4.54ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.92ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.24ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.33ba/s]100%|██████████| 4/4 [00:00<00:00,  5.36ba/s]100%|██████████| 4/4 [00:00<00:00,  4.82ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.69ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.66ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.31ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.49ba/s]100%|██████████| 8/8 [00:00<00:00, 10.81ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.40ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.09ba/s]100%|██████████| 4/4 [00:00<00:00, 11.34ba/s]
[INFO|trainer.py:414] 2023-08-28 13:17:01,130 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:17:01,137 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:17:01,137 >>   Num examples = 7553
[INFO|trainer.py:1149] 2023-08-28 13:17:01,137 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:17:01,137 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:17:01,137 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:17:01,137 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:17:01,137 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:53,  3.40it/s]  0%|          | 2/590 [00:00<02:50,  3.46it/s]  1%|          | 3/590 [00:00<02:48,  3.47it/s]  1%|          | 4/590 [00:01<02:48,  3.48it/s]  1%|          | 5/590 [00:01<02:47,  3.49it/s]  1%|          | 6/590 [00:01<02:47,  3.49it/s]  1%|          | 7/590 [00:02<02:46,  3.49it/s]  1%|▏         | 8/590 [00:02<02:46,  3.49it/s]  2%|▏         | 9/590 [00:02<02:46,  3.49it/s]  2%|▏         | 10/590 [00:02<02:46,  3.49it/s]  2%|▏         | 11/590 [00:03<02:45,  3.49it/s]  2%|▏         | 12/590 [00:03<02:45,  3.49it/s]  2%|▏         | 13/590 [00:03<02:45,  3.49it/s]  2%|▏         | 14/590 [00:04<02:44,  3.49it/s]  3%|▎         | 15/590 [00:04<02:44,  3.49it/s]  3%|▎         | 16/590 [00:04<02:44,  3.49it/s]  3%|▎         | 17/590 [00:04<02:44,  3.49it/s]  3%|▎         | 18/590 [00:05<02:43,  3.49it/s]  3%|▎         | 19/590 [00:05<02:43,  3.49it/s]  3%|▎         | 20/590 [00:05<02:43,  3.49it/s]  4%|▎         | 21/590 [00:06<02:42,  3.49it/s]  4%|▎         | 22/590 [00:06<02:42,  3.50it/s]  4%|▍         | 23/590 [00:06<02:42,  3.49it/s]  4%|▍         | 24/590 [00:06<02:42,  3.49it/s]  4%|▍         | 25/590 [00:07<02:41,  3.49it/s]  4%|▍         | 26/590 [00:07<02:41,  3.49it/s]  5%|▍         | 27/590 [00:07<02:41,  3.49it/s]  5%|▍         | 28/590 [00:08<02:40,  3.49it/s]  5%|▍         | 29/590 [00:08<02:40,  3.49it/s]  5%|▌         | 30/590 [00:08<02:40,  3.49it/s]  5%|▌         | 31/590 [00:08<02:40,  3.49it/s]  5%|▌         | 32/590 [00:09<02:39,  3.49it/s]  6%|▌         | 33/590 [00:09<02:39,  3.49it/s]  6%|▌         | 34/590 [00:09<02:39,  3.49it/s]  6%|▌         | 35/590 [00:10<02:39,  3.49it/s]  6%|▌         | 36/590 [00:10<02:38,  3.49it/s]  6%|▋         | 37/590 [00:10<02:38,  3.49it/s]  6%|▋         | 38/590 [00:10<02:38,  3.48it/s]  7%|▋         | 39/590 [00:11<02:38,  3.48it/s]  7%|▋         | 40/590 [00:11<02:37,  3.49it/s]  7%|▋         | 41/590 [00:11<02:37,  3.49it/s]  7%|▋         | 42/590 [00:12<02:37,  3.49it/s]  7%|▋         | 43/590 [00:12<02:36,  3.49it/s]  7%|▋         | 44/590 [00:12<02:36,  3.49it/s]  8%|▊         | 45/590 [00:12<02:36,  3.49it/s]  8%|▊         | 46/590 [00:13<02:35,  3.49it/s]  8%|▊         | 47/590 [00:13<02:35,  3.49it/s]  8%|▊         | 48/590 [00:13<02:35,  3.49it/s]  8%|▊         | 49/590 [00:14<02:35,  3.49it/s]  8%|▊         | 50/590 [00:14<02:34,  3.48it/s]  9%|▊         | 51/590 [00:14<02:34,  3.48it/s]  9%|▉         | 52/590 [00:14<02:34,  3.48it/s]  9%|▉         | 53/590 [00:15<02:34,  3.48it/s]  9%|▉         | 54/590 [00:15<02:34,  3.48it/s]  9%|▉         | 55/590 [00:15<02:33,  3.48it/s]  9%|▉         | 56/590 [00:16<02:33,  3.48it/s] 10%|▉         | 57/590 [00:16<02:32,  3.48it/s] 10%|▉         | 58/590 [00:16<02:32,  3.48it/s] 10%|█         | 59/590 [00:16<02:32,  3.48it/s] 10%|█         | 60/590 [00:17<02:32,  3.48it/s] 10%|█         | 61/590 [00:17<02:31,  3.48it/s] 11%|█         | 62/590 [00:17<02:31,  3.48it/s] 11%|█         | 63/590 [00:18<02:31,  3.48it/s] 11%|█         | 64/590 [00:18<02:31,  3.48it/s] 11%|█         | 65/590 [00:18<02:30,  3.48it/s] 11%|█         | 66/590 [00:18<02:30,  3.48it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.48it/s] 12%|█▏        | 68/590 [00:19<02:29,  3.48it/s] 12%|█▏        | 69/590 [00:19<02:29,  3.48it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.48it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.48it/s] 12%|█▏        | 72/590 [00:20<02:28,  3.48it/s] 12%|█▏        | 73/590 [00:20<02:28,  3.48it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.48it/s] 13%|█▎        | 75/590 [00:21<02:27,  3.48it/s] 13%|█▎        | 76/590 [00:21<02:27,  3.48it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.48it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.48it/s] 13%|█▎        | 79/590 [00:22<02:26,  3.48it/s] 14%|█▎        | 80/590 [00:22<02:26,  3.48it/s] 14%|█▎        | 81/590 [00:23<02:26,  3.48it/s] 14%|█▍        | 82/590 [00:23<02:25,  3.48it/s] 14%|█▍        | 83/590 [00:23<02:25,  3.48it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.48it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.48it/s] 15%|█▍        | 86/590 [00:24<02:24,  3.48it/s] 15%|█▍        | 87/590 [00:24<02:24,  3.48it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.48it/s] 15%|█▌        | 89/590 [00:25<02:23,  3.48it/s] 15%|█▌        | 90/590 [00:25<02:23,  3.48it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.48it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.48it/s] 16%|█▌        | 93/590 [00:26<02:22,  3.48it/s] 16%|█▌        | 94/590 [00:26<02:22,  3.48it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.48it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.48it/s] 16%|█▋        | 97/590 [00:27<02:21,  3.48it/s] 17%|█▋        | 98/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.48it/s] 17%|█▋        | 100/590 [00:28<02:20,  3.48it/s] 17%|█▋        | 101/590 [00:28<02:20,  3.48it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.48it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.48it/s] 18%|█▊        | 104/590 [00:29<02:19,  3.48it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.48it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.48it/s] 18%|█▊        | 107/590 [00:30<02:18,  3.48it/s] 18%|█▊        | 108/590 [00:30<02:18,  3.48it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.48it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.48it/s] 19%|█▉        | 111/590 [00:31<02:17,  3.48it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.48it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.48it/s] 19%|█▉        | 114/590 [00:32<02:16,  3.48it/s] 19%|█▉        | 115/590 [00:33<02:16,  3.48it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.48it/s] 20%|█▉        | 117/590 [00:33<02:15,  3.48it/s] 20%|██        | 118/590 [00:33<02:15,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 13:17:35,056 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:17:35,056 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:17:35,056 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.62it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.64it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.15it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.47it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.30it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.06it/s][A
 11%|█         | 48/437 [00:01<00:08, 47.00it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.79it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.86it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.84it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.89it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.82it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.87it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.88it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.83it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.84it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.81it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.76it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.85it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.79it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.86it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.82it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.65it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.65it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.63it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.61it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.66it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.65it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.66it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.77it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.81it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.80it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.71it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.78it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.79it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.81it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.81it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.85it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.75it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.85it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.80it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.72it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.73it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.79it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.72it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.79it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.79it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.51it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.56it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.55it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.50it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.48it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.45it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.48it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.46it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.63it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.71it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.64it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.71it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.72it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.62it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.70it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.75it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.74it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.83it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.87it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.83it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.78it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.71it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.71it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.75it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.79it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.69it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.68it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.76it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.77it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.83it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.79it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.76it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.49it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.75it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.78it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.75it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:43<02:15,  3.48it/s]
100%|██████████| 437/437 [00:09<00:00, 46.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:17:44,549 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 13:17:44,577 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:17:46,898 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:17:46,912 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:17:46,922 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:50<41:28,  5.28s/it] 20%|██        | 120/590 [00:51<29:38,  3.78s/it] 21%|██        | 121/590 [00:51<21:22,  2.74s/it] 21%|██        | 122/590 [00:51<15:36,  2.00s/it] 21%|██        | 123/590 [00:51<11:34,  1.49s/it] 21%|██        | 124/590 [00:52<08:45,  1.13s/it] 21%|██        | 125/590 [00:52<06:47,  1.14it/s] 21%|██▏       | 126/590 [00:52<05:24,  1.43it/s] 22%|██▏       | 127/590 [00:53<04:26,  1.74it/s] 22%|██▏       | 128/590 [00:53<03:45,  2.04it/s] 22%|██▏       | 129/590 [00:53<03:17,  2.33it/s] 22%|██▏       | 130/590 [00:53<02:57,  2.59it/s] 22%|██▏       | 131/590 [00:54<02:43,  2.81it/s] 22%|██▏       | 132/590 [00:54<02:33,  2.98it/s] 23%|██▎       | 133/590 [00:54<02:26,  3.11it/s] 23%|██▎       | 134/590 [00:55<02:21,  3.21it/s] 23%|██▎       | 135/590 [00:55<02:18,  3.29it/s] 23%|██▎       | 136/590 [00:55<02:15,  3.34it/s] 23%|██▎       | 137/590 [00:55<02:13,  3.38it/s] 23%|██▎       | 138/590 [00:56<02:12,  3.41it/s] 24%|██▎       | 139/590 [00:56<02:11,  3.44it/s] 24%|██▎       | 140/590 [00:56<02:10,  3.45it/s] 24%|██▍       | 141/590 [00:57<02:09,  3.46it/s] 24%|██▍       | 142/590 [00:57<02:09,  3.47it/s] 24%|██▍       | 143/590 [00:57<02:08,  3.47it/s] 24%|██▍       | 144/590 [00:58<02:08,  3.47it/s] 25%|██▍       | 145/590 [00:58<02:08,  3.47it/s] 25%|██▍       | 146/590 [00:58<02:07,  3.47it/s] 25%|██▍       | 147/590 [00:58<02:07,  3.47it/s] 25%|██▌       | 148/590 [00:59<02:07,  3.48it/s] 25%|██▌       | 149/590 [00:59<02:06,  3.48it/s] 25%|██▌       | 150/590 [00:59<02:06,  3.48it/s] 26%|██▌       | 151/590 [01:00<02:06,  3.48it/s] 26%|██▌       | 152/590 [01:00<02:05,  3.48it/s] 26%|██▌       | 153/590 [01:00<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:00<02:05,  3.46it/s] 26%|██▋       | 155/590 [01:01<02:05,  3.47it/s] 26%|██▋       | 156/590 [01:01<02:04,  3.47it/s] 27%|██▋       | 157/590 [01:01<02:04,  3.48it/s] 27%|██▋       | 158/590 [01:02<02:04,  3.48it/s] 27%|██▋       | 159/590 [01:02<02:04,  3.47it/s] 27%|██▋       | 160/590 [01:02<02:03,  3.48it/s] 27%|██▋       | 161/590 [01:02<02:03,  3.48it/s] 27%|██▋       | 162/590 [01:03<02:03,  3.48it/s] 28%|██▊       | 163/590 [01:03<02:02,  3.48it/s] 28%|██▊       | 164/590 [01:03<02:02,  3.47it/s] 28%|██▊       | 165/590 [01:04<02:02,  3.47it/s] 28%|██▊       | 166/590 [01:04<02:02,  3.47it/s] 28%|██▊       | 167/590 [01:04<02:01,  3.48it/s] 28%|██▊       | 168/590 [01:04<02:01,  3.48it/s] 29%|██▊       | 169/590 [01:05<02:01,  3.47it/s] 29%|██▉       | 170/590 [01:05<02:00,  3.48it/s] 29%|██▉       | 171/590 [01:05<02:00,  3.48it/s] 29%|██▉       | 172/590 [01:06<02:00,  3.48it/s] 29%|██▉       | 173/590 [01:06<01:59,  3.48it/s] 29%|██▉       | 174/590 [01:06<01:59,  3.48it/s] 30%|██▉       | 175/590 [01:06<01:59,  3.47it/s] 30%|██▉       | 176/590 [01:07<01:59,  3.47it/s] 30%|███       | 177/590 [01:07<01:58,  3.47it/s] 30%|███       | 178/590 [01:07<01:58,  3.48it/s] 30%|███       | 179/590 [01:08<01:58,  3.48it/s] 31%|███       | 180/590 [01:08<01:57,  3.48it/s] 31%|███       | 181/590 [01:08<01:57,  3.48it/s] 31%|███       | 182/590 [01:08<01:57,  3.48it/s] 31%|███       | 183/590 [01:09<01:57,  3.48it/s] 31%|███       | 184/590 [01:09<01:56,  3.48it/s] 31%|███▏      | 185/590 [01:09<01:56,  3.48it/s] 32%|███▏      | 186/590 [01:10<01:56,  3.48it/s] 32%|███▏      | 187/590 [01:10<01:56,  3.47it/s] 32%|███▏      | 188/590 [01:10<01:55,  3.47it/s] 32%|███▏      | 189/590 [01:10<01:55,  3.47it/s] 32%|███▏      | 190/590 [01:11<01:55,  3.47it/s] 32%|███▏      | 191/590 [01:11<01:54,  3.47it/s] 33%|███▎      | 192/590 [01:11<01:54,  3.47it/s] 33%|███▎      | 193/590 [01:12<01:54,  3.47it/s] 33%|███▎      | 194/590 [01:12<01:54,  3.47it/s] 33%|███▎      | 195/590 [01:12<01:53,  3.47it/s] 33%|███▎      | 196/590 [01:12<01:53,  3.48it/s] 33%|███▎      | 197/590 [01:13<01:53,  3.48it/s] 34%|███▎      | 198/590 [01:13<01:53,  3.47it/s] 34%|███▎      | 199/590 [01:13<01:52,  3.47it/s] 34%|███▍      | 200/590 [01:14<01:52,  3.47it/s] 34%|███▍      | 201/590 [01:14<01:51,  3.47it/s] 34%|███▍      | 202/590 [01:14<01:51,  3.47it/s] 34%|███▍      | 203/590 [01:14<01:51,  3.47it/s] 35%|███▍      | 204/590 [01:15<01:51,  3.47it/s] 35%|███▍      | 205/590 [01:15<01:50,  3.47it/s] 35%|███▍      | 206/590 [01:15<01:50,  3.47it/s] 35%|███▌      | 207/590 [01:16<01:50,  3.47it/s] 35%|███▌      | 208/590 [01:16<01:49,  3.47it/s] 35%|███▌      | 209/590 [01:16<01:49,  3.47it/s] 36%|███▌      | 210/590 [01:17<01:49,  3.47it/s] 36%|███▌      | 211/590 [01:17<01:49,  3.47it/s] 36%|███▌      | 212/590 [01:17<01:48,  3.47it/s] 36%|███▌      | 213/590 [01:17<01:48,  3.48it/s] 36%|███▋      | 214/590 [01:18<01:48,  3.47it/s] 36%|███▋      | 215/590 [01:18<01:47,  3.48it/s] 37%|███▋      | 216/590 [01:18<01:47,  3.47it/s] 37%|███▋      | 217/590 [01:19<01:47,  3.47it/s] 37%|███▋      | 218/590 [01:19<01:47,  3.47it/s] 37%|███▋      | 219/590 [01:19<01:46,  3.47it/s] 37%|███▋      | 220/590 [01:19<01:46,  3.47it/s] 37%|███▋      | 221/590 [01:20<01:46,  3.47it/s] 38%|███▊      | 222/590 [01:20<01:46,  3.47it/s] 38%|███▊      | 223/590 [01:20<01:45,  3.47it/s] 38%|███▊      | 224/590 [01:21<01:45,  3.47it/s] 38%|███▊      | 225/590 [01:21<01:45,  3.47it/s] 38%|███▊      | 226/590 [01:21<01:44,  3.47it/s] 38%|███▊      | 227/590 [01:21<01:44,  3.47it/s] 39%|███▊      | 228/590 [01:22<01:44,  3.47it/s] 39%|███▉      | 229/590 [01:22<01:43,  3.47it/s] 39%|███▉      | 230/590 [01:22<01:43,  3.47it/s] 39%|███▉      | 231/590 [01:23<01:44,  3.45it/s] 39%|███▉      | 232/590 [01:23<01:43,  3.46it/s] 39%|███▉      | 233/590 [01:23<01:43,  3.46it/s] 40%|███▉      | 234/590 [01:23<01:42,  3.47it/s] 40%|███▉      | 235/590 [01:24<01:42,  3.47it/s] 40%|████      | 236/590 [01:24<01:42,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 13:18:25,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:18:25,676 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:18:25,676 >>   Batch size = 8
{'eval_loss': 1.0070912837982178, 'eval_runtime': 9.3452, 'eval_samples_per_second': 373.774, 'eval_steps_per_second': 46.762, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.12it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.33it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.59it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.98it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.62it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.40it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.19it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.91it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.76it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.72it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.73it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.79it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.71it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.71it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.77it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.76it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.77it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.64it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.57it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.71it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.74it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.54it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.79it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.61it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.50it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.50it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.39it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.51it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.65it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.65it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.67it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.67it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.70it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.62it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.51it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.61it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 43.36it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.41it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 45.14it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 45.58it/s][A
 51%|█████     | 223/437 [00:04<00:04, 45.90it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.21it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.43it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.51it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.39it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.33it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.26it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.39it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.43it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.44it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.46it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.41it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.44it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.39it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.32it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.36it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.47it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.58it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.63it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.56it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.69it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.65it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.67it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 43.99it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 44.77it/s][A
 81%|████████  | 353/437 [00:07<00:01, 45.38it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 45.80it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.14it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.30it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.50it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.53it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.50it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.31it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.37it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.57it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.67it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.73it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.67it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.76it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.73it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.73it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.58it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:33<01:42,  3.47it/s]
100%|██████████| 437/437 [00:09<00:00, 46.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:18:35,221 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 13:18:35,242 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:18:37,583 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:18:37,597 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:18:37,604 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:41<31:23,  5.34s/it] 40%|████      | 238/590 [01:41<22:25,  3.82s/it] 41%|████      | 239/590 [01:42<16:09,  2.76s/it] 41%|████      | 240/590 [01:42<11:47,  2.02s/it] 41%|████      | 241/590 [01:42<08:43,  1.50s/it] 41%|████      | 242/590 [01:43<06:35,  1.14s/it] 41%|████      | 243/590 [01:43<05:06,  1.13it/s] 41%|████▏     | 244/590 [01:43<04:03,  1.42it/s] 42%|████▏     | 245/590 [01:43<03:19,  1.73it/s] 42%|████▏     | 246/590 [01:44<02:49,  2.03it/s] 42%|████▏     | 247/590 [01:44<02:27,  2.32it/s] 42%|████▏     | 248/590 [01:44<02:12,  2.58it/s] 42%|████▏     | 249/590 [01:45<02:02,  2.79it/s] 42%|████▏     | 250/590 [01:45<01:54,  2.97it/s] 43%|████▎     | 251/590 [01:45<01:49,  3.10it/s] 43%|████▎     | 252/590 [01:45<01:45,  3.21it/s] 43%|████▎     | 253/590 [01:46<01:42,  3.28it/s] 43%|████▎     | 254/590 [01:46<01:40,  3.34it/s] 43%|████▎     | 255/590 [01:46<01:39,  3.38it/s] 43%|████▎     | 256/590 [01:47<01:38,  3.41it/s] 44%|████▎     | 257/590 [01:47<01:37,  3.43it/s] 44%|████▎     | 258/590 [01:47<01:36,  3.44it/s] 44%|████▍     | 259/590 [01:47<01:35,  3.45it/s] 44%|████▍     | 260/590 [01:48<01:35,  3.45it/s] 44%|████▍     | 261/590 [01:48<01:35,  3.46it/s] 44%|████▍     | 262/590 [01:48<01:34,  3.46it/s] 45%|████▍     | 263/590 [01:49<01:34,  3.47it/s] 45%|████▍     | 264/590 [01:49<01:33,  3.47it/s] 45%|████▍     | 265/590 [01:49<01:33,  3.47it/s] 45%|████▌     | 266/590 [01:49<01:33,  3.47it/s] 45%|████▌     | 267/590 [01:50<01:32,  3.47it/s] 45%|████▌     | 268/590 [01:50<01:32,  3.47it/s] 46%|████▌     | 269/590 [01:50<01:32,  3.48it/s] 46%|████▌     | 270/590 [01:51<01:32,  3.48it/s] 46%|████▌     | 271/590 [01:51<01:32,  3.47it/s] 46%|████▌     | 272/590 [01:51<01:31,  3.47it/s] 46%|████▋     | 273/590 [01:51<01:31,  3.47it/s] 46%|████▋     | 274/590 [01:52<01:31,  3.45it/s] 47%|████▋     | 275/590 [01:52<01:31,  3.46it/s] 47%|████▋     | 276/590 [01:52<01:30,  3.47it/s] 47%|████▋     | 277/590 [01:53<01:30,  3.47it/s] 47%|████▋     | 278/590 [01:53<01:32,  3.38it/s] 47%|████▋     | 279/590 [01:53<01:31,  3.40it/s] 47%|████▋     | 280/590 [01:54<01:30,  3.42it/s] 48%|████▊     | 281/590 [01:54<01:29,  3.44it/s] 48%|████▊     | 282/590 [01:54<01:29,  3.43it/s] 48%|████▊     | 283/590 [01:54<01:29,  3.45it/s] 48%|████▊     | 284/590 [01:55<01:28,  3.45it/s] 48%|████▊     | 285/590 [01:55<01:28,  3.46it/s] 48%|████▊     | 286/590 [01:55<01:27,  3.46it/s] 49%|████▊     | 287/590 [01:56<01:27,  3.47it/s] 49%|████▉     | 288/590 [01:56<01:27,  3.47it/s] 49%|████▉     | 289/590 [01:56<01:26,  3.47it/s] 49%|████▉     | 290/590 [01:56<01:26,  3.47it/s] 49%|████▉     | 291/590 [01:57<01:26,  3.47it/s] 49%|████▉     | 292/590 [01:57<01:25,  3.47it/s] 50%|████▉     | 293/590 [01:57<01:25,  3.47it/s] 50%|████▉     | 294/590 [01:58<01:25,  3.47it/s] 50%|█████     | 295/590 [01:58<01:24,  3.47it/s] 50%|█████     | 296/590 [01:58<01:24,  3.47it/s] 50%|█████     | 297/590 [01:58<01:24,  3.47it/s] 51%|█████     | 298/590 [01:59<01:24,  3.47it/s] 51%|█████     | 299/590 [01:59<01:23,  3.47it/s] 51%|█████     | 300/590 [01:59<01:23,  3.47it/s] 51%|█████     | 301/590 [02:00<01:23,  3.47it/s] 51%|█████     | 302/590 [02:00<01:22,  3.47it/s] 51%|█████▏    | 303/590 [02:00<01:22,  3.47it/s] 52%|█████▏    | 304/590 [02:00<01:22,  3.46it/s] 52%|█████▏    | 305/590 [02:01<01:22,  3.46it/s] 52%|█████▏    | 306/590 [02:01<01:21,  3.47it/s] 52%|█████▏    | 307/590 [02:01<01:21,  3.47it/s] 52%|█████▏    | 308/590 [02:02<01:21,  3.47it/s] 52%|█████▏    | 309/590 [02:02<01:20,  3.47it/s] 53%|█████▎    | 310/590 [02:02<01:20,  3.47it/s] 53%|█████▎    | 311/590 [02:02<01:20,  3.47it/s] 53%|█████▎    | 312/590 [02:03<01:20,  3.47it/s] 53%|█████▎    | 313/590 [02:03<01:19,  3.47it/s] 53%|█████▎    | 314/590 [02:03<01:19,  3.47it/s] 53%|█████▎    | 315/590 [02:04<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:04<01:19,  3.46it/s] 54%|█████▎    | 317/590 [02:04<01:18,  3.47it/s] 54%|█████▍    | 318/590 [02:04<01:18,  3.47it/s] 54%|█████▍    | 319/590 [02:05<01:18,  3.47it/s] 54%|█████▍    | 320/590 [02:05<01:17,  3.47it/s] 54%|█████▍    | 321/590 [02:05<01:17,  3.47it/s] 55%|█████▍    | 322/590 [02:06<01:17,  3.47it/s] 55%|█████▍    | 323/590 [02:06<01:16,  3.47it/s] 55%|█████▍    | 324/590 [02:06<01:16,  3.47it/s] 55%|█████▌    | 325/590 [02:06<01:16,  3.47it/s] 55%|█████▌    | 326/590 [02:07<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:07<01:15,  3.46it/s] 56%|█████▌    | 328/590 [02:07<01:15,  3.47it/s] 56%|█████▌    | 329/590 [02:08<01:15,  3.47it/s] 56%|█████▌    | 330/590 [02:08<01:15,  3.46it/s] 56%|█████▌    | 331/590 [02:08<01:14,  3.47it/s] 56%|█████▋    | 332/590 [02:09<01:14,  3.47it/s] 56%|█████▋    | 333/590 [02:09<01:14,  3.47it/s] 57%|█████▋    | 334/590 [02:09<01:13,  3.47it/s] 57%|█████▋    | 335/590 [02:09<01:13,  3.47it/s] 57%|█████▋    | 336/590 [02:10<01:13,  3.47it/s] 57%|█████▋    | 337/590 [02:10<01:12,  3.47it/s] 57%|█████▋    | 338/590 [02:10<01:12,  3.47it/s] 57%|█████▋    | 339/590 [02:11<01:12,  3.47it/s] 58%|█████▊    | 340/590 [02:11<01:12,  3.47it/s] 58%|█████▊    | 341/590 [02:11<01:11,  3.47it/s] 58%|█████▊    | 342/590 [02:11<01:11,  3.46it/s] 58%|█████▊    | 343/590 [02:12<01:11,  3.47it/s] 58%|█████▊    | 344/590 [02:12<01:10,  3.47it/s] 58%|█████▊    | 345/590 [02:12<01:10,  3.47it/s] 59%|█████▊    | 346/590 [02:13<01:10,  3.47it/s] 59%|█████▉    | 347/590 [02:13<01:10,  3.47it/s] 59%|█████▉    | 348/590 [02:13<01:09,  3.47it/s] 59%|█████▉    | 349/590 [02:13<01:09,  3.47it/s] 59%|█████▉    | 350/590 [02:14<01:09,  3.47it/s] 59%|█████▉    | 351/590 [02:14<01:08,  3.47it/s] 60%|█████▉    | 352/590 [02:14<01:08,  3.47it/s] 60%|█████▉    | 353/590 [02:15<01:08,  3.44it/s] 60%|██████    | 354/590 [02:15<01:08,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:19:16,544 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:19:16,544 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:19:16,544 >>   Batch size = 8
{'eval_loss': 0.9932879209518433, 'eval_runtime': 9.4147, 'eval_samples_per_second': 371.015, 'eval_steps_per_second': 46.417, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.58it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.78it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.13it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.41it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.83it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.60it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.62it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.72it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.79it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.72it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.71it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.77it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.65it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.50it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.53it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.55it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.56it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.70it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.72it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.71it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.63it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.53it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.38it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.31it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.40it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.47it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.58it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.68it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.69it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.72it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.64it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.60it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.51it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.56it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.61it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.74it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.82it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.62it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.61it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.61it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.58it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.43it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.38it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.44it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.44it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.41it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.44it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.36it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.34it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.39it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.45it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.38it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.49it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.66it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.68it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.66it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.57it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.59it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.57it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.55it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.49it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.49it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.54it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.51it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.58it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.65it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.66it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.67it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.50it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.53it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 43.20it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 44.28it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 45.04it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 45.52it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 45.89it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.17it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.44it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:24<01:08,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.44it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:19:25,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 13:19:25,976 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:19:28,306 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:19:28,326 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:19:28,333 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:32<20:45,  5.30s/it] 60%|██████    | 356/590 [02:32<14:48,  3.80s/it] 61%|██████    | 357/590 [02:32<10:39,  2.74s/it] 61%|██████    | 358/590 [02:33<07:45,  2.01s/it] 61%|██████    | 359/590 [02:33<05:44,  1.49s/it] 61%|██████    | 360/590 [02:33<04:19,  1.13s/it] 61%|██████    | 361/590 [02:34<03:20,  1.14it/s] 61%|██████▏   | 362/590 [02:34<02:39,  1.43it/s] 62%|██████▏   | 363/590 [02:34<02:10,  1.73it/s] 62%|██████▏   | 364/590 [02:34<01:50,  2.04it/s] 62%|██████▏   | 365/590 [02:35<01:36,  2.33it/s] 62%|██████▏   | 366/590 [02:35<01:26,  2.59it/s] 62%|██████▏   | 367/590 [02:35<01:19,  2.80it/s] 62%|██████▏   | 368/590 [02:36<01:14,  2.97it/s] 63%|██████▎   | 369/590 [02:36<01:11,  3.11it/s] 63%|██████▎   | 370/590 [02:36<01:08,  3.21it/s] 63%|██████▎   | 371/590 [02:36<01:06,  3.29it/s] 63%|██████▎   | 372/590 [02:37<01:05,  3.34it/s] 63%|██████▎   | 373/590 [02:37<01:04,  3.38it/s] 63%|██████▎   | 374/590 [02:37<01:03,  3.41it/s] 64%|██████▎   | 375/590 [02:38<01:02,  3.43it/s] 64%|██████▎   | 376/590 [02:38<01:02,  3.44it/s] 64%|██████▍   | 377/590 [02:38<01:01,  3.45it/s] 64%|██████▍   | 378/590 [02:38<01:01,  3.44it/s] 64%|██████▍   | 379/590 [02:39<01:01,  3.45it/s] 64%|██████▍   | 380/590 [02:39<01:00,  3.46it/s] 65%|██████▍   | 381/590 [02:39<01:00,  3.46it/s] 65%|██████▍   | 382/590 [02:40<01:00,  3.47it/s] 65%|██████▍   | 383/590 [02:40<00:59,  3.47it/s] 65%|██████▌   | 384/590 [02:40<00:59,  3.47it/s] 65%|██████▌   | 385/590 [02:40<00:59,  3.47it/s] 65%|██████▌   | 386/590 [02:41<00:58,  3.47it/s] 66%|██████▌   | 387/590 [02:41<00:58,  3.47it/s] 66%|██████▌   | 388/590 [02:41<00:58,  3.47it/s] 66%|██████▌   | 389/590 [02:42<00:57,  3.47it/s] 66%|██████▌   | 390/590 [02:42<00:57,  3.46it/s] 66%|██████▋   | 391/590 [02:42<00:57,  3.47it/s] 66%|██████▋   | 392/590 [02:43<00:57,  3.47it/s] 67%|██████▋   | 393/590 [02:43<00:56,  3.47it/s] 67%|██████▋   | 394/590 [02:43<00:56,  3.47it/s] 67%|██████▋   | 395/590 [02:43<00:56,  3.47it/s] 67%|██████▋   | 396/590 [02:44<00:55,  3.47it/s] 67%|██████▋   | 397/590 [02:44<00:55,  3.47it/s] 67%|██████▋   | 398/590 [02:44<00:55,  3.47it/s] 68%|██████▊   | 399/590 [02:45<00:54,  3.47it/s] 68%|██████▊   | 400/590 [02:45<00:54,  3.47it/s] 68%|██████▊   | 401/590 [02:45<00:54,  3.45it/s] 68%|██████▊   | 402/590 [02:45<00:54,  3.46it/s] 68%|██████▊   | 403/590 [02:46<00:53,  3.46it/s] 68%|██████▊   | 404/590 [02:46<00:53,  3.46it/s] 69%|██████▊   | 405/590 [02:46<00:53,  3.47it/s] 69%|██████▉   | 406/590 [02:47<00:53,  3.47it/s] 69%|██████▉   | 407/590 [02:47<00:52,  3.47it/s] 69%|██████▉   | 408/590 [02:47<00:52,  3.47it/s] 69%|██████▉   | 409/590 [02:47<00:52,  3.47it/s] 69%|██████▉   | 410/590 [02:48<00:51,  3.47it/s] 70%|██████▉   | 411/590 [02:48<00:51,  3.47it/s] 70%|██████▉   | 412/590 [02:48<00:51,  3.46it/s] 70%|███████   | 413/590 [02:49<00:51,  3.46it/s] 70%|███████   | 414/590 [02:49<00:50,  3.47it/s] 70%|███████   | 415/590 [02:49<00:50,  3.47it/s] 71%|███████   | 416/590 [02:49<00:50,  3.47it/s] 71%|███████   | 417/590 [02:50<00:49,  3.47it/s] 71%|███████   | 418/590 [02:50<00:49,  3.47it/s] 71%|███████   | 419/590 [02:50<00:49,  3.47it/s] 71%|███████   | 420/590 [02:51<00:48,  3.47it/s] 71%|███████▏  | 421/590 [02:51<00:48,  3.47it/s] 72%|███████▏  | 422/590 [02:51<00:48,  3.47it/s] 72%|███████▏  | 423/590 [02:51<00:48,  3.46it/s] 72%|███████▏  | 424/590 [02:52<00:47,  3.47it/s] 72%|███████▏  | 425/590 [02:52<00:47,  3.47it/s] 72%|███████▏  | 426/590 [02:52<00:47,  3.47it/s] 72%|███████▏  | 427/590 [02:53<00:46,  3.47it/s] 73%|███████▎  | 428/590 [02:53<00:46,  3.47it/s] 73%|███████▎  | 429/590 [02:53<00:46,  3.47it/s] 73%|███████▎  | 430/590 [02:53<00:46,  3.47it/s] 73%|███████▎  | 431/590 [02:54<00:45,  3.47it/s] 73%|███████▎  | 432/590 [02:54<00:45,  3.47it/s] 73%|███████▎  | 433/590 [02:54<00:45,  3.47it/s] 74%|███████▎  | 434/590 [02:55<00:45,  3.46it/s] 74%|███████▎  | 435/590 [02:55<00:44,  3.46it/s] 74%|███████▍  | 436/590 [02:55<00:44,  3.47it/s] 74%|███████▍  | 437/590 [02:55<00:44,  3.47it/s] 74%|███████▍  | 438/590 [02:56<00:43,  3.47it/s] 74%|███████▍  | 439/590 [02:56<00:43,  3.47it/s] 75%|███████▍  | 440/590 [02:56<00:43,  3.47it/s] 75%|███████▍  | 441/590 [02:57<00:42,  3.47it/s] 75%|███████▍  | 442/590 [02:57<00:42,  3.47it/s] 75%|███████▌  | 443/590 [02:57<00:42,  3.47it/s] 75%|███████▌  | 444/590 [02:58<00:42,  3.47it/s] 75%|███████▌  | 445/590 [02:58<00:41,  3.46it/s] 76%|███████▌  | 446/590 [02:58<00:41,  3.47it/s] 76%|███████▌  | 447/590 [02:58<00:41,  3.47it/s] 76%|███████▌  | 448/590 [02:59<00:40,  3.47it/s] 76%|███████▌  | 449/590 [02:59<00:40,  3.47it/s] 76%|███████▋  | 450/590 [02:59<00:40,  3.47it/s] 76%|███████▋  | 451/590 [03:00<00:40,  3.47it/s] 77%|███████▋  | 452/590 [03:00<00:39,  3.47it/s] 77%|███████▋  | 453/590 [03:00<00:39,  3.47it/s] 77%|███████▋  | 454/590 [03:00<00:39,  3.47it/s] 77%|███████▋  | 455/590 [03:01<00:38,  3.47it/s] 77%|███████▋  | 456/590 [03:01<00:38,  3.46it/s] 77%|███████▋  | 457/590 [03:01<00:38,  3.47it/s] 78%|███████▊  | 458/590 [03:02<00:38,  3.47it/s] 78%|███████▊  | 459/590 [03:02<00:37,  3.47it/s] 78%|███████▊  | 460/590 [03:02<00:37,  3.47it/s] 78%|███████▊  | 461/590 [03:02<00:37,  3.47it/s] 78%|███████▊  | 462/590 [03:03<00:36,  3.47it/s] 78%|███████▊  | 463/590 [03:03<00:36,  3.47it/s] 79%|███████▊  | 464/590 [03:03<00:36,  3.47it/s] 79%|███████▉  | 465/590 [03:04<00:35,  3.47it/s] 79%|███████▉  | 466/590 [03:04<00:35,  3.47it/s] 79%|███████▉  | 467/590 [03:04<00:35,  3.44it/s] 79%|███████▉  | 468/590 [03:04<00:35,  3.45it/s] 79%|███████▉  | 469/590 [03:05<00:34,  3.46it/s] 80%|███████▉  | 470/590 [03:05<00:34,  3.46it/s] 80%|███████▉  | 471/590 [03:05<00:34,  3.47it/s] 80%|████████  | 472/590 [03:06<00:34,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 13:20:07,257 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:20:07,257 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:20:07,257 >>   Batch size = 8
{'eval_loss': 0.9900286197662354, 'eval_runtime': 9.4029, 'eval_samples_per_second': 371.483, 'eval_steps_per_second': 46.475, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.33it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.62it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.87it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.18it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.31it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.21it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.93it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.67it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.67it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.67it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.57it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.69it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.76it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.75it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.79it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.72it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.64it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.50it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.47it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.54it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.54it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.50it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.49it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.43it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.53it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.56it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.45it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.47it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.52it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.61it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.65it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.53it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.63it/s][A
 41%|████      | 178/437 [00:03<00:06, 41.68it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 43.04it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 44.13it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 44.89it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 45.45it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 45.83it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.08it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.26it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.14it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.21it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.26it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.27it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.33it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.38it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.40it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.42it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.33it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.30it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.26it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.34it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.47it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.48it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.56it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.55it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.56it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.61it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.58it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 44.73it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 45.36it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.72it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.04it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.17it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.30it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.45it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.54it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.38it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.46it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.43it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.54it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.53it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.55it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.68it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.66it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.65it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.59it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.59it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.63it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.58it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.62it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.60it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:15<00:34,  3.47it/s]
100%|██████████| 437/437 [00:09<00:00, 46.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:20:16,709 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 13:20:16,727 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:20:19,168 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:20:19,210 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:20:19,221 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:25<11:47,  6.05s/it] 80%|████████  | 474/590 [03:25<08:21,  4.32s/it] 81%|████████  | 475/590 [03:26<05:57,  3.11s/it] 81%|████████  | 476/590 [03:26<04:18,  2.26s/it] 81%|████████  | 477/590 [03:26<03:08,  1.67s/it] 81%|████████  | 478/590 [03:27<02:20,  1.26s/it] 81%|████████  | 479/590 [03:27<01:47,  1.04it/s] 81%|████████▏ | 480/590 [03:27<01:23,  1.31it/s] 82%|████████▏ | 481/590 [03:27<01:07,  1.61it/s] 82%|████████▏ | 482/590 [03:28<00:56,  1.92it/s] 82%|████████▏ | 483/590 [03:28<00:48,  2.22it/s] 82%|████████▏ | 484/590 [03:28<00:42,  2.49it/s] 82%|████████▏ | 485/590 [03:29<00:38,  2.71it/s] 82%|████████▏ | 486/590 [03:29<00:35,  2.90it/s] 83%|████████▎ | 487/590 [03:29<00:33,  3.05it/s] 83%|████████▎ | 488/590 [03:29<00:32,  3.17it/s] 83%|████████▎ | 489/590 [03:30<00:31,  3.26it/s] 83%|████████▎ | 490/590 [03:30<00:30,  3.32it/s] 83%|████████▎ | 491/590 [03:30<00:29,  3.37it/s] 83%|████████▎ | 492/590 [03:31<00:28,  3.40it/s] 84%|████████▎ | 493/590 [03:31<00:28,  3.42it/s] 84%|████████▎ | 494/590 [03:31<00:27,  3.44it/s] 84%|████████▍ | 495/590 [03:31<00:27,  3.45it/s] 84%|████████▍ | 496/590 [03:32<00:27,  3.45it/s] 84%|████████▍ | 497/590 [03:32<00:26,  3.46it/s] 84%|████████▍ | 498/590 [03:32<00:26,  3.46it/s] 85%|████████▍ | 499/590 [03:33<00:26,  3.46it/s] 85%|████████▍ | 500/590 [03:33<00:25,  3.47it/s]                                                  85%|████████▍ | 500/590 [03:33<00:25,  3.47it/s] 85%|████████▍ | 501/590 [03:33<00:25,  3.47it/s] 85%|████████▌ | 502/590 [03:33<00:25,  3.47it/s] 85%|████████▌ | 503/590 [03:34<00:25,  3.47it/s] 85%|████████▌ | 504/590 [03:34<00:24,  3.47it/s] 86%|████████▌ | 505/590 [03:34<00:24,  3.47it/s] 86%|████████▌ | 506/590 [03:35<00:24,  3.47it/s] 86%|████████▌ | 507/590 [03:35<00:24,  3.45it/s] 86%|████████▌ | 508/590 [03:35<00:23,  3.45it/s] 86%|████████▋ | 509/590 [03:35<00:23,  3.46it/s] 86%|████████▋ | 510/590 [03:36<00:23,  3.46it/s] 87%|████████▋ | 511/590 [03:36<00:22,  3.47it/s] 87%|████████▋ | 512/590 [03:36<00:22,  3.47it/s] 87%|████████▋ | 513/590 [03:37<00:22,  3.47it/s] 87%|████████▋ | 514/590 [03:37<00:21,  3.47it/s] 87%|████████▋ | 515/590 [03:37<00:21,  3.47it/s] 87%|████████▋ | 516/590 [03:37<00:21,  3.48it/s] 88%|████████▊ | 517/590 [03:38<00:21,  3.47it/s] 88%|████████▊ | 518/590 [03:38<00:20,  3.46it/s] 88%|████████▊ | 519/590 [03:38<00:20,  3.46it/s] 88%|████████▊ | 520/590 [03:39<00:20,  3.47it/s] 88%|████████▊ | 521/590 [03:39<00:19,  3.47it/s] 88%|████████▊ | 522/590 [03:39<00:19,  3.47it/s] 89%|████████▊ | 523/590 [03:39<00:19,  3.47it/s] 89%|████████▉ | 524/590 [03:40<00:19,  3.47it/s] 89%|████████▉ | 525/590 [03:40<00:18,  3.47it/s] 89%|████████▉ | 526/590 [03:40<00:18,  3.48it/s] 89%|████████▉ | 527/590 [03:41<00:18,  3.48it/s] 89%|████████▉ | 528/590 [03:41<00:17,  3.48it/s] 90%|████████▉ | 529/590 [03:41<00:17,  3.48it/s] 90%|████████▉ | 530/590 [03:41<00:17,  3.48it/s] 90%|█████████ | 531/590 [03:42<00:16,  3.48it/s] 90%|█████████ | 532/590 [03:42<00:16,  3.47it/s] 90%|█████████ | 533/590 [03:42<00:16,  3.47it/s] 91%|█████████ | 534/590 [03:43<00:16,  3.47it/s] 91%|█████████ | 535/590 [03:43<00:15,  3.47it/s] 91%|█████████ | 536/590 [03:43<00:15,  3.47it/s] 91%|█████████ | 537/590 [03:44<00:15,  3.45it/s] 91%|█████████ | 538/590 [03:44<00:15,  3.46it/s] 91%|█████████▏| 539/590 [03:44<00:14,  3.46it/s] 92%|█████████▏| 540/590 [03:44<00:14,  3.47it/s] 92%|█████████▏| 541/590 [03:45<00:14,  3.47it/s] 92%|█████████▏| 542/590 [03:45<00:13,  3.47it/s] 92%|█████████▏| 543/590 [03:45<00:13,  3.47it/s] 92%|█████████▏| 544/590 [03:46<00:13,  3.47it/s] 92%|█████████▏| 545/590 [03:46<00:12,  3.47it/s] 93%|█████████▎| 546/590 [03:46<00:12,  3.47it/s] 93%|█████████▎| 547/590 [03:46<00:12,  3.47it/s] 93%|█████████▎| 548/590 [03:47<00:12,  3.46it/s] 93%|█████████▎| 549/590 [03:47<00:11,  3.47it/s] 93%|█████████▎| 550/590 [03:47<00:11,  3.47it/s] 93%|█████████▎| 551/590 [03:48<00:11,  3.47it/s] 94%|█████████▎| 552/590 [03:48<00:10,  3.47it/s] 94%|█████████▎| 553/590 [03:48<00:10,  3.47it/s] 94%|█████████▍| 554/590 [03:48<00:10,  3.47it/s] 94%|█████████▍| 555/590 [03:49<00:10,  3.47it/s] 94%|█████████▍| 556/590 [03:49<00:09,  3.47it/s] 94%|█████████▍| 557/590 [03:49<00:09,  3.47it/s] 95%|█████████▍| 558/590 [03:50<00:09,  3.47it/s] 95%|█████████▍| 559/590 [03:50<00:08,  3.46it/s] 95%|█████████▍| 560/590 [03:50<00:08,  3.47it/s] 95%|█████████▌| 561/590 [03:50<00:08,  3.47it/s] 95%|█████████▌| 562/590 [03:51<00:08,  3.47it/s] 95%|█████████▌| 563/590 [03:51<00:07,  3.47it/s] 96%|█████████▌| 564/590 [03:51<00:07,  3.47it/s] 96%|█████████▌| 565/590 [03:52<00:07,  3.47it/s] 96%|█████████▌| 566/590 [03:52<00:06,  3.46it/s] 96%|█████████▌| 567/590 [03:52<00:06,  3.46it/s] 96%|█████████▋| 568/590 [03:52<00:06,  3.47it/s] 96%|█████████▋| 569/590 [03:53<00:06,  3.47it/s] 97%|█████████▋| 570/590 [03:53<00:05,  3.39it/s] 97%|█████████▋| 571/590 [03:53<00:05,  3.41it/s] 97%|█████████▋| 572/590 [03:54<00:05,  3.42it/s] 97%|█████████▋| 573/590 [03:54<00:04,  3.44it/s] 97%|█████████▋| 574/590 [03:54<00:04,  3.45it/s] 97%|█████████▋| 575/590 [03:54<00:04,  3.46it/s] 98%|█████████▊| 576/590 [03:55<00:04,  3.46it/s] 98%|█████████▊| 577/590 [03:55<00:03,  3.46it/s] 98%|█████████▊| 578/590 [03:55<00:03,  3.47it/s] 98%|█████████▊| 579/590 [03:56<00:03,  3.46it/s] 98%|█████████▊| 580/590 [03:56<00:02,  3.47it/s] 98%|█████████▊| 581/590 [03:56<00:02,  3.46it/s] 99%|█████████▊| 582/590 [03:57<00:02,  3.46it/s] 99%|█████████▉| 583/590 [03:57<00:02,  3.46it/s] 99%|█████████▉| 584/590 [03:57<00:01,  3.47it/s] 99%|█████████▉| 585/590 [03:57<00:01,  3.47it/s] 99%|█████████▉| 586/590 [03:58<00:01,  3.47it/s] 99%|█████████▉| 587/590 [03:58<00:00,  3.47it/s]100%|█████████▉| 588/590 [03:58<00:00,  3.47it/s]100%|█████████▉| 589/590 [03:59<00:00,  3.47it/s]100%|██████████| 590/590 [03:59<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 13:21:00,456 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:21:00,457 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:21:00,457 >>   Batch size = 8
{'eval_loss': 0.9918655753135681, 'eval_runtime': 9.4343, 'eval_samples_per_second': 370.245, 'eval_steps_per_second': 46.32, 'epoch': 4.0}
{'loss': 0.8388, 'learning_rate': 5.783898305084746e-06, 'epoch': 4.24}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.81it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.10it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.62it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.27it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.78it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.67it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.65it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.73it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.77it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.84it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.76it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.72it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.64it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.61it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.54it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.57it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.54it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.55it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.66it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.73it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.78it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.71it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.58it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.51it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.47it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.55it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.57it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.53it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.63it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.67it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.70it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.54it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.55it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.48it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.52it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.62it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.64it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.64it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.57it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.51it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.55it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.49it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.54it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.56it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.50it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.54it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.60it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.67it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.58it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.62it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.56it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.54it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.58it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.53it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.53it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.50it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.56it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.61it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.60it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.59it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.50it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.52it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.57it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.49it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.51it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.59it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.55it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.55it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.58it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.55it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.61it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.59it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.61it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.59it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.49it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.57it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.54it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.54it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.49it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.44it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.53it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:08<00:00,  3.47it/s]
100%|██████████| 437/437 [00:09<00:00, 46.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:21:09,849 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 13:21:09,871 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:21:12,387 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:21:12,401 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:21:12,414 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:21:17,851 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:21:17,856 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354 (score: 0.9900286197662354).
                                                 100%|██████████| 590/590 [04:18<00:00,  3.47it/s]100%|██████████| 590/590 [04:18<00:00,  2.28it/s]
[INFO|trainer.py:1894] 2023-08-28 13:21:19,763 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 13:21:19,778 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:21:22,498 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:21:22,533 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:21:22,546 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:21:22,729 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   train_loss               =     0.8334
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   train_runtime            = 0:04:18.62
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   train_samples            =       7553
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   train_samples_per_second =    146.024
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:22,729 >>   train_steps_per_second   =      2.281
{'eval_loss': 0.992085337638855, 'eval_runtime': 9.3756, 'eval_samples_per_second': 372.561, 'eval_steps_per_second': 46.61, 'epoch': 5.0}
{'train_runtime': 258.6215, 'train_samples_per_second': 146.024, 'train_steps_per_second': 2.281, 'train_loss': 0.8333813812773107, 'epoch': 5.0}
08/28/2023 13:21:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:21:22,762 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:21:22,762 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 13:21:22,762 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.49it/s]  3%|▎         | 12/437 [00:00<00:08, 51.13it/s]  4%|▍         | 18/437 [00:00<00:08, 49.21it/s]  5%|▌         | 23/437 [00:00<00:08, 48.36it/s]  6%|▋         | 28/437 [00:00<00:08, 47.93it/s]  8%|▊         | 33/437 [00:00<00:08, 47.62it/s]  9%|▊         | 38/437 [00:00<00:08, 47.43it/s] 10%|▉         | 43/437 [00:00<00:08, 47.29it/s] 11%|█         | 48/437 [00:00<00:08, 47.12it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.76it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.83it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.95it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.91it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.00it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.98it/s] 19%|█▉        | 83/437 [00:01<00:07, 46.79it/s] 20%|██        | 88/437 [00:01<00:07, 46.92it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.91it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.83it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.87it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.87it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.81it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.75it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.84it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.88it/s] 30%|███       | 133/437 [00:02<00:06, 46.82it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.73it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.69it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.76it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.90it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.86it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.77it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.82it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.80it/s] 41%|████      | 178/437 [00:03<00:05, 46.89it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.75it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.78it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.79it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.86it/s] 46%|████▋     | 203/437 [00:04<00:04, 46.86it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.89it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.77it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.78it/s] 51%|█████     | 223/437 [00:04<00:04, 46.83it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.80it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.76it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.82it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.81it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.81it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.87it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.76it/s] 60%|██████    | 263/437 [00:05<00:03, 46.71it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.82it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.79it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.74it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.76it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.76it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.81it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.85it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.86it/s] 70%|███████   | 308/437 [00:06<00:02, 46.84it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.79it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.78it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.86it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.80it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.75it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.78it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.70it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.69it/s] 81%|████████  | 353/437 [00:07<00:01, 46.78it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.73it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.76it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.82it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.79it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.82it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.79it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.81it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.85it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.88it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.79it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.79it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.80it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.84it/s] 97%|█████████▋| 423/437 [00:09<00:00, 46.83it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.89it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.74it/s]100%|██████████| 437/437 [00:09<00:00, 46.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:21:32,097 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   eval_loss               =       0.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   eval_runtime            = 0:00:09.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   eval_samples_per_second =    374.189
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   eval_steps_per_second   =     46.814
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:21:32,097 >>   perplexity              =     2.6913
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:38,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:38,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:38,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:38,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:38,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:21:39,369 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:21:39,370 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:21:39,922 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:21:40,953 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:21:40,953 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:43,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:43,757 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:43,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:43,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:21:43,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:21:44,600 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:21:44,601 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:21:45,162 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:21:45,297 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:21:45,297 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.54it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:10,  1.57it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.55it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:20,  1.59it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.55it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.60it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.56it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.60it/s]Extractor Predicting: 54it [00:35,  1.60it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:36,  1.57it/s]Extractor Predicting: 57it [00:37,  1.59it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.50it/s]Extractor Predicting: 68it [00:44,  1.47it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:45,  1.50it/s]Extractor Predicting: 71it [00:46,  1.52it/s]Extractor Predicting: 72it [00:46,  1.55it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.51it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.46it/s]Extractor Predicting: 91it [00:59,  1.47it/s]Extractor Predicting: 92it [01:00,  1.48it/s]Extractor Predicting: 93it [01:01,  1.49it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:02,  1.44it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:03,  1.48it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:07,  1.51it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.52it/s]Extractor Predicting: 106it [01:09,  1.50it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:15,  1.44it/s]Extractor Predicting: 116it [01:16,  1.45it/s]Extractor Predicting: 117it [01:17,  1.45it/s]Extractor Predicting: 118it [01:17,  1.45it/s]Extractor Predicting: 119it [01:18,  1.45it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:19,  1.45it/s]Extractor Predicting: 122it [01:20,  1.44it/s]Extractor Predicting: 123it [01:21,  1.50it/s]Extractor Predicting: 124it [01:21,  1.49it/s]Extractor Predicting: 125it [01:22,  1.46it/s]Extractor Predicting: 126it [01:23,  1.47it/s]Extractor Predicting: 127it [01:24,  1.46it/s]Extractor Predicting: 128it [01:24,  1.46it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:26,  1.45it/s]Extractor Predicting: 131it [01:26,  1.43it/s]Extractor Predicting: 132it [01:27,  1.42it/s]Extractor Predicting: 133it [01:28,  1.42it/s]Extractor Predicting: 134it [01:29,  1.41it/s]Extractor Predicting: 135it [01:29,  1.43it/s]Extractor Predicting: 136it [01:29,  1.74it/s]Extractor Predicting: 136it [01:29,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:23,454 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:23,461 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:23,461 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:23,461 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:23,461 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:23:24,050 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:23:24,051 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:23:24,594 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:23:25,624 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:23:25,624 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:28,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:28,509 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:28,509 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:28,509 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:23:28,509 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:23:29,134 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:23:29,135 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:23:29,694 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:23:29,850 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:23:29,850 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6789473684210526,
  "recall": 0.11079301460062983,
  "score": 0.19049963081466895,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.66it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.67it/s]Extractor Predicting: 33it [00:20,  1.69it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:23,  1.56it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.64it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.64it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:31,  1.69it/s]Extractor Predicting: 52it [00:32,  1.70it/s]Extractor Predicting: 53it [00:32,  1.65it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:34,  1.65it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:38,  1.61it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.64it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.66it/s]Extractor Predicting: 69it [00:42,  1.68it/s]Extractor Predicting: 70it [00:43,  1.66it/s]Extractor Predicting: 71it [00:43,  1.66it/s]Extractor Predicting: 72it [00:44,  1.62it/s]Extractor Predicting: 73it [00:45,  1.64it/s]Extractor Predicting: 74it [00:45,  1.60it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:47,  1.60it/s]Extractor Predicting: 77it [00:47,  1.59it/s]Extractor Predicting: 78it [00:48,  1.60it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:49,  1.61it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:50,  1.62it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:52,  1.56it/s]Extractor Predicting: 86it [00:53,  1.53it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:54,  1.54it/s]Extractor Predicting: 89it [00:55,  1.53it/s]Extractor Predicting: 90it [00:56,  1.50it/s]Extractor Predicting: 91it [00:56,  1.51it/s]Extractor Predicting: 92it [00:57,  1.50it/s]Extractor Predicting: 93it [00:58,  1.48it/s]Extractor Predicting: 94it [00:58,  1.49it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [01:00,  1.51it/s]Extractor Predicting: 97it [01:00,  1.49it/s]Extractor Predicting: 98it [01:01,  1.49it/s]Extractor Predicting: 99it [01:02,  1.47it/s]Extractor Predicting: 100it [01:02,  1.47it/s]Extractor Predicting: 101it [01:03,  1.47it/s]Extractor Predicting: 102it [01:04,  1.46it/s]Extractor Predicting: 103it [01:04,  1.49it/s]Extractor Predicting: 104it [01:05,  1.50it/s]Extractor Predicting: 105it [01:06,  1.51it/s]Extractor Predicting: 106it [01:06,  1.47it/s]Extractor Predicting: 107it [01:07,  1.48it/s]Extractor Predicting: 108it [01:08,  1.45it/s]Extractor Predicting: 109it [01:08,  1.45it/s]Extractor Predicting: 110it [01:09,  1.45it/s]Extractor Predicting: 111it [01:10,  1.34it/s]Extractor Predicting: 112it [01:11,  1.37it/s]Extractor Predicting: 113it [01:11,  1.40it/s]Extractor Predicting: 114it [01:12,  1.41it/s]Extractor Predicting: 115it [01:13,  1.42it/s]Extractor Predicting: 116it [01:13,  1.44it/s]Extractor Predicting: 117it [01:14,  1.44it/s]Extractor Predicting: 118it [01:15,  1.45it/s]Extractor Predicting: 119it [01:15,  1.48it/s]Extractor Predicting: 120it [01:16,  1.53it/s]Extractor Predicting: 121it [01:17,  1.52it/s]Extractor Predicting: 122it [01:17,  1.51it/s]Extractor Predicting: 123it [01:18,  1.50it/s]Extractor Predicting: 124it [01:19,  1.50it/s]Extractor Predicting: 125it [01:19,  1.52it/s]Extractor Predicting: 126it [01:20,  1.56it/s]Extractor Predicting: 127it [01:21,  1.56it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.56it/s]Extractor Predicting: 130it [01:23,  1.52it/s]Extractor Predicting: 131it [01:23,  1.53it/s]Extractor Predicting: 132it [01:24,  1.54it/s]Extractor Predicting: 133it [01:25,  1.57it/s]Extractor Predicting: 134it [01:25,  1.59it/s]Extractor Predicting: 135it [01:26,  1.56it/s]Extractor Predicting: 136it [01:26,  1.58it/s]Extractor Predicting: 137it [01:27,  1.60it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:28,  1.58it/s]Extractor Predicting: 140it [01:29,  1.58it/s]Extractor Predicting: 141it [01:30,  1.53it/s]Extractor Predicting: 142it [01:30,  1.52it/s]Extractor Predicting: 143it [01:31,  1.52it/s]Extractor Predicting: 144it [01:32,  1.51it/s]Extractor Predicting: 145it [01:32,  1.51it/s]Extractor Predicting: 146it [01:33,  1.54it/s]Extractor Predicting: 147it [01:34,  1.56it/s]Extractor Predicting: 148it [01:34,  1.53it/s]Extractor Predicting: 149it [01:35,  1.53it/s]Extractor Predicting: 150it [01:36,  1.53it/s]Extractor Predicting: 151it [01:36,  1.49it/s]Extractor Predicting: 152it [01:37,  1.50it/s]Extractor Predicting: 153it [01:38,  1.50it/s]Extractor Predicting: 154it [01:38,  1.54it/s]Extractor Predicting: 155it [01:39,  1.51it/s]Extractor Predicting: 156it [01:40,  1.51it/s]Extractor Predicting: 157it [01:40,  1.48it/s]Extractor Predicting: 158it [01:41,  1.50it/s]Extractor Predicting: 159it [01:42,  1.49it/s]Extractor Predicting: 160it [01:42,  1.48it/s]Extractor Predicting: 161it [01:43,  1.49it/s]Extractor Predicting: 162it [01:44,  1.48it/s]Extractor Predicting: 163it [01:44,  1.48it/s]Extractor Predicting: 164it [01:45,  1.49it/s]Extractor Predicting: 165it [01:46,  1.48it/s]Extractor Predicting: 166it [01:46,  1.49it/s]Extractor Predicting: 167it [01:47,  1.48it/s]Extractor Predicting: 168it [01:48,  1.48it/s]Extractor Predicting: 169it [01:48,  1.47it/s]Extractor Predicting: 170it [01:49,  1.46it/s]Extractor Predicting: 171it [01:50,  1.47it/s]Extractor Predicting: 172it [01:50,  1.47it/s]Extractor Predicting: 173it [01:51,  1.47it/s]Extractor Predicting: 174it [01:52,  1.47it/s]Extractor Predicting: 175it [01:52,  1.52it/s]Extractor Predicting: 176it [01:53,  1.51it/s]Extractor Predicting: 177it [01:54,  1.53it/s]Extractor Predicting: 178it [01:54,  1.54it/s]Extractor Predicting: 179it [01:55,  1.55it/s]Extractor Predicting: 180it [01:56,  1.54it/s]Extractor Predicting: 181it [01:56,  1.55it/s]Extractor Predicting: 182it [01:57,  1.57it/s]Extractor Predicting: 183it [01:58,  1.52it/s]Extractor Predicting: 184it [01:58,  1.58it/s]Extractor Predicting: 185it [01:59,  1.56it/s]Extractor Predicting: 186it [01:59,  1.57it/s]Extractor Predicting: 187it [02:00,  1.57it/s]Extractor Predicting: 188it [02:01,  1.53it/s]Extractor Predicting: 189it [02:01,  1.52it/s]Extractor Predicting: 190it [02:02,  1.53it/s]Extractor Predicting: 191it [02:03,  1.51it/s]Extractor Predicting: 192it [02:03,  1.51it/s]Extractor Predicting: 193it [02:04,  1.55it/s]Extractor Predicting: 194it [02:05,  1.54it/s]Extractor Predicting: 195it [02:05,  1.51it/s]Extractor Predicting: 196it [02:06,  1.45it/s]Extractor Predicting: 197it [02:07,  1.49it/s]Extractor Predicting: 198it [02:07,  1.48it/s]Extractor Predicting: 199it [02:08,  1.51it/s]Extractor Predicting: 200it [02:09,  1.37it/s]Extractor Predicting: 201it [02:10,  1.41it/s]Extractor Predicting: 202it [02:10,  1.45it/s]Extractor Predicting: 203it [02:11,  1.48it/s]Extractor Predicting: 204it [02:12,  1.52it/s]Extractor Predicting: 205it [02:12,  1.52it/s]Extractor Predicting: 206it [02:13,  1.55it/s]Extractor Predicting: 207it [02:13,  1.53it/s]Extractor Predicting: 208it [02:14,  1.52it/s]Extractor Predicting: 209it [02:15,  1.53it/s]Extractor Predicting: 210it [02:15,  1.54it/s]Extractor Predicting: 211it [02:16,  1.51it/s]Extractor Predicting: 212it [02:17,  1.53it/s]Extractor Predicting: 213it [02:17,  1.55it/s]Extractor Predicting: 214it [02:18,  1.54it/s]Extractor Predicting: 215it [02:19,  1.54it/s]Extractor Predicting: 216it [02:19,  1.54it/s]Extractor Predicting: 217it [02:20,  1.53it/s]Extractor Predicting: 218it [02:21,  1.54it/s]Extractor Predicting: 219it [02:21,  1.51it/s]Extractor Predicting: 220it [02:22,  1.54it/s]Extractor Predicting: 221it [02:23,  1.57it/s]Extractor Predicting: 222it [02:23,  1.56it/s]Extractor Predicting: 223it [02:24,  1.52it/s]Extractor Predicting: 224it [02:24,  1.56it/s]Extractor Predicting: 225it [02:25,  1.55it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:26,  1.53it/s]Extractor Predicting: 228it [02:27,  1.55it/s]Extractor Predicting: 229it [02:28,  1.55it/s]Extractor Predicting: 230it [02:28,  1.56it/s]Extractor Predicting: 231it [02:29,  1.57it/s]Extractor Predicting: 232it [02:30,  1.55it/s]Extractor Predicting: 233it [02:30,  1.56it/s]Extractor Predicting: 234it [02:31,  1.54it/s]Extractor Predicting: 235it [02:32,  1.52it/s]Extractor Predicting: 236it [02:32,  1.52it/s]Extractor Predicting: 237it [02:33,  1.52it/s]Extractor Predicting: 238it [02:34,  1.51it/s]Extractor Predicting: 239it [02:34,  1.50it/s]Extractor Predicting: 240it [02:35,  1.51it/s]Extractor Predicting: 241it [02:36,  1.51it/s]Extractor Predicting: 242it [02:36,  1.51it/s]Extractor Predicting: 243it [02:37,  1.53it/s]Extractor Predicting: 244it [02:38,  1.50it/s]Extractor Predicting: 245it [02:38,  1.52it/s]Extractor Predicting: 246it [02:39,  1.52it/s]Extractor Predicting: 247it [02:40,  1.53it/s]Extractor Predicting: 248it [02:40,  1.53it/s]Extractor Predicting: 249it [02:41,  1.55it/s]Extractor Predicting: 250it [02:41,  1.54it/s]Extractor Predicting: 251it [02:42,  1.58it/s]Extractor Predicting: 252it [02:43,  1.55it/s]Extractor Predicting: 253it [02:43,  1.54it/s]Extractor Predicting: 254it [02:44,  1.55it/s]Extractor Predicting: 255it [02:45,  1.54it/s]Extractor Predicting: 256it [02:45,  1.50it/s]Extractor Predicting: 257it [02:46,  1.51it/s]Extractor Predicting: 258it [02:47,  1.51it/s]Extractor Predicting: 259it [02:47,  1.56it/s]Extractor Predicting: 260it [02:48,  1.54it/s]Extractor Predicting: 261it [02:49,  1.56it/s]Extractor Predicting: 262it [02:49,  1.54it/s]Extractor Predicting: 263it [02:50,  1.53it/s]Extractor Predicting: 264it [02:50,  1.60it/s]Extractor Predicting: 265it [02:51,  1.58it/s]Extractor Predicting: 266it [02:52,  1.60it/s]Extractor Predicting: 267it [02:52,  1.57it/s]Extractor Predicting: 268it [02:53,  1.56it/s]Extractor Predicting: 269it [02:54,  1.55it/s]Extractor Predicting: 270it [02:54,  1.52it/s]Extractor Predicting: 271it [02:55,  1.51it/s]Extractor Predicting: 272it [02:56,  1.52it/s]Extractor Predicting: 273it [02:56,  1.51it/s]Extractor Predicting: 274it [02:57,  1.49it/s]Extractor Predicting: 275it [02:58,  1.46it/s]Extractor Predicting: 276it [02:58,  1.49it/s]Extractor Predicting: 277it [02:59,  1.49it/s]Extractor Predicting: 278it [03:00,  1.53it/s]Extractor Predicting: 279it [03:00,  1.52it/s]Extractor Predicting: 280it [03:01,  1.51it/s]Extractor Predicting: 281it [03:02,  1.54it/s]Extractor Predicting: 282it [03:02,  1.52it/s]Extractor Predicting: 283it [03:03,  1.51it/s]Extractor Predicting: 284it [03:04,  1.38it/s]Extractor Predicting: 285it [03:05,  1.41it/s]Extractor Predicting: 286it [03:05,  1.47it/s]Extractor Predicting: 287it [03:06,  1.46it/s]Extractor Predicting: 288it [03:06,  1.67it/s]Extractor Predicting: 288it [03:06,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:44,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:44,297 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:44,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:44,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:44,298 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:26:44,906 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:26:44,907 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:26:45,470 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:26:46,530 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:26:46,531 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:49,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:49,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:49,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:49,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:49,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:26:50,013 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:26:50,014 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:26:50,575 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:26:50,728 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:26:50,728 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6093344857389801,
  "recall": 0.10217391304347827,
  "score": 0.17500310289189527,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.29it/s]Extractor Predicting: 3it [00:02,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:26:53,308 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:26:53,308 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:26:53,312 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:26:53,312 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:26:53,315 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:26:56,335 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:26:56,337 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:26:56,355 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:26:56,356 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:26:56,361 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:26:56,363 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:26:56,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:57,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:58,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:59,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:59,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:00,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:01,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:02,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:03,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:03,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:04,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:05,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:06,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:06,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:07,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:08,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:09,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:10,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:11,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:11,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:12,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:13,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:04, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:14,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:14,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:16,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:17,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:18,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:19,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:19,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:20,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:22,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:23,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:23,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:24,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:25,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:26,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:27,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:28,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:29,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:29,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:30,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:31,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:32,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:33,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:37<04:10, 19.26s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:34,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:35,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:36,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:36,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:37,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:38,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:39,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:40,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:40,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:41,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:42,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:42,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:43,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:44,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:45,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:45,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:46,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:47,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:47,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:48,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:49,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:50,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:51,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:55<03:40, 18.35s/it][WARNING|generation_utils.py:914] 2023-08-28 13:27:51,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:52,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:53,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:54,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:54,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:55,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:56,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:57,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:58,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:59,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:27:59,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:00,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:01,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:02,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:02,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:03,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:04,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:05,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:05,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:06,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:07,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:08,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:08,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:12<03:19, 18.10s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:09,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:10,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:11,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:11,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:12,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:13,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:14,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:14,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:15,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:16,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:17,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:17,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:18,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:19,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:20,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:21,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:21,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:22,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:23,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:24,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:25,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:25,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:30<02:57, 17.79s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:26,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:27,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:28,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:29,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:29,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:30,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:31,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:31,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:32,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:33,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:34,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:35,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:36,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:36,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:37,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:38,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:39,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:39,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:40,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:41,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:42,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:43,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:47<02:38, 17.66s/it][WARNING|generation_utils.py:914] 2023-08-28 13:28:44,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:45,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:45,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:46,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:47,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:48,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:48,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:49,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:50,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:51,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:52,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:52,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:53,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:54,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:55,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:55,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:56,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:57,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:58,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:28:59,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:00,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:01,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:01,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:02,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:07<02:25, 18.23s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:03,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:04,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:05,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:05,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:06,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:07,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:08,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:09,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:09,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:10,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:11,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:11,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:12,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:13,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:14,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:15,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:15,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:16,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:17,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:18,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:19,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:20,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:21,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:25<02:08, 18.40s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:22,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:23,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:24,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:25,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:26,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:27,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:28,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:29,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:30,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:31,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:32,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:33,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:34,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:35,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:36,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:37,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:38,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:39,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:39,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:40,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:42,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:46<01:54, 19.06s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:42,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:43,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:44,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:45,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:45,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:46,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:47,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:48,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:49,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:50,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:50,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:51,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:52,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:52,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:53,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:54,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:55,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:56,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:56,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:57,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:58,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:02<01:30, 18.20s/it][WARNING|generation_utils.py:914] 2023-08-28 13:29:59,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:29:59,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:00,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:01,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:02,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:03,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:03,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:04,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:05,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:06,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:07,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:07,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:08,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:09,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:10,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:11,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:11,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:12,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:13,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:14,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:15,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:15,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:16,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:21<01:13, 18.29s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:17,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:18,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:19,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:20,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:20,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:21,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:22,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:23,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:24,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:24,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:25,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:26,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:27,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:29,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:30,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:30,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:31,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:32,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:33,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:33,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:34,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:35,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:39<00:54, 18.32s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:36,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:36,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:37,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:38,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:39,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:39,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:40,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:41,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:41,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:42,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:43,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:44,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:44,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:45,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:45,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:46,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:47,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:48,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:48,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:49,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:50,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:54<00:34, 17.41s/it][WARNING|generation_utils.py:914] 2023-08-28 13:30:51,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:52,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:53,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:53,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:55,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:55,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:56,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:57,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:58,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:58,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:30:59,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:00,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:01,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:01,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:02,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:03,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:04,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:04,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:05,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:06,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:07,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:07,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:08,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:09,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:13<00:17, 17.80s/it][WARNING|generation_utils.py:914] 2023-08-28 13:31:10,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:10,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:11,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:12,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:13,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:13,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:14,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:15,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:16,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:16,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:17,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:18,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:19,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:20,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:20,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:21,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:22,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:23,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:24,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:25,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:25,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:26,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:31:27,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:31<00:00, 17.89s/it]Generating: 100%|██████████| 15/15 [04:31<00:00, 18.10s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:34,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:34,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:34,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:34,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:34,773 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:31:35,360 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:31:35,361 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:31:35,942 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:31:37,007 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:31:37,007 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:40,108 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:40,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:40,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:40,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:31:40,113 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:31:40,754 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:31:40,755 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:31:41,312 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:31:41,470 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:31:41,470 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8622159090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8206521739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7916666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.842391304347826, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : work location . Context : Later in the year , the band formed the single " The Way To The West " . Head Entity : The Way To The West , Tail Entity : music venue .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 13315
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13415, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.38it/s]Extractor Estimating: 7it [00:04,  1.44it/s]Extractor Estimating: 8it [00:05,  1.38it/s]Extractor Estimating: 9it [00:06,  1.36it/s]Extractor Estimating: 10it [00:07,  1.43it/s]Extractor Estimating: 11it [00:07,  1.42it/s]Extractor Estimating: 12it [00:08,  1.44it/s]Extractor Estimating: 13it [00:09,  1.44it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.44it/s]Extractor Estimating: 16it [00:11,  1.42it/s]Extractor Estimating: 17it [00:12,  1.40it/s]Extractor Estimating: 18it [00:12,  1.33it/s]Extractor Estimating: 19it [00:13,  1.32it/s]Extractor Estimating: 20it [00:14,  1.32it/s]Extractor Estimating: 21it [00:15,  1.33it/s]Extractor Estimating: 22it [00:15,  1.39it/s]Extractor Estimating: 23it [00:16,  1.42it/s]Extractor Estimating: 24it [00:17,  1.44it/s]Extractor Estimating: 25it [00:17,  1.43it/s]Extractor Estimating: 26it [00:18,  1.46it/s]Extractor Estimating: 27it [00:19,  1.49it/s]Extractor Estimating: 28it [00:19,  1.51it/s]Extractor Estimating: 29it [00:20,  1.50it/s]Extractor Estimating: 30it [00:21,  1.53it/s]Extractor Estimating: 31it [00:21,  1.48it/s]Extractor Estimating: 32it [00:22,  1.50it/s]Extractor Estimating: 33it [00:23,  1.49it/s]Extractor Estimating: 34it [00:23,  1.50it/s]Extractor Estimating: 35it [00:24,  1.49it/s]Extractor Estimating: 36it [00:25,  1.46it/s]Extractor Estimating: 37it [00:25,  1.47it/s]Extractor Estimating: 38it [00:26,  1.43it/s]Extractor Estimating: 39it [00:27,  1.47it/s]Extractor Estimating: 40it [00:27,  1.46it/s]Extractor Estimating: 41it [00:28,  1.49it/s]Extractor Estimating: 42it [00:29,  1.49it/s]Extractor Estimating: 43it [00:29,  1.46it/s]Extractor Estimating: 44it [00:30,  1.46it/s]Extractor Estimating: 45it [00:31,  1.51it/s]Extractor Estimating: 46it [00:31,  1.47it/s]Extractor Estimating: 47it [00:32,  1.55it/s]Extractor Estimating: 48it [00:33,  1.53it/s]Extractor Estimating: 49it [00:33,  1.53it/s]Extractor Estimating: 50it [00:34,  1.44it/s]Extractor Estimating: 51it [00:35,  1.46it/s]Extractor Estimating: 52it [00:35,  1.51it/s]Extractor Estimating: 53it [00:36,  1.41it/s]Extractor Estimating: 54it [00:37,  1.43it/s]Extractor Estimating: 55it [00:38,  1.41it/s]Extractor Estimating: 56it [00:38,  1.44it/s]Extractor Estimating: 57it [00:39,  1.40it/s]Extractor Estimating: 58it [00:40,  1.44it/s]Extractor Estimating: 59it [00:40,  1.44it/s]Extractor Estimating: 60it [00:41,  1.52it/s]Extractor Estimating: 61it [00:42,  1.56it/s]Extractor Estimating: 62it [00:42,  1.56it/s]Extractor Estimating: 63it [00:43,  1.57it/s]Extractor Estimating: 64it [00:43,  1.59it/s]Extractor Estimating: 65it [00:44,  1.58it/s]Extractor Estimating: 66it [00:45,  1.60it/s]Extractor Estimating: 67it [00:45,  1.56it/s]Extractor Estimating: 68it [00:46,  1.54it/s]Extractor Estimating: 69it [00:47,  1.56it/s]Extractor Estimating: 70it [00:47,  1.50it/s]Extractor Estimating: 71it [00:48,  1.47it/s]Extractor Estimating: 72it [00:49,  1.43it/s]Extractor Estimating: 73it [00:50,  1.45it/s]Extractor Estimating: 74it [00:50,  1.45it/s]Extractor Estimating: 75it [00:51,  1.50it/s]Extractor Estimating: 76it [00:51,  1.50it/s]Extractor Estimating: 77it [00:52,  1.52it/s]Extractor Estimating: 78it [00:53,  1.57it/s]Extractor Estimating: 79it [00:53,  1.61it/s]Extractor Estimating: 80it [00:54,  1.58it/s]Extractor Estimating: 81it [00:54,  1.66it/s]Extractor Estimating: 82it [00:55,  1.62it/s]Extractor Estimating: 83it [00:56,  1.67it/s]Extractor Estimating: 84it [00:56,  1.63it/s]Extractor Estimating: 85it [00:57,  1.63it/s]Extractor Estimating: 86it [00:58,  1.55it/s]Extractor Estimating: 87it [00:58,  1.56it/s]Extractor Estimating: 88it [00:59,  1.61it/s]Extractor Estimating: 89it [01:00,  1.57it/s]Extractor Estimating: 90it [01:00,  1.55it/s]Extractor Estimating: 91it [01:01,  1.59it/s]Extractor Estimating: 92it [01:01,  1.61it/s]Extractor Estimating: 93it [01:02,  1.62it/s]Extractor Estimating: 94it [01:03,  1.65it/s]Extractor Estimating: 95it [01:03,  1.64it/s]Extractor Estimating: 96it [01:04,  1.66it/s]Extractor Estimating: 97it [01:04,  1.63it/s]Extractor Estimating: 98it [01:05,  1.62it/s]Extractor Estimating: 99it [01:06,  1.63it/s]Extractor Estimating: 100it [01:06,  1.63it/s]Extractor Estimating: 101it [01:07,  1.58it/s]Extractor Estimating: 102it [01:08,  1.52it/s]Extractor Estimating: 103it [01:08,  1.53it/s]Extractor Estimating: 104it [01:09,  1.55it/s]Extractor Estimating: 105it [01:10,  1.56it/s]Extractor Estimating: 106it [01:10,  1.51it/s]Extractor Estimating: 107it [01:11,  1.54it/s]Extractor Estimating: 108it [01:12,  1.56it/s]Extractor Estimating: 109it [01:12,  1.52it/s]Extractor Estimating: 110it [01:13,  1.54it/s]Extractor Estimating: 111it [01:14,  1.49it/s]Extractor Estimating: 112it [01:14,  1.41it/s]Extractor Estimating: 113it [01:15,  1.44it/s]Extractor Estimating: 114it [01:16,  1.43it/s]Extractor Estimating: 115it [01:16,  1.47it/s]Extractor Estimating: 116it [01:17,  1.51it/s]Extractor Estimating: 117it [01:18,  1.48it/s]Extractor Estimating: 118it [01:18,  1.47it/s]Extractor Estimating: 119it [01:19,  1.36it/s]Extractor Estimating: 120it [01:20,  1.42it/s]Extractor Estimating: 121it [01:21,  1.48it/s]Extractor Estimating: 122it [01:21,  1.49it/s]Extractor Estimating: 123it [01:22,  1.49it/s]Extractor Estimating: 124it [01:23,  1.47it/s]Extractor Estimating: 125it [01:23,  1.49it/s]Extractor Estimating: 126it [01:24,  1.49it/s]Extractor Estimating: 127it [01:25,  1.47it/s]Extractor Estimating: 128it [01:25,  1.48it/s]Extractor Estimating: 129it [01:26,  1.50it/s]Extractor Estimating: 130it [01:26,  1.53it/s]Extractor Estimating: 131it [01:27,  1.54it/s]Extractor Estimating: 132it [01:28,  1.57it/s]Extractor Estimating: 133it [01:28,  1.58it/s]Extractor Estimating: 134it [01:29,  1.52it/s]Extractor Estimating: 135it [01:30,  1.51it/s]Extractor Estimating: 136it [01:30,  1.50it/s]Extractor Estimating: 137it [01:31,  1.46it/s]Extractor Estimating: 138it [01:32,  1.43it/s]Extractor Estimating: 139it [01:33,  1.47it/s]Extractor Estimating: 140it [01:33,  1.50it/s]Extractor Estimating: 141it [01:34,  1.51it/s]Extractor Estimating: 142it [01:34,  1.50it/s]Extractor Estimating: 143it [01:35,  1.52it/s]Extractor Estimating: 144it [01:36,  1.53it/s]Extractor Estimating: 145it [01:36,  1.52it/s]Extractor Estimating: 146it [01:37,  1.49it/s]Extractor Estimating: 147it [01:38,  1.48it/s]Extractor Estimating: 148it [01:39,  1.40it/s]Extractor Estimating: 149it [01:39,  1.41it/s]Extractor Estimating: 150it [01:40,  1.46it/s]Extractor Estimating: 151it [01:41,  1.43it/s]Extractor Estimating: 152it [01:41,  1.47it/s]Extractor Estimating: 153it [01:42,  1.50it/s]Extractor Estimating: 154it [01:43,  1.48it/s]Extractor Estimating: 155it [01:43,  1.49it/s]Extractor Estimating: 156it [01:44,  1.48it/s]Extractor Estimating: 157it [01:45,  1.45it/s]Extractor Estimating: 158it [01:45,  1.43it/s]Extractor Estimating: 159it [01:46,  1.37it/s]Extractor Estimating: 160it [01:47,  1.46it/s]Extractor Estimating: 161it [01:48,  1.45it/s]Extractor Estimating: 162it [01:48,  1.49it/s]Extractor Estimating: 163it [01:49,  1.43it/s]Extractor Estimating: 164it [01:50,  1.45it/s]Extractor Estimating: 165it [01:50,  1.46it/s]Extractor Estimating: 166it [01:51,  1.48it/s]Extractor Estimating: 167it [01:52,  1.46it/s]Extractor Estimating: 168it [01:52,  1.44it/s]Extractor Estimating: 169it [01:53,  1.48it/s]Extractor Estimating: 170it [01:54,  1.51it/s]Extractor Estimating: 171it [01:54,  1.52it/s]Extractor Estimating: 172it [01:55,  1.52it/s]Extractor Estimating: 173it [01:56,  1.51it/s]Extractor Estimating: 174it [01:56,  1.52it/s]Extractor Estimating: 175it [01:57,  1.53it/s]Extractor Estimating: 176it [01:57,  1.56it/s]Extractor Estimating: 177it [01:58,  1.52it/s]Extractor Estimating: 178it [01:59,  1.47it/s]Extractor Estimating: 179it [02:00,  1.50it/s]Extractor Estimating: 180it [02:00,  1.50it/s]Extractor Estimating: 181it [02:01,  1.45it/s]Extractor Estimating: 182it [02:02,  1.44it/s]Extractor Estimating: 183it [02:03,  1.35it/s]Extractor Estimating: 184it [02:03,  1.40it/s]Extractor Estimating: 185it [02:04,  1.45it/s]Extractor Estimating: 186it [02:04,  1.46it/s]Extractor Estimating: 187it [02:05,  1.48it/s]Extractor Estimating: 188it [02:06,  1.46it/s]Extractor Estimating: 189it [02:07,  1.42it/s]Extractor Estimating: 190it [02:07,  1.45it/s]Extractor Estimating: 191it [02:08,  1.47it/s]Extractor Estimating: 192it [02:08,  1.51it/s]Extractor Estimating: 193it [02:09,  1.48it/s]Extractor Estimating: 194it [02:10,  1.50it/s]Extractor Estimating: 195it [02:11,  1.45it/s]Extractor Estimating: 196it [02:11,  1.48it/s]Extractor Estimating: 197it [02:12,  1.50it/s]Extractor Estimating: 198it [02:13,  1.49it/s]Extractor Estimating: 199it [02:13,  1.44it/s]Extractor Estimating: 200it [02:14,  1.41it/s]Extractor Estimating: 201it [02:15,  1.40it/s]Extractor Estimating: 202it [02:15,  1.40it/s]Extractor Estimating: 203it [02:16,  1.42it/s]Extractor Estimating: 204it [02:17,  1.43it/s]Extractor Estimating: 205it [02:18,  1.46it/s]Extractor Estimating: 206it [02:18,  1.43it/s]Extractor Estimating: 207it [02:19,  1.44it/s]Extractor Estimating: 208it [02:20,  1.42it/s]Extractor Estimating: 209it [02:20,  1.41it/s]Extractor Estimating: 210it [02:21,  1.40it/s]Extractor Estimating: 211it [02:22,  1.43it/s]Extractor Estimating: 212it [02:22,  1.48it/s]Extractor Estimating: 213it [02:23,  1.41it/s]Extractor Estimating: 214it [02:24,  1.42it/s]Extractor Estimating: 215it [02:25,  1.42it/s]Extractor Estimating: 216it [02:25,  1.33it/s]Extractor Estimating: 217it [02:26,  1.27it/s]Extractor Estimating: 218it [02:27,  1.33it/s]Extractor Estimating: 219it [02:28,  1.34it/s]Extractor Estimating: 220it [02:28,  1.39it/s]Extractor Estimating: 221it [02:29,  1.39it/s]Extractor Estimating: 222it [02:30,  1.40it/s]Extractor Estimating: 223it [02:30,  1.42it/s]Extractor Estimating: 224it [02:31,  1.42it/s]Extractor Estimating: 225it [02:32,  1.44it/s]Extractor Estimating: 226it [02:32,  1.51it/s]Extractor Estimating: 227it [02:33,  1.47it/s]Extractor Estimating: 228it [02:34,  1.47it/s]Extractor Estimating: 229it [02:34,  1.50it/s]Extractor Estimating: 230it [02:35,  1.51it/s]Extractor Estimating: 231it [02:36,  1.52it/s]Extractor Estimating: 232it [02:36,  1.55it/s]Extractor Estimating: 233it [02:37,  1.62it/s]Extractor Estimating: 234it [02:38,  1.58it/s]Extractor Estimating: 235it [02:38,  1.54it/s]Extractor Estimating: 236it [02:39,  1.53it/s]Extractor Estimating: 237it [02:40,  1.55it/s]Extractor Estimating: 238it [02:40,  1.56it/s]Extractor Estimating: 239it [02:41,  1.58it/s]Extractor Estimating: 240it [02:41,  1.58it/s]Extractor Estimating: 241it [02:42,  1.59it/s]Extractor Estimating: 242it [02:43,  1.61it/s]Extractor Estimating: 243it [02:43,  1.63it/s]Extractor Estimating: 244it [02:44,  1.65it/s]Extractor Estimating: 245it [02:44,  1.64it/s]Extractor Estimating: 246it [02:45,  1.61it/s]Extractor Estimating: 247it [02:46,  1.58it/s]Extractor Estimating: 248it [02:46,  1.59it/s]Extractor Estimating: 249it [02:47,  1.55it/s]Extractor Estimating: 250it [02:48,  1.57it/s]Extractor Estimating: 251it [02:48,  1.56it/s]Extractor Estimating: 252it [02:49,  1.54it/s]Extractor Estimating: 253it [02:50,  1.57it/s]Extractor Estimating: 254it [02:50,  1.59it/s]Extractor Estimating: 255it [02:51,  1.53it/s]Extractor Estimating: 256it [02:52,  1.51it/s]Extractor Estimating: 257it [02:52,  1.53it/s]Extractor Estimating: 258it [02:53,  1.48it/s]Extractor Estimating: 259it [02:54,  1.49it/s]Extractor Estimating: 260it [02:54,  1.49it/s]Extractor Estimating: 261it [02:55,  1.50it/s]Extractor Estimating: 262it [02:56,  1.45it/s]Extractor Estimating: 263it [02:56,  1.47it/s]Extractor Estimating: 264it [02:57,  1.43it/s]Extractor Estimating: 265it [02:58,  1.48it/s]Extractor Estimating: 266it [02:58,  1.50it/s]Extractor Estimating: 267it [02:59,  1.37it/s]Extractor Estimating: 268it [03:00,  1.41it/s]Extractor Estimating: 269it [03:01,  1.49it/s]Extractor Estimating: 270it [03:01,  1.54it/s]Extractor Estimating: 271it [03:02,  1.53it/s]Extractor Estimating: 272it [03:02,  1.54it/s]Extractor Estimating: 273it [03:03,  1.53it/s]Extractor Estimating: 274it [03:04,  1.53it/s]Extractor Estimating: 275it [03:05,  1.40it/s]Extractor Estimating: 276it [03:05,  1.44it/s]Extractor Estimating: 277it [03:06,  1.45it/s]Extractor Estimating: 278it [03:07,  1.51it/s]Extractor Estimating: 279it [03:07,  1.50it/s]Extractor Estimating: 280it [03:08,  1.51it/s]Extractor Estimating: 281it [03:08,  1.55it/s]Extractor Estimating: 282it [03:09,  1.52it/s]Extractor Estimating: 283it [03:10,  1.56it/s]Extractor Estimating: 284it [03:10,  1.55it/s]Extractor Estimating: 285it [03:11,  1.58it/s]Extractor Estimating: 286it [03:12,  1.50it/s]Extractor Estimating: 287it [03:12,  1.51it/s]Extractor Estimating: 288it [03:13,  1.58it/s]Extractor Estimating: 289it [03:14,  1.59it/s]Extractor Estimating: 290it [03:14,  1.51it/s]Extractor Estimating: 291it [03:15,  1.55it/s]Extractor Estimating: 292it [03:16,  1.59it/s]Extractor Estimating: 293it [03:16,  1.59it/s]Extractor Estimating: 294it [03:17,  1.59it/s]Extractor Estimating: 295it [03:17,  1.60it/s]Extractor Estimating: 296it [03:18,  1.63it/s]Extractor Estimating: 297it [03:19,  1.63it/s]Extractor Estimating: 298it [03:19,  1.67it/s]Extractor Estimating: 299it [03:20,  1.61it/s]Extractor Estimating: 300it [03:20,  1.59it/s]Extractor Estimating: 301it [03:21,  1.55it/s]Extractor Estimating: 302it [03:22,  1.50it/s]Extractor Estimating: 303it [03:23,  1.44it/s]Extractor Estimating: 304it [03:23,  1.46it/s]Extractor Estimating: 305it [03:24,  1.47it/s]Extractor Estimating: 306it [03:25,  1.50it/s]Extractor Estimating: 307it [03:25,  1.48it/s]Extractor Estimating: 308it [03:26,  1.48it/s]Extractor Estimating: 309it [03:27,  1.50it/s]Extractor Estimating: 310it [03:27,  1.46it/s]Extractor Estimating: 311it [03:28,  1.48it/s]Extractor Estimating: 312it [03:29,  1.47it/s]Extractor Estimating: 313it [03:29,  1.49it/s]Extractor Estimating: 314it [03:30,  1.49it/s]Extractor Estimating: 315it [03:31,  1.52it/s]Extractor Estimating: 316it [03:31,  1.54it/s]Extractor Estimating: 317it [03:32,  1.56it/s]Extractor Estimating: 318it [03:33,  1.53it/s]Extractor Estimating: 319it [03:33,  1.60it/s]Extractor Estimating: 320it [03:34,  1.54it/s]Extractor Estimating: 321it [03:35,  1.52it/s]Extractor Estimating: 322it [03:35,  1.49it/s]Extractor Estimating: 323it [03:36,  1.53it/s]Extractor Estimating: 324it [03:37,  1.43it/s]Extractor Estimating: 325it [03:37,  1.40it/s]Extractor Estimating: 326it [03:38,  1.44it/s]Extractor Estimating: 327it [03:39,  1.39it/s]Extractor Estimating: 328it [03:39,  1.47it/s]Extractor Estimating: 329it [03:40,  1.54it/s]Extractor Estimating: 330it [03:41,  1.56it/s]Extractor Estimating: 331it [03:41,  1.51it/s]Extractor Estimating: 332it [03:42,  1.52it/s]Extractor Estimating: 333it [03:43,  1.47it/s]Extractor Estimating: 334it [03:43,  1.50it/s]Extractor Estimating: 335it [03:44,  1.45it/s]Extractor Estimating: 336it [03:45,  1.48it/s]Extractor Estimating: 337it [03:45,  1.52it/s]Extractor Estimating: 338it [03:46,  1.54it/s]Extractor Estimating: 339it [03:47,  1.59it/s]Extractor Estimating: 340it [03:47,  1.55it/s]Extractor Estimating: 341it [03:48,  1.58it/s]Extractor Estimating: 342it [03:48,  1.58it/s]Extractor Estimating: 343it [03:49,  1.59it/s]Extractor Estimating: 344it [03:50,  1.59it/s]Extractor Estimating: 345it [03:50,  1.63it/s]Extractor Estimating: 346it [03:51,  1.58it/s]Extractor Estimating: 347it [03:52,  1.58it/s]Extractor Estimating: 348it [03:52,  1.61it/s]Extractor Estimating: 349it [03:53,  1.64it/s]Extractor Estimating: 350it [03:53,  1.70it/s]Extractor Estimating: 351it [03:54,  1.47it/s]Extractor Estimating: 352it [03:55,  1.50it/s]Extractor Estimating: 353it [03:56,  1.46it/s]Extractor Estimating: 354it [03:56,  1.46it/s]Extractor Estimating: 355it [03:57,  1.51it/s]Extractor Estimating: 356it [03:58,  1.49it/s]Extractor Estimating: 357it [03:58,  1.46it/s]Extractor Estimating: 358it [03:59,  1.48it/s]Extractor Estimating: 359it [04:00,  1.45it/s]Extractor Estimating: 360it [04:00,  1.46it/s]Extractor Estimating: 361it [04:01,  1.45it/s]Extractor Estimating: 362it [04:02,  1.45it/s]Extractor Estimating: 363it [04:02,  1.45it/s]Extractor Estimating: 364it [04:03,  1.40it/s]Extractor Estimating: 365it [04:04,  1.44it/s]Extractor Estimating: 366it [04:05,  1.45it/s]Extractor Estimating: 367it [04:05,  1.44it/s]Extractor Estimating: 368it [04:06,  1.32it/s]Extractor Estimating: 369it [04:07,  1.33it/s]Extractor Estimating: 370it [04:08,  1.36it/s]Extractor Estimating: 371it [04:08,  1.40it/s]Extractor Estimating: 372it [04:09,  1.35it/s]Extractor Estimating: 373it [04:10,  1.39it/s]Extractor Estimating: 374it [04:10,  1.38it/s]Extractor Estimating: 375it [04:11,  1.41it/s]Extractor Estimating: 375it [04:11,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:05,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:05,877 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:05,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:05,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:05,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:36:06,175 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:36:06,177 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:36:06,443 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:36:07,499 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:36:07,499 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:08,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:08,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:08,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:08,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:36:08,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:36:09,124 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:36:09,130 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:36:09,390 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:36:09,542 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:36:09,542 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 15:55:16,914 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 15:55:16,948 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7505 mean pseudo reward: 0.9297860173832608
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 25226
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25326, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25326, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.088, loss:936.8487
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.125, loss:876.1834
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.081, loss:829.9434
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.085, loss:800.7548
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.085, loss:837.0576
>> valid entity prec:0.6167, rec:0.5526, f1:0.5829
>> valid relation prec:0.4020, rec:0.1017, f1:0.1623
>> valid relation with NER prec:0.4020, rec:0.1017, f1:0.1623
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.367, loss:875.4383
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.086, loss:786.4931
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.093, loss:818.8602
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.088, loss:873.9073
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.081, loss:818.9110
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5736, rec:0.5150, f1:0.5427
>> valid relation prec:0.3092, rec:0.0882, f1:0.1373
>> valid relation with NER prec:0.3092, rec:0.0882, f1:0.1373
g_step 1100, step 161, avg_time 2.362, loss:813.2361
g_step 1200, step 261, avg_time 1.095, loss:818.6733
g_step 1300, step 48, avg_time 1.082, loss:822.1493
g_step 1400, step 148, avg_time 1.077, loss:771.8445
g_step 1500, step 248, avg_time 1.100, loss:766.1616
>> valid entity prec:0.6007, rec:0.5935, f1:0.5971
>> valid relation prec:0.3795, rec:0.0879, f1:0.1428
>> valid relation with NER prec:0.3795, rec:0.0879, f1:0.1428
new max entity f1 on valid!
g_step 1600, step 35, avg_time 2.370, loss:749.9634
g_step 1700, step 135, avg_time 1.089, loss:711.3713
g_step 1800, step 235, avg_time 1.083, loss:772.7616
g_step 1900, step 22, avg_time 1.088, loss:714.6239
g_step 2000, step 122, avg_time 1.077, loss:696.9416
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5490, rec:0.5107, f1:0.5292
>> valid relation prec:0.3550, rec:0.1034, f1:0.1602
>> valid relation with NER prec:0.3550, rec:0.1034, f1:0.1602
g_step 2100, step 222, avg_time 2.362, loss:694.1214
g_step 2200, step 9, avg_time 1.095, loss:739.7872
g_step 2300, step 109, avg_time 1.087, loss:629.5184
g_step 2400, step 209, avg_time 1.088, loss:666.1119
g_step 2500, step 309, avg_time 1.094, loss:689.8522
>> valid entity prec:0.6291, rec:0.5226, f1:0.5709
>> valid relation prec:0.3773, rec:0.0885, f1:0.1434
>> valid relation with NER prec:0.3773, rec:0.0885, f1:0.1434
g_step 2600, step 96, avg_time 2.354, loss:609.7856
g_step 2700, step 196, avg_time 1.079, loss:636.0732
g_step 2800, step 296, avg_time 1.090, loss:634.3445
g_step 2900, step 83, avg_time 1.088, loss:582.5930
g_step 3000, step 183, avg_time 1.083, loss:608.0564
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5958, rec:0.5090, f1:0.5490
>> valid relation prec:0.3434, rec:0.1008, f1:0.1559
>> valid relation with NER prec:0.3434, rec:0.1008, f1:0.1559
g_step 3100, step 283, avg_time 2.366, loss:633.4841
g_step 3200, step 70, avg_time 1.074, loss:566.9225
g_step 3300, step 170, avg_time 1.100, loss:570.0874
g_step 3400, step 270, avg_time 1.100, loss:564.4665
g_step 3500, step 57, avg_time 1.079, loss:574.2420
>> valid entity prec:0.6121, rec:0.5052, f1:0.5535
>> valid relation prec:0.3228, rec:0.0848, f1:0.1343
>> valid relation with NER prec:0.3228, rec:0.0848, f1:0.1343
g_step 3600, step 157, avg_time 2.359, loss:550.6588
g_step 3700, step 257, avg_time 1.085, loss:573.1618
g_step 3800, step 44, avg_time 1.086, loss:523.7602
g_step 3900, step 144, avg_time 1.087, loss:533.5470
g_step 4000, step 244, avg_time 1.095, loss:539.4977
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5947, rec:0.5282, f1:0.5595
>> valid relation prec:0.2505, rec:0.0676, f1:0.1065
>> valid relation with NER prec:0.2505, rec:0.0676, f1:0.1065
g_step 4100, step 31, avg_time 2.375, loss:535.5603
g_step 4200, step 131, avg_time 1.085, loss:487.5565
g_step 4300, step 231, avg_time 1.082, loss:510.0885
g_step 4400, step 18, avg_time 1.084, loss:510.5069
g_step 4500, step 118, avg_time 1.089, loss:482.1651
>> valid entity prec:0.5956, rec:0.4926, f1:0.5392
>> valid relation prec:0.2921, rec:0.0914, f1:0.1392
>> valid relation with NER prec:0.2921, rec:0.0914, f1:0.1392
g_step 4600, step 218, avg_time 2.366, loss:486.7507
g_step 4700, step 5, avg_time 1.097, loss:477.6333
g_step 4800, step 105, avg_time 1.080, loss:458.3030
g_step 4900, step 205, avg_time 1.081, loss:457.0796
g_step 5000, step 305, avg_time 1.092, loss:478.2833
learning rate was adjusted to 0.0008
>> valid entity prec:0.5871, rec:0.5721, f1:0.5795
>> valid relation prec:0.2590, rec:0.0928, f1:0.1367
>> valid relation with NER prec:0.2590, rec:0.0928, f1:0.1367
g_step 5100, step 92, avg_time 2.381, loss:425.5336
g_step 5200, step 192, avg_time 1.087, loss:466.3431
g_step 5300, step 292, avg_time 1.086, loss:448.6073
g_step 5400, step 79, avg_time 1.078, loss:416.6395
g_step 5500, step 179, avg_time 1.097, loss:445.7189
>> valid entity prec:0.5717, rec:0.5353, f1:0.5529
>> valid relation prec:0.2448, rec:0.0902, f1:0.1319
>> valid relation with NER prec:0.2448, rec:0.0902, f1:0.1319
g_step 5600, step 279, avg_time 2.368, loss:440.5254
g_step 5700, step 66, avg_time 1.077, loss:403.8324
g_step 5800, step 166, avg_time 1.089, loss:411.2562
g_step 5900, step 266, avg_time 1.088, loss:422.1449
g_step 6000, step 53, avg_time 1.097, loss:410.0458
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5655, rec:0.5978, f1:0.5812
>> valid relation prec:0.2425, rec:0.1023, f1:0.1439
>> valid relation with NER prec:0.2425, rec:0.1023, f1:0.1439
g_step 6100, step 153, avg_time 2.370, loss:379.3915
g_step 6200, step 253, avg_time 1.092, loss:399.5950
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:55:16 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:55:16 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-55-16_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:55:18 - WARNING - datasets.builder -   Using custom data configuration default-7b73383743199d24
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7b73383743199d24/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:55:20,381 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:55:20,384 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:55:20,384 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:55:20,385 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:55:20,392 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,397 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,397 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,398 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,398 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,398 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:55:20,398 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:55:20,550 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:55:23,663 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:55:23,668 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7b73383743199d24/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.17ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.98ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.22ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.34ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.44ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.47ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  3.82ba/s]100%|██████████| 8/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  4.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.86ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.22ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.31ba/s]100%|██████████| 4/4 [00:00<00:00,  5.38ba/s]100%|██████████| 4/4 [00:00<00:00,  4.87ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.35ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.95ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.38ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.43ba/s]100%|██████████| 8/8 [00:00<00:00, 10.95ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.96ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.92ba/s]100%|██████████| 4/4 [00:00<00:00, 11.26ba/s]
[INFO|trainer.py:414] 2023-08-28 15:55:27,884 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:55:27,907 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:55:27,907 >>   Num examples = 7514
[INFO|trainer.py:1149] 2023-08-28 15:55:27,907 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:55:27,907 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:55:27,907 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:55:27,907 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:55:27,907 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:56,  3.30it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:46,  3.48it/s]  1%|▏         | 8/585 [00:02<02:46,  3.46it/s]  2%|▏         | 9/585 [00:02<02:45,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.47it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.48it/s]  2%|▏         | 13/585 [00:03<02:44,  3.49it/s]  2%|▏         | 14/585 [00:04<02:43,  3.49it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:43,  3.49it/s]  3%|▎         | 17/585 [00:04<02:42,  3.49it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:41,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.49it/s]  4%|▍         | 23/585 [00:06<02:41,  3.49it/s]  4%|▍         | 24/585 [00:06<02:40,  3.49it/s]  4%|▍         | 25/585 [00:07<02:40,  3.49it/s]  4%|▍         | 26/585 [00:07<02:40,  3.49it/s]  5%|▍         | 27/585 [00:07<02:39,  3.49it/s]  5%|▍         | 28/585 [00:08<02:39,  3.49it/s]  5%|▍         | 29/585 [00:08<02:39,  3.49it/s]  5%|▌         | 30/585 [00:08<02:40,  3.46it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:36,  3.49it/s]  7%|▋         | 39/585 [00:11<02:36,  3.48it/s]  7%|▋         | 40/585 [00:11<02:36,  3.49it/s]  7%|▋         | 41/585 [00:11<02:37,  3.46it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:33,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.48it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:14<02:34,  3.46it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:32,  3.47it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:32,  3.48it/s] 10%|▉         | 57/585 [00:16<02:31,  3.48it/s] 10%|▉         | 58/585 [00:16<02:31,  3.48it/s] 10%|█         | 59/585 [00:16<02:30,  3.48it/s] 10%|█         | 60/585 [00:17<02:30,  3.48it/s] 10%|█         | 61/585 [00:17<02:30,  3.48it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:29,  3.48it/s] 11%|█         | 65/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 67/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.48it/s] 12%|█▏        | 73/585 [00:20<02:27,  3.48it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.48it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.48it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.45it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:22,  3.47it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.45it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 107/585 [00:30<02:17,  3.47it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 110/585 [00:31<02:16,  3.47it/s] 19%|█▉        | 111/585 [00:31<02:16,  3.47it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:32<02:15,  3.47it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:56:01,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:56:01,700 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:56:01,700 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.52it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.71it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.07it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.50it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.29it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.08it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.04it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.94it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.88it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.81it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.87it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.74it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.74it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.74it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.74it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.73it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.72it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.70it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.75it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.66it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.61it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.65it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.64it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.56it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.48it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.49it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.57it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.64it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.59it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.70it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.57it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.68it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.64it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.70it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.74it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 44.08it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.88it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 45.37it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 45.78it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.02it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.20it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.40it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.46it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.53it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.56it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.66it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.63it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.61it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.51it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.51it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.50it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.44it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.34it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 46.32it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.30it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.29it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.26it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.43it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.52it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.49it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.53it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.60it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 43.01it/s][A
 80%|███████▉  | 348/437 [00:07<00:02, 44.04it/s][A
 81%|████████  | 353/437 [00:07<00:01, 44.81it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 45.29it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 45.67it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 45.98it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.19it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.30it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.33it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.50it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.58it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.54it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.64it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.68it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.65it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.70it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.64it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.68it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.60it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 46.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:56:11,278 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 15:56:11,544 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:56:14,159 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:56:14,190 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:56:14,200 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<43:30,  5.59s/it] 20%|██        | 119/585 [00:51<31:04,  4.00s/it] 21%|██        | 120/585 [00:52<22:22,  2.89s/it] 21%|██        | 121/585 [00:52<16:17,  2.11s/it] 21%|██        | 122/585 [00:52<12:02,  1.56s/it] 21%|██        | 123/585 [00:53<09:04,  1.18s/it] 21%|██        | 124/585 [00:53<06:59,  1.10it/s] 21%|██▏       | 125/585 [00:53<05:32,  1.38it/s] 22%|██▏       | 126/585 [00:53<04:32,  1.69it/s] 22%|██▏       | 127/585 [00:54<03:49,  2.00it/s] 22%|██▏       | 128/585 [00:54<03:19,  2.29it/s] 22%|██▏       | 129/585 [00:54<02:58,  2.55it/s] 22%|██▏       | 130/585 [00:55<02:44,  2.76it/s] 22%|██▏       | 131/585 [00:55<02:34,  2.94it/s] 23%|██▎       | 132/585 [00:55<02:26,  3.09it/s] 23%|██▎       | 133/585 [00:55<02:21,  3.19it/s] 23%|██▎       | 134/585 [00:56<02:17,  3.28it/s] 23%|██▎       | 135/585 [00:56<02:15,  3.33it/s] 23%|██▎       | 136/585 [00:56<02:12,  3.38it/s] 23%|██▎       | 137/585 [00:57<02:11,  3.41it/s] 24%|██▎       | 138/585 [00:57<02:10,  3.43it/s] 24%|██▍       | 139/585 [00:57<02:09,  3.44it/s] 24%|██▍       | 140/585 [00:57<02:08,  3.45it/s] 24%|██▍       | 141/585 [00:58<02:08,  3.46it/s] 24%|██▍       | 142/585 [00:58<02:07,  3.46it/s] 24%|██▍       | 143/585 [00:58<02:07,  3.47it/s] 25%|██▍       | 144/585 [00:59<02:07,  3.47it/s] 25%|██▍       | 145/585 [00:59<02:06,  3.47it/s] 25%|██▍       | 146/585 [00:59<02:06,  3.47it/s] 25%|██▌       | 147/585 [00:59<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:00<02:05,  3.48it/s] 25%|██▌       | 149/585 [01:00<02:05,  3.48it/s] 26%|██▌       | 150/585 [01:00<02:05,  3.48it/s] 26%|██▌       | 151/585 [01:01<02:04,  3.48it/s] 26%|██▌       | 152/585 [01:01<02:04,  3.47it/s] 26%|██▌       | 153/585 [01:01<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:01<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:02<02:03,  3.48it/s] 27%|██▋       | 158/585 [01:03<02:02,  3.47it/s] 27%|██▋       | 159/585 [01:03<02:02,  3.48it/s] 27%|██▋       | 160/585 [01:03<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:04<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:04<02:01,  3.48it/s] 28%|██▊       | 163/585 [01:04<02:02,  3.45it/s] 28%|██▊       | 164/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:05<02:00,  3.47it/s] 29%|██▊       | 167/585 [01:05<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:06<02:00,  3.47it/s] 29%|██▉       | 169/585 [01:06<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:06<01:59,  3.48it/s] 29%|██▉       | 171/585 [01:06<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:07<01:58,  3.47it/s] 30%|██▉       | 173/585 [01:07<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:07<01:58,  3.48it/s] 30%|██▉       | 175/585 [01:08<01:57,  3.48it/s] 30%|███       | 176/585 [01:08<01:57,  3.48it/s] 30%|███       | 177/585 [01:08<01:57,  3.47it/s] 30%|███       | 178/585 [01:08<01:57,  3.47it/s] 31%|███       | 179/585 [01:09<01:57,  3.45it/s] 31%|███       | 180/585 [01:09<01:57,  3.46it/s] 31%|███       | 181/585 [01:09<01:56,  3.46it/s] 31%|███       | 182/585 [01:10<01:56,  3.47it/s] 31%|███▏      | 183/585 [01:10<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:10<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:10<01:55,  3.47it/s] 32%|███▏      | 186/585 [01:11<01:54,  3.47it/s] 32%|███▏      | 187/585 [01:11<01:54,  3.47it/s] 32%|███▏      | 188/585 [01:11<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:12<01:54,  3.47it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:12<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:12<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.47it/s] 33%|███▎      | 194/585 [01:13<01:52,  3.47it/s] 33%|███▎      | 195/585 [01:13<01:52,  3.47it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.47it/s] 34%|███▎      | 197/585 [01:14<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:14<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:14<01:51,  3.47it/s] 34%|███▍      | 200/585 [01:15<01:50,  3.47it/s] 34%|███▍      | 201/585 [01:15<01:50,  3.46it/s] 35%|███▍      | 202/585 [01:15<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.47it/s] 35%|███▍      | 204/585 [01:16<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:16<01:49,  3.48it/s] 35%|███▌      | 206/585 [01:16<01:49,  3.47it/s] 35%|███▌      | 207/585 [01:17<01:48,  3.48it/s] 36%|███▌      | 208/585 [01:17<01:48,  3.48it/s] 36%|███▌      | 209/585 [01:17<01:48,  3.47it/s] 36%|███▌      | 210/585 [01:18<01:47,  3.47it/s] 36%|███▌      | 211/585 [01:18<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:18<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:18<01:47,  3.47it/s] 37%|███▋      | 214/585 [01:19<01:46,  3.47it/s] 37%|███▋      | 215/585 [01:19<01:46,  3.47it/s] 37%|███▋      | 216/585 [01:19<01:46,  3.47it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.47it/s] 37%|███▋      | 218/585 [01:20<01:45,  3.47it/s] 37%|███▋      | 219/585 [01:20<01:45,  3.47it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.47it/s] 38%|███▊      | 221/585 [01:21<01:44,  3.47it/s] 38%|███▊      | 222/585 [01:21<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:21<01:44,  3.47it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.47it/s] 38%|███▊      | 225/585 [01:22<01:43,  3.47it/s] 39%|███▊      | 226/585 [01:22<01:43,  3.47it/s] 39%|███▉      | 227/585 [01:23<01:43,  3.47it/s] 39%|███▉      | 228/585 [01:23<01:42,  3.47it/s] 39%|███▉      | 229/585 [01:23<01:42,  3.47it/s] 39%|███▉      | 230/585 [01:23<01:42,  3.48it/s] 39%|███▉      | 231/585 [01:24<01:41,  3.47it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:24<01:41,  3.46it/s] 40%|████      | 234/585 [01:25<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 15:56:53,072 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:56:53,072 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:56:53,072 >>   Batch size = 8
{'eval_loss': 0.9880424737930298, 'eval_runtime': 9.4176, 'eval_samples_per_second': 370.902, 'eval_steps_per_second': 46.403, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.52it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.89it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.95it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.28it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.75it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.43it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.19it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.86it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.83it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.84it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.92it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.93it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.88it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.79it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.72it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.67it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.63it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.69it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.72it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.85it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.85it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.81it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.77it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.68it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.63it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.65it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.73it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.78it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.85it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.92it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.86it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.76it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.69it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.57it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.59it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.55it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.54it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.48it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.54it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.71it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.75it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.70it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.64it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.67it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.71it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.79it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.79it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.74it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.68it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.78it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.76it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.79it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.68it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.66it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.70it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.74it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.79it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.78it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.68it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.63it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.68it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.63it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.56it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.50it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.54it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.55it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.53it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.48it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.42it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.37it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.40it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.46it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.50it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.59it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.64it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.70it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.73it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.64it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.60it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.69it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.68it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.76it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.64it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:34<01:41,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 46.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:57:02,631 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:57:02,909 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:57:06,699 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:57:06,752 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:57:06,761 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:44<34:59,  6.00s/it] 40%|████      | 236/585 [01:44<24:55,  4.29s/it] 41%|████      | 237/585 [01:44<17:53,  3.09s/it] 41%|████      | 238/585 [01:45<12:59,  2.25s/it] 41%|████      | 239/585 [01:45<09:33,  1.66s/it] 41%|████      | 240/585 [01:45<07:10,  1.25s/it] 41%|████      | 241/585 [01:46<05:29,  1.04it/s] 41%|████▏     | 242/585 [01:46<04:19,  1.32it/s] 42%|████▏     | 243/585 [01:46<03:30,  1.62it/s] 42%|████▏     | 244/585 [01:46<02:56,  1.93it/s] 42%|████▏     | 245/585 [01:47<02:32,  2.23it/s] 42%|████▏     | 246/585 [01:47<02:15,  2.50it/s] 42%|████▏     | 247/585 [01:47<02:04,  2.71it/s] 42%|████▏     | 248/585 [01:48<01:56,  2.90it/s] 43%|████▎     | 249/585 [01:48<01:50,  3.05it/s] 43%|████▎     | 250/585 [01:48<01:45,  3.17it/s] 43%|████▎     | 251/585 [01:48<01:42,  3.26it/s] 43%|████▎     | 252/585 [01:49<01:40,  3.32it/s] 43%|████▎     | 253/585 [01:49<01:38,  3.37it/s] 43%|████▎     | 254/585 [01:49<01:37,  3.40it/s] 44%|████▎     | 255/585 [01:50<01:36,  3.42it/s] 44%|████▍     | 256/585 [01:50<01:35,  3.44it/s] 44%|████▍     | 257/585 [01:50<01:35,  3.45it/s] 44%|████▍     | 258/585 [01:50<01:34,  3.45it/s] 44%|████▍     | 259/585 [01:51<01:34,  3.46it/s] 44%|████▍     | 260/585 [01:51<01:33,  3.46it/s] 45%|████▍     | 261/585 [01:51<01:33,  3.46it/s] 45%|████▍     | 262/585 [01:52<01:33,  3.47it/s] 45%|████▍     | 263/585 [01:52<01:32,  3.47it/s] 45%|████▌     | 264/585 [01:52<01:32,  3.47it/s] 45%|████▌     | 265/585 [01:53<01:32,  3.47it/s] 45%|████▌     | 266/585 [01:53<01:31,  3.47it/s] 46%|████▌     | 267/585 [01:53<01:31,  3.48it/s] 46%|████▌     | 268/585 [01:53<01:31,  3.47it/s] 46%|████▌     | 269/585 [01:54<01:31,  3.47it/s] 46%|████▌     | 270/585 [01:54<01:30,  3.47it/s] 46%|████▋     | 271/585 [01:54<01:30,  3.47it/s] 46%|████▋     | 272/585 [01:55<01:30,  3.47it/s] 47%|████▋     | 273/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 274/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 275/585 [01:55<01:29,  3.47it/s] 47%|████▋     | 276/585 [01:56<01:28,  3.47it/s] 47%|████▋     | 277/585 [01:56<01:28,  3.48it/s] 48%|████▊     | 278/585 [01:56<01:28,  3.47it/s] 48%|████▊     | 279/585 [01:57<01:28,  3.47it/s] 48%|████▊     | 280/585 [01:57<01:27,  3.47it/s] 48%|████▊     | 281/585 [01:57<01:27,  3.47it/s] 48%|████▊     | 282/585 [01:57<01:27,  3.47it/s] 48%|████▊     | 283/585 [01:58<01:26,  3.47it/s] 49%|████▊     | 284/585 [01:58<01:26,  3.48it/s] 49%|████▊     | 285/585 [01:58<01:26,  3.48it/s] 49%|████▉     | 286/585 [01:59<01:26,  3.47it/s] 49%|████▉     | 287/585 [01:59<01:25,  3.48it/s] 49%|████▉     | 288/585 [01:59<01:25,  3.47it/s] 49%|████▉     | 289/585 [01:59<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:00<01:25,  3.46it/s] 50%|████▉     | 291/585 [02:00<01:24,  3.46it/s] 50%|████▉     | 292/585 [02:00<01:24,  3.47it/s] 50%|█████     | 293/585 [02:01<01:24,  3.47it/s] 50%|█████     | 294/585 [02:01<01:23,  3.47it/s] 50%|█████     | 295/585 [02:01<01:23,  3.47it/s] 51%|█████     | 296/585 [02:01<01:25,  3.39it/s] 51%|█████     | 297/585 [02:02<01:24,  3.41it/s] 51%|█████     | 298/585 [02:02<01:23,  3.43it/s] 51%|█████     | 299/585 [02:02<01:23,  3.44it/s] 51%|█████▏    | 300/585 [02:03<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:03<01:22,  3.44it/s] 52%|█████▏    | 302/585 [02:03<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:03<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:04<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:04<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:04<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:05<01:20,  3.47it/s] 53%|█████▎    | 308/585 [02:05<01:19,  3.47it/s] 53%|█████▎    | 309/585 [02:05<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:05<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:06<01:19,  3.47it/s] 53%|█████▎    | 312/585 [02:06<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:06<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:07<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:07<01:17,  3.47it/s] 54%|█████▍    | 316/585 [02:07<01:17,  3.47it/s] 54%|█████▍    | 317/585 [02:08<01:17,  3.47it/s] 54%|█████▍    | 318/585 [02:08<01:16,  3.47it/s] 55%|█████▍    | 319/585 [02:08<01:16,  3.47it/s] 55%|█████▍    | 320/585 [02:08<01:16,  3.47it/s] 55%|█████▍    | 321/585 [02:09<01:16,  3.47it/s] 55%|█████▌    | 322/585 [02:09<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:09<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:10<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:10<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:10<01:14,  3.47it/s] 56%|█████▌    | 327/585 [02:10<01:14,  3.47it/s] 56%|█████▌    | 328/585 [02:11<01:14,  3.47it/s] 56%|█████▌    | 329/585 [02:11<01:13,  3.47it/s] 56%|█████▋    | 330/585 [02:11<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:12<01:13,  3.47it/s] 57%|█████▋    | 332/585 [02:12<01:12,  3.47it/s] 57%|█████▋    | 333/585 [02:12<01:12,  3.47it/s] 57%|█████▋    | 334/585 [02:12<01:12,  3.47it/s] 57%|█████▋    | 335/585 [02:13<01:11,  3.47it/s] 57%|█████▋    | 336/585 [02:13<01:11,  3.47it/s] 58%|█████▊    | 337/585 [02:13<01:11,  3.47it/s] 58%|█████▊    | 338/585 [02:14<01:11,  3.47it/s] 58%|█████▊    | 339/585 [02:14<01:10,  3.47it/s] 58%|█████▊    | 340/585 [02:14<01:10,  3.47it/s] 58%|█████▊    | 341/585 [02:14<01:10,  3.47it/s] 58%|█████▊    | 342/585 [02:15<01:10,  3.47it/s] 59%|█████▊    | 343/585 [02:15<01:09,  3.47it/s] 59%|█████▉    | 344/585 [02:15<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:16<01:09,  3.47it/s] 59%|█████▉    | 346/585 [02:16<01:08,  3.47it/s] 59%|█████▉    | 347/585 [02:16<01:08,  3.47it/s] 59%|█████▉    | 348/585 [02:16<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:17<01:08,  3.47it/s] 60%|█████▉    | 350/585 [02:17<01:07,  3.47it/s] 60%|██████    | 351/585 [02:17<01:07,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 15:57:45,840 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:57:45,840 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:57:45,840 >>   Batch size = 8
{'eval_loss': 1.0022289752960205, 'eval_runtime': 9.3544, 'eval_samples_per_second': 373.408, 'eval_steps_per_second': 46.716, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.39it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.57it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.91it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.21it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.67it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.49it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.14it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.85it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.75it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.76it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.81it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.83it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.87it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.81it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.85it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.77it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.71it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.55it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.59it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.69it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.77it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.83it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.81it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.83it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.80it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.73it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.54it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.69it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.74it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.84it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.80it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.79it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.73it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.63it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.70it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.67it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.67it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.74it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.71it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.67it/s][A
 50%|████▉     | 218/437 [00:04<00:05, 43.69it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.50it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 45.12it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 45.61it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 45.96it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.32it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.42it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.55it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.46it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.46it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.50it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.62it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.68it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.76it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.85it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.71it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.73it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.72it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.63it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.51it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.60it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.78it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.84it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.74it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.73it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.72it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.67it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 41.70it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 43.12it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 44.20it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.94it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.48it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.81it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.06it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.24it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.07it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.09it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.21it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.22it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.28it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.43it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.59it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:27<01:07,  3.47it/s]
100%|██████████| 437/437 [00:09<00:00, 46.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:57:55,411 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 15:57:55,685 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:58:00,339 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:58:00,566 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:58:00,629 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:39<25:55,  6.68s/it] 60%|██████    | 353/585 [02:39<18:24,  4.76s/it] 61%|██████    | 354/585 [02:39<13:09,  3.42s/it] 61%|██████    | 355/585 [02:40<09:30,  2.48s/it] 61%|██████    | 356/585 [02:40<06:57,  1.82s/it] 61%|██████    | 357/585 [02:40<05:10,  1.36s/it] 61%|██████    | 358/585 [02:41<03:55,  1.04s/it] 61%|██████▏   | 359/585 [02:41<03:03,  1.23it/s] 62%|██████▏   | 360/585 [02:41<02:27,  1.53it/s] 62%|██████▏   | 361/585 [02:41<02:02,  1.83it/s] 62%|██████▏   | 362/585 [02:42<01:44,  2.14it/s] 62%|██████▏   | 363/585 [02:42<01:31,  2.42it/s] 62%|██████▏   | 364/585 [02:42<01:23,  2.66it/s] 62%|██████▏   | 365/585 [02:43<01:16,  2.86it/s] 63%|██████▎   | 366/585 [02:43<01:12,  3.02it/s] 63%|██████▎   | 367/585 [02:43<01:09,  3.15it/s] 63%|██████▎   | 368/585 [02:44<01:06,  3.24it/s] 63%|██████▎   | 369/585 [02:44<01:05,  3.31it/s] 63%|██████▎   | 370/585 [02:44<01:04,  3.36it/s] 63%|██████▎   | 371/585 [02:44<01:03,  3.39it/s] 64%|██████▎   | 372/585 [02:45<01:02,  3.41it/s] 64%|██████▍   | 373/585 [02:45<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:45<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:46<01:01,  3.44it/s] 64%|██████▍   | 376/585 [02:46<01:00,  3.45it/s] 64%|██████▍   | 377/585 [02:46<01:00,  3.46it/s] 65%|██████▍   | 378/585 [02:46<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:47<00:59,  3.47it/s] 65%|██████▍   | 380/585 [02:47<00:59,  3.47it/s] 65%|██████▌   | 381/585 [02:47<00:58,  3.47it/s] 65%|██████▌   | 382/585 [02:48<00:58,  3.47it/s] 65%|██████▌   | 383/585 [02:48<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:48<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:48<00:57,  3.47it/s] 66%|██████▌   | 386/585 [02:49<00:57,  3.46it/s] 66%|██████▌   | 387/585 [02:49<00:57,  3.46it/s] 66%|██████▋   | 388/585 [02:49<00:56,  3.47it/s] 66%|██████▋   | 389/585 [02:50<00:56,  3.47it/s] 67%|██████▋   | 390/585 [02:50<00:56,  3.47it/s] 67%|██████▋   | 391/585 [02:50<00:55,  3.47it/s] 67%|██████▋   | 392/585 [02:50<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:51<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:51<00:54,  3.47it/s] 68%|██████▊   | 395/585 [02:51<00:54,  3.48it/s] 68%|██████▊   | 396/585 [02:52<00:54,  3.48it/s] 68%|██████▊   | 397/585 [02:52<00:54,  3.46it/s] 68%|██████▊   | 398/585 [02:52<00:53,  3.47it/s] 68%|██████▊   | 399/585 [02:52<00:53,  3.46it/s] 68%|██████▊   | 400/585 [02:53<00:53,  3.47it/s] 69%|██████▊   | 401/585 [02:53<00:53,  3.47it/s] 69%|██████▊   | 402/585 [02:53<00:52,  3.47it/s] 69%|██████▉   | 403/585 [02:54<00:52,  3.47it/s] 69%|██████▉   | 404/585 [02:54<00:52,  3.47it/s] 69%|██████▉   | 405/585 [02:54<00:51,  3.47it/s] 69%|██████▉   | 406/585 [02:54<00:51,  3.47it/s] 70%|██████▉   | 407/585 [02:55<00:51,  3.47it/s] 70%|██████▉   | 408/585 [02:55<00:50,  3.47it/s] 70%|██████▉   | 409/585 [02:55<00:50,  3.47it/s] 70%|███████   | 410/585 [02:56<00:50,  3.47it/s] 70%|███████   | 411/585 [02:56<00:50,  3.47it/s] 70%|███████   | 412/585 [02:56<00:50,  3.46it/s] 71%|███████   | 413/585 [02:56<00:49,  3.46it/s] 71%|███████   | 414/585 [02:57<00:49,  3.46it/s] 71%|███████   | 415/585 [02:57<00:49,  3.47it/s] 71%|███████   | 416/585 [02:57<00:48,  3.47it/s] 71%|███████▏  | 417/585 [02:58<00:48,  3.47it/s] 71%|███████▏  | 418/585 [02:58<00:48,  3.47it/s] 72%|███████▏  | 419/585 [02:58<00:47,  3.47it/s] 72%|███████▏  | 420/585 [02:58<00:47,  3.47it/s] 72%|███████▏  | 421/585 [02:59<00:47,  3.47it/s] 72%|███████▏  | 422/585 [02:59<00:46,  3.47it/s] 72%|███████▏  | 423/585 [02:59<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:00<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:00<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:00<00:45,  3.47it/s] 73%|███████▎  | 427/585 [03:01<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:01<00:45,  3.47it/s] 73%|███████▎  | 429/585 [03:01<00:44,  3.47it/s] 74%|███████▎  | 430/585 [03:01<00:44,  3.46it/s] 74%|███████▎  | 431/585 [03:02<00:47,  3.26it/s] 74%|███████▍  | 432/585 [03:02<00:46,  3.31it/s] 74%|███████▍  | 433/585 [03:02<00:45,  3.36it/s] 74%|███████▍  | 434/585 [03:03<00:44,  3.38it/s] 74%|███████▍  | 435/585 [03:03<00:43,  3.41it/s] 75%|███████▍  | 436/585 [03:03<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:03<00:43,  3.44it/s] 75%|███████▍  | 438/585 [03:04<00:42,  3.45it/s] 75%|███████▌  | 439/585 [03:04<00:42,  3.46it/s] 75%|███████▌  | 440/585 [03:04<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:05<00:41,  3.46it/s] 76%|███████▌  | 442/585 [03:05<00:41,  3.47it/s] 76%|███████▌  | 443/585 [03:05<00:40,  3.47it/s] 76%|███████▌  | 444/585 [03:05<00:40,  3.47it/s] 76%|███████▌  | 445/585 [03:06<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:06<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:06<00:39,  3.47it/s] 77%|███████▋  | 448/585 [03:07<00:39,  3.47it/s] 77%|███████▋  | 449/585 [03:07<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:07<00:38,  3.47it/s] 77%|███████▋  | 451/585 [03:07<00:38,  3.47it/s] 77%|███████▋  | 452/585 [03:08<00:38,  3.47it/s] 77%|███████▋  | 453/585 [03:08<00:38,  3.47it/s] 78%|███████▊  | 454/585 [03:08<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:09<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:09<00:37,  3.42it/s] 78%|███████▊  | 457/585 [03:09<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:10<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:10<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:10<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:10<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:11<00:35,  3.47it/s] 79%|███████▉  | 463/585 [03:11<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:11<00:34,  3.47it/s] 79%|███████▉  | 465/585 [03:12<00:34,  3.47it/s] 80%|███████▉  | 466/585 [03:12<00:34,  3.47it/s] 80%|███████▉  | 467/585 [03:12<00:34,  3.45it/s] 80%|████████  | 468/585 [03:12<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 15:58:40,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:58:40,934 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:58:40,934 >>   Batch size = 8
{'eval_loss': 1.0067094564437866, 'eval_runtime': 9.4203, 'eval_samples_per_second': 370.793, 'eval_steps_per_second': 46.389, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.65it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.85it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.10it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.38it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.15it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.84it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.68it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.71it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.77it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.79it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.85it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.87it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.76it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.69it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.63it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.63it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.58it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.58it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.61it/s][A
 26%|██▌       | 113/437 [00:02<00:10, 30.85it/s][A
 27%|██▋       | 118/437 [00:02<00:09, 34.40it/s][A
 28%|██▊       | 123/437 [00:02<00:08, 37.31it/s][A
 29%|██▉       | 128/437 [00:02<00:07, 39.71it/s][A
 30%|███       | 133/437 [00:03<00:07, 41.58it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 43.07it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 44.11it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.93it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.29it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.64it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.09it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.27it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.38it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 46.69it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.69it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.68it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.59it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.64it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.65it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.65it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.62it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.70it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 46.75it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.74it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.81it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.70it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 44.58it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 45.25it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 45.71it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.05it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.33it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.40it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.54it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.63it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.49it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.56it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.59it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.63it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.65it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.72it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.77it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.76it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.79it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.68it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.59it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.62it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.68it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.75it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.74it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.84it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 46.65it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.68it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.72it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.57it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.63it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.68it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.64it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.77it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.87it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 46.86it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.69it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.64it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.59it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.64it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:22<00:33,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 46.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:58:50,496 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 15:58:50,514 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:58:53,359 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:58:53,379 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:58:53,394 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:30<10:49,  5.60s/it] 80%|████████  | 470/585 [03:31<07:40,  4.00s/it] 81%|████████  | 471/585 [03:31<05:29,  2.89s/it] 81%|████████  | 472/585 [03:31<03:58,  2.11s/it] 81%|████████  | 473/585 [03:32<02:55,  1.56s/it] 81%|████████  | 474/585 [03:32<02:11,  1.18s/it] 81%|████████  | 475/585 [03:32<01:40,  1.10it/s] 81%|████████▏ | 476/585 [03:32<01:19,  1.38it/s] 82%|████████▏ | 477/585 [03:33<01:04,  1.68it/s] 82%|████████▏ | 478/585 [03:33<00:53,  1.99it/s] 82%|████████▏ | 479/585 [03:33<00:46,  2.29it/s] 82%|████████▏ | 480/585 [03:34<00:41,  2.55it/s] 82%|████████▏ | 481/585 [03:34<00:37,  2.76it/s] 82%|████████▏ | 482/585 [03:34<00:34,  2.94it/s] 83%|████████▎ | 483/585 [03:34<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:35<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:35<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:35<00:29,  3.33it/s] 83%|████████▎ | 487/585 [03:36<00:29,  3.37it/s] 83%|████████▎ | 488/585 [03:36<00:28,  3.40it/s] 84%|████████▎ | 489/585 [03:36<00:28,  3.42it/s] 84%|████████▍ | 490/585 [03:36<00:27,  3.44it/s] 84%|████████▍ | 491/585 [03:37<00:27,  3.45it/s] 84%|████████▍ | 492/585 [03:37<00:26,  3.45it/s] 84%|████████▍ | 493/585 [03:37<00:26,  3.45it/s] 84%|████████▍ | 494/585 [03:38<00:26,  3.46it/s] 85%|████████▍ | 495/585 [03:38<00:25,  3.46it/s] 85%|████████▍ | 496/585 [03:38<00:25,  3.47it/s] 85%|████████▍ | 497/585 [03:38<00:25,  3.47it/s] 85%|████████▌ | 498/585 [03:39<00:25,  3.47it/s] 85%|████████▌ | 499/585 [03:39<00:24,  3.47it/s] 85%|████████▌ | 500/585 [03:39<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:39<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:40<00:24,  3.47it/s] 86%|████████▌ | 502/585 [03:40<00:23,  3.47it/s] 86%|████████▌ | 503/585 [03:40<00:23,  3.47it/s] 86%|████████▌ | 504/585 [03:40<00:23,  3.47it/s] 86%|████████▋ | 505/585 [03:41<00:23,  3.47it/s] 86%|████████▋ | 506/585 [03:41<00:22,  3.47it/s] 87%|████████▋ | 507/585 [03:41<00:22,  3.47it/s] 87%|████████▋ | 508/585 [03:42<00:22,  3.47it/s] 87%|████████▋ | 509/585 [03:42<00:21,  3.47it/s] 87%|████████▋ | 510/585 [03:42<00:21,  3.47it/s] 87%|████████▋ | 511/585 [03:42<00:21,  3.47it/s] 88%|████████▊ | 512/585 [03:43<00:21,  3.47it/s] 88%|████████▊ | 513/585 [03:43<00:20,  3.48it/s] 88%|████████▊ | 514/585 [03:43<00:20,  3.45it/s] 88%|████████▊ | 515/585 [03:44<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:44<00:19,  3.46it/s] 88%|████████▊ | 517/585 [03:44<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:45<00:19,  3.47it/s] 89%|████████▊ | 519/585 [03:45<00:19,  3.47it/s] 89%|████████▉ | 520/585 [03:45<00:18,  3.47it/s] 89%|████████▉ | 521/585 [03:45<00:18,  3.47it/s] 89%|████████▉ | 522/585 [03:46<00:18,  3.47it/s] 89%|████████▉ | 523/585 [03:46<00:17,  3.47it/s] 90%|████████▉ | 524/585 [03:46<00:17,  3.47it/s] 90%|████████▉ | 525/585 [03:47<00:17,  3.46it/s] 90%|████████▉ | 526/585 [03:47<00:17,  3.46it/s] 90%|█████████ | 527/585 [03:47<00:16,  3.47it/s] 90%|█████████ | 528/585 [03:47<00:16,  3.47it/s] 90%|█████████ | 529/585 [03:48<00:16,  3.47it/s] 91%|█████████ | 530/585 [03:48<00:15,  3.47it/s] 91%|█████████ | 531/585 [03:48<00:15,  3.47it/s] 91%|█████████ | 532/585 [03:49<00:15,  3.47it/s] 91%|█████████ | 533/585 [03:49<00:14,  3.47it/s] 91%|█████████▏| 534/585 [03:49<00:14,  3.47it/s] 91%|█████████▏| 535/585 [03:49<00:14,  3.47it/s] 92%|█████████▏| 536/585 [03:50<00:14,  3.46it/s] 92%|█████████▏| 537/585 [03:50<00:13,  3.46it/s] 92%|█████████▏| 538/585 [03:50<00:13,  3.46it/s] 92%|█████████▏| 539/585 [03:51<00:13,  3.47it/s] 92%|█████████▏| 540/585 [03:51<00:12,  3.47it/s] 92%|█████████▏| 541/585 [03:51<00:12,  3.47it/s] 93%|█████████▎| 542/585 [03:51<00:12,  3.47it/s] 93%|█████████▎| 543/585 [03:52<00:12,  3.47it/s] 93%|█████████▎| 544/585 [03:52<00:11,  3.47it/s] 93%|█████████▎| 545/585 [03:52<00:11,  3.47it/s] 93%|█████████▎| 546/585 [03:53<00:11,  3.47it/s] 94%|█████████▎| 547/585 [03:53<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:53<00:10,  3.46it/s] 94%|█████████▍| 549/585 [03:53<00:10,  3.47it/s] 94%|█████████▍| 550/585 [03:54<00:10,  3.46it/s] 94%|█████████▍| 551/585 [03:54<00:09,  3.47it/s] 94%|█████████▍| 552/585 [03:54<00:09,  3.47it/s] 95%|█████████▍| 553/585 [03:55<00:09,  3.47it/s] 95%|█████████▍| 554/585 [03:55<00:08,  3.47it/s] 95%|█████████▍| 555/585 [03:55<00:08,  3.47it/s] 95%|█████████▌| 556/585 [03:55<00:08,  3.47it/s] 95%|█████████▌| 557/585 [03:56<00:08,  3.47it/s] 95%|█████████▌| 558/585 [03:56<00:07,  3.47it/s] 96%|█████████▌| 559/585 [03:56<00:07,  3.47it/s] 96%|█████████▌| 560/585 [03:57<00:07,  3.47it/s] 96%|█████████▌| 561/585 [03:57<00:06,  3.47it/s] 96%|█████████▌| 562/585 [03:57<00:06,  3.47it/s] 96%|█████████▌| 563/585 [03:57<00:06,  3.44it/s] 96%|█████████▋| 564/585 [03:58<00:06,  3.45it/s] 97%|█████████▋| 565/585 [03:58<00:05,  3.45it/s] 97%|█████████▋| 566/585 [03:58<00:05,  3.46it/s] 97%|█████████▋| 567/585 [03:59<00:05,  3.46it/s] 97%|█████████▋| 568/585 [03:59<00:04,  3.47it/s] 97%|█████████▋| 569/585 [03:59<00:04,  3.47it/s] 97%|█████████▋| 570/585 [04:00<00:04,  3.47it/s] 98%|█████████▊| 571/585 [04:00<00:04,  3.47it/s] 98%|█████████▊| 572/585 [04:00<00:03,  3.47it/s] 98%|█████████▊| 573/585 [04:00<00:03,  3.47it/s] 98%|█████████▊| 574/585 [04:01<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:01<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:01<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:02<00:02,  3.44it/s] 99%|█████████▉| 578/585 [04:02<00:02,  3.34it/s] 99%|█████████▉| 579/585 [04:02<00:01,  3.37it/s] 99%|█████████▉| 580/585 [04:02<00:01,  3.40it/s] 99%|█████████▉| 581/585 [04:03<00:01,  3.42it/s] 99%|█████████▉| 582/585 [04:03<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:03<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:04<00:00,  3.45it/s]100%|██████████| 585/585 [04:04<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:59:32,302 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:59:32,302 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:59:32,302 >>   Batch size = 8
{'eval_loss': 1.0120725631713867, 'eval_runtime': 9.5496, 'eval_samples_per_second': 365.775, 'eval_steps_per_second': 45.761, 'epoch': 4.0}
{'loss': 0.732, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.34it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.62it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.76it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.11it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.45it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.06it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.66it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.77it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.76it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.82it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.75it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.78it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.80it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.83it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.63it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.60it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.56it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.60it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.76it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.69it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.78it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.82it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.68it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.37it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.46it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.44it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.64it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.63it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.65it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.76it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.67it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.59it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.63it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.57it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.62it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.65it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.68it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.77it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.79it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.60it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.61it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.60it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.55it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.66it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.65it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.57it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.67it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.77it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.72it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.67it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.49it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.58it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.65it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.57it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.70it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.73it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.64it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.68it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.63it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.56it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.63it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.60it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.54it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.62it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.65it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.70it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.60it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.48it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.56it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.55it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.60it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.61it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.65it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.63it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.65it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.66it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.57it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.55it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.54it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.61it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:13<00:00,  3.42it/s]
100%|██████████| 437/437 [00:09<00:00, 46.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:59:41,694 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 15:59:41,715 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:59:44,161 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:59:44,173 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:59:44,181 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:59:49,408 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:59:49,412 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117 (score: 0.9880424737930298).
                                                 100%|██████████| 585/585 [04:23<00:00,  3.42it/s]100%|██████████| 585/585 [04:23<00:00,  2.22it/s]
[INFO|trainer.py:1894] 2023-08-28 15:59:51,382 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 15:59:51,398 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:59:54,042 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:59:54,059 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:59:54,075 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:59:54,263 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,263 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,263 >>   train_loss               =     0.7277
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,263 >>   train_runtime            = 0:04:23.47
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,263 >>   train_samples            =       7514
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,264 >>   train_samples_per_second =    142.596
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:59:54,264 >>   train_steps_per_second   =       2.22
{'eval_loss': 1.0182031393051147, 'eval_runtime': 9.3636, 'eval_samples_per_second': 373.042, 'eval_steps_per_second': 46.67, 'epoch': 5.0}
{'train_runtime': 263.4709, 'train_samples_per_second': 142.596, 'train_steps_per_second': 2.22, 'train_loss': 0.7277444301507412, 'epoch': 5.0}
08/28/2023 15:59:54 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:59:54,305 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:59:54,305 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 15:59:54,305 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.14it/s]  3%|▎         | 12/437 [00:00<00:08, 51.40it/s]  4%|▍         | 18/437 [00:00<00:08, 49.49it/s]  5%|▌         | 23/437 [00:00<00:08, 48.53it/s]  6%|▋         | 28/437 [00:00<00:08, 48.05it/s]  8%|▊         | 33/437 [00:00<00:08, 47.71it/s]  9%|▊         | 38/437 [00:00<00:08, 47.58it/s] 10%|▉         | 43/437 [00:00<00:08, 47.39it/s] 11%|█         | 48/437 [00:00<00:08, 47.12it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.11it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.12it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.17it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.12it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.05it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.14it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.14it/s] 20%|██        | 88/437 [00:01<00:07, 47.10it/s] 21%|██▏       | 93/437 [00:01<00:07, 46.96it/s] 22%|██▏       | 98/437 [00:02<00:07, 46.62it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.77it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.94it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.99it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.91it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.04it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.98it/s] 30%|███       | 133/437 [00:02<00:06, 47.00it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.97it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.83it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.89it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.00it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.08it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.91it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.92it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.04it/s] 41%|████      | 178/437 [00:03<00:05, 46.98it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.03it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.96it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.76it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.86it/s] 46%|████▋     | 203/437 [00:04<00:04, 46.91it/s] 48%|████▊     | 208/437 [00:04<00:04, 46.89it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.97it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.92it/s] 51%|█████     | 223/437 [00:04<00:04, 46.93it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.98it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.00it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.85it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.82it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.87it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.73it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.86it/s] 60%|██████    | 263/437 [00:05<00:03, 46.91it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.81it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.85it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.91it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.88it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.88it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.87it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.83it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.88it/s] 70%|███████   | 308/437 [00:06<00:02, 46.92it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.84it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.81it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.85it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.85it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.84it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.86it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.84it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.81it/s] 81%|████████  | 353/437 [00:07<00:01, 46.80it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.84it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.87it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.84it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.89it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.83it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.77it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.84it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.88it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.89it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.86it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.82it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.78it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.87it/s] 97%|█████████▋| 423/437 [00:08<00:00, 46.87it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.85it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.84it/s]100%|██████████| 437/437 [00:09<00:00, 47.03it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:00:03,621 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   eval_loss               =      0.988
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   eval_samples_per_second =    374.948
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   eval_steps_per_second   =     46.909
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:00:03,621 >>   perplexity              =      2.686
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:10,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:10,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:10,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:10,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:10,485 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:00:11,114 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:00:11,115 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:11,668 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:12,705 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:12,705 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:15,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:15,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:15,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:15,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:00:15,588 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:00:16,253 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:00:16,255 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:00:16,812 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:00:16,975 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:00:16,975 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.54it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:10,  1.56it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.56it/s]Extractor Predicting: 18it [00:12,  1.55it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.50it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:21,  1.57it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.57it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.55it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.59it/s]Extractor Predicting: 49it [00:32,  1.61it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:33,  1.56it/s]Extractor Predicting: 52it [00:34,  1.58it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.60it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:36,  1.57it/s]Extractor Predicting: 57it [00:37,  1.59it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:38,  1.55it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:39,  1.53it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.49it/s]Extractor Predicting: 68it [00:44,  1.47it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:45,  1.49it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.52it/s]Extractor Predicting: 74it [00:48,  1.50it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:53,  1.48it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:57,  1.39it/s]Extractor Predicting: 89it [00:58,  1.42it/s]Extractor Predicting: 90it [00:59,  1.40it/s]Extractor Predicting: 91it [01:00,  1.42it/s]Extractor Predicting: 92it [01:00,  1.43it/s]Extractor Predicting: 93it [01:01,  1.44it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:02,  1.51it/s]Extractor Predicting: 96it [01:03,  1.53it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:04,  1.50it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.48it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:14,  1.45it/s]Extractor Predicting: 114it [01:15,  1.44it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:16,  1.45it/s]Extractor Predicting: 117it [01:17,  1.44it/s]Extractor Predicting: 118it [01:18,  1.44it/s]Extractor Predicting: 119it [01:18,  1.44it/s]Extractor Predicting: 120it [01:19,  1.46it/s]Extractor Predicting: 121it [01:20,  1.44it/s]Extractor Predicting: 122it [01:21,  1.44it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.46it/s]Extractor Predicting: 126it [01:23,  1.47it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:25,  1.47it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:26,  1.45it/s]Extractor Predicting: 131it [01:27,  1.43it/s]Extractor Predicting: 132it [01:27,  1.42it/s]Extractor Predicting: 133it [01:28,  1.42it/s]Extractor Predicting: 134it [01:29,  1.40it/s]Extractor Predicting: 135it [01:30,  1.42it/s]Extractor Predicting: 136it [01:30,  1.73it/s]Extractor Predicting: 136it [01:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:01:55,689 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:01:55,694 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:01:55,694 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:01:55,694 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:01:55,694 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:01:56,314 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:01:56,315 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:01:56,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:01:57,930 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:01:57,930 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:02:00,207 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:02:00,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:02:00,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:02:00,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:02:00,212 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:02:00,537 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:02:00,539 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:02:00,805 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:02:00,955 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:02:00,955 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7018633540372671,
  "recall": 0.1294016604637847,
  "score": 0.21851583272903066,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.65it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.66it/s]Extractor Predicting: 33it [00:20,  1.68it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:26,  1.64it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:29,  1.65it/s]Extractor Predicting: 48it [00:29,  1.64it/s]Extractor Predicting: 49it [00:30,  1.65it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:31,  1.70it/s]Extractor Predicting: 52it [00:32,  1.70it/s]Extractor Predicting: 53it [00:32,  1.65it/s]Extractor Predicting: 54it [00:33,  1.62it/s]Extractor Predicting: 55it [00:34,  1.65it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:35,  1.63it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:39,  1.57it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.66it/s]Extractor Predicting: 69it [00:42,  1.67it/s]Extractor Predicting: 70it [00:43,  1.66it/s]Extractor Predicting: 71it [00:43,  1.65it/s]Extractor Predicting: 72it [00:44,  1.62it/s]Extractor Predicting: 73it [00:45,  1.64it/s]Extractor Predicting: 74it [00:45,  1.60it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:47,  1.60it/s]Extractor Predicting: 77it [00:47,  1.59it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:49,  1.61it/s]Extractor Predicting: 81it [00:50,  1.61it/s]Extractor Predicting: 82it [00:50,  1.62it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:52,  1.55it/s]Extractor Predicting: 86it [00:53,  1.52it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:55,  1.52it/s]Extractor Predicting: 90it [00:56,  1.50it/s]Extractor Predicting: 91it [00:56,  1.51it/s]Extractor Predicting: 92it [00:57,  1.49it/s]Extractor Predicting: 93it [00:58,  1.47it/s]Extractor Predicting: 94it [00:58,  1.47it/s]Extractor Predicting: 95it [00:59,  1.50it/s]Extractor Predicting: 96it [01:00,  1.50it/s]Extractor Predicting: 97it [01:00,  1.47it/s]Extractor Predicting: 98it [01:01,  1.48it/s]Extractor Predicting: 99it [01:02,  1.45it/s]Extractor Predicting: 100it [01:02,  1.45it/s]Extractor Predicting: 101it [01:03,  1.33it/s]Extractor Predicting: 102it [01:04,  1.36it/s]Extractor Predicting: 103it [01:05,  1.40it/s]Extractor Predicting: 104it [01:05,  1.43it/s]Extractor Predicting: 105it [01:06,  1.46it/s]Extractor Predicting: 106it [01:07,  1.44it/s]Extractor Predicting: 107it [01:07,  1.45it/s]Extractor Predicting: 108it [01:08,  1.43it/s]Extractor Predicting: 109it [01:09,  1.44it/s]Extractor Predicting: 110it [01:10,  1.44it/s]Extractor Predicting: 111it [01:10,  1.46it/s]Extractor Predicting: 112it [01:11,  1.45it/s]Extractor Predicting: 113it [01:12,  1.45it/s]Extractor Predicting: 114it [01:12,  1.46it/s]Extractor Predicting: 115it [01:13,  1.45it/s]Extractor Predicting: 116it [01:14,  1.46it/s]Extractor Predicting: 117it [01:14,  1.46it/s]Extractor Predicting: 118it [01:15,  1.48it/s]Extractor Predicting: 119it [01:16,  1.49it/s]Extractor Predicting: 120it [01:16,  1.54it/s]Extractor Predicting: 121it [01:17,  1.53it/s]Extractor Predicting: 122it [01:18,  1.52it/s]Extractor Predicting: 123it [01:18,  1.52it/s]Extractor Predicting: 124it [01:19,  1.51it/s]Extractor Predicting: 125it [01:20,  1.53it/s]Extractor Predicting: 126it [01:20,  1.57it/s]Extractor Predicting: 127it [01:21,  1.57it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.56it/s]Extractor Predicting: 130it [01:23,  1.52it/s]Extractor Predicting: 131it [01:23,  1.53it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:25,  1.56it/s]Extractor Predicting: 134it [01:25,  1.58it/s]Extractor Predicting: 135it [01:26,  1.56it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:27,  1.60it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:28,  1.58it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:30,  1.52it/s]Extractor Predicting: 142it [01:30,  1.52it/s]Extractor Predicting: 143it [01:31,  1.52it/s]Extractor Predicting: 144it [01:32,  1.52it/s]Extractor Predicting: 145it [01:32,  1.50it/s]Extractor Predicting: 146it [01:33,  1.54it/s]Extractor Predicting: 147it [01:34,  1.56it/s]Extractor Predicting: 148it [01:34,  1.53it/s]Extractor Predicting: 149it [01:35,  1.54it/s]Extractor Predicting: 150it [01:36,  1.53it/s]Extractor Predicting: 151it [01:36,  1.48it/s]Extractor Predicting: 152it [01:37,  1.50it/s]Extractor Predicting: 153it [01:38,  1.50it/s]Extractor Predicting: 154it [01:38,  1.54it/s]Extractor Predicting: 155it [01:39,  1.50it/s]Extractor Predicting: 156it [01:40,  1.50it/s]Extractor Predicting: 157it [01:40,  1.48it/s]Extractor Predicting: 158it [01:41,  1.50it/s]Extractor Predicting: 159it [01:42,  1.49it/s]Extractor Predicting: 160it [01:42,  1.47it/s]Extractor Predicting: 161it [01:43,  1.48it/s]Extractor Predicting: 162it [01:44,  1.48it/s]Extractor Predicting: 163it [01:44,  1.47it/s]Extractor Predicting: 164it [01:45,  1.49it/s]Extractor Predicting: 165it [01:46,  1.48it/s]Extractor Predicting: 166it [01:46,  1.49it/s]Extractor Predicting: 167it [01:47,  1.47it/s]Extractor Predicting: 168it [01:48,  1.47it/s]Extractor Predicting: 169it [01:49,  1.47it/s]Extractor Predicting: 170it [01:49,  1.46it/s]Extractor Predicting: 171it [01:50,  1.47it/s]Extractor Predicting: 172it [01:51,  1.47it/s]Extractor Predicting: 173it [01:51,  1.47it/s]Extractor Predicting: 174it [01:52,  1.47it/s]Extractor Predicting: 175it [01:53,  1.50it/s]Extractor Predicting: 176it [01:53,  1.50it/s]Extractor Predicting: 177it [01:54,  1.52it/s]Extractor Predicting: 178it [01:54,  1.53it/s]Extractor Predicting: 179it [01:55,  1.55it/s]Extractor Predicting: 180it [01:56,  1.50it/s]Extractor Predicting: 181it [01:56,  1.53it/s]Extractor Predicting: 182it [01:57,  1.55it/s]Extractor Predicting: 183it [01:58,  1.50it/s]Extractor Predicting: 184it [01:58,  1.57it/s]Extractor Predicting: 185it [01:59,  1.55it/s]Extractor Predicting: 186it [02:00,  1.42it/s]Extractor Predicting: 187it [02:01,  1.46it/s]Extractor Predicting: 188it [02:01,  1.45it/s]Extractor Predicting: 189it [02:02,  1.47it/s]Extractor Predicting: 190it [02:03,  1.49it/s]Extractor Predicting: 191it [02:03,  1.48it/s]Extractor Predicting: 192it [02:04,  1.49it/s]Extractor Predicting: 193it [02:04,  1.53it/s]Extractor Predicting: 194it [02:05,  1.53it/s]Extractor Predicting: 195it [02:06,  1.51it/s]Extractor Predicting: 196it [02:07,  1.47it/s]Extractor Predicting: 197it [02:07,  1.50it/s]Extractor Predicting: 198it [02:08,  1.49it/s]Extractor Predicting: 199it [02:08,  1.52it/s]Extractor Predicting: 200it [02:09,  1.51it/s]Extractor Predicting: 201it [02:10,  1.53it/s]Extractor Predicting: 202it [02:10,  1.53it/s]Extractor Predicting: 203it [02:11,  1.54it/s]Extractor Predicting: 204it [02:12,  1.56it/s]Extractor Predicting: 205it [02:12,  1.55it/s]Extractor Predicting: 206it [02:13,  1.58it/s]Extractor Predicting: 207it [02:14,  1.55it/s]Extractor Predicting: 208it [02:14,  1.53it/s]Extractor Predicting: 209it [02:15,  1.55it/s]Extractor Predicting: 210it [02:16,  1.55it/s]Extractor Predicting: 211it [02:16,  1.53it/s]Extractor Predicting: 212it [02:17,  1.54it/s]Extractor Predicting: 213it [02:18,  1.56it/s]Extractor Predicting: 214it [02:18,  1.54it/s]Extractor Predicting: 215it [02:19,  1.55it/s]Extractor Predicting: 216it [02:19,  1.55it/s]Extractor Predicting: 217it [02:20,  1.53it/s]Extractor Predicting: 218it [02:21,  1.54it/s]Extractor Predicting: 219it [02:21,  1.51it/s]Extractor Predicting: 220it [02:22,  1.54it/s]Extractor Predicting: 221it [02:23,  1.57it/s]Extractor Predicting: 222it [02:23,  1.56it/s]Extractor Predicting: 223it [02:24,  1.52it/s]Extractor Predicting: 224it [02:25,  1.56it/s]Extractor Predicting: 225it [02:25,  1.54it/s]Extractor Predicting: 226it [02:26,  1.53it/s]Extractor Predicting: 227it [02:27,  1.53it/s]Extractor Predicting: 228it [02:27,  1.55it/s]Extractor Predicting: 229it [02:28,  1.55it/s]Extractor Predicting: 230it [02:29,  1.56it/s]Extractor Predicting: 231it [02:29,  1.57it/s]Extractor Predicting: 232it [02:30,  1.56it/s]Extractor Predicting: 233it [02:30,  1.57it/s]Extractor Predicting: 234it [02:31,  1.54it/s]Extractor Predicting: 235it [02:32,  1.52it/s]Extractor Predicting: 236it [02:32,  1.52it/s]Extractor Predicting: 237it [02:33,  1.52it/s]Extractor Predicting: 238it [02:34,  1.52it/s]Extractor Predicting: 239it [02:34,  1.50it/s]Extractor Predicting: 240it [02:35,  1.51it/s]Extractor Predicting: 241it [02:36,  1.51it/s]Extractor Predicting: 242it [02:36,  1.51it/s]Extractor Predicting: 243it [02:37,  1.52it/s]Extractor Predicting: 244it [02:38,  1.49it/s]Extractor Predicting: 245it [02:38,  1.52it/s]Extractor Predicting: 246it [02:39,  1.52it/s]Extractor Predicting: 247it [02:40,  1.53it/s]Extractor Predicting: 248it [02:40,  1.52it/s]Extractor Predicting: 249it [02:41,  1.55it/s]Extractor Predicting: 250it [02:42,  1.54it/s]Extractor Predicting: 251it [02:42,  1.58it/s]Extractor Predicting: 252it [02:43,  1.55it/s]Extractor Predicting: 253it [02:44,  1.54it/s]Extractor Predicting: 254it [02:44,  1.53it/s]Extractor Predicting: 255it [02:45,  1.53it/s]Extractor Predicting: 256it [02:46,  1.49it/s]Extractor Predicting: 257it [02:46,  1.51it/s]Extractor Predicting: 258it [02:47,  1.51it/s]Extractor Predicting: 259it [02:48,  1.54it/s]Extractor Predicting: 260it [02:48,  1.53it/s]Extractor Predicting: 261it [02:49,  1.55it/s]Extractor Predicting: 262it [02:49,  1.53it/s]Extractor Predicting: 263it [02:50,  1.52it/s]Extractor Predicting: 264it [02:51,  1.58it/s]Extractor Predicting: 265it [02:51,  1.58it/s]Extractor Predicting: 266it [02:52,  1.59it/s]Extractor Predicting: 267it [02:53,  1.58it/s]Extractor Predicting: 268it [02:53,  1.57it/s]Extractor Predicting: 269it [02:54,  1.55it/s]Extractor Predicting: 270it [02:55,  1.52it/s]Extractor Predicting: 271it [02:55,  1.50it/s]Extractor Predicting: 272it [02:56,  1.52it/s]Extractor Predicting: 273it [02:57,  1.51it/s]Extractor Predicting: 274it [02:57,  1.49it/s]Extractor Predicting: 275it [02:58,  1.46it/s]Extractor Predicting: 276it [02:59,  1.49it/s]Extractor Predicting: 277it [02:59,  1.49it/s]Extractor Predicting: 278it [03:00,  1.38it/s]Extractor Predicting: 279it [03:01,  1.41it/s]Extractor Predicting: 280it [03:02,  1.44it/s]Extractor Predicting: 281it [03:02,  1.48it/s]Extractor Predicting: 282it [03:03,  1.48it/s]Extractor Predicting: 283it [03:03,  1.48it/s]Extractor Predicting: 284it [03:04,  1.49it/s]Extractor Predicting: 285it [03:05,  1.49it/s]Extractor Predicting: 286it [03:05,  1.54it/s]Extractor Predicting: 287it [03:06,  1.50it/s]Extractor Predicting: 288it [03:07,  1.72it/s]Extractor Predicting: 288it [03:07,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:15,662 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:15,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:15,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:15,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:15,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:05:16,411 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:05:16,412 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:05:16,981 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:05:18,012 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:05:18,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:20,900 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:20,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:20,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:20,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:05:20,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:05:21,519 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:05:21,520 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:05:22,080 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:05:22,235 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:05:22,235 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6413730803974707,
  "recall": 0.10289855072463767,
  "score": 0.1773448232796303,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:01,  2.29it/s]Extractor Predicting: 3it [00:01,  1.98it/s]
[INFO|configuration_utils.py:515] 2023-08-28 16:05:24,112 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:05:24,113 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:05:24,117 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:05:24,118 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:05:24,122 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:05:27,286 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:05:27,289 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:05:27,301 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:05:27,302 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:05:27,311 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:05:27,318 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:05:27,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:28,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:28,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:29,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:30,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:30,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:31,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:32,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:33,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:33,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:34,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:35,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:36,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:37,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:37,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:38,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:39,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:40,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:41,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:41,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:42,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:39, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-28 16:05:43,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:43,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:44,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:45,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:46,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:46,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:47,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:48,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:49,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:49,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:51,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:52,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:52,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:53,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:54,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:55,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:56,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:56,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:57,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:58,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:59,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:05:59,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:39, 16.87s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:00,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:01,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:02,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:03,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:04,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:04,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:05,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:06,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:06,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:07,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:08,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:09,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:09,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:10,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:11,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:12,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:12,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:14,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:15,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:15,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:16,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:17,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:24, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:18,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:18,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:19,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:20,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:20,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:21,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:22,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:23,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:23,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:24,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:25,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:26,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:26,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:27,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:28,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:28,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:29,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:30,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:30,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:31,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:32,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<03:00, 16.44s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:33,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:34,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:35,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:35,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:37,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:38,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:38,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:39,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:40,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:40,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:41,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:42,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:43,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:44,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:45,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:46,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:47,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:48,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:49,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:50,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:23<02:48, 16.87s/it][WARNING|generation_utils.py:914] 2023-08-28 16:06:51,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:52,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:52,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:53,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:54,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:55,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:56,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:56,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:58,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:06:59,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:00,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:01,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:01,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:02,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:03,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:04,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:05,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:06,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:07,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:40<02:31, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:08,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:08,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:09,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:10,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:11,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:12,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:13,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:13,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:14,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:15,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:15,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:16,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:17,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:18,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:18,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:19,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:20,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:21,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:21,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:22,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:23,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:24,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:57<02:15, 16.90s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:25,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:25,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:26,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:27,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:28,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:29,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:29,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:30,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:31,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:31,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:32,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:33,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:34,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:35,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:35,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:36,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:37,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:38,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:38,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:39,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:40,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:41,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:42,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:15<02:00, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 16:07:42,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:43,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:44,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:45,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:46,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:47,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:47,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:48,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:49,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:50,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:51,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:52,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:53,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:53,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:54,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:55,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:56,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:57,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:58,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:07:59,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:00,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:01,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:34<01:47, 17.85s/it][WARNING|generation_utils.py:914] 2023-08-28 16:08:02,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:03,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:03,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:04,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:05,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:06,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:06,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:07,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:08,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:09,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:10,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:10,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:11,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:12,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:13,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:14,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:15,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:15,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:16,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:17,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:18,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:51<01:27, 17.48s/it][WARNING|generation_utils.py:914] 2023-08-28 16:08:18,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:19,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:20,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:21,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:21,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:22,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:23,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:24,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:25,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:26,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:27,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:28,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:28,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:29,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:30,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:31,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:31,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:32,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:33,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:34,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:35,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:35,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:36,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:09<01:11, 17.78s/it][WARNING|generation_utils.py:914] 2023-08-28 16:08:37,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:38,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:39,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:40,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:40,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:41,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:42,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:43,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:43,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:44,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:45,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:46,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:47,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:47,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:48,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:49,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:49,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:50,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:51,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:52,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:52,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:26<00:52, 17.39s/it][WARNING|generation_utils.py:914] 2023-08-28 16:08:53,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:54,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:55,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:55,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:56,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:57,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:57,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:58,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:08:59,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:00,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:00,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:01,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:02,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:02,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:03,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:04,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:04,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:06,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:07,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:40<00:32, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 16:09:08,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:08,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:09,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:10,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:10,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:11,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:12,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:12,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:13,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:14,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:15,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:16,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:17,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:17,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:18,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:19,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:20,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:20,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:21,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:22,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:23,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:24,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:25,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:58<00:16, 16.87s/it][WARNING|generation_utils.py:914] 2023-08-28 16:09:25,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:26,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:27,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:28,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:29,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:29,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:30,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:31,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:31,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:32,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:33,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:34,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:34,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:35,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:36,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:37,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:38,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:39,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:40,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:09:41,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:14<00:00, 16.73s/it]Generating: 100%|██████████| 15/15 [04:14<00:00, 16.98s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:48,980 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:48,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:48,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:48,987 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:48,987 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:09:49,626 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:09:49,627 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:09:50,206 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:09:51,281 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:09:51,281 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:54,143 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:54,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:54,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:54,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:09:54,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:09:54,807 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:09:54,808 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:09:55,381 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:09:55,548 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:09:55,548 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8328804347826086, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8505434782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 11839
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11939, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.51it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.43it/s]Extractor Estimating: 5it [00:03,  1.46it/s]Extractor Estimating: 6it [00:04,  1.48it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.43it/s]Extractor Estimating: 10it [00:06,  1.45it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.51it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.46it/s]Extractor Estimating: 16it [00:11,  1.41it/s]Extractor Estimating: 17it [00:11,  1.41it/s]Extractor Estimating: 18it [00:12,  1.41it/s]Extractor Estimating: 19it [00:13,  1.40it/s]Extractor Estimating: 20it [00:13,  1.44it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:15,  1.42it/s]Extractor Estimating: 23it [00:15,  1.41it/s]Extractor Estimating: 24it [00:16,  1.43it/s]Extractor Estimating: 25it [00:17,  1.46it/s]Extractor Estimating: 26it [00:17,  1.49it/s]Extractor Estimating: 27it [00:18,  1.41it/s]Extractor Estimating: 28it [00:19,  1.39it/s]Extractor Estimating: 29it [00:20,  1.44it/s]Extractor Estimating: 30it [00:20,  1.46it/s]Extractor Estimating: 31it [00:21,  1.55it/s]Extractor Estimating: 32it [00:21,  1.56it/s]Extractor Estimating: 33it [00:22,  1.60it/s]Extractor Estimating: 34it [00:23,  1.54it/s]Extractor Estimating: 35it [00:23,  1.53it/s]Extractor Estimating: 36it [00:24,  1.53it/s]Extractor Estimating: 37it [00:25,  1.59it/s]Extractor Estimating: 38it [00:25,  1.54it/s]Extractor Estimating: 39it [00:26,  1.51it/s]Extractor Estimating: 40it [00:27,  1.54it/s]Extractor Estimating: 41it [00:27,  1.56it/s]Extractor Estimating: 42it [00:28,  1.54it/s]Extractor Estimating: 43it [00:29,  1.55it/s]Extractor Estimating: 44it [00:29,  1.48it/s]Extractor Estimating: 45it [00:30,  1.46it/s]Extractor Estimating: 46it [00:31,  1.47it/s]Extractor Estimating: 47it [00:31,  1.49it/s]Extractor Estimating: 48it [00:32,  1.50it/s]Extractor Estimating: 49it [00:33,  1.53it/s]Extractor Estimating: 50it [00:33,  1.52it/s]Extractor Estimating: 51it [00:34,  1.53it/s]Extractor Estimating: 52it [00:35,  1.52it/s]Extractor Estimating: 53it [00:35,  1.52it/s]Extractor Estimating: 54it [00:36,  1.53it/s]Extractor Estimating: 55it [00:37,  1.55it/s]Extractor Estimating: 56it [00:37,  1.60it/s]Extractor Estimating: 57it [00:38,  1.57it/s]Extractor Estimating: 58it [00:38,  1.55it/s]Extractor Estimating: 59it [00:39,  1.55it/s]Extractor Estimating: 60it [00:40,  1.58it/s]Extractor Estimating: 61it [00:40,  1.55it/s]Extractor Estimating: 62it [00:41,  1.56it/s]Extractor Estimating: 63it [00:42,  1.53it/s]Extractor Estimating: 64it [00:42,  1.53it/s]Extractor Estimating: 65it [00:43,  1.52it/s]Extractor Estimating: 66it [00:44,  1.51it/s]Extractor Estimating: 67it [00:44,  1.55it/s]Extractor Estimating: 68it [00:45,  1.54it/s]Extractor Estimating: 69it [00:46,  1.52it/s]Extractor Estimating: 70it [00:46,  1.59it/s]Extractor Estimating: 71it [00:47,  1.57it/s]Extractor Estimating: 72it [00:47,  1.56it/s]Extractor Estimating: 73it [00:48,  1.53it/s]Extractor Estimating: 74it [00:49,  1.58it/s]Extractor Estimating: 75it [00:49,  1.62it/s]Extractor Estimating: 76it [00:50,  1.66it/s]Extractor Estimating: 77it [00:50,  1.67it/s]Extractor Estimating: 78it [00:51,  1.67it/s]Extractor Estimating: 79it [00:52,  1.69it/s]Extractor Estimating: 80it [00:52,  1.69it/s]Extractor Estimating: 81it [00:53,  1.61it/s]Extractor Estimating: 82it [00:54,  1.63it/s]Extractor Estimating: 83it [00:54,  1.66it/s]Extractor Estimating: 84it [00:55,  1.66it/s]Extractor Estimating: 85it [00:55,  1.65it/s]Extractor Estimating: 86it [00:56,  1.63it/s]Extractor Estimating: 87it [00:57,  1.66it/s]Extractor Estimating: 88it [00:57,  1.69it/s]Extractor Estimating: 89it [00:58,  1.75it/s]Extractor Estimating: 90it [00:58,  1.73it/s]Extractor Estimating: 91it [00:59,  1.76it/s]Extractor Estimating: 92it [00:59,  1.73it/s]Extractor Estimating: 93it [01:00,  1.73it/s]Extractor Estimating: 94it [01:00,  1.78it/s]Extractor Estimating: 95it [01:01,  1.72it/s]Extractor Estimating: 96it [01:02,  1.79it/s]Extractor Estimating: 97it [01:02,  1.80it/s]Extractor Estimating: 98it [01:03,  1.85it/s]Extractor Estimating: 99it [01:03,  1.79it/s]Extractor Estimating: 100it [01:04,  1.79it/s]Extractor Estimating: 101it [01:04,  1.74it/s]Extractor Estimating: 102it [01:05,  1.68it/s]Extractor Estimating: 103it [01:06,  1.60it/s]Extractor Estimating: 104it [01:06,  1.62it/s]Extractor Estimating: 105it [01:07,  1.57it/s]Extractor Estimating: 106it [01:08,  1.45it/s]Extractor Estimating: 107it [01:08,  1.47it/s]Extractor Estimating: 108it [01:09,  1.52it/s]Extractor Estimating: 109it [01:10,  1.52it/s]Extractor Estimating: 110it [01:10,  1.50it/s]Extractor Estimating: 111it [01:11,  1.53it/s]Extractor Estimating: 112it [01:12,  1.55it/s]Extractor Estimating: 113it [01:12,  1.51it/s]Extractor Estimating: 114it [01:13,  1.54it/s]Extractor Estimating: 115it [01:14,  1.59it/s]Extractor Estimating: 116it [01:14,  1.56it/s]Extractor Estimating: 117it [01:15,  1.60it/s]Extractor Estimating: 118it [01:16,  1.53it/s]Extractor Estimating: 119it [01:16,  1.49it/s]Extractor Estimating: 120it [01:17,  1.41it/s]Extractor Estimating: 121it [01:18,  1.45it/s]Extractor Estimating: 122it [01:18,  1.43it/s]Extractor Estimating: 123it [01:19,  1.41it/s]Extractor Estimating: 124it [01:20,  1.41it/s]Extractor Estimating: 125it [01:20,  1.48it/s]Extractor Estimating: 126it [01:21,  1.43it/s]Extractor Estimating: 127it [01:22,  1.47it/s]Extractor Estimating: 128it [01:22,  1.52it/s]Extractor Estimating: 129it [01:23,  1.49it/s]Extractor Estimating: 130it [01:24,  1.47it/s]Extractor Estimating: 131it [01:25,  1.49it/s]Extractor Estimating: 132it [01:25,  1.46it/s]Extractor Estimating: 133it [01:26,  1.48it/s]Extractor Estimating: 134it [01:27,  1.51it/s]Extractor Estimating: 135it [01:27,  1.59it/s]Extractor Estimating: 136it [01:28,  1.61it/s]Extractor Estimating: 137it [01:28,  1.66it/s]Extractor Estimating: 138it [01:29,  1.59it/s]Extractor Estimating: 139it [01:30,  1.52it/s]Extractor Estimating: 140it [01:30,  1.47it/s]Extractor Estimating: 141it [01:31,  1.49it/s]Extractor Estimating: 142it [01:32,  1.51it/s]Extractor Estimating: 143it [01:32,  1.51it/s]Extractor Estimating: 144it [01:33,  1.53it/s]Extractor Estimating: 145it [01:34,  1.48it/s]Extractor Estimating: 146it [01:34,  1.44it/s]Extractor Estimating: 147it [01:35,  1.47it/s]Extractor Estimating: 148it [01:36,  1.44it/s]Extractor Estimating: 149it [01:36,  1.45it/s]Extractor Estimating: 150it [01:37,  1.38it/s]Extractor Estimating: 151it [01:38,  1.45it/s]Extractor Estimating: 152it [01:39,  1.45it/s]Extractor Estimating: 153it [01:39,  1.47it/s]Extractor Estimating: 154it [01:40,  1.53it/s]Extractor Estimating: 155it [01:41,  1.51it/s]Extractor Estimating: 156it [01:41,  1.50it/s]Extractor Estimating: 157it [01:42,  1.51it/s]Extractor Estimating: 158it [01:42,  1.53it/s]Extractor Estimating: 159it [01:43,  1.59it/s]Extractor Estimating: 160it [01:44,  1.64it/s]Extractor Estimating: 161it [01:44,  1.62it/s]Extractor Estimating: 162it [01:45,  1.56it/s]Extractor Estimating: 163it [01:46,  1.62it/s]Extractor Estimating: 164it [01:46,  1.53it/s]Extractor Estimating: 165it [01:47,  1.53it/s]Extractor Estimating: 166it [01:48,  1.50it/s]Extractor Estimating: 167it [01:48,  1.48it/s]Extractor Estimating: 168it [01:49,  1.50it/s]Extractor Estimating: 169it [01:50,  1.56it/s]Extractor Estimating: 170it [01:50,  1.51it/s]Extractor Estimating: 171it [01:51,  1.48it/s]Extractor Estimating: 172it [01:52,  1.49it/s]Extractor Estimating: 173it [01:52,  1.47it/s]Extractor Estimating: 174it [01:53,  1.44it/s]Extractor Estimating: 175it [01:54,  1.47it/s]Extractor Estimating: 176it [01:54,  1.47it/s]Extractor Estimating: 177it [01:55,  1.41it/s]Extractor Estimating: 178it [01:56,  1.38it/s]Extractor Estimating: 179it [01:57,  1.38it/s]Extractor Estimating: 180it [01:57,  1.45it/s]Extractor Estimating: 181it [01:58,  1.49it/s]Extractor Estimating: 182it [01:59,  1.49it/s]Extractor Estimating: 183it [01:59,  1.51it/s]Extractor Estimating: 184it [02:00,  1.50it/s]Extractor Estimating: 185it [02:00,  1.53it/s]Extractor Estimating: 186it [02:01,  1.53it/s]Extractor Estimating: 187it [02:02,  1.54it/s]Extractor Estimating: 188it [02:02,  1.49it/s]Extractor Estimating: 189it [02:03,  1.42it/s]Extractor Estimating: 190it [02:04,  1.47it/s]Extractor Estimating: 191it [02:05,  1.47it/s]Extractor Estimating: 192it [02:05,  1.52it/s]Extractor Estimating: 193it [02:06,  1.56it/s]Extractor Estimating: 194it [02:06,  1.55it/s]Extractor Estimating: 195it [02:07,  1.58it/s]Extractor Estimating: 196it [02:08,  1.56it/s]Extractor Estimating: 197it [02:08,  1.53it/s]Extractor Estimating: 198it [02:09,  1.49it/s]Extractor Estimating: 199it [02:10,  1.44it/s]Extractor Estimating: 200it [02:11,  1.38it/s]Extractor Estimating: 201it [02:11,  1.39it/s]Extractor Estimating: 202it [02:12,  1.45it/s]Extractor Estimating: 203it [02:13,  1.52it/s]Extractor Estimating: 204it [02:13,  1.52it/s]Extractor Estimating: 205it [02:14,  1.45it/s]Extractor Estimating: 206it [02:15,  1.46it/s]Extractor Estimating: 207it [02:15,  1.48it/s]Extractor Estimating: 208it [02:16,  1.42it/s]Extractor Estimating: 209it [02:17,  1.42it/s]Extractor Estimating: 210it [02:18,  1.40it/s]Extractor Estimating: 211it [02:18,  1.43it/s]Extractor Estimating: 212it [02:19,  1.43it/s]Extractor Estimating: 213it [02:20,  1.44it/s]Extractor Estimating: 214it [02:20,  1.42it/s]Extractor Estimating: 215it [02:21,  1.39it/s]Extractor Estimating: 216it [02:22,  1.43it/s]Extractor Estimating: 217it [02:22,  1.41it/s]Extractor Estimating: 218it [02:23,  1.46it/s]Extractor Estimating: 219it [02:24,  1.49it/s]Extractor Estimating: 220it [02:24,  1.48it/s]Extractor Estimating: 221it [02:25,  1.39it/s]Extractor Estimating: 222it [02:26,  1.40it/s]Extractor Estimating: 223it [02:27,  1.44it/s]Extractor Estimating: 224it [02:27,  1.46it/s]Extractor Estimating: 225it [02:28,  1.48it/s]Extractor Estimating: 226it [02:28,  1.54it/s]Extractor Estimating: 227it [02:29,  1.55it/s]Extractor Estimating: 228it [02:30,  1.57it/s]Extractor Estimating: 229it [02:30,  1.62it/s]Extractor Estimating: 230it [02:31,  1.61it/s]Extractor Estimating: 231it [02:31,  1.69it/s]Extractor Estimating: 232it [02:32,  1.69it/s]Extractor Estimating: 233it [02:33,  1.63it/s]Extractor Estimating: 234it [02:33,  1.57it/s]Extractor Estimating: 235it [02:34,  1.58it/s]Extractor Estimating: 236it [02:35,  1.63it/s]Extractor Estimating: 237it [02:35,  1.62it/s]Extractor Estimating: 238it [02:36,  1.61it/s]Extractor Estimating: 239it [02:36,  1.64it/s]Extractor Estimating: 240it [02:37,  1.62it/s]Extractor Estimating: 241it [02:38,  1.59it/s]Extractor Estimating: 242it [02:38,  1.62it/s]Extractor Estimating: 243it [02:39,  1.65it/s]Extractor Estimating: 244it [02:40,  1.61it/s]Extractor Estimating: 245it [02:40,  1.65it/s]Extractor Estimating: 246it [02:41,  1.67it/s]Extractor Estimating: 247it [02:41,  1.61it/s]Extractor Estimating: 248it [02:42,  1.66it/s]Extractor Estimating: 249it [02:43,  1.67it/s]Extractor Estimating: 250it [02:43,  1.67it/s]Extractor Estimating: 251it [02:44,  1.63it/s]Extractor Estimating: 252it [02:44,  1.62it/s]Extractor Estimating: 253it [02:45,  1.63it/s]Extractor Estimating: 254it [02:46,  1.62it/s]Extractor Estimating: 255it [02:46,  1.60it/s]Extractor Estimating: 256it [02:47,  1.59it/s]Extractor Estimating: 257it [02:48,  1.48it/s]Extractor Estimating: 258it [02:48,  1.47it/s]Extractor Estimating: 259it [02:49,  1.47it/s]Extractor Estimating: 260it [02:50,  1.44it/s]Extractor Estimating: 261it [02:50,  1.45it/s]Extractor Estimating: 262it [02:51,  1.48it/s]Extractor Estimating: 263it [02:52,  1.48it/s]Extractor Estimating: 264it [02:52,  1.52it/s]Extractor Estimating: 265it [02:53,  1.51it/s]Extractor Estimating: 266it [02:54,  1.56it/s]Extractor Estimating: 267it [02:54,  1.58it/s]Extractor Estimating: 268it [02:55,  1.53it/s]Extractor Estimating: 269it [02:56,  1.57it/s]Extractor Estimating: 270it [02:56,  1.51it/s]Extractor Estimating: 271it [02:57,  1.53it/s]Extractor Estimating: 272it [02:58,  1.52it/s]Extractor Estimating: 273it [02:58,  1.55it/s]Extractor Estimating: 274it [02:59,  1.58it/s]Extractor Estimating: 275it [02:59,  1.62it/s]Extractor Estimating: 276it [03:00,  1.64it/s]Extractor Estimating: 277it [03:01,  1.67it/s]Extractor Estimating: 278it [03:01,  1.64it/s]Extractor Estimating: 279it [03:02,  1.67it/s]Extractor Estimating: 280it [03:02,  1.69it/s]Extractor Estimating: 281it [03:03,  1.69it/s]Extractor Estimating: 282it [03:04,  1.70it/s]Extractor Estimating: 283it [03:04,  1.72it/s]Extractor Estimating: 284it [03:05,  1.66it/s]Extractor Estimating: 285it [03:05,  1.58it/s]Extractor Estimating: 286it [03:06,  1.61it/s]Extractor Estimating: 287it [03:07,  1.52it/s]Extractor Estimating: 288it [03:07,  1.52it/s]Extractor Estimating: 289it [03:08,  1.54it/s]Extractor Estimating: 290it [03:09,  1.62it/s]Extractor Estimating: 291it [03:09,  1.67it/s]Extractor Estimating: 292it [03:10,  1.68it/s]Extractor Estimating: 293it [03:10,  1.69it/s]Extractor Estimating: 294it [03:11,  1.71it/s]Extractor Estimating: 295it [03:12,  1.68it/s]Extractor Estimating: 296it [03:12,  1.64it/s]Extractor Estimating: 297it [03:13,  1.66it/s]Extractor Estimating: 298it [03:13,  1.63it/s]Extractor Estimating: 299it [03:14,  1.68it/s]Extractor Estimating: 300it [03:15,  1.65it/s]Extractor Estimating: 301it [03:15,  1.59it/s]Extractor Estimating: 302it [03:16,  1.57it/s]Extractor Estimating: 303it [03:17,  1.56it/s]Extractor Estimating: 304it [03:17,  1.58it/s]Extractor Estimating: 305it [03:18,  1.61it/s]Extractor Estimating: 306it [03:18,  1.65it/s]Extractor Estimating: 307it [03:19,  1.63it/s]Extractor Estimating: 308it [03:20,  1.62it/s]Extractor Estimating: 309it [03:20,  1.51it/s]Extractor Estimating: 310it [03:21,  1.52it/s]Extractor Estimating: 311it [03:22,  1.57it/s]Extractor Estimating: 312it [03:22,  1.58it/s]Extractor Estimating: 313it [03:23,  1.59it/s]Extractor Estimating: 314it [03:23,  1.62it/s]Extractor Estimating: 315it [03:24,  1.55it/s]Extractor Estimating: 316it [03:25,  1.53it/s]Extractor Estimating: 317it [03:25,  1.56it/s]Extractor Estimating: 318it [03:26,  1.59it/s]Extractor Estimating: 319it [03:27,  1.53it/s]Extractor Estimating: 320it [03:27,  1.55it/s]Extractor Estimating: 321it [03:28,  1.52it/s]Extractor Estimating: 322it [03:29,  1.56it/s]Extractor Estimating: 323it [03:29,  1.52it/s]Extractor Estimating: 324it [03:30,  1.49it/s]Extractor Estimating: 325it [03:31,  1.50it/s]Extractor Estimating: 326it [03:31,  1.55it/s]Extractor Estimating: 327it [03:32,  1.56it/s]Extractor Estimating: 328it [03:33,  1.55it/s]Extractor Estimating: 329it [03:33,  1.60it/s]Extractor Estimating: 330it [03:34,  1.66it/s]Extractor Estimating: 331it [03:34,  1.70it/s]Extractor Estimating: 332it [03:35,  1.64it/s]Extractor Estimating: 333it [03:36,  1.65it/s]Extractor Estimating: 334it [03:36,  1.63it/s]Extractor Estimating: 335it [03:37,  1.67it/s]Extractor Estimating: 336it [03:37,  1.65it/s]Extractor Estimating: 337it [03:38,  1.68it/s]Extractor Estimating: 338it [03:38,  1.71it/s]Extractor Estimating: 339it [03:39,  1.70it/s]Extractor Estimating: 340it [03:40,  1.63it/s]Extractor Estimating: 341it [03:40,  1.64it/s]Extractor Estimating: 342it [03:41,  1.68it/s]Extractor Estimating: 343it [03:42,  1.70it/s]Extractor Estimating: 344it [03:42,  1.68it/s]Extractor Estimating: 345it [03:43,  1.65it/s]Extractor Estimating: 346it [03:43,  1.68it/s]Extractor Estimating: 347it [03:44,  1.68it/s]Extractor Estimating: 348it [03:44,  1.70it/s]Extractor Estimating: 349it [03:45,  1.70it/s]Extractor Estimating: 350it [03:46,  1.65it/s]Extractor Estimating: 351it [03:46,  1.56it/s]Extractor Estimating: 352it [03:47,  1.56it/s]Extractor Estimating: 353it [03:48,  1.48it/s]Extractor Estimating: 354it [03:48,  1.52it/s]Extractor Estimating: 355it [03:49,  1.53it/s]Extractor Estimating: 356it [03:50,  1.58it/s]Extractor Estimating: 357it [03:50,  1.56it/s]Extractor Estimating: 358it [03:51,  1.58it/s]Extractor Estimating: 359it [03:52,  1.59it/s]Extractor Estimating: 360it [03:52,  1.54it/s]Extractor Estimating: 361it [03:53,  1.48it/s]Extractor Estimating: 362it [03:54,  1.47it/s]Extractor Estimating: 363it [03:54,  1.46it/s]Extractor Estimating: 364it [03:55,  1.44it/s]Extractor Estimating: 365it [03:56,  1.47it/s]Extractor Estimating: 366it [03:57,  1.42it/s]Extractor Estimating: 367it [03:57,  1.45it/s]Extractor Estimating: 368it [03:58,  1.42it/s]Extractor Estimating: 369it [03:59,  1.30it/s]Extractor Estimating: 370it [04:00,  1.32it/s]Extractor Estimating: 371it [04:00,  1.32it/s]Extractor Estimating: 372it [04:01,  1.37it/s]Extractor Estimating: 373it [04:02,  1.40it/s]Extractor Estimating: 374it [04:02,  1.41it/s]Extractor Estimating: 375it [04:03,  1.35it/s]Extractor Estimating: 375it [04:03,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:11,732 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:11,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:11,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:11,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:11,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:14:12,371 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:14:12,372 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:14:12,939 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:14:13,985 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:14:13,985 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:16,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:16,848 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:16,849 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:16,849 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:14:16,849 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:14:17,599 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:14:17,601 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:14:18,155 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:14:18,309 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:14:18,309 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:34:11,605 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:34:11,615 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7496 mean pseudo reward: 0.9310502004263431
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 22864
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22964, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22964, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.096, loss:806.9648
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.099, loss:781.6305
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.094, loss:795.1964
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.091, loss:716.8321
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.100, loss:765.2449
>> valid entity prec:0.6122, rec:0.6090, f1:0.6106
>> valid relation prec:0.4323, rec:0.1364, f1:0.2073
>> valid relation with NER prec:0.4323, rec:0.1364, f1:0.2073
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.376, loss:784.5440
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.086, loss:764.3196
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.102, loss:772.6849
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.092, loss:770.4609
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.089, loss:730.8117
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5706, rec:0.6792, f1:0.6202
>> valid relation prec:0.3834, rec:0.1163, f1:0.1785
>> valid relation with NER prec:0.3834, rec:0.1163, f1:0.1785
new max entity f1 on valid!
g_step 1100, step 161, avg_time 2.359, loss:756.4114
g_step 1200, step 261, avg_time 1.107, loss:766.2712
g_step 1300, step 48, avg_time 1.091, loss:746.4154
g_step 1400, step 148, avg_time 1.088, loss:693.5837
g_step 1500, step 248, avg_time 1.103, loss:724.7289
>> valid entity prec:0.6059, rec:0.6035, f1:0.6047
>> valid relation prec:0.4066, rec:0.0911, f1:0.1488
>> valid relation with NER prec:0.4066, rec:0.0911, f1:0.1488
g_step 1600, step 35, avg_time 2.381, loss:710.4639
g_step 1700, step 135, avg_time 1.104, loss:661.5184
g_step 1800, step 235, avg_time 1.084, loss:693.2071
g_step 1900, step 22, avg_time 1.098, loss:668.6596
g_step 2000, step 122, avg_time 1.101, loss:614.4779
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5838, rec:0.6170, f1:0.6000
>> valid relation prec:0.4283, rec:0.1232, f1:0.1913
>> valid relation with NER prec:0.4283, rec:0.1232, f1:0.1913
g_step 2100, step 222, avg_time 2.373, loss:654.5924
g_step 2200, step 9, avg_time 1.086, loss:642.8249
g_step 2300, step 109, avg_time 1.097, loss:591.9138
g_step 2400, step 209, avg_time 1.106, loss:612.0094
g_step 2500, step 309, avg_time 1.095, loss:620.4983
>> valid entity prec:0.6387, rec:0.5013, f1:0.5617
>> valid relation prec:0.3511, rec:0.0980, f1:0.1532
>> valid relation with NER prec:0.3511, rec:0.0980, f1:0.1532
g_step 2600, step 96, avg_time 2.364, loss:565.7440
g_step 2700, step 196, avg_time 1.099, loss:604.9504
g_step 2800, step 296, avg_time 1.099, loss:573.5810
g_step 2900, step 83, avg_time 1.102, loss:547.2582
g_step 3000, step 183, avg_time 1.101, loss:550.7128
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6316, rec:0.5028, f1:0.5599
>> valid relation prec:0.3232, rec:0.0854, f1:0.1351
>> valid relation with NER prec:0.3232, rec:0.0854, f1:0.1351
g_step 3100, step 283, avg_time 2.366, loss:563.9771
g_step 3200, step 70, avg_time 1.091, loss:513.2083
g_step 3300, step 170, avg_time 1.103, loss:537.0332
g_step 3400, step 270, avg_time 1.088, loss:531.6761
g_step 3500, step 57, avg_time 1.092, loss:497.7109
>> valid entity prec:0.5550, rec:0.4945, f1:0.5230
>> valid relation prec:0.3343, rec:0.0954, f1:0.1484
>> valid relation with NER prec:0.3343, rec:0.0954, f1:0.1484
g_step 3600, step 157, avg_time 2.381, loss:505.9439
g_step 3700, step 257, avg_time 1.100, loss:509.5357
g_step 3800, step 44, avg_time 1.083, loss:504.0040
g_step 3900, step 144, avg_time 1.099, loss:469.3420
g_step 4000, step 244, avg_time 1.099, loss:493.2218
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5807, rec:0.5700, f1:0.5753
>> valid relation prec:0.3250, rec:0.1048, f1:0.1585
>> valid relation with NER prec:0.3250, rec:0.1048, f1:0.1585
g_step 4100, step 31, avg_time 2.378, loss:466.2655
g_step 4200, step 131, avg_time 1.093, loss:443.1974
g_step 4300, step 231, avg_time 1.095, loss:457.1171
g_step 4400, step 18, avg_time 1.096, loss:481.3330
g_step 4500, step 118, avg_time 1.081, loss:417.8250
>> valid entity prec:0.5869, rec:0.5637, f1:0.5751
>> valid relation prec:0.2650, rec:0.0836, f1:0.1272
>> valid relation with NER prec:0.2650, rec:0.0836, f1:0.1272
g_step 4600, step 218, avg_time 2.382, loss:455.2292
g_step 4700, step 5, avg_time 1.104, loss:450.9111
g_step 4800, step 105, avg_time 1.101, loss:417.5261
g_step 4900, step 205, avg_time 1.096, loss:425.5172
g_step 5000, step 305, avg_time 1.094, loss:431.7937
learning rate was adjusted to 0.0008
>> valid entity prec:0.5944, rec:0.5394, f1:0.5656
>> valid relation prec:0.2920, rec:0.0925, f1:0.1405
>> valid relation with NER prec:0.2920, rec:0.0925, f1:0.1405
g_step 5100, step 92, avg_time 2.361, loss:412.8124
g_step 5200, step 192, avg_time 1.092, loss:398.4210
g_step 5300, step 292, avg_time 1.101, loss:407.1880
g_step 5400, step 79, avg_time 1.074, loss:370.2632
g_step 5500, step 179, avg_time 1.092, loss:389.9201
>> valid entity prec:0.6044, rec:0.5154, f1:0.5564
>> valid relation prec:0.3020, rec:0.0828, f1:0.1299
>> valid relation with NER prec:0.3020, rec:0.0828, f1:0.1299
g_step 5600, step 279, avg_time 2.384, loss:397.8119
g_step 5700, step 66, avg_time 1.104, loss:384.1678
g_step 5800, step 166, avg_time 1.103, loss:369.5983
g_step 5900, step 266, avg_time 1.102, loss:379.2185
g_step 6000, step 53, avg_time 1.078, loss:360.9481
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5350, rec:0.6297, f1:0.5785
>> valid relation prec:0.3018, rec:0.1280, f1:0.1798
>> valid relation with NER prec:0.3018, rec:0.1280, f1:0.1798
g_step 6100, step 153, avg_time 2.371, loss:341.0281
g_step 6200, step 253, avg_time 1.098, loss:363.9115
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:34:11 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:34:11 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-34-11_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:34:12 - WARNING - datasets.builder -   Using custom data configuration default-94f09710791e0a16
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-94f09710791e0a16/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:34:13,091 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:13,092 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:34:13,092 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:34:13,093 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:34:13,105 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:34:13,109 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:34:13,513 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:34:16,688 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:34:16,697 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-94f09710791e0a16/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.73ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.88ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.52ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.92ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.15ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.30ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.42ba/s]100%|██████████| 8/8 [00:01<00:00,  5.29ba/s]100%|██████████| 8/8 [00:01<00:00,  4.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.34ba/s]100%|██████████| 4/4 [00:00<00:00,  5.39ba/s]100%|██████████| 4/4 [00:00<00:00,  4.90ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.01ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.44ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.02ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.33ba/s]100%|██████████| 8/8 [00:00<00:00, 10.65ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.36ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.16ba/s]100%|██████████| 4/4 [00:00<00:00, 10.18ba/s]
[INFO|trainer.py:414] 2023-08-28 18:34:21,502 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:34:21,508 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:34:21,508 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 18:34:21,508 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:34:21,509 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:34:21,509 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:34:21,509 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:34:21,509 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:54,  3.34it/s]  0%|          | 2/585 [00:00<02:50,  3.43it/s]  1%|          | 3/585 [00:00<02:48,  3.46it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<03:02,  3.17it/s]  1%|          | 7/585 [00:02<02:56,  3.27it/s]  1%|▏         | 8/585 [00:02<02:52,  3.34it/s]  2%|▏         | 9/585 [00:02<02:50,  3.38it/s]  2%|▏         | 10/585 [00:02<02:48,  3.42it/s]  2%|▏         | 11/585 [00:03<02:46,  3.44it/s]  2%|▏         | 12/585 [00:03<02:45,  3.45it/s]  2%|▏         | 13/585 [00:03<02:45,  3.46it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.48it/s]  3%|▎         | 17/585 [00:04<02:42,  3.48it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.49it/s]  3%|▎         | 20/585 [00:05<02:41,  3.49it/s]  4%|▎         | 21/585 [00:06<02:41,  3.49it/s]  4%|▍         | 22/585 [00:06<02:41,  3.49it/s]  4%|▍         | 23/585 [00:06<02:41,  3.49it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.48it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:39,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:09<02:48,  3.28it/s]  5%|▌         | 32/585 [00:09<02:45,  3.34it/s]  6%|▌         | 33/585 [00:09<02:43,  3.38it/s]  6%|▌         | 34/585 [00:09<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:39,  3.45it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:11<02:38,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.46it/s]  7%|▋         | 40/585 [00:11<02:37,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.44it/s]  8%|▊         | 44/585 [00:12<02:36,  3.45it/s]  8%|▊         | 45/585 [00:13<02:35,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.47it/s]  8%|▊         | 47/585 [00:13<02:34,  3.47it/s]  8%|▊         | 48/585 [00:13<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.47it/s]  9%|▉         | 52/585 [00:15<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:33,  3.48it/s]  9%|▉         | 54/585 [00:15<02:32,  3.47it/s]  9%|▉         | 55/585 [00:15<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.48it/s] 10%|█         | 59/585 [00:17<02:33,  3.42it/s] 10%|█         | 60/585 [00:17<02:32,  3.43it/s] 10%|█         | 61/585 [00:17<02:32,  3.45it/s] 11%|█         | 62/585 [00:17<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 69/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.48it/s] 13%|█▎        | 76/585 [00:22<02:26,  3.48it/s] 13%|█▎        | 77/585 [00:22<02:42,  3.13it/s] 13%|█▎        | 78/585 [00:22<02:37,  3.23it/s] 14%|█▎        | 79/585 [00:22<02:33,  3.29it/s] 14%|█▎        | 80/585 [00:23<02:30,  3.35it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.39it/s] 14%|█▍        | 82/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.45it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 90/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 93/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:26,  3.35it/s] 16%|█▋        | 96/585 [00:27<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.41it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 107/585 [00:31<02:17,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 111/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 18:34:55,532 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:34:55,533 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:34:55,533 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.34it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.47it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.77it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.98it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.53it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.20it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.04it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.98it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.93it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.74it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.79it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 42.33it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 43.57it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 44.52it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 45.06it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 45.52it/s][A
 20%|██        | 88/437 [00:01<00:07, 45.84it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.00it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.10it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.20it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.26it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 46.26it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.24it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.30it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.12it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.26it/s][A
 32%|███▏      | 138/437 [00:03<00:08, 37.22it/s][A
 33%|███▎      | 143/437 [00:03<00:07, 39.61it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 41.48it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 42.94it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 43.97it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 44.75it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 45.34it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 45.72it/s][A
 41%|████      | 178/437 [00:03<00:05, 45.94it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 46.24it/s][A
 43%|████▎     | 188/437 [00:04<00:10, 23.24it/s][A
 44%|████▍     | 192/437 [00:04<00:09, 25.67it/s][A
 45%|████▌     | 197/437 [00:04<00:08, 29.86it/s][A
 46%|████▌     | 202/437 [00:04<00:06, 33.64it/s][A
 47%|████▋     | 207/437 [00:04<00:06, 36.81it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 39.36it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 41.39it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.91it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.98it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.80it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 45.39it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 45.78it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 46.07it/s][A
 58%|█████▊    | 252/437 [00:05<00:03, 46.35it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 46.44it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 46.59it/s][A
 61%|██████    | 267/437 [00:06<00:03, 46.67it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 46.70it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 46.80it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 46.76it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 46.78it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 46.70it/s][A
 68%|██████▊   | 297/437 [00:06<00:02, 46.83it/s][A
 69%|██████▉   | 302/437 [00:06<00:02, 46.83it/s][A
 70%|███████   | 307/437 [00:07<00:02, 46.77it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 46.77it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 46.80it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 46.85it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 46.82it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 46.87it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 39.88it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 41.71it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.14it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.19it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.95it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 45.52it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 45.86it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 46.09it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 46.40it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 46.54it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 46.59it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 46.63it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 46.73it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 46.73it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 46.76it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 46.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 46.85it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 46.89it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 46.86it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 46.76it/s][A
100%|██████████| 437/437 [00:09<00:00, 46.69it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.69it/s][A 20%|██        | 117/585 [00:43<02:15,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:35:05,510 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 18:35:05,538 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:35:11,799 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:35:11,864 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:35:11,880 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:59<1:02:04,  7.98s/it] 20%|██        | 119/585 [01:00<44:11,  5.69s/it]   21%|██        | 120/585 [01:00<31:32,  4.07s/it] 21%|██        | 121/585 [01:00<22:41,  2.93s/it] 21%|██        | 122/585 [01:01<16:31,  2.14s/it] 21%|██        | 123/585 [01:01<12:12,  1.58s/it] 21%|██        | 124/585 [01:01<09:11,  1.20s/it] 21%|██▏       | 125/585 [01:01<07:04,  1.08it/s] 22%|██▏       | 126/585 [01:02<05:36,  1.37it/s] 22%|██▏       | 127/585 [01:02<04:34,  1.67it/s] 22%|██▏       | 128/585 [01:02<03:51,  1.98it/s] 22%|██▏       | 129/585 [01:03<03:20,  2.27it/s] 22%|██▏       | 130/585 [01:03<03:03,  2.48it/s] 22%|██▏       | 131/585 [01:03<02:47,  2.71it/s] 23%|██▎       | 132/585 [01:04<02:35,  2.91it/s] 23%|██▎       | 133/585 [01:04<02:27,  3.06it/s] 23%|██▎       | 134/585 [01:04<02:22,  3.17it/s] 23%|██▎       | 135/585 [01:04<02:18,  3.25it/s] 23%|██▎       | 136/585 [01:05<02:15,  3.32it/s] 23%|██▎       | 137/585 [01:05<02:13,  3.37it/s] 24%|██▎       | 138/585 [01:05<02:11,  3.40it/s] 24%|██▍       | 139/585 [01:06<02:10,  3.42it/s] 24%|██▍       | 140/585 [01:06<02:09,  3.44it/s] 24%|██▍       | 141/585 [01:06<02:09,  3.44it/s] 24%|██▍       | 142/585 [01:06<02:08,  3.45it/s] 24%|██▍       | 143/585 [01:07<02:07,  3.46it/s] 25%|██▍       | 144/585 [01:07<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:07<02:07,  3.46it/s] 25%|██▍       | 146/585 [01:08<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:08<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:08<02:05,  3.47it/s] 25%|██▌       | 149/585 [01:08<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:09<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:09<02:04,  3.47it/s] 26%|██▌       | 152/585 [01:09<02:05,  3.46it/s] 26%|██▌       | 153/585 [01:10<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:10<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:10<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:10<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:11<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:11<02:02,  3.47it/s] 27%|██▋       | 159/585 [01:11<02:02,  3.48it/s] 27%|██▋       | 160/585 [01:12<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:12<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:12<02:01,  3.47it/s] 28%|██▊       | 163/585 [01:12<02:07,  3.32it/s] 28%|██▊       | 164/585 [01:13<02:05,  3.37it/s] 28%|██▊       | 165/585 [01:13<02:03,  3.40it/s] 28%|██▊       | 166/585 [01:13<02:02,  3.42it/s] 29%|██▊       | 167/585 [01:14<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:14<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:14<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:14<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:15<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:15<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:15<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:16<01:59,  3.43it/s] 30%|██▉       | 175/585 [01:16<01:59,  3.44it/s] 30%|███       | 176/585 [01:16<01:58,  3.45it/s] 30%|███       | 177/585 [01:17<01:58,  3.46it/s] 30%|███       | 178/585 [01:17<01:57,  3.46it/s] 31%|███       | 179/585 [01:17<01:57,  3.47it/s] 31%|███       | 180/585 [01:17<01:56,  3.47it/s] 31%|███       | 181/585 [01:18<01:56,  3.47it/s] 31%|███       | 182/585 [01:18<01:56,  3.47it/s] 31%|███▏      | 183/585 [01:18<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:19<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:19<01:55,  3.45it/s] 32%|███▏      | 186/585 [01:19<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:19<01:54,  3.46it/s] 32%|███▏      | 188/585 [01:20<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:20<01:54,  3.47it/s] 32%|███▏      | 190/585 [01:20<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:21<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:21<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:21<01:52,  3.47it/s] 33%|███▎      | 194/585 [01:21<01:52,  3.47it/s] 33%|███▎      | 195/585 [01:22<01:52,  3.47it/s] 34%|███▎      | 196/585 [01:22<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:22<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:23<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:23<01:51,  3.47it/s] 34%|███▍      | 200/585 [01:23<01:50,  3.47it/s] 34%|███▍      | 201/585 [01:23<01:50,  3.47it/s] 35%|███▍      | 202/585 [01:24<01:50,  3.47it/s] 35%|███▍      | 203/585 [01:24<01:49,  3.47it/s] 35%|███▍      | 204/585 [01:24<01:49,  3.47it/s] 35%|███▌      | 205/585 [01:25<01:49,  3.47it/s] 35%|███▌      | 206/585 [01:25<01:49,  3.47it/s] 35%|███▌      | 207/585 [01:25<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:25<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:26<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:26<01:48,  3.47it/s] 36%|███▌      | 211/585 [01:26<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:27<01:47,  3.47it/s] 36%|███▋      | 213/585 [01:27<01:47,  3.47it/s] 37%|███▋      | 214/585 [01:27<01:46,  3.47it/s] 37%|███▋      | 215/585 [01:27<01:46,  3.47it/s] 37%|███▋      | 216/585 [01:28<01:46,  3.47it/s] 37%|███▋      | 217/585 [01:28<01:46,  3.47it/s] 37%|███▋      | 218/585 [01:28<01:50,  3.32it/s] 37%|███▋      | 219/585 [01:29<01:48,  3.36it/s] 38%|███▊      | 220/585 [01:29<01:47,  3.39it/s] 38%|███▊      | 221/585 [01:29<01:46,  3.41it/s] 38%|███▊      | 222/585 [01:30<01:45,  3.43it/s] 38%|███▊      | 223/585 [01:30<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:30<01:44,  3.44it/s] 38%|███▊      | 225/585 [01:31<02:34,  2.34it/s] 39%|███▊      | 226/585 [01:31<02:18,  2.59it/s] 39%|███▉      | 227/585 [01:31<02:07,  2.80it/s] 39%|███▉      | 228/585 [01:32<02:00,  2.97it/s] 39%|███▉      | 229/585 [01:32<01:54,  3.11it/s] 39%|███▉      | 230/585 [01:32<01:50,  3.21it/s] 39%|███▉      | 231/585 [01:33<01:47,  3.28it/s] 40%|███▉      | 232/585 [01:33<01:45,  3.33it/s] 40%|███▉      | 233/585 [01:33<01:44,  3.37it/s] 40%|████      | 234/585 [01:34<01:52,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 18:35:55,603 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:35:55,603 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:35:55,603 >>   Batch size = 8
{'eval_loss': 1.0058021545410156, 'eval_runtime': 9.928, 'eval_samples_per_second': 351.832, 'eval_steps_per_second': 44.017, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.16it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.50it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.78it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.00it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.38it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.19it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.99it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.89it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.88it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.84it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.71it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.78it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.80it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.76it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.85it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.84it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.80it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.77it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.75it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.72it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.63it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.68it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.64it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.56it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.50it/s][A
 32%|███▏      | 138/437 [00:03<00:07, 37.79it/s][A
 33%|███▎      | 143/437 [00:03<00:07, 39.97it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 41.76it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 43.16it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 44.16it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 45.01it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 45.56it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 45.89it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.13it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.31it/s][A
 43%|████▎     | 188/437 [00:04<00:16, 14.90it/s][A
 44%|████▍     | 193/437 [00:04<00:12, 18.85it/s][A
 45%|████▌     | 198/437 [00:05<00:10, 22.98it/s][A
 46%|████▋     | 203/437 [00:05<00:08, 27.10it/s][A
 48%|████▊     | 208/437 [00:05<00:07, 31.03it/s][A
 49%|████▊     | 213/437 [00:05<00:06, 34.55it/s][A
 50%|████▉     | 218/437 [00:05<00:05, 37.49it/s][A
 51%|█████     | 223/437 [00:05<00:05, 39.89it/s][A
 52%|█████▏    | 228/437 [00:05<00:05, 41.79it/s][A
 53%|█████▎    | 233/437 [00:05<00:05, 36.25it/s][A
 54%|█████▍    | 238/437 [00:05<00:05, 38.77it/s][A
 56%|█████▌    | 243/437 [00:06<00:04, 40.90it/s][A
 57%|█████▋    | 248/437 [00:06<00:04, 42.56it/s][A
 58%|█████▊    | 253/437 [00:06<00:04, 43.75it/s][A
 59%|█████▉    | 258/437 [00:06<00:04, 44.60it/s][A
 60%|██████    | 263/437 [00:06<00:03, 45.29it/s][A
 61%|██████▏   | 268/437 [00:06<00:03, 45.74it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.09it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.34it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.50it/s][A
 66%|██████▌   | 288/437 [00:07<00:03, 46.64it/s][A
 67%|██████▋   | 293/437 [00:07<00:03, 46.63it/s][A
 68%|██████▊   | 298/437 [00:07<00:02, 46.74it/s][A
 69%|██████▉   | 303/437 [00:07<00:02, 46.81it/s][A
 70%|███████   | 308/437 [00:07<00:02, 46.73it/s][A
 72%|███████▏  | 313/437 [00:07<00:02, 46.74it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 46.79it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 46.84it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.87it/s][A
 76%|███████▌  | 333/437 [00:08<00:02, 46.89it/s][A
 77%|███████▋  | 338/437 [00:08<00:02, 46.83it/s][A
 78%|███████▊  | 343/437 [00:08<00:02, 46.87it/s][A
 80%|███████▉  | 348/437 [00:08<00:01, 46.87it/s][A
 81%|████████  | 353/437 [00:08<00:01, 46.90it/s][A
 82%|████████▏ | 358/437 [00:08<00:01, 46.84it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 46.75it/s][A
 84%|████████▍ | 368/437 [00:09<00:01, 46.84it/s][A
 85%|████████▌ | 373/437 [00:09<00:03, 16.16it/s][A
 86%|████████▋ | 378/437 [00:09<00:02, 20.12it/s][A
 88%|████████▊ | 383/437 [00:09<00:02, 24.26it/s][A
 89%|████████▉ | 388/437 [00:09<00:01, 28.35it/s][A
 90%|████████▉ | 393/437 [00:09<00:01, 32.18it/s][A
 91%|█████████ | 398/437 [00:10<00:01, 35.53it/s][A
 92%|█████████▏| 403/437 [00:10<00:00, 38.27it/s][A
 93%|█████████▎| 408/437 [00:10<00:00, 40.52it/s][A
 95%|█████████▍| 413/437 [00:10<00:00, 42.24it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 43.41it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 44.41it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 45.10it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 45.65it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 45.65it/s][A 40%|████      | 234/585 [01:45<01:52,  3.13it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:36:06,737 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:36:07,283 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:36:11,730 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:36:11,807 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:36:11,818 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:02<50:56,  8.73s/it] 40%|████      | 236/585 [02:02<36:03,  6.20s/it] 41%|████      | 237/585 [02:02<25:39,  4.43s/it] 41%|████      | 238/585 [02:03<18:24,  3.18s/it] 41%|████      | 239/585 [02:03<13:20,  2.31s/it] 41%|████      | 240/585 [02:03<09:48,  1.71s/it] 41%|████      | 241/585 [02:04<07:20,  1.28s/it] 41%|████▏     | 242/585 [02:04<05:37,  1.02it/s] 42%|████▏     | 243/585 [02:04<04:25,  1.29it/s] 42%|████▏     | 244/585 [02:05<03:34,  1.59it/s] 42%|████▏     | 245/585 [02:05<02:59,  1.90it/s] 42%|████▏     | 246/585 [02:05<02:34,  2.20it/s] 42%|████▏     | 247/585 [02:05<02:16,  2.47it/s] 42%|████▏     | 248/585 [02:06<02:04,  2.71it/s] 43%|████▎     | 249/585 [02:06<01:55,  2.90it/s] 43%|████▎     | 250/585 [02:06<01:49,  3.06it/s] 43%|████▎     | 251/585 [02:07<01:45,  3.17it/s] 43%|████▎     | 252/585 [02:07<01:42,  3.26it/s] 43%|████▎     | 253/585 [02:07<01:39,  3.33it/s] 43%|████▎     | 254/585 [02:07<01:38,  3.35it/s] 44%|████▎     | 255/585 [02:08<01:37,  3.39it/s] 44%|████▍     | 256/585 [02:08<01:36,  3.42it/s] 44%|████▍     | 257/585 [02:08<01:35,  3.44it/s] 44%|████▍     | 258/585 [02:09<01:34,  3.45it/s] 44%|████▍     | 259/585 [02:09<01:34,  3.46it/s] 44%|████▍     | 260/585 [02:09<01:33,  3.47it/s] 45%|████▍     | 261/585 [02:09<01:33,  3.47it/s] 45%|████▍     | 262/585 [02:10<01:32,  3.48it/s] 45%|████▍     | 263/585 [02:10<01:32,  3.48it/s] 45%|████▌     | 264/585 [02:10<01:32,  3.48it/s] 45%|████▌     | 265/585 [02:11<01:32,  3.47it/s] 45%|████▌     | 266/585 [02:11<01:31,  3.47it/s] 46%|████▌     | 267/585 [02:11<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:11<01:31,  3.48it/s] 46%|████▌     | 269/585 [02:12<01:30,  3.47it/s] 46%|████▌     | 270/585 [02:12<01:30,  3.48it/s] 46%|████▋     | 271/585 [02:12<01:30,  3.48it/s] 46%|████▋     | 272/585 [02:13<01:29,  3.48it/s] 47%|████▋     | 273/585 [02:13<01:29,  3.48it/s] 47%|████▋     | 274/585 [02:13<01:29,  3.48it/s] 47%|████▋     | 275/585 [02:13<01:28,  3.48it/s] 47%|████▋     | 276/585 [02:14<01:29,  3.46it/s] 47%|████▋     | 277/585 [02:14<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:14<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:15<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:15<01:27,  3.47it/s] 48%|████▊     | 281/585 [02:15<01:27,  3.48it/s] 48%|████▊     | 282/585 [02:15<01:27,  3.48it/s] 48%|████▊     | 283/585 [02:16<01:26,  3.48it/s] 49%|████▊     | 284/585 [02:16<01:26,  3.48it/s] 49%|████▊     | 285/585 [02:16<01:26,  3.48it/s] 49%|████▉     | 286/585 [02:17<01:25,  3.48it/s] 49%|████▉     | 287/585 [02:17<01:26,  3.45it/s] 49%|████▉     | 288/585 [02:17<01:25,  3.46it/s] 49%|████▉     | 289/585 [02:17<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:18<01:24,  3.47it/s] 50%|████▉     | 291/585 [02:18<01:24,  3.47it/s] 50%|████▉     | 292/585 [02:18<01:24,  3.48it/s] 50%|█████     | 293/585 [02:19<01:23,  3.48it/s] 50%|█████     | 294/585 [02:19<01:23,  3.48it/s] 50%|█████     | 295/585 [02:19<01:23,  3.48it/s] 51%|█████     | 296/585 [02:19<01:23,  3.48it/s] 51%|█████     | 297/585 [02:20<01:22,  3.48it/s] 51%|█████     | 298/585 [02:20<01:22,  3.47it/s] 51%|█████     | 299/585 [02:20<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:21<01:21,  3.48it/s] 51%|█████▏    | 301/585 [02:21<01:21,  3.48it/s] 52%|█████▏    | 302/585 [02:21<01:21,  3.48it/s] 52%|█████▏    | 303/585 [02:21<01:21,  3.48it/s] 52%|█████▏    | 304/585 [02:22<01:20,  3.48it/s] 52%|█████▏    | 305/585 [02:22<01:20,  3.48it/s] 52%|█████▏    | 306/585 [02:22<01:20,  3.48it/s] 52%|█████▏    | 307/585 [02:23<01:19,  3.48it/s] 53%|█████▎    | 308/585 [02:23<01:19,  3.48it/s] 53%|█████▎    | 309/585 [02:23<01:20,  3.42it/s] 53%|█████▎    | 310/585 [02:24<01:19,  3.44it/s] 53%|█████▎    | 311/585 [02:24<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:24<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:24<01:18,  3.47it/s] 54%|█████▎    | 314/585 [02:25<01:18,  3.47it/s] 54%|█████▍    | 315/585 [02:25<01:17,  3.47it/s] 54%|█████▍    | 316/585 [02:25<01:17,  3.48it/s] 54%|█████▍    | 317/585 [02:26<01:17,  3.48it/s] 54%|█████▍    | 318/585 [02:26<01:16,  3.48it/s] 55%|█████▍    | 319/585 [02:26<01:16,  3.48it/s] 55%|█████▍    | 320/585 [02:26<01:22,  3.22it/s] 55%|█████▍    | 321/585 [02:27<01:20,  3.29it/s] 55%|█████▌    | 322/585 [02:27<01:18,  3.35it/s] 55%|█████▌    | 323/585 [02:27<01:17,  3.38it/s] 55%|█████▌    | 324/585 [02:28<01:16,  3.41it/s] 56%|█████▌    | 325/585 [02:28<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:28<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:28<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:29<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:29<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:29<01:13,  3.47it/s] 57%|█████▋    | 331/585 [02:30<01:19,  3.21it/s] 57%|█████▋    | 332/585 [02:30<01:16,  3.29it/s] 57%|█████▋    | 333/585 [02:30<01:15,  3.34it/s] 57%|█████▋    | 334/585 [02:31<01:14,  3.38it/s] 57%|█████▋    | 335/585 [02:31<01:13,  3.40it/s] 57%|█████▋    | 336/585 [02:31<01:12,  3.43it/s] 58%|█████▊    | 337/585 [02:31<01:12,  3.44it/s] 58%|█████▊    | 338/585 [02:32<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:32<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:32<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:33<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:33<01:10,  3.47it/s] 59%|█████▊    | 343/585 [02:33<01:09,  3.47it/s] 59%|█████▉    | 344/585 [02:33<01:09,  3.47it/s] 59%|█████▉    | 345/585 [02:34<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:34<01:09,  3.46it/s] 59%|█████▉    | 347/585 [02:34<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:35<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:35<01:10,  3.34it/s] 60%|█████▉    | 350/585 [02:35<01:09,  3.38it/s] 60%|██████    | 351/585 [02:35<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 18:36:57,551 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:36:57,551 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:36:57,552 >>   Batch size = 8
{'eval_loss': 1.0164378881454468, 'eval_runtime': 10.9334, 'eval_samples_per_second': 319.481, 'eval_steps_per_second': 39.969, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.25it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.69it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.01it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.51it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.29it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.09it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.93it/s][A
 11%|█         | 48/437 [00:01<00:10, 37.88it/s][A
 12%|█▏        | 53/437 [00:01<00:09, 40.14it/s][A
 13%|█▎        | 58/437 [00:01<00:09, 41.90it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 43.29it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 44.21it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 44.94it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 45.49it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 45.85it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.00it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 46.19it/s][A
 22%|██▏       | 98/437 [00:02<00:08, 41.52it/s][A
 24%|██▎       | 103/437 [00:02<00:08, 39.33it/s][A
 25%|██▍       | 108/437 [00:03<00:35,  9.30it/s][A
 26%|██▌       | 113/437 [00:03<00:26, 12.25it/s][A
 27%|██▋       | 118/437 [00:04<00:20, 15.73it/s][A
 28%|██▊       | 123/437 [00:04<00:15, 19.65it/s][A
 29%|██▉       | 128/437 [00:04<00:12, 23.79it/s][A
 30%|███       | 133/437 [00:04<00:10, 27.90it/s][A
 32%|███▏      | 138/437 [00:04<00:09, 31.74it/s][A
 33%|███▎      | 143/437 [00:04<00:08, 35.16it/s][A
 34%|███▍      | 148/437 [00:04<00:07, 38.01it/s][A
 35%|███▌      | 153/437 [00:04<00:07, 40.33it/s][A
 36%|███▌      | 158/437 [00:04<00:06, 42.09it/s][A
 37%|███▋      | 163/437 [00:05<00:06, 43.44it/s][A
 38%|███▊      | 168/437 [00:05<00:06, 44.31it/s][A
 40%|███▉      | 173/437 [00:05<00:07, 35.48it/s][A
 41%|████      | 178/437 [00:05<00:06, 38.21it/s][A
 42%|████▏     | 183/437 [00:05<00:06, 40.46it/s][A
 43%|████▎     | 188/437 [00:05<00:05, 42.18it/s][A
 44%|████▍     | 193/437 [00:05<00:05, 43.47it/s][A
 45%|████▌     | 198/437 [00:05<00:05, 44.48it/s][A
 46%|████▋     | 203/437 [00:05<00:05, 45.18it/s][A
 48%|████▊     | 208/437 [00:06<00:05, 45.67it/s][A
 49%|████▊     | 213/437 [00:06<00:04, 46.02it/s][A
 50%|████▉     | 218/437 [00:06<00:04, 46.27it/s][A
 51%|█████     | 223/437 [00:06<00:04, 46.40it/s][A
 52%|█████▏    | 228/437 [00:06<00:04, 46.57it/s][A
 53%|█████▎    | 233/437 [00:06<00:04, 46.69it/s][A
 54%|█████▍    | 238/437 [00:06<00:04, 46.67it/s][A
 56%|█████▌    | 243/437 [00:06<00:04, 46.70it/s][A
 57%|█████▋    | 248/437 [00:06<00:04, 46.77it/s][A
 58%|█████▊    | 253/437 [00:07<00:03, 46.79it/s][A
 59%|█████▉    | 258/437 [00:07<00:03, 46.83it/s][A
 60%|██████    | 263/437 [00:07<00:03, 46.77it/s][A
 61%|██████▏   | 268/437 [00:07<00:03, 46.81it/s][A
 62%|██████▏   | 273/437 [00:07<00:03, 46.81it/s][A
 64%|██████▎   | 278/437 [00:07<00:03, 46.85it/s][A
 65%|██████▍   | 283/437 [00:07<00:03, 46.75it/s][A
 66%|██████▌   | 288/437 [00:07<00:03, 46.74it/s][A
 67%|██████▋   | 293/437 [00:07<00:03, 46.62it/s][A
 68%|██████▊   | 298/437 [00:07<00:02, 46.58it/s][A
 69%|██████▉   | 303/437 [00:08<00:02, 46.56it/s][A
 70%|███████   | 308/437 [00:08<00:02, 46.61it/s][A
 72%|███████▏  | 313/437 [00:08<00:02, 41.57it/s][A
 73%|███████▎  | 318/437 [00:08<00:02, 42.97it/s][A
 74%|███████▍  | 323/437 [00:08<00:02, 44.04it/s][A
 75%|███████▌  | 328/437 [00:08<00:02, 44.83it/s][A
 76%|███████▌  | 333/437 [00:08<00:02, 45.45it/s][A
 77%|███████▋  | 338/437 [00:08<00:02, 45.88it/s][A
 78%|███████▊  | 343/437 [00:09<00:02, 46.17it/s][A
 80%|███████▉  | 348/437 [00:09<00:01, 46.39it/s][A
 81%|████████  | 353/437 [00:09<00:01, 46.52it/s][A
 82%|████████▏ | 358/437 [00:09<00:01, 46.53it/s][A
 83%|████████▎ | 363/437 [00:09<00:01, 46.65it/s][A
 84%|████████▍ | 368/437 [00:09<00:01, 46.66it/s][A
 85%|████████▌ | 373/437 [00:09<00:01, 46.74it/s][A
 86%|████████▋ | 378/437 [00:09<00:01, 46.82it/s][A
 88%|████████▊ | 383/437 [00:09<00:01, 46.77it/s][A
 89%|████████▉ | 388/437 [00:09<00:01, 46.80it/s][A
 90%|████████▉ | 393/437 [00:10<00:00, 46.84it/s][A
 91%|█████████ | 398/437 [00:10<00:00, 46.87it/s][A
 92%|█████████▏| 403/437 [00:10<00:00, 46.78it/s][A
 93%|█████████▎| 408/437 [00:10<00:00, 46.84it/s][A
 95%|█████████▍| 413/437 [00:10<00:00, 46.88it/s][A
 96%|█████████▌| 418/437 [00:10<00:00, 46.80it/s][A
 97%|█████████▋| 423/437 [00:10<00:00, 46.84it/s][A
 98%|█████████▊| 428/437 [00:10<00:00, 46.70it/s][A
 99%|█████████▉| 433/437 [00:10<00:00, 46.87it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:11<00:00, 46.87it/s][A 60%|██████    | 351/585 [02:47<01:08,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:37:09,482 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 18:37:10,457 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:37:16,256 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:37:16,565 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:37:17,236 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:06<36:46,  9.47s/it] 60%|██████    | 353/585 [03:07<26:00,  6.72s/it] 61%|██████    | 354/585 [03:07<18:27,  4.79s/it] 61%|██████    | 355/585 [03:07<13:11,  3.44s/it] 61%|██████    | 356/585 [03:08<09:31,  2.50s/it] 61%|██████    | 357/585 [03:08<06:57,  1.83s/it] 61%|██████    | 358/585 [03:08<05:10,  1.37s/it] 61%|██████▏   | 359/585 [03:08<03:56,  1.04s/it] 62%|██████▏   | 360/585 [03:09<03:03,  1.22it/s] 62%|██████▏   | 361/585 [03:09<02:27,  1.52it/s] 62%|██████▏   | 362/585 [03:09<02:01,  1.83it/s] 62%|██████▏   | 363/585 [03:10<01:44,  2.13it/s] 62%|██████▏   | 364/585 [03:10<01:32,  2.40it/s] 62%|██████▏   | 365/585 [03:10<01:23,  2.64it/s] 63%|██████▎   | 366/585 [03:10<01:16,  2.85it/s] 63%|██████▎   | 367/585 [03:11<01:12,  3.01it/s] 63%|██████▎   | 368/585 [03:11<01:09,  3.14it/s] 63%|██████▎   | 369/585 [03:11<01:06,  3.24it/s] 63%|██████▎   | 370/585 [03:12<01:05,  3.31it/s] 63%|██████▎   | 371/585 [03:12<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:12<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:12<01:02,  3.42it/s] 64%|██████▍   | 374/585 [03:13<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:13<01:01,  3.44it/s] 64%|██████▍   | 376/585 [03:13<01:00,  3.45it/s] 64%|██████▍   | 377/585 [03:14<01:00,  3.46it/s] 65%|██████▍   | 378/585 [03:14<00:59,  3.47it/s] 65%|██████▍   | 379/585 [03:14<00:59,  3.47it/s] 65%|██████▍   | 380/585 [03:14<00:59,  3.47it/s] 65%|██████▌   | 381/585 [03:15<00:58,  3.48it/s] 65%|██████▌   | 382/585 [03:15<00:58,  3.48it/s] 65%|██████▌   | 383/585 [03:15<00:58,  3.48it/s] 66%|██████▌   | 384/585 [03:16<00:57,  3.48it/s] 66%|██████▌   | 385/585 [03:16<00:57,  3.48it/s] 66%|██████▌   | 386/585 [03:16<00:57,  3.46it/s] 66%|██████▌   | 387/585 [03:16<00:57,  3.46it/s] 66%|██████▋   | 388/585 [03:17<00:56,  3.47it/s] 66%|██████▋   | 389/585 [03:17<00:56,  3.47it/s] 67%|██████▋   | 390/585 [03:17<00:56,  3.47it/s] 67%|██████▋   | 391/585 [03:18<00:55,  3.48it/s] 67%|██████▋   | 392/585 [03:18<00:55,  3.48it/s] 67%|██████▋   | 393/585 [03:18<00:55,  3.48it/s] 67%|██████▋   | 394/585 [03:18<00:54,  3.48it/s] 68%|██████▊   | 395/585 [03:19<00:54,  3.48it/s] 68%|██████▊   | 396/585 [03:19<00:54,  3.48it/s] 68%|██████▊   | 397/585 [03:19<00:54,  3.46it/s] 68%|██████▊   | 398/585 [03:20<00:53,  3.47it/s] 68%|██████▊   | 399/585 [03:20<00:53,  3.47it/s] 68%|██████▊   | 400/585 [03:20<00:53,  3.47it/s] 69%|██████▊   | 401/585 [03:21<00:52,  3.48it/s] 69%|██████▊   | 402/585 [03:21<00:52,  3.48it/s] 69%|██████▉   | 403/585 [03:21<00:52,  3.48it/s] 69%|██████▉   | 404/585 [03:21<00:52,  3.48it/s] 69%|██████▉   | 405/585 [03:22<00:51,  3.48it/s] 69%|██████▉   | 406/585 [03:22<00:51,  3.48it/s] 70%|██████▉   | 407/585 [03:22<00:51,  3.48it/s] 70%|██████▉   | 408/585 [03:23<00:51,  3.46it/s] 70%|██████▉   | 409/585 [03:23<00:50,  3.46it/s] 70%|███████   | 410/585 [03:23<00:50,  3.47it/s] 70%|███████   | 411/585 [03:23<00:50,  3.47it/s] 70%|███████   | 412/585 [03:24<00:49,  3.47it/s] 71%|███████   | 413/585 [03:24<00:49,  3.48it/s] 71%|███████   | 414/585 [03:24<00:49,  3.48it/s] 71%|███████   | 415/585 [03:25<00:48,  3.48it/s] 71%|███████   | 416/585 [03:25<00:48,  3.48it/s] 71%|███████▏  | 417/585 [03:25<00:48,  3.48it/s] 71%|███████▏  | 418/585 [03:25<00:48,  3.48it/s] 72%|███████▏  | 419/585 [03:26<00:48,  3.44it/s] 72%|███████▏  | 420/585 [03:26<00:47,  3.45it/s] 72%|███████▏  | 421/585 [03:26<00:47,  3.46it/s] 72%|███████▏  | 422/585 [03:27<00:47,  3.47it/s] 72%|███████▏  | 423/585 [03:27<00:46,  3.47it/s] 72%|███████▏  | 424/585 [03:27<00:46,  3.47it/s] 73%|███████▎  | 425/585 [03:27<00:46,  3.48it/s] 73%|███████▎  | 426/585 [03:28<00:45,  3.48it/s] 73%|███████▎  | 427/585 [03:28<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:28<00:45,  3.48it/s] 73%|███████▎  | 429/585 [03:29<00:44,  3.48it/s] 74%|███████▎  | 430/585 [03:29<00:51,  3.03it/s] 74%|███████▎  | 431/585 [03:29<00:48,  3.15it/s] 74%|███████▍  | 432/585 [03:30<00:47,  3.24it/s] 74%|███████▍  | 433/585 [03:30<00:45,  3.31it/s] 74%|███████▍  | 434/585 [03:30<00:44,  3.36it/s] 74%|███████▍  | 435/585 [03:30<00:44,  3.39it/s] 75%|███████▍  | 436/585 [03:31<00:43,  3.42it/s] 75%|███████▍  | 437/585 [03:31<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:31<00:42,  3.45it/s] 75%|███████▌  | 439/585 [03:32<00:42,  3.46it/s] 75%|███████▌  | 440/585 [03:32<00:42,  3.41it/s] 75%|███████▌  | 441/585 [03:32<00:41,  3.43it/s] 76%|███████▌  | 442/585 [03:32<00:41,  3.44it/s] 76%|███████▌  | 443/585 [03:33<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:33<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:33<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:34<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:34<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:34<00:39,  3.47it/s] 77%|███████▋  | 449/585 [03:34<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:35<00:38,  3.47it/s] 77%|███████▋  | 451/585 [03:35<00:38,  3.47it/s] 77%|███████▋  | 452/585 [03:35<00:38,  3.47it/s] 77%|███████▋  | 453/585 [03:36<00:38,  3.47it/s] 78%|███████▊  | 454/585 [03:36<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:36<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:37<00:57,  2.23it/s] 78%|███████▊  | 457/585 [03:37<00:51,  2.49it/s] 78%|███████▊  | 458/585 [03:38<00:46,  2.72it/s] 78%|███████▊  | 459/585 [03:38<00:56,  2.22it/s] 79%|███████▊  | 460/585 [03:39<00:50,  2.48it/s] 79%|███████▉  | 461/585 [03:39<00:45,  2.72it/s] 79%|███████▉  | 462/585 [03:40<01:11,  1.72it/s] 79%|███████▉  | 463/585 [03:40<01:07,  1.82it/s] 79%|███████▉  | 464/585 [03:41<00:57,  2.12it/s] 79%|███████▉  | 465/585 [03:41<00:49,  2.40it/s] 80%|███████▉  | 466/585 [03:41<00:44,  2.65it/s] 80%|███████▉  | 467/585 [03:42<00:41,  2.85it/s] 80%|████████  | 468/585 [03:42<00:38,  3.01it/s][INFO|trainer.py:2140] 2023-08-28 18:38:03,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:38:03,889 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:38:03,889 >>   Batch size = 8
{'eval_loss': 1.0278432369232178, 'eval_runtime': 11.04, 'eval_samples_per_second': 316.394, 'eval_steps_per_second': 39.583, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.56it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.56it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.82it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.12it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.37it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.23it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.09it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.98it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.92it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.78it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.86it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.78it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.81it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.79it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.77it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.77it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.79it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.76it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.81it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.81it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.71it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.73it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.73it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.70it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.71it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.74it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.70it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.74it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.73it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.65it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.72it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.66it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.66it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.62it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.68it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.71it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 45.95it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.20it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.34it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.47it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.53it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.63it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.67it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.72it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.72it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.67it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.76it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.75it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.76it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.71it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.74it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.75it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.73it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.77it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.74it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.72it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.78it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.76it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.69it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.71it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.66it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.59it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.57it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.53it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.31it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 45.67it/s][A
 81%|████████  | 353/437 [00:07<00:01, 45.99it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.16it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.39it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.45it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.49it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.55it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.57it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.56it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.65it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.66it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.58it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.67it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.67it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.69it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.67it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.68it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.68it/s][A 80%|████████  | 468/585 [03:51<00:38,  3.01it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:38:13,998 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 18:38:14,431 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:38:19,163 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:38:19,180 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:38:19,190 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:09<16:14,  8.40s/it] 80%|████████  | 470/585 [04:09<11:26,  5.97s/it] 81%|████████  | 471/585 [04:10<08:06,  4.27s/it] 81%|████████  | 472/585 [04:10<05:47,  3.07s/it] 81%|████████  | 473/585 [04:10<04:10,  2.24s/it] 81%|████████  | 474/585 [04:11<03:03,  1.65s/it] 81%|████████  | 475/585 [04:11<02:16,  1.24s/it] 81%|████████▏ | 476/585 [04:11<01:44,  1.05it/s] 82%|████████▏ | 477/585 [04:11<01:21,  1.33it/s] 82%|████████▏ | 478/585 [04:12<01:05,  1.63it/s] 82%|████████▏ | 479/585 [04:12<00:54,  1.94it/s] 82%|████████▏ | 480/585 [04:12<00:46,  2.24it/s] 82%|████████▏ | 481/585 [04:13<00:42,  2.44it/s] 82%|████████▏ | 482/585 [04:13<00:38,  2.68it/s] 83%|████████▎ | 483/585 [04:13<00:35,  2.88it/s] 83%|████████▎ | 484/585 [04:13<00:33,  3.03it/s] 83%|████████▎ | 485/585 [04:14<00:31,  3.16it/s] 83%|████████▎ | 486/585 [04:14<00:30,  3.25it/s] 83%|████████▎ | 487/585 [04:14<00:29,  3.31it/s] 83%|████████▎ | 488/585 [04:15<00:28,  3.37it/s] 84%|████████▎ | 489/585 [04:15<00:28,  3.40it/s] 84%|████████▍ | 490/585 [04:15<00:27,  3.43it/s] 84%|████████▍ | 491/585 [04:15<00:27,  3.44it/s] 84%|████████▍ | 492/585 [04:16<00:30,  3.07it/s] 84%|████████▍ | 493/585 [04:16<00:28,  3.18it/s] 84%|████████▍ | 494/585 [04:16<00:27,  3.27it/s] 85%|████████▍ | 495/585 [04:17<00:27,  3.33it/s] 85%|████████▍ | 496/585 [04:17<00:26,  3.37it/s] 85%|████████▍ | 497/585 [04:17<00:25,  3.41it/s] 85%|████████▌ | 498/585 [04:18<00:25,  3.43it/s] 85%|████████▌ | 499/585 [04:18<00:24,  3.45it/s] 85%|████████▌ | 500/585 [04:18<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:18<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:18<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:19<00:23,  3.47it/s] 86%|████████▌ | 503/585 [04:19<00:23,  3.46it/s] 86%|████████▌ | 504/585 [04:19<00:23,  3.47it/s] 86%|████████▋ | 505/585 [04:20<00:23,  3.47it/s] 86%|████████▋ | 506/585 [04:20<00:22,  3.48it/s] 87%|████████▋ | 507/585 [04:20<00:22,  3.48it/s] 87%|████████▋ | 508/585 [04:20<00:22,  3.48it/s] 87%|████████▋ | 509/585 [04:21<00:21,  3.48it/s] 87%|████████▋ | 510/585 [04:21<00:21,  3.48it/s] 87%|████████▋ | 511/585 [04:21<00:21,  3.48it/s] 88%|████████▊ | 512/585 [04:22<00:20,  3.48it/s] 88%|████████▊ | 513/585 [04:22<00:20,  3.49it/s] 88%|████████▊ | 514/585 [04:22<00:20,  3.43it/s] 88%|████████▊ | 515/585 [04:22<00:20,  3.45it/s] 88%|████████▊ | 516/585 [04:23<00:19,  3.46it/s] 88%|████████▊ | 517/585 [04:23<00:19,  3.47it/s] 89%|████████▊ | 518/585 [04:23<00:19,  3.47it/s] 89%|████████▊ | 519/585 [04:24<00:18,  3.48it/s] 89%|████████▉ | 520/585 [04:24<00:18,  3.48it/s] 89%|████████▉ | 521/585 [04:24<00:18,  3.48it/s] 89%|████████▉ | 522/585 [04:24<00:18,  3.48it/s] 89%|████████▉ | 523/585 [04:25<00:17,  3.48it/s] 90%|████████▉ | 524/585 [04:25<00:17,  3.48it/s] 90%|████████▉ | 525/585 [04:25<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:26<00:17,  3.46it/s] 90%|█████████ | 527/585 [04:26<00:16,  3.47it/s] 90%|█████████ | 528/585 [04:26<00:16,  3.47it/s] 90%|█████████ | 529/585 [04:26<00:16,  3.47it/s] 91%|█████████ | 530/585 [04:27<00:15,  3.47it/s] 91%|█████████ | 531/585 [04:27<00:15,  3.47it/s] 91%|█████████ | 532/585 [04:27<00:15,  3.48it/s] 91%|█████████ | 533/585 [04:28<00:14,  3.48it/s] 91%|█████████▏| 534/585 [04:28<00:14,  3.48it/s] 91%|█████████▏| 535/585 [04:28<00:14,  3.48it/s] 92%|█████████▏| 536/585 [04:29<00:14,  3.29it/s] 92%|█████████▏| 537/585 [04:29<00:14,  3.34it/s] 92%|█████████▏| 538/585 [04:29<00:13,  3.38it/s] 92%|█████████▏| 539/585 [04:29<00:13,  3.41it/s] 92%|█████████▏| 540/585 [04:30<00:13,  3.43it/s] 92%|█████████▏| 541/585 [04:30<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:30<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:31<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:31<00:11,  3.47it/s] 93%|█████████▎| 545/585 [04:31<00:11,  3.47it/s] 93%|█████████▎| 546/585 [04:31<00:11,  3.47it/s] 94%|█████████▎| 547/585 [04:32<00:11,  3.41it/s] 94%|█████████▎| 548/585 [04:32<00:10,  3.43it/s] 94%|█████████▍| 549/585 [04:32<00:10,  3.44it/s] 94%|█████████▍| 550/585 [04:33<00:10,  3.45it/s] 94%|█████████▍| 551/585 [04:33<00:09,  3.46it/s] 94%|█████████▍| 552/585 [04:33<00:09,  3.47it/s] 95%|█████████▍| 553/585 [04:33<00:09,  3.47it/s] 95%|█████████▍| 554/585 [04:34<00:08,  3.47it/s] 95%|█████████▍| 555/585 [04:34<00:08,  3.47it/s] 95%|█████████▌| 556/585 [04:34<00:08,  3.48it/s] 95%|█████████▌| 557/585 [04:35<00:08,  3.48it/s] 95%|█████████▌| 558/585 [04:35<00:09,  2.87it/s] 96%|█████████▌| 559/585 [04:35<00:08,  3.03it/s] 96%|█████████▌| 560/585 [04:36<00:07,  3.15it/s] 96%|█████████▌| 561/585 [04:36<00:07,  3.24it/s] 96%|█████████▌| 562/585 [04:36<00:06,  3.31it/s] 96%|█████████▌| 563/585 [04:37<00:06,  3.36it/s] 96%|█████████▋| 564/585 [04:37<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:37<00:07,  2.71it/s] 97%|█████████▋| 566/585 [04:38<00:06,  2.89it/s] 97%|█████████▋| 567/585 [04:39<00:10,  1.67it/s] 97%|█████████▋| 568/585 [04:39<00:09,  1.87it/s] 97%|█████████▋| 569/585 [04:40<00:07,  2.17it/s] 97%|█████████▋| 570/585 [04:40<00:06,  2.45it/s] 98%|█████████▊| 571/585 [04:40<00:05,  2.65it/s] 98%|█████████▊| 572/585 [04:40<00:04,  2.85it/s] 98%|█████████▊| 573/585 [04:41<00:03,  3.01it/s] 98%|█████████▊| 574/585 [04:41<00:03,  3.13it/s] 98%|█████████▊| 575/585 [04:41<00:03,  3.23it/s] 98%|█████████▊| 576/585 [04:42<00:02,  3.30it/s] 99%|█████████▊| 577/585 [04:42<00:02,  3.35it/s] 99%|█████████▉| 578/585 [04:42<00:02,  3.39it/s] 99%|█████████▉| 579/585 [04:42<00:01,  3.41it/s] 99%|█████████▉| 580/585 [04:43<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:43<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:43<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:44<00:00,  3.46it/s]100%|█████████▉| 584/585 [04:44<00:00,  3.47it/s]100%|██████████| 585/585 [04:44<00:00,  3.19it/s][INFO|trainer.py:2140] 2023-08-28 18:39:06,230 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:39:06,231 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:39:06,231 >>   Batch size = 8
{'eval_loss': 1.0377821922302246, 'eval_runtime': 9.3763, 'eval_samples_per_second': 372.537, 'eval_steps_per_second': 46.607, 'epoch': 4.0}
{'loss': 0.6389, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.60it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.97it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.15it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.25it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.91it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.67it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.41it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.10it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.02it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.93it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.02it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.04it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.02it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.01it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.08it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.03it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.94it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.84it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.76it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.72it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.91it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.87it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.86it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.85it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.82it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.53it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.61it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.71it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.74it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.80it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.90it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.97it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.97it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.88it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.91it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.91it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.91it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.90it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.90it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.89it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.88it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.87it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.98it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.95it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.88it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.88it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.91it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.93it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.88it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.91it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.99it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.91it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.97it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 46.95it/s][A
 64%|██████▎   | 278/437 [00:06<00:06, 24.56it/s][A
 65%|██████▍   | 283/437 [00:06<00:05, 28.68it/s][A
 66%|██████▌   | 288/437 [00:06<00:04, 32.42it/s][A
 67%|██████▋   | 293/437 [00:06<00:04, 35.77it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 38.57it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 40.77it/s][A
 70%|███████   | 308/437 [00:06<00:03, 42.46it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 43.74it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 44.50it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 45.12it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.71it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.12it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.22it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.44it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.60it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.71it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.78it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 46.74it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 46.73it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.74it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.77it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.72it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.85it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.85it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.85it/s][A
 92%|█████████▏| 403/437 [00:09<00:00, 46.91it/s][A
 93%|█████████▎| 408/437 [00:09<00:01, 21.51it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 25.64it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 29.67it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 33.31it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 36.46it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 39.03it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 39.03it/s][A100%|██████████| 585/585 [04:54<00:00,  3.19it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:39:16,451 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 18:39:16,565 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:39:27,419 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:39:27,453 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:39:27,466 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:39:47,415 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:39:47,417 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117 (score: 1.0058021545410156).
                                                 100%|██████████| 585/585 [05:32<00:00,  3.19it/s]100%|██████████| 585/585 [05:32<00:00,  1.76it/s]
[INFO|trainer.py:1894] 2023-08-28 18:39:53,797 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 18:39:53,816 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:39:57,997 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:39:58,012 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:39:58,023 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:39:58,269 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,269 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,270 >>   train_loss               =     0.6347
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,270 >>   train_runtime            = 0:05:32.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,270 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,270 >>   train_samples_per_second =     112.86
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:39:58,270 >>   train_steps_per_second   =      1.761
{'eval_loss': 1.0458942651748657, 'eval_runtime': 10.0593, 'eval_samples_per_second': 347.24, 'eval_steps_per_second': 43.442, 'epoch': 5.0}
{'train_runtime': 332.2708, 'train_samples_per_second': 112.86, 'train_steps_per_second': 1.761, 'train_loss': 0.6347160078521468, 'epoch': 5.0}
08/28/2023 18:39:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:39:58,327 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:39:58,327 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:39:58,327 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.71it/s]  3%|▎         | 12/437 [00:00<00:08, 51.54it/s]  4%|▍         | 18/437 [00:00<00:08, 49.57it/s]  5%|▌         | 23/437 [00:00<00:08, 48.88it/s]  6%|▋         | 28/437 [00:00<00:08, 48.29it/s]  8%|▊         | 33/437 [00:00<00:08, 48.14it/s]  9%|▊         | 38/437 [00:00<00:08, 47.93it/s] 10%|▉         | 43/437 [00:00<00:08, 47.81it/s] 11%|█         | 48/437 [00:00<00:08, 47.67it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.67it/s] 13%|█▎        | 58/437 [00:01<00:07, 47.51it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.48it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.46it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.41it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.38it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.37it/s] 20%|██        | 88/437 [00:01<00:07, 47.39it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.45it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.43it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.41it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.43it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.37it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.26it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.34it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.33it/s] 30%|███       | 133/437 [00:02<00:06, 44.32it/s] 32%|███▏      | 138/437 [00:02<00:06, 45.40it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.03it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.33it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.01it/s] 36%|███▌      | 158/437 [00:03<00:06, 43.20it/s] 38%|███▊      | 164/437 [00:03<00:06, 45.21it/s] 39%|███▊      | 169/437 [00:03<00:05, 45.87it/s] 40%|███▉      | 174/437 [00:03<00:05, 46.33it/s] 41%|████      | 179/437 [00:03<00:05, 46.59it/s] 42%|████▏     | 184/437 [00:03<00:05, 46.86it/s] 43%|████▎     | 189/437 [00:04<00:05, 47.07it/s] 44%|████▍     | 194/437 [00:04<00:05, 47.23it/s] 46%|████▌     | 199/437 [00:04<00:05, 47.21it/s] 47%|████▋     | 204/437 [00:04<00:04, 47.11it/s] 48%|████▊     | 209/437 [00:04<00:04, 47.07it/s] 49%|████▉     | 214/437 [00:04<00:04, 47.07it/s] 50%|█████     | 219/437 [00:04<00:04, 47.20it/s] 51%|█████▏    | 224/437 [00:04<00:04, 47.32it/s] 52%|█████▏    | 229/437 [00:04<00:04, 47.34it/s] 54%|█████▎    | 234/437 [00:04<00:04, 47.38it/s] 55%|█████▍    | 239/437 [00:05<00:04, 47.34it/s] 56%|█████▌    | 244/437 [00:05<00:04, 47.24it/s] 57%|█████▋    | 249/437 [00:05<00:03, 47.28it/s] 58%|█████▊    | 254/437 [00:05<00:03, 47.19it/s] 59%|█████▉    | 259/437 [00:05<00:03, 47.21it/s] 60%|██████    | 264/437 [00:05<00:03, 47.21it/s] 62%|██████▏   | 269/437 [00:05<00:03, 47.22it/s] 63%|██████▎   | 274/437 [00:05<00:03, 47.25it/s] 64%|██████▍   | 279/437 [00:05<00:03, 47.31it/s] 65%|██████▍   | 284/437 [00:06<00:03, 47.32it/s] 66%|██████▌   | 289/437 [00:06<00:03, 47.34it/s] 67%|██████▋   | 294/437 [00:06<00:03, 47.38it/s] 68%|██████▊   | 299/437 [00:06<00:02, 47.24it/s] 70%|██████▉   | 304/437 [00:06<00:02, 47.19it/s] 71%|███████   | 309/437 [00:06<00:02, 47.17it/s] 72%|███████▏  | 314/437 [00:06<00:02, 47.23it/s] 73%|███████▎  | 319/437 [00:06<00:02, 47.28it/s] 74%|███████▍  | 324/437 [00:06<00:02, 47.33it/s] 75%|███████▌  | 329/437 [00:06<00:02, 47.33it/s] 76%|███████▋  | 334/437 [00:07<00:02, 47.29it/s] 78%|███████▊  | 339/437 [00:07<00:02, 47.24it/s] 79%|███████▊  | 344/437 [00:07<00:01, 47.30it/s] 80%|███████▉  | 349/437 [00:07<00:01, 47.26it/s] 81%|████████  | 354/437 [00:07<00:01, 47.13it/s] 82%|████████▏ | 359/437 [00:07<00:01, 47.27it/s] 83%|████████▎ | 364/437 [00:07<00:01, 47.20it/s] 84%|████████▍ | 369/437 [00:07<00:01, 47.09it/s] 86%|████████▌ | 374/437 [00:07<00:01, 47.25it/s] 87%|████████▋ | 379/437 [00:08<00:01, 47.36it/s] 88%|████████▊ | 384/437 [00:08<00:01, 47.26it/s] 89%|████████▉ | 389/437 [00:08<00:01, 47.15it/s] 90%|█████████ | 394/437 [00:08<00:00, 47.23it/s] 91%|█████████▏| 399/437 [00:08<00:00, 47.19it/s] 92%|█████████▏| 404/437 [00:08<00:00, 47.22it/s] 94%|█████████▎| 409/437 [00:08<00:00, 47.24it/s] 95%|█████████▍| 414/437 [00:08<00:00, 33.76it/s] 96%|█████████▌| 419/437 [00:09<00:00, 36.94it/s] 97%|█████████▋| 424/437 [00:09<00:00, 39.53it/s] 98%|█████████▊| 429/437 [00:09<00:00, 41.56it/s] 99%|█████████▉| 434/437 [00:09<00:00, 43.15it/s]100%|██████████| 437/437 [00:09<00:00, 46.48it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:40:07,754 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   eval_loss               =     1.0058
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   eval_runtime            = 0:00:09.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   eval_samples_per_second =    370.525
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   eval_steps_per_second   =     46.355
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:40:07,754 >>   perplexity              =     2.7341
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:15,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:15,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:15,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:15,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:15,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:40:16,069 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:40:16,070 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:40:16,771 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:40:17,779 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:40:17,779 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:20,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:20,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:20,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:20,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:40:20,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:40:21,500 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:40:21,502 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:40:22,106 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:40:22,258 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:40:22,258 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:12,  1.53it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:18,  1.53it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:20,  1.58it/s]Extractor Predicting: 31it [00:20,  1.58it/s]Extractor Predicting: 32it [00:21,  1.57it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.58it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:29,  1.59it/s]Extractor Predicting: 45it [00:29,  1.46it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:31,  1.47it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:32,  1.56it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:36,  1.58it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:44,  1.46it/s]Extractor Predicting: 69it [00:45,  1.47it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:48,  1.53it/s]Extractor Predicting: 74it [00:48,  1.45it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:50,  1.48it/s]Extractor Predicting: 78it [00:51,  1.47it/s]Extractor Predicting: 79it [00:52,  1.41it/s]Extractor Predicting: 80it [00:53,  1.43it/s]Extractor Predicting: 81it [00:53,  1.44it/s]Extractor Predicting: 82it [00:54,  1.45it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:57,  1.45it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [00:59,  1.46it/s]Extractor Predicting: 91it [01:00,  1.46it/s]Extractor Predicting: 92it [01:01,  1.48it/s]Extractor Predicting: 93it [01:01,  1.48it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:03,  1.54it/s]Extractor Predicting: 96it [01:03,  1.55it/s]Extractor Predicting: 97it [01:04,  1.52it/s]Extractor Predicting: 98it [01:05,  1.52it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:07,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.50it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.46it/s]Extractor Predicting: 111it [01:13,  1.47it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:15,  1.43it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:18,  1.43it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.43it/s]Extractor Predicting: 120it [01:20,  1.34it/s]Extractor Predicting: 121it [01:21,  1.35it/s]Extractor Predicting: 122it [01:21,  1.37it/s]Extractor Predicting: 123it [01:22,  1.43it/s]Extractor Predicting: 124it [01:23,  1.44it/s]Extractor Predicting: 125it [01:23,  1.42it/s]Extractor Predicting: 126it [01:24,  1.44it/s]Extractor Predicting: 127it [01:25,  1.45it/s]Extractor Predicting: 128it [01:25,  1.44it/s]Extractor Predicting: 129it [01:26,  1.43it/s]Extractor Predicting: 130it [01:27,  1.43it/s]Extractor Predicting: 131it [01:27,  1.41it/s]Extractor Predicting: 132it [01:28,  1.40it/s]Extractor Predicting: 133it [01:29,  1.39it/s]Extractor Predicting: 134it [01:30,  1.39it/s]Extractor Predicting: 135it [01:30,  1.41it/s]Extractor Predicting: 136it [01:31,  1.72it/s]Extractor Predicting: 136it [01:31,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:02,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:02,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:02,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:02,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:02,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:42:03,251 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:42:03,252 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:42:03,540 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:42:04,558 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:42:04,562 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:06,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:06,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:06,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:06,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:42:06,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:42:06,785 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:42:06,786 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:42:07,532 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:42:07,670 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:42:07,681 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6356673960612691,
  "recall": 0.16633266533066132,
  "score": 0.2636714318130247,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:21,  1.66it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.62it/s]Extractor Predicting: 42it [00:26,  1.62it/s]Extractor Predicting: 43it [00:27,  1.65it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.62it/s]Extractor Predicting: 47it [00:29,  1.64it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.63it/s]Extractor Predicting: 50it [00:31,  1.64it/s]Extractor Predicting: 51it [00:32,  1.67it/s]Extractor Predicting: 52it [00:32,  1.67it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:34,  1.61it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:35,  1.59it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:41,  1.59it/s]Extractor Predicting: 67it [00:42,  1.60it/s]Extractor Predicting: 68it [00:42,  1.65it/s]Extractor Predicting: 69it [00:43,  1.67it/s]Extractor Predicting: 70it [00:44,  1.65it/s]Extractor Predicting: 71it [00:44,  1.65it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:45,  1.63it/s]Extractor Predicting: 74it [00:46,  1.59it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:47,  1.60it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:49,  1.59it/s]Extractor Predicting: 80it [00:50,  1.61it/s]Extractor Predicting: 81it [00:50,  1.59it/s]Extractor Predicting: 82it [00:51,  1.61it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:52,  1.57it/s]Extractor Predicting: 85it [00:53,  1.55it/s]Extractor Predicting: 86it [00:54,  1.53it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.53it/s]Extractor Predicting: 89it [00:56,  1.52it/s]Extractor Predicting: 90it [00:56,  1.50it/s]Extractor Predicting: 91it [00:57,  1.51it/s]Extractor Predicting: 92it [00:58,  1.49it/s]Extractor Predicting: 93it [00:58,  1.47it/s]Extractor Predicting: 94it [00:59,  1.48it/s]Extractor Predicting: 95it [01:00,  1.51it/s]Extractor Predicting: 96it [01:00,  1.51it/s]Extractor Predicting: 97it [01:01,  1.48it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:02,  1.46it/s]Extractor Predicting: 100it [01:03,  1.45it/s]Extractor Predicting: 101it [01:04,  1.45it/s]Extractor Predicting: 102it [01:04,  1.45it/s]Extractor Predicting: 103it [01:05,  1.48it/s]Extractor Predicting: 104it [01:06,  1.48it/s]Extractor Predicting: 105it [01:06,  1.49it/s]Extractor Predicting: 106it [01:07,  1.46it/s]Extractor Predicting: 107it [01:08,  1.47it/s]Extractor Predicting: 108it [01:09,  1.45it/s]Extractor Predicting: 109it [01:09,  1.43it/s]Extractor Predicting: 110it [01:10,  1.44it/s]Extractor Predicting: 111it [01:11,  1.46it/s]Extractor Predicting: 112it [01:11,  1.45it/s]Extractor Predicting: 113it [01:12,  1.45it/s]Extractor Predicting: 114it [01:13,  1.45it/s]Extractor Predicting: 115it [01:13,  1.44it/s]Extractor Predicting: 116it [01:14,  1.45it/s]Extractor Predicting: 117it [01:15,  1.46it/s]Extractor Predicting: 118it [01:15,  1.47it/s]Extractor Predicting: 119it [01:16,  1.35it/s]Extractor Predicting: 120it [01:17,  1.43it/s]Extractor Predicting: 121it [01:18,  1.44it/s]Extractor Predicting: 122it [01:18,  1.45it/s]Extractor Predicting: 123it [01:19,  1.46it/s]Extractor Predicting: 124it [01:20,  1.46it/s]Extractor Predicting: 125it [01:20,  1.49it/s]Extractor Predicting: 126it [01:21,  1.53it/s]Extractor Predicting: 127it [01:22,  1.53it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:23,  1.53it/s]Extractor Predicting: 130it [01:24,  1.50it/s]Extractor Predicting: 131it [01:24,  1.51it/s]Extractor Predicting: 132it [01:25,  1.52it/s]Extractor Predicting: 133it [01:25,  1.55it/s]Extractor Predicting: 134it [01:26,  1.57it/s]Extractor Predicting: 135it [01:27,  1.54it/s]Extractor Predicting: 136it [01:27,  1.57it/s]Extractor Predicting: 137it [01:28,  1.59it/s]Extractor Predicting: 138it [01:29,  1.56it/s]Extractor Predicting: 139it [01:29,  1.57it/s]Extractor Predicting: 140it [01:30,  1.56it/s]Extractor Predicting: 141it [01:31,  1.51it/s]Extractor Predicting: 142it [01:31,  1.50it/s]Extractor Predicting: 143it [01:32,  1.51it/s]Extractor Predicting: 144it [01:33,  1.49it/s]Extractor Predicting: 145it [01:33,  1.49it/s]Extractor Predicting: 146it [01:34,  1.53it/s]Extractor Predicting: 147it [01:35,  1.55it/s]Extractor Predicting: 148it [01:35,  1.53it/s]Extractor Predicting: 149it [01:36,  1.52it/s]Extractor Predicting: 150it [01:37,  1.52it/s]Extractor Predicting: 151it [01:37,  1.48it/s]Extractor Predicting: 152it [01:38,  1.49it/s]Extractor Predicting: 153it [01:39,  1.50it/s]Extractor Predicting: 154it [01:39,  1.54it/s]Extractor Predicting: 155it [01:40,  1.48it/s]Extractor Predicting: 156it [01:41,  1.49it/s]Extractor Predicting: 157it [01:41,  1.47it/s]Extractor Predicting: 158it [01:42,  1.49it/s]Extractor Predicting: 159it [01:43,  1.48it/s]Extractor Predicting: 160it [01:43,  1.47it/s]Extractor Predicting: 161it [01:44,  1.47it/s]Extractor Predicting: 162it [01:45,  1.47it/s]Extractor Predicting: 163it [01:45,  1.47it/s]Extractor Predicting: 164it [01:46,  1.49it/s]Extractor Predicting: 165it [01:47,  1.47it/s]Extractor Predicting: 166it [01:47,  1.48it/s]Extractor Predicting: 167it [01:48,  1.46it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:49,  1.46it/s]Extractor Predicting: 170it [01:50,  1.45it/s]Extractor Predicting: 171it [01:51,  1.46it/s]Extractor Predicting: 172it [01:51,  1.46it/s]Extractor Predicting: 173it [01:52,  1.46it/s]Extractor Predicting: 174it [01:53,  1.46it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:54,  1.49it/s]Extractor Predicting: 177it [01:55,  1.51it/s]Extractor Predicting: 178it [01:55,  1.52it/s]Extractor Predicting: 179it [01:56,  1.54it/s]Extractor Predicting: 180it [01:57,  1.51it/s]Extractor Predicting: 181it [01:57,  1.53it/s]Extractor Predicting: 182it [01:58,  1.55it/s]Extractor Predicting: 183it [01:59,  1.50it/s]Extractor Predicting: 184it [01:59,  1.56it/s]Extractor Predicting: 185it [02:00,  1.54it/s]Extractor Predicting: 186it [02:01,  1.55it/s]Extractor Predicting: 187it [02:01,  1.55it/s]Extractor Predicting: 188it [02:02,  1.51it/s]Extractor Predicting: 189it [02:03,  1.51it/s]Extractor Predicting: 190it [02:03,  1.51it/s]Extractor Predicting: 191it [02:04,  1.49it/s]Extractor Predicting: 192it [02:05,  1.50it/s]Extractor Predicting: 193it [02:05,  1.53it/s]Extractor Predicting: 194it [02:06,  1.53it/s]Extractor Predicting: 195it [02:07,  1.49it/s]Extractor Predicting: 196it [02:07,  1.46it/s]Extractor Predicting: 197it [02:08,  1.49it/s]Extractor Predicting: 198it [02:09,  1.48it/s]Extractor Predicting: 199it [02:09,  1.51it/s]Extractor Predicting: 200it [02:10,  1.37it/s]Extractor Predicting: 201it [02:11,  1.41it/s]Extractor Predicting: 202it [02:12,  1.44it/s]Extractor Predicting: 203it [02:12,  1.47it/s]Extractor Predicting: 204it [02:13,  1.51it/s]Extractor Predicting: 205it [02:13,  1.50it/s]Extractor Predicting: 206it [02:14,  1.54it/s]Extractor Predicting: 207it [02:15,  1.51it/s]Extractor Predicting: 208it [02:15,  1.50it/s]Extractor Predicting: 209it [02:16,  1.52it/s]Extractor Predicting: 210it [02:17,  1.53it/s]Extractor Predicting: 211it [02:17,  1.50it/s]Extractor Predicting: 212it [02:18,  1.52it/s]Extractor Predicting: 213it [02:19,  1.54it/s]Extractor Predicting: 214it [02:19,  1.53it/s]Extractor Predicting: 215it [02:20,  1.54it/s]Extractor Predicting: 216it [02:21,  1.54it/s]Extractor Predicting: 217it [02:21,  1.52it/s]Extractor Predicting: 218it [02:22,  1.53it/s]Extractor Predicting: 219it [02:23,  1.50it/s]Extractor Predicting: 220it [02:23,  1.53it/s]Extractor Predicting: 221it [02:24,  1.56it/s]Extractor Predicting: 222it [02:25,  1.55it/s]Extractor Predicting: 223it [02:25,  1.51it/s]Extractor Predicting: 224it [02:26,  1.56it/s]Extractor Predicting: 225it [02:27,  1.54it/s]Extractor Predicting: 226it [02:27,  1.53it/s]Extractor Predicting: 227it [02:28,  1.53it/s]Extractor Predicting: 228it [02:28,  1.55it/s]Extractor Predicting: 229it [02:29,  1.54it/s]Extractor Predicting: 230it [02:30,  1.56it/s]Extractor Predicting: 231it [02:30,  1.55it/s]Extractor Predicting: 232it [02:31,  1.54it/s]Extractor Predicting: 233it [02:32,  1.55it/s]Extractor Predicting: 234it [02:32,  1.54it/s]Extractor Predicting: 235it [02:33,  1.51it/s]Extractor Predicting: 236it [02:34,  1.51it/s]Extractor Predicting: 237it [02:34,  1.51it/s]Extractor Predicting: 238it [02:35,  1.51it/s]Extractor Predicting: 239it [02:36,  1.50it/s]Extractor Predicting: 240it [02:36,  1.50it/s]Extractor Predicting: 241it [02:37,  1.49it/s]Extractor Predicting: 242it [02:38,  1.50it/s]Extractor Predicting: 243it [02:38,  1.51it/s]Extractor Predicting: 244it [02:39,  1.49it/s]Extractor Predicting: 245it [02:40,  1.51it/s]Extractor Predicting: 246it [02:40,  1.51it/s]Extractor Predicting: 247it [02:41,  1.53it/s]Extractor Predicting: 248it [02:42,  1.51it/s]Extractor Predicting: 249it [02:42,  1.54it/s]Extractor Predicting: 250it [02:43,  1.53it/s]Extractor Predicting: 251it [02:44,  1.56it/s]Extractor Predicting: 252it [02:44,  1.54it/s]Extractor Predicting: 253it [02:45,  1.53it/s]Extractor Predicting: 254it [02:46,  1.53it/s]Extractor Predicting: 255it [02:46,  1.53it/s]Extractor Predicting: 256it [02:47,  1.49it/s]Extractor Predicting: 257it [02:48,  1.51it/s]Extractor Predicting: 258it [02:48,  1.50it/s]Extractor Predicting: 259it [02:49,  1.55it/s]Extractor Predicting: 260it [02:50,  1.54it/s]Extractor Predicting: 261it [02:50,  1.56it/s]Extractor Predicting: 262it [02:51,  1.54it/s]Extractor Predicting: 263it [02:51,  1.53it/s]Extractor Predicting: 264it [02:52,  1.59it/s]Extractor Predicting: 265it [02:53,  1.58it/s]Extractor Predicting: 266it [02:53,  1.59it/s]Extractor Predicting: 267it [02:54,  1.57it/s]Extractor Predicting: 268it [02:55,  1.52it/s]Extractor Predicting: 269it [02:55,  1.52it/s]Extractor Predicting: 270it [02:56,  1.50it/s]Extractor Predicting: 271it [02:57,  1.49it/s]Extractor Predicting: 272it [02:57,  1.50it/s]Extractor Predicting: 273it [02:58,  1.49it/s]Extractor Predicting: 274it [02:59,  1.48it/s]Extractor Predicting: 275it [02:59,  1.46it/s]Extractor Predicting: 276it [03:00,  1.48it/s]Extractor Predicting: 277it [03:01,  1.48it/s]Extractor Predicting: 278it [03:01,  1.45it/s]Extractor Predicting: 279it [03:02,  1.46it/s]Extractor Predicting: 280it [03:03,  1.47it/s]Extractor Predicting: 281it [03:03,  1.50it/s]Extractor Predicting: 282it [03:04,  1.50it/s]Extractor Predicting: 283it [03:05,  1.48it/s]Extractor Predicting: 284it [03:05,  1.50it/s]Extractor Predicting: 285it [03:06,  1.50it/s]Extractor Predicting: 286it [03:07,  1.54it/s]Extractor Predicting: 287it [03:07,  1.51it/s]Extractor Predicting: 288it [03:08,  1.70it/s]Extractor Predicting: 288it [03:08,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:24,655 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:24,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:24,665 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:24,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:24,666 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:45:24,969 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:45:24,970 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:45:25,233 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:26,246 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:26,246 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:28,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:28,256 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:28,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:28,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:45:28,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:45:29,002 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:45:29,003 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:45:29,266 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:45:29,406 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:45:29,406 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5658783783783784,
  "recall": 0.14565217391304347,
  "score": 0.23167358229598892,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:01,  2.30it/s]Extractor Predicting: 3it [00:01,  1.98it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:45:31,862 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:31,863 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:45:31,900 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:31,901 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:45:31,912 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:45:36,874 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:45:36,882 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:45:36,903 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:45:36,904 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:45:36,911 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,917 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,917 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,917 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,918 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:45:36,918 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 1.0,
  "recall": 0.02,
  "score": 0.0392156862745098,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:45:37,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:38,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:38,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:39,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:40,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:41,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:42,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:43,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:43,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:44,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:45,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:46,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:46,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:47,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:48,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:49,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:50,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:50,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:51,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:52,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:52,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:48, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 18:45:53,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:54,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:55,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:56,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:56,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:57,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:58,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:59,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:45:59,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:00,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:01,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:01,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:02,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:03,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:04,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:04,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:05,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:06,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:06,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:08,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:08,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:09,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:10,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:43, 17.19s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:11,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:12,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:13,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:13,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:14,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:15,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:15,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:16,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:17,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:18,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:18,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:19,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:20,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:21,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:21,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:22,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:23,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:23,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:24,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:25,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:25,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:26,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:27,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:24, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:28,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:29,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:29,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:30,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:31,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:31,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:32,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:33,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:33,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:34,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:35,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:35,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:36,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:37,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:37,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:38,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:39,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:39,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:41,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:41,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:42,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:43,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<03:02, 16.59s/it][WARNING|generation_utils.py:914] 2023-08-28 18:46:44,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:44,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:45,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:46,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:47,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:47,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:48,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:49,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:50,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:51,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:52,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:52,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:53,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:54,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:54,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:55,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:56,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:57,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:57,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:58,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:46:59,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:22<02:42, 16.30s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:00,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:00,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:01,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:02,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:03,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:04,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:05,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:05,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:06,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:07,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:07,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:08,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:09,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:10,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:10,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:11,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:12,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:13,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:13,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:14,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:15,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:38<02:25, 16.17s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:15,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:16,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:17,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:18,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:18,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:19,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:20,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:20,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:21,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:22,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:23,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:24,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:25,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:26,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:27,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:27,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:28,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:29,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:29,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:30,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:53<02:06, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:31,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:31,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:32,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:33,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:33,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:34,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:35,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:36,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:37,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:37,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:38,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:39,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:39,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:41,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:42,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:42,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:43,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:44,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:45,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:45,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:09<01:50, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-28 18:47:46,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:47,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:48,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:48,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:49,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:50,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:51,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:52,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:52,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:53,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:54,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:55,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:55,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:56,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:57,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:59,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:47:59,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:00,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:01,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:02,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:25<01:35, 15.96s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:03,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:03,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:04,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:05,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:06,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:06,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:07,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:08,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:08,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:09,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:10,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:11,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:11,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:12,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:13,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:14,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:14,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:15,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:16,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:16,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:40<01:17, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:17,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:18,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:19,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:20,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:20,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:21,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:22,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:22,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:23,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:24,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:25,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:25,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:26,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:27,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:27,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:28,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:29,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:30,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:31,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:31,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:32,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:33,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:33,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:57<01:03, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:34,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:35,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:35,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:36,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:37,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:38,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:38,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:39,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:40,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:41,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:41,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:42,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:43,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:43,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:44,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:45,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:46,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:46,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:47,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:48,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:49,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:12<00:47, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 18:48:49,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:50,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:51,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:52,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:52,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:53,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:54,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:54,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:55,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:56,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:56,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:57,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:58,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:59,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:48:59,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:00,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:01,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:01,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:02,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:03,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:26<00:30, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:49:04,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:04,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:05,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:06,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:06,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:07,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:08,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:08,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:09,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:10,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:11,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:11,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:12,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:13,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:14,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:14,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:15,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:16,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:16,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:17,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:18,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:19,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:42<00:15, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-28 18:49:19,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:20,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:21,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:22,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:22,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:23,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:24,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:24,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:25,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:26,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:26,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:27,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:28,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:29,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:30,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:30,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:31,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:32,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:33,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:34,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:49:34,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:58<00:00, 15.55s/it]Generating: 100%|██████████| 15/15 [03:58<00:00, 15.88s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:42,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:42,797 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:42,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:42,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:42,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:49:43,395 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:49:43,396 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:49:43,984 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:49:45,031 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:49:45,032 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:48,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:48,073 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:48,073 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:48,073 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:49:48,073 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:49:48,769 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:49:48,770 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:49:49,353 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:49:49,508 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:49:49,508 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9330357142857143, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : after a work by . Context : Later in the year , the film was shot with a cast of actors who included Will Rogers , John Goodman , Mike Rowe , and William Shatner . Head Entity : Will Rogers , Tail Entity : The Dark Knight .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9345238095238095, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9453125, 'errors': {''}}
['Relation : occupant . Context : Later in the year , the house was listed on a real estate listing in the Port Angeles County Courthouse , in downtown Los Angeles . Head Entity : Port Angeles County Courthouse , Tail Entity : Los Angeles County Department of Corrections .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8464673913043478, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 10353
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10453, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.29it/s]Extractor Estimating: 2it [00:01,  1.34it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.43it/s]Extractor Estimating: 5it [00:03,  1.43it/s]Extractor Estimating: 6it [00:04,  1.35it/s]Extractor Estimating: 7it [00:05,  1.37it/s]Extractor Estimating: 8it [00:05,  1.33it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:07,  1.44it/s]Extractor Estimating: 11it [00:07,  1.44it/s]Extractor Estimating: 12it [00:08,  1.42it/s]Extractor Estimating: 13it [00:09,  1.37it/s]Extractor Estimating: 14it [00:10,  1.38it/s]Extractor Estimating: 15it [00:10,  1.43it/s]Extractor Estimating: 16it [00:11,  1.49it/s]Extractor Estimating: 17it [00:12,  1.44it/s]Extractor Estimating: 18it [00:12,  1.35it/s]Extractor Estimating: 19it [00:13,  1.39it/s]Extractor Estimating: 20it [00:14,  1.42it/s]Extractor Estimating: 21it [00:14,  1.45it/s]Extractor Estimating: 22it [00:15,  1.48it/s]Extractor Estimating: 23it [00:16,  1.49it/s]Extractor Estimating: 24it [00:16,  1.53it/s]Extractor Estimating: 25it [00:17,  1.46it/s]Extractor Estimating: 26it [00:18,  1.49it/s]Extractor Estimating: 27it [00:18,  1.48it/s]Extractor Estimating: 28it [00:19,  1.49it/s]Extractor Estimating: 29it [00:20,  1.47it/s]Extractor Estimating: 30it [00:21,  1.44it/s]Extractor Estimating: 31it [00:21,  1.49it/s]Extractor Estimating: 32it [00:22,  1.47it/s]Extractor Estimating: 33it [00:22,  1.48it/s]Extractor Estimating: 34it [00:23,  1.50it/s]Extractor Estimating: 35it [00:24,  1.50it/s]Extractor Estimating: 36it [00:24,  1.50it/s]Extractor Estimating: 37it [00:25,  1.56it/s]Extractor Estimating: 38it [00:26,  1.53it/s]Extractor Estimating: 39it [00:26,  1.52it/s]Extractor Estimating: 40it [00:27,  1.47it/s]Extractor Estimating: 41it [00:28,  1.48it/s]Extractor Estimating: 42it [00:28,  1.50it/s]Extractor Estimating: 43it [00:29,  1.47it/s]Extractor Estimating: 44it [00:30,  1.52it/s]Extractor Estimating: 45it [00:30,  1.52it/s]Extractor Estimating: 46it [00:31,  1.55it/s]Extractor Estimating: 47it [00:32,  1.55it/s]Extractor Estimating: 48it [00:32,  1.52it/s]Extractor Estimating: 49it [00:33,  1.46it/s]Extractor Estimating: 50it [00:34,  1.45it/s]Extractor Estimating: 51it [00:34,  1.46it/s]Extractor Estimating: 52it [00:35,  1.42it/s]Extractor Estimating: 53it [00:36,  1.52it/s]Extractor Estimating: 54it [00:36,  1.52it/s]Extractor Estimating: 55it [00:37,  1.49it/s]Extractor Estimating: 56it [00:38,  1.48it/s]Extractor Estimating: 57it [00:38,  1.54it/s]Extractor Estimating: 58it [00:39,  1.57it/s]Extractor Estimating: 59it [00:40,  1.56it/s]Extractor Estimating: 60it [00:40,  1.54it/s]Extractor Estimating: 61it [00:41,  1.52it/s]Extractor Estimating: 62it [00:42,  1.59it/s]Extractor Estimating: 63it [00:42,  1.57it/s]Extractor Estimating: 64it [00:43,  1.45it/s]Extractor Estimating: 65it [00:44,  1.51it/s]Extractor Estimating: 66it [00:44,  1.53it/s]Extractor Estimating: 67it [00:45,  1.52it/s]Extractor Estimating: 68it [00:46,  1.55it/s]Extractor Estimating: 69it [00:46,  1.57it/s]Extractor Estimating: 70it [00:47,  1.56it/s]Extractor Estimating: 71it [00:48,  1.54it/s]Extractor Estimating: 72it [00:48,  1.56it/s]Extractor Estimating: 73it [00:49,  1.57it/s]Extractor Estimating: 74it [00:49,  1.56it/s]Extractor Estimating: 75it [00:50,  1.61it/s]Extractor Estimating: 76it [00:51,  1.55it/s]Extractor Estimating: 77it [00:51,  1.62it/s]Extractor Estimating: 78it [00:52,  1.68it/s]Extractor Estimating: 79it [00:52,  1.70it/s]Extractor Estimating: 80it [00:53,  1.73it/s]Extractor Estimating: 81it [00:53,  1.75it/s]Extractor Estimating: 82it [00:54,  1.70it/s]Extractor Estimating: 83it [00:55,  1.75it/s]Extractor Estimating: 84it [00:55,  1.80it/s]Extractor Estimating: 85it [00:56,  1.82it/s]Extractor Estimating: 86it [00:56,  1.83it/s]Extractor Estimating: 87it [00:57,  1.80it/s]Extractor Estimating: 88it [00:57,  1.82it/s]Extractor Estimating: 89it [00:58,  1.85it/s]Extractor Estimating: 90it [00:58,  1.86it/s]Extractor Estimating: 91it [00:59,  1.83it/s]Extractor Estimating: 92it [00:59,  1.90it/s]Extractor Estimating: 93it [01:00,  1.85it/s]Extractor Estimating: 94it [01:01,  1.86it/s]Extractor Estimating: 95it [01:01,  1.80it/s]Extractor Estimating: 96it [01:02,  1.79it/s]Extractor Estimating: 97it [01:02,  1.78it/s]Extractor Estimating: 98it [01:03,  1.61it/s]Extractor Estimating: 99it [01:04,  1.62it/s]Extractor Estimating: 100it [01:04,  1.63it/s]Extractor Estimating: 101it [01:05,  1.65it/s]Extractor Estimating: 102it [01:05,  1.64it/s]Extractor Estimating: 103it [01:06,  1.65it/s]Extractor Estimating: 104it [01:07,  1.63it/s]Extractor Estimating: 105it [01:07,  1.61it/s]Extractor Estimating: 106it [01:08,  1.55it/s]Extractor Estimating: 107it [01:09,  1.52it/s]Extractor Estimating: 108it [01:09,  1.56it/s]Extractor Estimating: 109it [01:10,  1.53it/s]Extractor Estimating: 110it [01:11,  1.56it/s]Extractor Estimating: 111it [01:11,  1.57it/s]Extractor Estimating: 112it [01:12,  1.62it/s]Extractor Estimating: 113it [01:12,  1.68it/s]Extractor Estimating: 114it [01:13,  1.67it/s]Extractor Estimating: 115it [01:14,  1.61it/s]Extractor Estimating: 116it [01:14,  1.62it/s]Extractor Estimating: 117it [01:15,  1.63it/s]Extractor Estimating: 118it [01:16,  1.56it/s]Extractor Estimating: 119it [01:16,  1.55it/s]Extractor Estimating: 120it [01:17,  1.54it/s]Extractor Estimating: 121it [01:17,  1.62it/s]Extractor Estimating: 122it [01:18,  1.52it/s]Extractor Estimating: 123it [01:19,  1.62it/s]Extractor Estimating: 124it [01:19,  1.60it/s]Extractor Estimating: 125it [01:20,  1.64it/s]Extractor Estimating: 126it [01:21,  1.60it/s]Extractor Estimating: 127it [01:21,  1.48it/s]Extractor Estimating: 128it [01:22,  1.49it/s]Extractor Estimating: 129it [01:23,  1.49it/s]Extractor Estimating: 130it [01:23,  1.48it/s]Extractor Estimating: 131it [01:24,  1.57it/s]Extractor Estimating: 132it [01:25,  1.56it/s]Extractor Estimating: 133it [01:25,  1.59it/s]Extractor Estimating: 134it [01:26,  1.59it/s]Extractor Estimating: 135it [01:26,  1.58it/s]Extractor Estimating: 136it [01:27,  1.54it/s]Extractor Estimating: 137it [01:28,  1.51it/s]Extractor Estimating: 138it [01:28,  1.51it/s]Extractor Estimating: 139it [01:29,  1.54it/s]Extractor Estimating: 140it [01:30,  1.58it/s]Extractor Estimating: 141it [01:30,  1.58it/s]Extractor Estimating: 142it [01:31,  1.61it/s]Extractor Estimating: 143it [01:32,  1.55it/s]Extractor Estimating: 144it [01:32,  1.59it/s]Extractor Estimating: 145it [01:33,  1.58it/s]Extractor Estimating: 146it [01:34,  1.35it/s]Extractor Estimating: 147it [01:34,  1.41it/s]Extractor Estimating: 148it [01:35,  1.44it/s]Extractor Estimating: 149it [01:36,  1.43it/s]Extractor Estimating: 150it [01:36,  1.49it/s]Extractor Estimating: 151it [01:37,  1.49it/s]Extractor Estimating: 152it [01:38,  1.48it/s]Extractor Estimating: 153it [01:38,  1.51it/s]Extractor Estimating: 154it [01:39,  1.52it/s]Extractor Estimating: 155it [01:40,  1.54it/s]Extractor Estimating: 156it [01:40,  1.57it/s]Extractor Estimating: 157it [01:41,  1.63it/s]Extractor Estimating: 158it [01:41,  1.64it/s]Extractor Estimating: 159it [01:42,  1.60it/s]Extractor Estimating: 160it [01:43,  1.55it/s]Extractor Estimating: 161it [01:43,  1.55it/s]Extractor Estimating: 162it [01:44,  1.50it/s]Extractor Estimating: 163it [01:45,  1.51it/s]Extractor Estimating: 164it [01:46,  1.51it/s]Extractor Estimating: 165it [01:46,  1.51it/s]Extractor Estimating: 166it [01:47,  1.50it/s]Extractor Estimating: 167it [01:47,  1.55it/s]Extractor Estimating: 168it [01:48,  1.59it/s]Extractor Estimating: 169it [01:49,  1.57it/s]Extractor Estimating: 170it [01:49,  1.53it/s]Extractor Estimating: 171it [01:50,  1.53it/s]Extractor Estimating: 172it [01:51,  1.57it/s]Extractor Estimating: 173it [01:51,  1.62it/s]Extractor Estimating: 174it [01:52,  1.61it/s]Extractor Estimating: 175it [01:52,  1.61it/s]Extractor Estimating: 176it [01:53,  1.62it/s]Extractor Estimating: 177it [01:54,  1.62it/s]Extractor Estimating: 178it [01:54,  1.61it/s]Extractor Estimating: 179it [01:55,  1.63it/s]Extractor Estimating: 180it [01:56,  1.58it/s]Extractor Estimating: 181it [01:56,  1.53it/s]Extractor Estimating: 182it [01:57,  1.59it/s]Extractor Estimating: 183it [01:57,  1.61it/s]Extractor Estimating: 184it [01:58,  1.50it/s]Extractor Estimating: 185it [01:59,  1.50it/s]Extractor Estimating: 186it [02:00,  1.51it/s]Extractor Estimating: 187it [02:00,  1.56it/s]Extractor Estimating: 188it [02:01,  1.57it/s]Extractor Estimating: 189it [02:01,  1.58it/s]Extractor Estimating: 190it [02:02,  1.61it/s]Extractor Estimating: 191it [02:03,  1.58it/s]Extractor Estimating: 192it [02:03,  1.59it/s]Extractor Estimating: 193it [02:04,  1.60it/s]Extractor Estimating: 194it [02:05,  1.50it/s]Extractor Estimating: 195it [02:05,  1.52it/s]Extractor Estimating: 196it [02:06,  1.55it/s]Extractor Estimating: 197it [02:06,  1.59it/s]Extractor Estimating: 198it [02:07,  1.54it/s]Extractor Estimating: 199it [02:08,  1.56it/s]Extractor Estimating: 200it [02:08,  1.54it/s]Extractor Estimating: 201it [02:09,  1.54it/s]Extractor Estimating: 202it [02:10,  1.52it/s]Extractor Estimating: 203it [02:10,  1.53it/s]Extractor Estimating: 204it [02:11,  1.50it/s]Extractor Estimating: 205it [02:12,  1.52it/s]Extractor Estimating: 206it [02:12,  1.52it/s]Extractor Estimating: 207it [02:13,  1.49it/s]Extractor Estimating: 208it [02:14,  1.43it/s]Extractor Estimating: 209it [02:15,  1.48it/s]Extractor Estimating: 210it [02:15,  1.53it/s]Extractor Estimating: 211it [02:16,  1.51it/s]Extractor Estimating: 212it [02:17,  1.47it/s]Extractor Estimating: 213it [02:17,  1.50it/s]Extractor Estimating: 214it [02:18,  1.51it/s]Extractor Estimating: 215it [02:19,  1.46it/s]Extractor Estimating: 216it [02:19,  1.48it/s]Extractor Estimating: 217it [02:20,  1.48it/s]Extractor Estimating: 218it [02:21,  1.50it/s]Extractor Estimating: 219it [02:21,  1.54it/s]Extractor Estimating: 220it [02:22,  1.51it/s]Extractor Estimating: 221it [02:22,  1.53it/s]Extractor Estimating: 222it [02:23,  1.40it/s]Extractor Estimating: 223it [02:24,  1.37it/s]Extractor Estimating: 224it [02:25,  1.33it/s]Extractor Estimating: 225it [02:26,  1.36it/s]Extractor Estimating: 226it [02:26,  1.47it/s]Extractor Estimating: 227it [02:27,  1.50it/s]Extractor Estimating: 228it [02:27,  1.53it/s]Extractor Estimating: 229it [02:28,  1.62it/s]Extractor Estimating: 230it [02:28,  1.66it/s]Extractor Estimating: 231it [02:29,  1.72it/s]Extractor Estimating: 232it [02:30,  1.75it/s]Extractor Estimating: 233it [02:30,  1.78it/s]Extractor Estimating: 234it [02:31,  1.77it/s]Extractor Estimating: 235it [02:31,  1.75it/s]Extractor Estimating: 236it [02:32,  1.69it/s]Extractor Estimating: 237it [02:33,  1.68it/s]Extractor Estimating: 238it [02:33,  1.71it/s]Extractor Estimating: 239it [02:34,  1.74it/s]Extractor Estimating: 240it [02:34,  1.68it/s]Extractor Estimating: 241it [02:35,  1.63it/s]Extractor Estimating: 242it [02:35,  1.70it/s]Extractor Estimating: 243it [02:36,  1.71it/s]Extractor Estimating: 244it [02:37,  1.67it/s]Extractor Estimating: 245it [02:37,  1.68it/s]Extractor Estimating: 246it [02:38,  1.72it/s]Extractor Estimating: 247it [02:38,  1.77it/s]Extractor Estimating: 248it [02:39,  1.76it/s]Extractor Estimating: 249it [02:39,  1.81it/s]Extractor Estimating: 250it [02:40,  1.67it/s]Extractor Estimating: 251it [02:41,  1.65it/s]Extractor Estimating: 252it [02:41,  1.66it/s]Extractor Estimating: 253it [02:42,  1.64it/s]Extractor Estimating: 254it [02:43,  1.67it/s]Extractor Estimating: 255it [02:43,  1.64it/s]Extractor Estimating: 256it [02:44,  1.66it/s]Extractor Estimating: 257it [02:44,  1.66it/s]Extractor Estimating: 258it [02:45,  1.70it/s]Extractor Estimating: 259it [02:46,  1.69it/s]Extractor Estimating: 260it [02:46,  1.61it/s]Extractor Estimating: 261it [02:47,  1.65it/s]Extractor Estimating: 262it [02:47,  1.67it/s]Extractor Estimating: 263it [02:48,  1.69it/s]Extractor Estimating: 264it [02:49,  1.71it/s]Extractor Estimating: 265it [02:49,  1.72it/s]Extractor Estimating: 266it [02:50,  1.68it/s]Extractor Estimating: 267it [02:50,  1.64it/s]Extractor Estimating: 268it [02:51,  1.63it/s]Extractor Estimating: 269it [02:52,  1.62it/s]Extractor Estimating: 270it [02:52,  1.63it/s]Extractor Estimating: 271it [02:53,  1.62it/s]Extractor Estimating: 272it [02:53,  1.60it/s]Extractor Estimating: 273it [02:54,  1.62it/s]Extractor Estimating: 274it [02:55,  1.65it/s]Extractor Estimating: 275it [02:55,  1.63it/s]Extractor Estimating: 276it [02:56,  1.59it/s]Extractor Estimating: 277it [02:57,  1.61it/s]Extractor Estimating: 278it [02:57,  1.64it/s]Extractor Estimating: 279it [02:58,  1.61it/s]Extractor Estimating: 280it [02:58,  1.65it/s]Extractor Estimating: 281it [02:59,  1.69it/s]Extractor Estimating: 282it [03:00,  1.67it/s]Extractor Estimating: 283it [03:00,  1.63it/s]Extractor Estimating: 284it [03:01,  1.66it/s]Extractor Estimating: 285it [03:01,  1.66it/s]Extractor Estimating: 286it [03:02,  1.72it/s]Extractor Estimating: 287it [03:02,  1.74it/s]Extractor Estimating: 288it [03:03,  1.78it/s]Extractor Estimating: 289it [03:04,  1.77it/s]Extractor Estimating: 290it [03:04,  1.76it/s]Extractor Estimating: 291it [03:05,  1.73it/s]Extractor Estimating: 292it [03:05,  1.72it/s]Extractor Estimating: 293it [03:06,  1.71it/s]Extractor Estimating: 294it [03:07,  1.66it/s]Extractor Estimating: 295it [03:07,  1.74it/s]Extractor Estimating: 296it [03:08,  1.73it/s]Extractor Estimating: 297it [03:08,  1.73it/s]Extractor Estimating: 298it [03:09,  1.68it/s]Extractor Estimating: 299it [03:09,  1.73it/s]Extractor Estimating: 300it [03:10,  1.73it/s]Extractor Estimating: 301it [03:11,  1.69it/s]Extractor Estimating: 302it [03:11,  1.50it/s]Extractor Estimating: 303it [03:12,  1.54it/s]Extractor Estimating: 304it [03:13,  1.61it/s]Extractor Estimating: 305it [03:13,  1.58it/s]Extractor Estimating: 306it [03:14,  1.60it/s]Extractor Estimating: 307it [03:14,  1.60it/s]Extractor Estimating: 308it [03:15,  1.60it/s]Extractor Estimating: 309it [03:16,  1.59it/s]Extractor Estimating: 310it [03:16,  1.57it/s]Extractor Estimating: 311it [03:17,  1.55it/s]Extractor Estimating: 312it [03:18,  1.54it/s]Extractor Estimating: 313it [03:18,  1.57it/s]Extractor Estimating: 314it [03:19,  1.60it/s]Extractor Estimating: 315it [03:20,  1.57it/s]Extractor Estimating: 316it [03:20,  1.57it/s]Extractor Estimating: 317it [03:21,  1.60it/s]Extractor Estimating: 318it [03:22,  1.54it/s]Extractor Estimating: 319it [03:22,  1.58it/s]Extractor Estimating: 320it [03:23,  1.59it/s]Extractor Estimating: 321it [03:23,  1.57it/s]Extractor Estimating: 322it [03:24,  1.50it/s]Extractor Estimating: 323it [03:25,  1.51it/s]Extractor Estimating: 324it [03:25,  1.49it/s]Extractor Estimating: 325it [03:26,  1.47it/s]Extractor Estimating: 326it [03:27,  1.54it/s]Extractor Estimating: 327it [03:27,  1.58it/s]Extractor Estimating: 328it [03:28,  1.65it/s]Extractor Estimating: 329it [03:28,  1.68it/s]Extractor Estimating: 330it [03:29,  1.79it/s]Extractor Estimating: 331it [03:30,  1.74it/s]Extractor Estimating: 332it [03:30,  1.72it/s]Extractor Estimating: 333it [03:31,  1.63it/s]Extractor Estimating: 334it [03:32,  1.53it/s]Extractor Estimating: 335it [03:32,  1.58it/s]Extractor Estimating: 336it [03:33,  1.66it/s]Extractor Estimating: 337it [03:33,  1.62it/s]Extractor Estimating: 338it [03:34,  1.62it/s]Extractor Estimating: 339it [03:35,  1.63it/s]Extractor Estimating: 340it [03:35,  1.69it/s]Extractor Estimating: 341it [03:36,  1.66it/s]Extractor Estimating: 342it [03:36,  1.68it/s]Extractor Estimating: 343it [03:37,  1.71it/s]Extractor Estimating: 344it [03:38,  1.67it/s]Extractor Estimating: 345it [03:38,  1.68it/s]Extractor Estimating: 346it [03:39,  1.74it/s]Extractor Estimating: 347it [03:39,  1.71it/s]Extractor Estimating: 348it [03:40,  1.61it/s]Extractor Estimating: 349it [03:41,  1.65it/s]Extractor Estimating: 350it [03:41,  1.75it/s]Extractor Estimating: 351it [03:42,  1.66it/s]Extractor Estimating: 352it [03:42,  1.51it/s]Extractor Estimating: 353it [03:43,  1.55it/s]Extractor Estimating: 354it [03:44,  1.58it/s]Extractor Estimating: 355it [03:44,  1.56it/s]Extractor Estimating: 356it [03:45,  1.51it/s]Extractor Estimating: 357it [03:46,  1.54it/s]Extractor Estimating: 358it [03:46,  1.57it/s]Extractor Estimating: 359it [03:47,  1.51it/s]Extractor Estimating: 360it [03:48,  1.51it/s]Extractor Estimating: 361it [03:48,  1.48it/s]Extractor Estimating: 362it [03:49,  1.51it/s]Extractor Estimating: 363it [03:50,  1.50it/s]Extractor Estimating: 364it [03:50,  1.52it/s]Extractor Estimating: 365it [03:51,  1.51it/s]Extractor Estimating: 366it [03:52,  1.51it/s]Extractor Estimating: 367it [03:52,  1.54it/s]Extractor Estimating: 368it [03:53,  1.56it/s]Extractor Estimating: 369it [03:54,  1.57it/s]Extractor Estimating: 370it [03:54,  1.61it/s]Extractor Estimating: 371it [03:55,  1.58it/s]Extractor Estimating: 372it [03:56,  1.50it/s]Extractor Estimating: 373it [03:56,  1.47it/s]Extractor Estimating: 374it [03:57,  1.50it/s]Extractor Estimating: 375it [03:58,  1.49it/s]Extractor Estimating: 375it [03:58,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:00,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:00,072 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:00,072 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:00,072 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:00,072 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:54:00,447 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:54:00,448 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:54:01,138 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:54:02,180 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:54:02,180 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:03,943 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:03,949 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:03,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:03,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:54:03,950 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:54:04,263 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:54:04,264 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:54:04,524 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:54:04,671 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:54:04,671 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:15:50,247 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:15:50,253 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7496 mean pseudo reward: 0.9328987010348684
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 20244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20344, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.096, loss:737.2566
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.109, loss:722.4123
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.129, loss:697.2777
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.103, loss:660.3639
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.114, loss:712.1822
>> valid entity prec:0.5862, rec:0.6082, f1:0.5970
>> valid relation prec:0.4520, rec:0.1469, f1:0.2218
>> valid relation with NER prec:0.4520, rec:0.1469, f1:0.2218
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.372, loss:687.3327
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.104, loss:671.8947
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.111, loss:670.4455
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.123, loss:711.7984
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.118, loss:664.8819
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5879, rec:0.6114, f1:0.5994
>> valid relation prec:0.4049, rec:0.1080, f1:0.1705
>> valid relation with NER prec:0.4049, rec:0.1080, f1:0.1705
new max entity f1 on valid!
g_step 1100, step 161, avg_time 2.386, loss:663.2659
g_step 1200, step 261, avg_time 1.110, loss:682.8413
g_step 1300, step 48, avg_time 1.124, loss:643.2405
g_step 1400, step 148, avg_time 1.118, loss:646.4342
g_step 1500, step 248, avg_time 1.107, loss:627.4796
>> valid entity prec:0.6327, rec:0.5210, f1:0.5715
>> valid relation prec:0.3888, rec:0.1017, f1:0.1612
>> valid relation with NER prec:0.3888, rec:0.1017, f1:0.1612
g_step 1600, step 35, avg_time 2.389, loss:624.8606
g_step 1700, step 135, avg_time 1.117, loss:593.2480
g_step 1800, step 235, avg_time 1.112, loss:593.5026
g_step 1900, step 22, avg_time 1.102, loss:591.8318
g_step 2000, step 122, avg_time 1.108, loss:585.7120
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5512, rec:0.6166, f1:0.5821
>> valid relation prec:0.3869, rec:0.1318, f1:0.1966
>> valid relation with NER prec:0.3869, rec:0.1318, f1:0.1966
g_step 2100, step 222, avg_time 2.400, loss:559.2975
g_step 2200, step 9, avg_time 1.117, loss:567.9229
g_step 2300, step 109, avg_time 1.102, loss:528.2278
g_step 2400, step 209, avg_time 1.122, loss:550.4759
g_step 2500, step 309, avg_time 1.118, loss:549.7301
>> valid entity prec:0.5698, rec:0.5546, f1:0.5621
>> valid relation prec:0.3591, rec:0.0985, f1:0.1546
>> valid relation with NER prec:0.3591, rec:0.0985, f1:0.1546
g_step 2600, step 96, avg_time 2.401, loss:490.9926
g_step 2700, step 196, avg_time 1.109, loss:514.1212
g_step 2800, step 296, avg_time 1.109, loss:528.8902
g_step 2900, step 83, avg_time 1.121, loss:461.9810
g_step 3000, step 183, avg_time 1.123, loss:471.4686
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5617, rec:0.5858, f1:0.5735
>> valid relation prec:0.4074, rec:0.1323, f1:0.1998
>> valid relation with NER prec:0.4074, rec:0.1323, f1:0.1998
g_step 3100, step 283, avg_time 2.390, loss:503.2387
g_step 3200, step 70, avg_time 1.113, loss:455.9739
g_step 3300, step 170, avg_time 1.124, loss:473.5692
g_step 3400, step 270, avg_time 1.113, loss:475.6763
g_step 3500, step 57, avg_time 1.093, loss:429.5364
>> valid entity prec:0.5803, rec:0.5554, f1:0.5676
>> valid relation prec:0.3629, rec:0.1266, f1:0.1877
>> valid relation with NER prec:0.3629, rec:0.1266, f1:0.1877
g_step 3600, step 157, avg_time 2.396, loss:435.1828
g_step 3700, step 257, avg_time 1.119, loss:471.3597
g_step 3800, step 44, avg_time 1.121, loss:425.1693
g_step 3900, step 144, avg_time 1.109, loss:415.0890
g_step 4000, step 244, avg_time 1.116, loss:433.4920
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5931, rec:0.4995, f1:0.5423
>> valid relation prec:0.3264, rec:0.0897, f1:0.1407
>> valid relation with NER prec:0.3264, rec:0.0897, f1:0.1407
g_step 4100, step 31, avg_time 2.390, loss:413.2618
g_step 4200, step 131, avg_time 1.121, loss:391.9659
g_step 4300, step 231, avg_time 1.119, loss:414.1428
g_step 4400, step 18, avg_time 1.098, loss:403.4683
g_step 4500, step 118, avg_time 1.124, loss:355.2111
>> valid entity prec:0.6194, rec:0.5497, f1:0.5825
>> valid relation prec:0.3814, rec:0.1341, f1:0.1984
>> valid relation with NER prec:0.3814, rec:0.1341, f1:0.1984
g_step 4600, step 218, avg_time 2.386, loss:395.4442
g_step 4700, step 5, avg_time 1.106, loss:407.6169
g_step 4800, step 105, avg_time 1.115, loss:341.1963
g_step 4900, step 205, avg_time 1.111, loss:387.0778
g_step 5000, step 305, avg_time 1.118, loss:389.1119
learning rate was adjusted to 0.0008
>> valid entity prec:0.5783, rec:0.5949, f1:0.5864
>> valid relation prec:0.3604, rec:0.1323, f1:0.1936
>> valid relation with NER prec:0.3604, rec:0.1323, f1:0.1936
g_step 5100, step 92, avg_time 2.382, loss:354.8993
g_step 5200, step 192, avg_time 1.125, loss:351.1352
g_step 5300, step 292, avg_time 1.114, loss:370.3836
g_step 5400, step 79, avg_time 1.105, loss:326.7200
g_step 5500, step 179, avg_time 1.131, loss:334.9503
>> valid entity prec:0.5773, rec:0.5414, f1:0.5588
>> valid relation prec:0.3485, rec:0.1051, f1:0.1615
>> valid relation with NER prec:0.3485, rec:0.1051, f1:0.1615
g_step 5600, step 279, avg_time 2.383, loss:343.6910
g_step 5700, step 66, avg_time 1.088, loss:314.9076
g_step 5800, step 166, avg_time 1.117, loss:318.0119
g_step 5900, step 266, avg_time 1.133, loss:357.4479
g_step 6000, step 53, avg_time 1.118, loss:319.7549
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5928, rec:0.5276, f1:0.5583
>> valid relation prec:0.3360, rec:0.1200, f1:0.1769
>> valid relation with NER prec:0.3360, rec:0.1200, f1:0.1769
g_step 6100, step 153, avg_time 2.378, loss:305.8109
g_step 6200, step 253, avg_time 1.135, loss:318.4631
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:15:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:15:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-15-50_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:15:51 - WARNING - datasets.builder -   Using custom data configuration default-4218c9c3a2527c3c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4218c9c3a2527c3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:15:52,195 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:15:52,197 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:15:52,197 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:15:52,198 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:15:52,207 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:15:52,213 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:15:52,385 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:15:55,595 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:15:55,604 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4218c9c3a2527c3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.53ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.82ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.48ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.91ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.20ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.37ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.49ba/s]100%|██████████| 8/8 [00:01<00:00,  5.37ba/s]100%|██████████| 8/8 [00:01<00:00,  4.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35ba/s]100%|██████████| 4/4 [00:00<00:00,  5.44ba/s]100%|██████████| 4/4 [00:00<00:00,  4.93ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.88ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.37ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.51ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.01ba/s]100%|██████████| 8/8 [00:00<00:00, 10.00ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.08ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.71ba/s]100%|██████████| 4/4 [00:00<00:00, 11.11ba/s]
[INFO|trainer.py:414] 2023-08-28 21:16:00,365 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:16:00,388 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:16:00,388 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 21:16:00,388 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:16:00,388 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:16:00,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:16:00,388 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:16:00,388 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.29it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:47,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:47,  3.45it/s]  1%|▏         | 8/585 [00:02<02:46,  3.46it/s]  2%|▏         | 9/585 [00:02<02:45,  3.47it/s]  2%|▏         | 10/585 [00:02<02:45,  3.48it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.49it/s]  2%|▏         | 13/585 [00:03<02:44,  3.49it/s]  2%|▏         | 14/585 [00:04<02:43,  3.49it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:43,  3.49it/s]  3%|▎         | 17/585 [00:05<05:12,  1.82it/s]  3%|▎         | 18/585 [00:06<04:27,  2.12it/s]  3%|▎         | 19/585 [00:06<03:55,  2.40it/s]  3%|▎         | 20/585 [00:06<03:33,  2.65it/s]  4%|▎         | 21/585 [00:06<03:17,  2.85it/s]  4%|▍         | 22/585 [00:07<03:07,  3.01it/s]  4%|▍         | 23/585 [00:07<02:59,  3.14it/s]  4%|▍         | 24/585 [00:07<02:53,  3.23it/s]  4%|▍         | 25/585 [00:08<03:38,  2.57it/s]  4%|▍         | 26/585 [00:08<03:20,  2.79it/s]  5%|▍         | 27/585 [00:08<03:08,  2.97it/s]  5%|▍         | 28/585 [00:09<02:59,  3.11it/s]  5%|▍         | 29/585 [00:09<02:53,  3.21it/s]  5%|▌         | 30/585 [00:09<02:48,  3.29it/s]  5%|▌         | 31/585 [00:10<02:45,  3.35it/s]  5%|▌         | 32/585 [00:10<02:43,  3.39it/s]  6%|▌         | 33/585 [00:10<02:41,  3.42it/s]  6%|▌         | 34/585 [00:10<02:40,  3.44it/s]  6%|▌         | 35/585 [00:11<04:09,  2.21it/s]  6%|▌         | 36/585 [00:12<03:41,  2.48it/s]  6%|▋         | 37/585 [00:12<03:21,  2.71it/s]  6%|▋         | 38/585 [00:12<03:08,  2.91it/s]  7%|▋         | 39/585 [00:12<02:58,  3.06it/s]  7%|▋         | 40/585 [00:13<02:51,  3.18it/s]  7%|▋         | 41/585 [00:13<02:46,  3.26it/s]  7%|▋         | 42/585 [00:13<02:43,  3.33it/s]  7%|▋         | 43/585 [00:14<02:40,  3.38it/s]  8%|▊         | 44/585 [00:14<02:54,  3.10it/s]  8%|▊         | 45/585 [00:14<02:48,  3.20it/s]  8%|▊         | 46/585 [00:15<02:44,  3.29it/s]  8%|▊         | 47/585 [00:15<02:40,  3.34it/s]  8%|▊         | 48/585 [00:15<02:38,  3.38it/s]  8%|▊         | 49/585 [00:15<02:37,  3.41it/s]  9%|▊         | 50/585 [00:16<02:35,  3.43it/s]  9%|▊         | 51/585 [00:16<02:34,  3.45it/s]  9%|▉         | 52/585 [00:16<02:34,  3.46it/s]  9%|▉         | 53/585 [00:17<02:33,  3.47it/s]  9%|▉         | 54/585 [00:17<02:33,  3.47it/s]  9%|▉         | 55/585 [00:17<02:44,  3.22it/s] 10%|▉         | 56/585 [00:17<02:40,  3.30it/s] 10%|▉         | 57/585 [00:18<02:37,  3.35it/s] 10%|▉         | 58/585 [00:18<02:35,  3.39it/s] 10%|█         | 59/585 [00:18<02:33,  3.42it/s] 10%|█         | 60/585 [00:19<02:32,  3.43it/s] 10%|█         | 61/585 [00:19<02:31,  3.45it/s] 11%|█         | 62/585 [00:19<02:31,  3.46it/s] 11%|█         | 63/585 [00:19<02:30,  3.47it/s] 11%|█         | 64/585 [00:20<02:30,  3.47it/s] 11%|█         | 65/585 [00:20<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:21<03:04,  2.81it/s] 11%|█▏        | 67/585 [00:21<02:53,  2.98it/s] 12%|█▏        | 68/585 [00:21<02:46,  3.11it/s] 12%|█▏        | 69/585 [00:21<02:40,  3.21it/s] 12%|█▏        | 70/585 [00:22<02:36,  3.29it/s] 12%|█▏        | 71/585 [00:22<02:33,  3.34it/s] 12%|█▏        | 72/585 [00:22<02:31,  3.38it/s] 12%|█▏        | 73/585 [00:23<02:30,  3.41it/s] 13%|█▎        | 74/585 [00:23<02:43,  3.13it/s] 13%|█▎        | 75/585 [00:23<02:37,  3.23it/s] 13%|█▎        | 76/585 [00:24<02:34,  3.30it/s] 13%|█▎        | 77/585 [00:24<02:31,  3.35it/s] 13%|█▎        | 78/585 [00:24<02:29,  3.39it/s] 14%|█▎        | 79/585 [00:24<02:28,  3.41it/s] 14%|█▎        | 80/585 [00:25<02:27,  3.43it/s] 14%|█▍        | 81/585 [00:25<02:26,  3.44it/s] 14%|█▍        | 82/585 [00:25<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:26<02:24,  3.46it/s] 14%|█▍        | 84/585 [00:26<02:27,  3.39it/s] 15%|█▍        | 85/585 [00:26<02:26,  3.42it/s] 15%|█▍        | 86/585 [00:26<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:27<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:27<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:27<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:28<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:28<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:28<02:21,  3.47it/s] 16%|█▌        | 93/585 [00:28<02:21,  3.48it/s] 16%|█▌        | 94/585 [00:29<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:29<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:29<02:21,  3.47it/s] 17%|█▋        | 97/585 [00:30<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:30<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:30<02:19,  3.47it/s] 17%|█▋        | 100/585 [00:30<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:31<02:19,  3.48it/s] 17%|█▋        | 102/585 [00:31<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:31<02:18,  3.48it/s] 18%|█▊        | 104/585 [00:32<02:18,  3.48it/s] 18%|█▊        | 105/585 [00:32<02:18,  3.48it/s] 18%|█▊        | 106/585 [00:32<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:32<02:17,  3.47it/s] 18%|█▊        | 108/585 [00:33<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:33<02:17,  3.47it/s] 19%|█▉        | 110/585 [00:33<02:16,  3.47it/s] 19%|█▉        | 111/585 [00:34<02:16,  3.47it/s] 19%|█▉        | 112/585 [00:34<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:34<02:15,  3.47it/s] 19%|█▉        | 114/585 [00:35<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:35<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:35<02:15,  3.47it/s] 20%|██        | 117/585 [00:35<02:22,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 21:16:36,363 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:16:36,363 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:16:36,363 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.94it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.70it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.03it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.37it/s][A
  6%|▋         | 28/437 [00:00<00:08, 48.03it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.62it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.32it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.96it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.86it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.91it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.99it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.02it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.94it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.98it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.95it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.01it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.87it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.81it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.78it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.82it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.87it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.92it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.99it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.93it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.87it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.87it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.85it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.86it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.80it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.93it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.94it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.95it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.94it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.93it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.91it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.87it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.89it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.89it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.90it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.84it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.97it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.96it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.93it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.96it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.86it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.87it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.94it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.97it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.84it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.94it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.93it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.92it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.99it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.95it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.82it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.87it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.88it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 43.00it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 44.88it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 45.48it/s][A
 70%|███████   | 308/437 [00:06<00:02, 45.89it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.25it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.42it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.59it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.76it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.80it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.73it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.73it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.66it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.80it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.81it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.84it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.90it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.95it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.90it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.91it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.87it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.80it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.81it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.92it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.86it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 45.16it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 45.75it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.16it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.32it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.55it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.55it/s][A 20%|██        | 117/585 [00:45<02:22,  3.27it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:16:45,774 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:16:45,810 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:16:49,979 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:16:50,438 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:16:50,699 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:08<1:16:54,  9.88s/it] 20%|██        | 119/585 [01:08<54:31,  7.02s/it]   21%|██        | 120/585 [01:08<38:45,  5.00s/it] 21%|██        | 121/585 [01:09<27:44,  3.59s/it] 21%|██        | 122/585 [01:09<20:02,  2.60s/it] 21%|██        | 123/585 [01:09<14:39,  1.90s/it] 21%|██        | 124/585 [01:09<10:53,  1.42s/it] 21%|██▏       | 125/585 [01:10<08:16,  1.08s/it] 22%|██▏       | 126/585 [01:10<06:26,  1.19it/s] 22%|██▏       | 127/585 [01:10<05:09,  1.48it/s] 22%|██▏       | 128/585 [01:11<04:15,  1.79it/s] 22%|██▏       | 129/585 [01:11<03:37,  2.10it/s] 22%|██▏       | 130/585 [01:11<03:18,  2.29it/s] 22%|██▏       | 131/585 [01:11<02:57,  2.55it/s] 23%|██▎       | 132/585 [01:12<02:43,  2.78it/s] 23%|██▎       | 133/585 [01:12<02:32,  2.96it/s] 23%|██▎       | 134/585 [01:12<02:25,  3.10it/s] 23%|██▎       | 135/585 [01:13<02:20,  3.21it/s] 23%|██▎       | 136/585 [01:13<02:16,  3.29it/s] 23%|██▎       | 137/585 [01:13<02:13,  3.34it/s] 24%|██▎       | 138/585 [01:13<02:11,  3.39it/s] 24%|██▍       | 139/585 [01:14<02:10,  3.42it/s] 24%|██▍       | 140/585 [01:14<02:09,  3.44it/s] 24%|██▍       | 141/585 [01:14<02:20,  3.16it/s] 24%|██▍       | 142/585 [01:15<02:16,  3.25it/s] 24%|██▍       | 143/585 [01:15<02:13,  3.32it/s] 25%|██▍       | 144/585 [01:15<02:11,  3.36it/s] 25%|██▍       | 145/585 [01:16<02:09,  3.40it/s] 25%|██▍       | 146/585 [01:16<02:08,  3.43it/s] 25%|██▌       | 147/585 [01:16<02:07,  3.44it/s] 25%|██▌       | 148/585 [01:16<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:17<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:17<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:17<02:04,  3.48it/s] 26%|██▌       | 152/585 [01:18<03:07,  2.31it/s] 26%|██▌       | 153/585 [01:18<02:48,  2.57it/s] 26%|██▋       | 154/585 [01:19<02:34,  2.79it/s] 26%|██▋       | 155/585 [01:19<02:24,  2.97it/s] 27%|██▋       | 156/585 [01:19<02:18,  3.11it/s] 27%|██▋       | 157/585 [01:20<02:13,  3.21it/s] 27%|██▋       | 158/585 [01:20<02:09,  3.29it/s] 27%|██▋       | 159/585 [01:20<02:07,  3.34it/s] 27%|██▋       | 160/585 [01:20<02:05,  3.38it/s] 28%|██▊       | 161/585 [01:21<02:09,  3.27it/s] 28%|██▊       | 162/585 [01:21<02:07,  3.32it/s] 28%|██▊       | 163/585 [01:21<02:05,  3.37it/s] 28%|██▊       | 164/585 [01:22<02:03,  3.40it/s] 28%|██▊       | 165/585 [01:22<02:02,  3.43it/s] 28%|██▊       | 166/585 [01:22<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:22<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:23<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:23<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:23<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:24<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:24<02:15,  3.04it/s] 30%|██▉       | 173/585 [01:25<03:09,  2.17it/s] 30%|██▉       | 174/585 [01:25<02:47,  2.45it/s] 30%|██▉       | 175/585 [01:25<02:32,  2.69it/s] 30%|███       | 176/585 [01:26<02:21,  2.89it/s] 30%|███       | 177/585 [01:26<02:14,  3.04it/s] 30%|███       | 178/585 [01:26<02:09,  3.16it/s] 31%|███       | 179/585 [01:27<02:05,  3.24it/s] 31%|███       | 180/585 [01:27<02:02,  3.31it/s] 31%|███       | 181/585 [01:27<02:00,  3.36it/s] 31%|███       | 182/585 [01:28<03:03,  2.19it/s] 31%|███▏      | 183/585 [01:28<02:43,  2.47it/s] 31%|███▏      | 184/585 [01:28<02:28,  2.70it/s] 32%|███▏      | 185/585 [01:29<02:18,  2.89it/s] 32%|███▏      | 186/585 [01:29<02:10,  3.05it/s] 32%|███▏      | 187/585 [01:29<02:05,  3.17it/s] 32%|███▏      | 188/585 [01:30<02:02,  3.25it/s] 32%|███▏      | 189/585 [01:30<01:59,  3.32it/s] 32%|███▏      | 190/585 [01:30<01:57,  3.36it/s] 33%|███▎      | 191/585 [01:30<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:31<01:55,  3.41it/s] 33%|███▎      | 193/585 [01:31<01:54,  3.43it/s] 33%|███▎      | 194/585 [01:31<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:32<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:32<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:32<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:33<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:33<01:51,  3.47it/s] 34%|███▍      | 200/585 [01:33<01:50,  3.47it/s] 34%|███▍      | 201/585 [01:33<01:50,  3.47it/s] 35%|███▍      | 202/585 [01:34<01:54,  3.35it/s] 35%|███▍      | 203/585 [01:34<01:52,  3.39it/s] 35%|███▍      | 204/585 [01:34<01:51,  3.41it/s] 35%|███▌      | 205/585 [01:35<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:35<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:35<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:35<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:36<01:48,  3.47it/s] 36%|███▌      | 210/585 [01:36<01:48,  3.47it/s] 36%|███▌      | 211/585 [01:36<01:47,  3.47it/s] 36%|███▌      | 212/585 [01:37<01:47,  3.47it/s] 36%|███▋      | 213/585 [01:37<01:52,  3.32it/s] 37%|███▋      | 214/585 [01:37<01:50,  3.36it/s] 37%|███▋      | 215/585 [01:37<01:48,  3.40it/s] 37%|███▋      | 216/585 [01:38<01:47,  3.42it/s] 37%|███▋      | 217/585 [01:38<01:47,  3.44it/s] 37%|███▋      | 218/585 [01:38<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:39<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:39<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:39<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:39<01:44,  3.47it/s] 38%|███▊      | 223/585 [01:40<01:44,  3.47it/s] 38%|███▊      | 224/585 [01:40<01:47,  3.36it/s] 38%|███▊      | 225/585 [01:40<01:46,  3.40it/s] 39%|███▊      | 226/585 [01:41<01:45,  3.42it/s] 39%|███▉      | 227/585 [01:41<01:44,  3.44it/s] 39%|███▉      | 228/585 [01:41<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:42<01:43,  3.46it/s] 39%|███▉      | 230/585 [01:42<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:42<01:42,  3.47it/s] 40%|███▉      | 232/585 [01:42<01:41,  3.47it/s] 40%|███▉      | 233/585 [01:43<01:41,  3.47it/s] 40%|████      | 234/585 [01:43<01:41,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 21:17:43,921 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:17:43,921 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:17:43,921 >>   Batch size = 8
{'eval_loss': 1.031351089477539, 'eval_runtime': 9.3596, 'eval_samples_per_second': 373.2, 'eval_steps_per_second': 46.69, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.44it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.98it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.25it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.40it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.99it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.74it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.55it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.17it/s][A
 11%|█         | 48/437 [00:00<00:08, 47.08it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.01it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.06it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.09it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.08it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.11it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 47.01it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.02it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.96it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 47.02it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.92it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.93it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.99it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.95it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 47.03it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 47.02it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.97it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.98it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 47.03it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 47.06it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.87it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.93it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.95it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.98it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.91it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.89it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.82it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.84it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.88it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.85it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.97it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 47.00it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.89it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.97it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.97it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.94it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.99it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 47.04it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.96it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 47.02it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 47.06it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 47.00it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.99it/s][A
 60%|██████    | 263/437 [00:05<00:03, 47.05it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.94it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 47.00it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.96it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.03it/s][A
 67%|██████▋   | 293/437 [00:06<00:04, 34.12it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 37.15it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 39.70it/s][A
 70%|███████   | 308/437 [00:06<00:03, 41.58it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 43.09it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 44.24it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 44.99it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.61it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 45.78it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.12it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.45it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.66it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.66it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.77it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.83it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.91it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.92it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.85it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.83it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.90it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.82it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.88it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.92it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.84it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.84it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.79it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.73it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.94it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.19it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.19it/s][A 40%|████      | 234/585 [01:52<01:41,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:17:53,500 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 21:17:53,608 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:18:01,972 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:18:02,988 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:18:03,265 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:14<56:15,  9.64s/it] 40%|████      | 236/585 [02:15<39:46,  6.84s/it] 41%|████      | 237/585 [02:15<28:15,  4.87s/it] 41%|████      | 238/585 [02:15<20:13,  3.50s/it] 41%|████      | 239/585 [02:16<14:36,  2.53s/it] 41%|████      | 240/585 [02:16<10:41,  1.86s/it] 41%|████      | 241/585 [02:16<07:57,  1.39s/it] 41%|████▏     | 242/585 [02:16<06:02,  1.06s/it] 42%|████▏     | 243/585 [02:17<04:42,  1.21it/s] 42%|████▏     | 244/585 [02:17<03:46,  1.51it/s] 42%|████▏     | 245/585 [02:17<03:07,  1.81it/s] 42%|████▏     | 246/585 [02:18<02:39,  2.12it/s] 42%|████▏     | 247/585 [02:18<03:02,  1.86it/s] 42%|████▏     | 248/585 [02:19<02:36,  2.16it/s] 43%|████▎     | 249/585 [02:19<02:17,  2.44it/s] 43%|████▎     | 250/585 [02:19<02:05,  2.68it/s] 43%|████▎     | 251/585 [02:19<01:55,  2.88it/s] 43%|████▎     | 252/585 [02:20<01:49,  3.04it/s] 43%|████▎     | 253/585 [02:20<01:45,  3.16it/s] 43%|████▎     | 254/585 [02:20<01:41,  3.25it/s] 44%|████▎     | 255/585 [02:21<01:39,  3.32it/s] 44%|████▍     | 256/585 [02:21<01:37,  3.37it/s] 44%|████▍     | 257/585 [02:21<01:47,  3.04it/s] 44%|████▍     | 258/585 [02:22<01:43,  3.16it/s] 44%|████▍     | 259/585 [02:22<01:40,  3.25it/s] 44%|████▍     | 260/585 [02:22<01:38,  3.31it/s] 45%|████▍     | 261/585 [02:22<01:36,  3.36it/s] 45%|████▍     | 262/585 [02:23<01:35,  3.40it/s] 45%|████▍     | 263/585 [02:23<01:34,  3.42it/s] 45%|████▌     | 264/585 [02:23<01:33,  3.44it/s] 45%|████▌     | 265/585 [02:24<01:32,  3.46it/s] 45%|████▌     | 266/585 [02:24<01:32,  3.47it/s] 46%|████▌     | 267/585 [02:24<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:25<01:48,  2.93it/s] 46%|████▌     | 269/585 [02:25<01:42,  3.08it/s] 46%|████▌     | 270/585 [02:25<01:38,  3.19it/s] 46%|████▋     | 271/585 [02:25<01:35,  3.27it/s] 46%|████▋     | 272/585 [02:26<01:33,  3.33it/s] 47%|████▋     | 273/585 [02:26<01:32,  3.38it/s] 47%|████▋     | 274/585 [02:26<01:31,  3.41it/s] 47%|████▋     | 275/585 [02:27<01:30,  3.43it/s] 47%|████▋     | 276/585 [02:27<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:27<01:29,  3.46it/s] 48%|████▊     | 278/585 [02:27<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:28<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:28<01:27,  3.48it/s] 48%|████▊     | 281/585 [02:28<01:27,  3.48it/s] 48%|████▊     | 282/585 [02:29<01:31,  3.32it/s] 48%|████▊     | 283/585 [02:29<01:29,  3.37it/s] 49%|████▊     | 284/585 [02:29<01:28,  3.40it/s] 49%|████▊     | 285/585 [02:30<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:30<01:26,  3.44it/s] 49%|████▉     | 287/585 [02:30<01:26,  3.45it/s] 49%|████▉     | 288/585 [02:30<01:25,  3.46it/s] 49%|████▉     | 289/585 [02:31<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:31<01:25,  3.47it/s] 50%|████▉     | 291/585 [02:31<01:24,  3.47it/s] 50%|████▉     | 292/585 [02:32<01:24,  3.48it/s] 50%|█████     | 293/585 [02:32<01:32,  3.17it/s] 50%|█████     | 294/585 [02:32<01:29,  3.25it/s] 50%|█████     | 295/585 [02:33<01:27,  3.32it/s] 51%|█████     | 296/585 [02:33<01:25,  3.37it/s] 51%|█████     | 297/585 [02:33<01:24,  3.40it/s] 51%|█████     | 298/585 [02:33<01:23,  3.42it/s] 51%|█████     | 299/585 [02:34<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:34<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:34<01:22,  3.46it/s] 52%|█████▏    | 302/585 [02:35<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:35<01:21,  3.47it/s] 52%|█████▏    | 304/585 [02:35<01:22,  3.40it/s] 52%|█████▏    | 305/585 [02:35<01:21,  3.42it/s] 52%|█████▏    | 306/585 [02:36<01:21,  3.44it/s] 52%|█████▏    | 307/585 [02:36<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:36<01:20,  3.46it/s] 53%|█████▎    | 309/585 [02:37<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:37<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:37<01:18,  3.47it/s] 53%|█████▎    | 312/585 [02:37<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:38<01:18,  3.47it/s] 54%|█████▎    | 314/585 [02:38<01:18,  3.47it/s] 54%|█████▍    | 315/585 [02:38<01:20,  3.37it/s] 54%|█████▍    | 316/585 [02:39<01:19,  3.40it/s] 54%|█████▍    | 317/585 [02:39<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:39<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:39<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:40<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:40<01:16,  3.46it/s] 55%|█████▌    | 322/585 [02:40<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:41<01:15,  3.47it/s] 55%|█████▌    | 324/585 [02:41<01:15,  3.47it/s] 56%|█████▌    | 325/585 [02:41<01:14,  3.47it/s] 56%|█████▌    | 326/585 [02:41<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:42<01:18,  3.29it/s] 56%|█████▌    | 328/585 [02:42<01:17,  3.33it/s] 56%|█████▌    | 329/585 [02:42<01:15,  3.37it/s] 56%|█████▋    | 330/585 [02:43<01:15,  3.40it/s] 57%|█████▋    | 331/585 [02:43<01:14,  3.42it/s] 57%|█████▋    | 332/585 [02:43<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:44<01:13,  3.44it/s] 57%|█████▋    | 334/585 [02:44<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:44<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:44<01:11,  3.46it/s] 58%|█████▊    | 337/585 [02:45<01:14,  3.35it/s] 58%|█████▊    | 338/585 [02:45<01:12,  3.39it/s] 58%|█████▊    | 339/585 [02:45<01:12,  3.41it/s] 58%|█████▊    | 340/585 [02:46<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:46<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:46<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:46<01:09,  3.46it/s] 59%|█████▉    | 344/585 [02:47<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:47<01:09,  3.47it/s] 59%|█████▉    | 346/585 [02:47<01:08,  3.47it/s] 59%|█████▉    | 347/585 [02:48<01:08,  3.47it/s] 59%|█████▉    | 348/585 [02:48<01:30,  2.63it/s] 60%|█████▉    | 349/585 [02:49<01:23,  2.84it/s] 60%|█████▉    | 350/585 [02:49<01:18,  3.00it/s] 60%|██████    | 351/585 [02:49<01:14,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 21:18:50,018 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:18:50,018 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:18:50,018 >>   Batch size = 8
{'eval_loss': 1.041603684425354, 'eval_runtime': 9.4514, 'eval_samples_per_second': 369.577, 'eval_steps_per_second': 46.237, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.84it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.14it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.28it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.96it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.61it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.48it/s][A
 10%|▉         | 43/437 [00:00<00:08, 47.13it/s][A
 11%|█         | 48/437 [00:00<00:08, 46.95it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 47.06it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 47.14it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 47.13it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 47.06it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 47.02it/s][A
 18%|█▊        | 78/437 [00:01<00:10, 33.25it/s][A
 19%|█▉        | 83/437 [00:01<00:09, 36.45it/s][A
 20%|██        | 88/437 [00:01<00:08, 39.07it/s][A
 21%|██▏       | 93/437 [00:02<00:08, 41.18it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 42.78it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 44.03it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 44.89it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 45.41it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 45.79it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.10it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.40it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.61it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 46.74it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.85it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.91it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.98it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.91it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.85it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.94it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.92it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.96it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 46.93it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 47.00it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.96it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.99it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.94it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.98it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 45.80it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.24it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.48it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.64it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.71it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.83it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.93it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.99it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.90it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.90it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.86it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.85it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.93it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.96it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 47.00it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.96it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.97it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.92it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.88it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.82it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.77it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 44.42it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.09it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 45.55it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 45.85it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.17it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.48it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.63it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.80it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.67it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.73it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 46.81it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.91it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.91it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.94it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.91it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.97it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.91it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.94it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.89it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.95it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.95it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.94it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.96it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.96it/s][A 60%|██████    | 351/585 [02:59<01:14,  3.13it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:18:59,978 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 21:19:00,409 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:19:05,054 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:19:05,082 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:19:05,097 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:15<30:47,  7.93s/it] 60%|██████    | 353/585 [03:15<21:49,  5.65s/it] 61%|██████    | 354/585 [03:15<15:32,  4.04s/it] 61%|██████    | 355/585 [03:16<11:09,  2.91s/it] 61%|██████    | 356/585 [03:16<08:06,  2.13s/it] 61%|██████    | 357/585 [03:16<05:58,  1.57s/it] 61%|██████    | 358/585 [03:17<04:29,  1.19s/it] 61%|██████▏   | 359/585 [03:17<03:27,  1.09it/s] 62%|██████▏   | 360/585 [03:17<02:43,  1.37it/s] 62%|██████▏   | 361/585 [03:17<02:13,  1.68it/s] 62%|██████▏   | 362/585 [03:18<01:52,  1.99it/s] 62%|██████▏   | 363/585 [03:18<01:37,  2.28it/s] 62%|██████▏   | 364/585 [03:18<01:33,  2.37it/s] 62%|██████▏   | 365/585 [03:19<01:24,  2.62it/s] 63%|██████▎   | 366/585 [03:19<01:17,  2.83it/s] 63%|██████▎   | 367/585 [03:19<01:12,  3.00it/s] 63%|██████▎   | 368/585 [03:19<01:09,  3.13it/s] 63%|██████▎   | 369/585 [03:20<01:06,  3.23it/s] 63%|██████▎   | 370/585 [03:20<01:05,  3.30it/s] 63%|██████▎   | 371/585 [03:20<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:21<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:21<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:21<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:22<01:01,  3.43it/s] 64%|██████▍   | 376/585 [03:22<01:00,  3.44it/s] 64%|██████▍   | 377/585 [03:22<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:22<00:59,  3.46it/s] 65%|██████▍   | 379/585 [03:23<00:59,  3.47it/s] 65%|██████▍   | 380/585 [03:23<00:59,  3.47it/s] 65%|██████▌   | 381/585 [03:23<00:58,  3.48it/s] 65%|██████▌   | 382/585 [03:24<00:58,  3.48it/s] 65%|██████▌   | 383/585 [03:24<00:58,  3.48it/s] 66%|██████▌   | 384/585 [03:24<00:57,  3.48it/s] 66%|██████▌   | 385/585 [03:24<00:57,  3.48it/s] 66%|██████▌   | 386/585 [03:25<01:11,  2.79it/s] 66%|██████▌   | 387/585 [03:25<01:06,  2.96it/s] 66%|██████▋   | 388/585 [03:25<01:03,  3.10it/s] 66%|██████▋   | 389/585 [03:26<01:01,  3.20it/s] 67%|██████▋   | 390/585 [03:26<00:59,  3.28it/s] 67%|██████▋   | 391/585 [03:26<00:58,  3.34it/s] 67%|██████▋   | 392/585 [03:27<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:27<00:56,  3.41it/s] 67%|██████▋   | 394/585 [03:28<01:13,  2.61it/s] 68%|██████▊   | 395/585 [03:28<01:07,  2.83it/s] 68%|██████▊   | 396/585 [03:28<01:03,  2.99it/s] 68%|██████▊   | 397/585 [03:28<01:00,  3.12it/s] 68%|██████▊   | 398/585 [03:29<00:58,  3.22it/s] 68%|██████▊   | 399/585 [03:29<00:56,  3.30it/s] 68%|██████▊   | 400/585 [03:29<00:55,  3.35it/s] 69%|██████▊   | 401/585 [03:30<00:54,  3.39it/s] 69%|██████▊   | 402/585 [03:30<00:53,  3.41it/s] 69%|██████▉   | 403/585 [03:30<00:58,  3.10it/s] 69%|██████▉   | 404/585 [03:30<00:56,  3.21it/s] 69%|██████▉   | 405/585 [03:31<00:54,  3.29it/s] 69%|██████▉   | 406/585 [03:31<00:53,  3.34it/s] 70%|██████▉   | 407/585 [03:31<00:52,  3.38it/s] 70%|██████▉   | 408/585 [03:32<00:51,  3.41it/s] 70%|██████▉   | 409/585 [03:32<00:51,  3.43it/s] 70%|███████   | 410/585 [03:32<00:50,  3.44it/s] 70%|███████   | 411/585 [03:32<00:50,  3.46it/s] 70%|███████   | 412/585 [03:33<00:49,  3.46it/s] 71%|███████   | 413/585 [03:33<00:49,  3.47it/s] 71%|███████   | 414/585 [03:34<00:57,  2.97it/s] 71%|███████   | 415/585 [03:34<00:54,  3.11it/s] 71%|███████   | 416/585 [03:34<00:52,  3.21it/s] 71%|███████▏  | 417/585 [03:34<00:51,  3.28it/s] 71%|███████▏  | 418/585 [03:35<00:50,  3.34it/s] 72%|███████▏  | 419/585 [03:35<00:49,  3.38it/s] 72%|███████▏  | 420/585 [03:35<00:48,  3.41it/s] 72%|███████▏  | 421/585 [03:36<00:47,  3.43it/s] 72%|███████▏  | 422/585 [03:36<00:47,  3.44it/s] 72%|███████▏  | 423/585 [03:36<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:36<00:46,  3.44it/s] 73%|███████▎  | 425/585 [03:37<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:37<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:37<00:45,  3.46it/s] 73%|███████▎  | 428/585 [03:38<00:45,  3.46it/s] 73%|███████▎  | 429/585 [03:38<00:45,  3.46it/s] 74%|███████▎  | 430/585 [03:38<00:44,  3.47it/s] 74%|███████▎  | 431/585 [03:38<00:44,  3.47it/s] 74%|███████▍  | 432/585 [03:39<00:44,  3.47it/s] 74%|███████▍  | 433/585 [03:39<00:43,  3.47it/s] 74%|███████▍  | 434/585 [03:39<00:43,  3.47it/s] 74%|███████▍  | 435/585 [03:40<00:52,  2.85it/s] 75%|███████▍  | 436/585 [03:40<00:49,  3.01it/s] 75%|███████▍  | 437/585 [03:40<00:47,  3.14it/s] 75%|███████▍  | 438/585 [03:41<00:45,  3.23it/s] 75%|███████▌  | 439/585 [03:41<00:44,  3.30it/s] 75%|███████▌  | 440/585 [03:41<00:43,  3.35it/s] 75%|███████▌  | 441/585 [03:42<00:42,  3.39it/s] 76%|███████▌  | 442/585 [03:42<00:43,  3.30it/s] 76%|███████▌  | 443/585 [03:42<00:42,  3.34it/s] 76%|███████▌  | 444/585 [03:42<00:41,  3.38it/s] 76%|███████▌  | 445/585 [03:43<00:41,  3.36it/s] 76%|███████▌  | 446/585 [03:43<00:40,  3.39it/s] 76%|███████▋  | 447/585 [03:43<00:40,  3.42it/s] 77%|███████▋  | 448/585 [03:44<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:44<00:39,  3.44it/s] 77%|███████▋  | 450/585 [03:44<00:39,  3.45it/s] 77%|███████▋  | 451/585 [03:44<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:45<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:45<00:38,  3.47it/s] 78%|███████▊  | 454/585 [03:45<00:37,  3.47it/s] 78%|███████▊  | 455/585 [03:46<00:37,  3.47it/s] 78%|███████▊  | 456/585 [03:46<00:38,  3.35it/s] 78%|███████▊  | 457/585 [03:46<00:37,  3.39it/s] 78%|███████▊  | 458/585 [03:46<00:37,  3.41it/s] 78%|███████▊  | 459/585 [03:47<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:47<00:36,  3.44it/s] 79%|███████▉  | 461/585 [03:47<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:48<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:48<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:48<00:34,  3.47it/s] 79%|███████▉  | 465/585 [03:49<00:34,  3.47it/s] 80%|███████▉  | 466/585 [03:49<00:34,  3.47it/s] 80%|███████▉  | 467/585 [03:49<00:34,  3.45it/s] 80%|████████  | 468/585 [03:49<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:19:50,310 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:19:50,311 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:19:50,311 >>   Batch size = 8
{'eval_loss': 1.0555371046066284, 'eval_runtime': 9.4845, 'eval_samples_per_second': 368.286, 'eval_steps_per_second': 46.075, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.82it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.67it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.98it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.22it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.82it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.52it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.27it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.90it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.94it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.94it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.93it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.89it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.84it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.91it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.97it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 47.02it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.87it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.81it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.80it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.87it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.90it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.95it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 40.50it/s][A
 28%|██▊       | 123/437 [00:02<00:07, 42.24it/s][A
 29%|██▉       | 128/437 [00:02<00:07, 43.43it/s][A
 30%|███       | 133/437 [00:02<00:06, 44.49it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 45.13it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 45.76it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.13it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.36it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 46.45it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.53it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.58it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.74it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.78it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.77it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.86it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.79it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.87it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.90it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.89it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.82it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.93it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.93it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.78it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.89it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.81it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.90it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.87it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.78it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.84it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.83it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.87it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.93it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.82it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.72it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.78it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.82it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.78it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.90it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.89it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.87it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.90it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.85it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.78it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.82it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.86it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.89it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.90it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.85it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.87it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.90it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.84it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.82it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 39.65it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 41.48it/s][A
 90%|████████▉ | 393/437 [00:08<00:01, 43.06it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 44.21it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 44.99it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 45.63it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 45.96it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 46.03it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.32it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.47it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.54it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.54it/s][A 80%|████████  | 468/585 [03:59<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:19:59,836 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:19:59,889 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:20:06,838 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:20:06,915 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:20:06,935 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:17<16:26,  8.51s/it] 80%|████████  | 470/585 [04:17<11:34,  6.04s/it] 81%|████████  | 471/585 [04:18<08:12,  4.32s/it] 81%|████████  | 472/585 [04:18<05:51,  3.11s/it] 81%|████████  | 473/585 [04:18<04:13,  2.26s/it] 81%|████████  | 474/585 [04:18<03:05,  1.67s/it] 81%|████████  | 475/585 [04:19<02:17,  1.25s/it] 81%|████████▏ | 476/585 [04:19<01:45,  1.04it/s] 82%|████████▏ | 477/585 [04:19<01:22,  1.31it/s] 82%|████████▏ | 478/585 [04:20<01:06,  1.62it/s] 82%|████████▏ | 479/585 [04:20<00:55,  1.93it/s] 82%|████████▏ | 480/585 [04:20<00:47,  2.22it/s] 82%|████████▏ | 481/585 [04:21<00:42,  2.44it/s] 82%|████████▏ | 482/585 [04:21<00:38,  2.68it/s] 83%|████████▎ | 483/585 [04:21<00:35,  2.88it/s] 83%|████████▎ | 484/585 [04:21<00:33,  3.04it/s] 83%|████████▎ | 485/585 [04:22<00:31,  3.16it/s] 83%|████████▎ | 486/585 [04:22<00:30,  3.25it/s] 83%|████████▎ | 487/585 [04:22<00:29,  3.31it/s] 83%|████████▎ | 488/585 [04:23<00:28,  3.36it/s] 84%|████████▎ | 489/585 [04:23<00:28,  3.40it/s] 84%|████████▍ | 490/585 [04:23<00:27,  3.42it/s] 84%|████████▍ | 491/585 [04:23<00:27,  3.44it/s] 84%|████████▍ | 492/585 [04:24<00:27,  3.41it/s] 84%|████████▍ | 493/585 [04:24<00:26,  3.43it/s] 84%|████████▍ | 494/585 [04:24<00:26,  3.44it/s] 85%|████████▍ | 495/585 [04:25<00:26,  3.45it/s] 85%|████████▍ | 496/585 [04:25<00:25,  3.46it/s] 85%|████████▍ | 497/585 [04:25<00:25,  3.47it/s] 85%|████████▌ | 498/585 [04:25<00:25,  3.47it/s] 85%|████████▌ | 499/585 [04:26<00:24,  3.47it/s] 85%|████████▌ | 500/585 [04:26<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [04:26<00:24,  3.47it/s] 86%|████████▌ | 501/585 [04:26<00:24,  3.47it/s] 86%|████████▌ | 502/585 [04:27<00:23,  3.47it/s] 86%|████████▌ | 503/585 [04:27<00:24,  3.39it/s] 86%|████████▌ | 504/585 [04:27<00:23,  3.42it/s] 86%|████████▋ | 505/585 [04:27<00:23,  3.43it/s] 86%|████████▋ | 506/585 [04:28<00:22,  3.45it/s] 87%|████████▋ | 507/585 [04:28<00:22,  3.46it/s] 87%|████████▋ | 508/585 [04:28<00:22,  3.46it/s] 87%|████████▋ | 509/585 [04:29<00:21,  3.46it/s] 87%|████████▋ | 510/585 [04:29<00:21,  3.47it/s] 87%|████████▋ | 511/585 [04:29<00:21,  3.47it/s] 88%|████████▊ | 512/585 [04:29<00:21,  3.47it/s] 88%|████████▊ | 513/585 [04:30<00:20,  3.47it/s] 88%|████████▊ | 514/585 [04:30<00:20,  3.47it/s] 88%|████████▊ | 515/585 [04:30<00:20,  3.47it/s] 88%|████████▊ | 516/585 [04:31<00:19,  3.47it/s] 88%|████████▊ | 517/585 [04:31<00:19,  3.47it/s] 89%|████████▊ | 518/585 [04:31<00:19,  3.47it/s] 89%|████████▊ | 519/585 [04:32<00:19,  3.41it/s] 89%|████████▉ | 520/585 [04:32<00:18,  3.43it/s] 89%|████████▉ | 521/585 [04:32<00:18,  3.44it/s] 89%|████████▉ | 522/585 [04:32<00:18,  3.45it/s] 89%|████████▉ | 523/585 [04:33<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:33<00:17,  3.47it/s] 90%|████████▉ | 525/585 [04:33<00:17,  3.47it/s] 90%|████████▉ | 526/585 [04:34<00:17,  3.47it/s] 90%|█████████ | 527/585 [04:34<00:16,  3.47it/s] 90%|█████████ | 528/585 [04:34<00:16,  3.47it/s] 90%|█████████ | 529/585 [04:34<00:16,  3.47it/s] 91%|█████████ | 530/585 [04:35<00:15,  3.45it/s] 91%|█████████ | 531/585 [04:35<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:35<00:15,  3.47it/s] 91%|█████████ | 533/585 [04:36<00:14,  3.47it/s] 91%|█████████▏| 534/585 [04:36<00:14,  3.47it/s] 91%|█████████▏| 535/585 [04:36<00:14,  3.47it/s] 92%|█████████▏| 536/585 [04:36<00:14,  3.47it/s] 92%|█████████▏| 537/585 [04:37<00:13,  3.47it/s] 92%|█████████▏| 538/585 [04:37<00:13,  3.47it/s] 92%|█████████▏| 539/585 [04:37<00:13,  3.47it/s] 92%|█████████▏| 540/585 [04:38<00:12,  3.47it/s] 92%|█████████▏| 541/585 [04:38<00:13,  3.37it/s] 93%|█████████▎| 542/585 [04:38<00:12,  3.40it/s] 93%|█████████▎| 543/585 [04:38<00:12,  3.42it/s] 93%|█████████▎| 544/585 [04:39<00:11,  3.43it/s] 93%|█████████▎| 545/585 [04:39<00:11,  3.44it/s] 93%|█████████▎| 546/585 [04:39<00:11,  3.45it/s] 94%|█████████▎| 547/585 [04:40<00:10,  3.46it/s] 94%|█████████▎| 548/585 [04:40<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:40<00:10,  3.47it/s] 94%|█████████▍| 550/585 [04:40<00:10,  3.47it/s] 94%|█████████▍| 551/585 [04:41<00:09,  3.47it/s] 94%|█████████▍| 552/585 [04:41<00:09,  3.43it/s] 95%|█████████▍| 553/585 [04:41<00:09,  3.44it/s] 95%|█████████▍| 554/585 [04:42<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:42<00:08,  3.36it/s] 95%|█████████▌| 556/585 [04:42<00:08,  3.38it/s] 95%|█████████▌| 557/585 [04:43<00:08,  3.41it/s] 95%|█████████▌| 558/585 [04:43<00:07,  3.42it/s] 96%|█████████▌| 559/585 [04:43<00:07,  3.44it/s] 96%|█████████▌| 560/585 [04:43<00:07,  3.45it/s] 96%|█████████▌| 561/585 [04:44<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:44<00:06,  3.46it/s] 96%|█████████▌| 563/585 [04:44<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:45<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:45<00:05,  3.45it/s] 97%|█████████▋| 566/585 [04:45<00:05,  3.45it/s] 97%|█████████▋| 567/585 [04:45<00:05,  3.46it/s] 97%|█████████▋| 568/585 [04:46<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:46<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:46<00:04,  3.47it/s] 98%|█████████▊| 571/585 [04:47<00:04,  3.47it/s] 98%|█████████▊| 572/585 [04:47<00:03,  3.47it/s] 98%|█████████▊| 573/585 [04:47<00:03,  3.47it/s] 98%|█████████▊| 574/585 [04:47<00:03,  3.44it/s] 98%|█████████▊| 575/585 [04:48<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:48<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:48<00:02,  3.46it/s] 99%|█████████▉| 578/585 [04:49<00:02,  3.46it/s] 99%|█████████▉| 579/585 [04:49<00:01,  3.47it/s] 99%|█████████▉| 580/585 [04:49<00:01,  3.47it/s] 99%|█████████▉| 581/585 [04:49<00:01,  3.47it/s] 99%|█████████▉| 582/585 [04:50<00:00,  3.47it/s]100%|█████████▉| 583/585 [04:50<00:00,  3.47it/s]100%|█████████▉| 584/585 [04:50<00:00,  3.47it/s]100%|██████████| 585/585 [04:51<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:20:51,522 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:20:51,522 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:20:51,522 >>   Batch size = 8
{'eval_loss': 1.063217282295227, 'eval_runtime': 9.4482, 'eval_samples_per_second': 369.698, 'eval_steps_per_second': 46.252, 'epoch': 4.0}
{'loss': 0.5384, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.33it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.77it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.95it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.19it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.79it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.43it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.24it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.97it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.90it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.81it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.81it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.87it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.88it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.88it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.87it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.95it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.78it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.82it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.70it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.79it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.87it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.87it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.86it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.94it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.79it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.81it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.83it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.75it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.85it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.85it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.73it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.83it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.89it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.86it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.89it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.87it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.79it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.82it/s][A
 46%|████▋     | 203/437 [00:04<00:04, 46.85it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.78it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.79it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.86it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.91it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.85it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.90it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.90it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.84it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.74it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.78it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.79it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.78it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.85it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.89it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.77it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.83it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.82it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.71it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.81it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.86it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.70it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.80it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.87it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.87it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.83it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.80it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.80it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.88it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.83it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.77it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.81it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.84it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.88it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.83it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.72it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.76it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.80it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.80it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.86it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.73it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.71it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.82it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.81it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.79it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.89it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.84it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.84it/s][A100%|██████████| 585/585 [05:00<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:21:00,968 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 21:21:01,063 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:06,906 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:06,935 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:06,944 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:21:16,942 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:21:16,951 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117 (score: 1.031351089477539).
                                                 100%|██████████| 585/585 [05:21<00:00,  3.46it/s]100%|██████████| 585/585 [05:21<00:00,  1.82it/s]
[INFO|trainer.py:1894] 2023-08-28 21:21:22,384 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:21:22,628 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:21:27,305 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:21:27,334 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:21:27,349 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:21:27,549 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   train_loss               =     0.5343
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   train_runtime            = 0:05:21.93
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   train_samples_per_second =    116.466
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:27,549 >>   train_steps_per_second   =      1.817
{'eval_loss': 1.0715422630310059, 'eval_runtime': 9.3277, 'eval_samples_per_second': 374.475, 'eval_steps_per_second': 46.85, 'epoch': 5.0}
{'train_runtime': 321.9387, 'train_samples_per_second': 116.466, 'train_steps_per_second': 1.817, 'train_loss': 0.5343313494299212, 'epoch': 5.0}
08/28/2023 21:21:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:21:27,586 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:21:27,586 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:21:27,587 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 59.02it/s]  3%|▎         | 12/437 [00:00<00:08, 51.57it/s]  4%|▍         | 18/437 [00:00<00:08, 49.35it/s]  5%|▌         | 23/437 [00:00<00:08, 48.71it/s]  6%|▋         | 28/437 [00:00<00:08, 48.13it/s]  8%|▊         | 33/437 [00:00<00:08, 47.96it/s]  9%|▊         | 38/437 [00:00<00:08, 47.78it/s] 10%|▉         | 43/437 [00:00<00:08, 47.58it/s] 11%|█         | 48/437 [00:00<00:08, 47.27it/s] 12%|█▏        | 53/437 [00:01<00:08, 47.31it/s] 13%|█▎        | 58/437 [00:01<00:08, 47.30it/s] 14%|█▍        | 63/437 [00:01<00:07, 47.24it/s] 16%|█▌        | 68/437 [00:01<00:07, 47.24it/s] 17%|█▋        | 73/437 [00:01<00:07, 47.24it/s] 18%|█▊        | 78/437 [00:01<00:07, 47.18it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.24it/s] 20%|██        | 88/437 [00:01<00:07, 47.23it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.11it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.09it/s] 24%|██▎       | 103/437 [00:02<00:07, 47.12it/s] 25%|██▍       | 108/437 [00:02<00:06, 47.17it/s] 26%|██▌       | 113/437 [00:02<00:06, 47.17it/s] 27%|██▋       | 118/437 [00:02<00:06, 47.16it/s] 28%|██▊       | 123/437 [00:02<00:06, 47.16it/s] 29%|██▉       | 128/437 [00:02<00:06, 47.25it/s] 30%|███       | 133/437 [00:02<00:06, 47.10it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.51it/s] 33%|███▎      | 143/437 [00:03<00:06, 46.67it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.86it/s] 35%|███▌      | 153/437 [00:03<00:06, 46.91it/s] 36%|███▌      | 158/437 [00:03<00:05, 47.04it/s] 37%|███▋      | 163/437 [00:03<00:05, 47.02it/s] 38%|███▊      | 168/437 [00:03<00:05, 47.11it/s] 40%|███▉      | 173/437 [00:03<00:05, 47.10it/s] 41%|████      | 178/437 [00:03<00:05, 47.03it/s] 42%|████▏     | 183/437 [00:03<00:05, 47.03it/s] 43%|████▎     | 188/437 [00:03<00:05, 47.05it/s] 44%|████▍     | 193/437 [00:04<00:05, 47.06it/s] 45%|████▌     | 198/437 [00:04<00:05, 47.01it/s] 46%|████▋     | 203/437 [00:04<00:04, 47.03it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.13it/s] 49%|████▊     | 213/437 [00:04<00:04, 47.16it/s] 50%|████▉     | 218/437 [00:04<00:04, 47.12it/s] 51%|█████     | 223/437 [00:04<00:04, 47.05it/s] 52%|█████▏    | 228/437 [00:04<00:04, 47.13it/s] 53%|█████▎    | 233/437 [00:04<00:04, 47.11it/s] 54%|█████▍    | 238/437 [00:05<00:04, 47.04it/s] 56%|█████▌    | 243/437 [00:05<00:04, 47.09it/s] 57%|█████▋    | 248/437 [00:05<00:04, 47.05it/s] 58%|█████▊    | 253/437 [00:05<00:03, 47.17it/s] 59%|█████▉    | 258/437 [00:05<00:03, 47.11it/s] 60%|██████    | 263/437 [00:05<00:03, 47.22it/s] 61%|██████▏   | 268/437 [00:05<00:03, 47.08it/s] 62%|██████▏   | 273/437 [00:05<00:03, 47.12it/s] 64%|██████▎   | 278/437 [00:05<00:03, 47.09it/s] 65%|██████▍   | 283/437 [00:05<00:03, 47.09it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.01it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.43it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.66it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.87it/s] 70%|███████   | 308/437 [00:06<00:02, 46.92it/s] 72%|███████▏  | 313/437 [00:06<00:02, 47.01it/s] 73%|███████▎  | 318/437 [00:06<00:02, 47.08it/s] 74%|███████▍  | 323/437 [00:06<00:02, 47.05it/s] 75%|███████▌  | 328/437 [00:06<00:02, 47.02it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.99it/s] 77%|███████▋  | 338/437 [00:07<00:02, 47.05it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.96it/s] 80%|███████▉  | 348/437 [00:07<00:01, 47.04it/s] 81%|████████  | 353/437 [00:07<00:01, 47.07it/s] 82%|████████▏ | 358/437 [00:07<00:01, 47.12it/s] 83%|████████▎ | 363/437 [00:07<00:01, 47.11it/s] 84%|████████▍ | 368/437 [00:07<00:01, 47.12it/s] 85%|████████▌ | 373/437 [00:07<00:01, 47.03it/s] 86%|████████▋ | 378/437 [00:08<00:01, 47.01it/s] 88%|████████▊ | 383/437 [00:08<00:01, 47.09it/s] 89%|████████▉ | 388/437 [00:08<00:01, 47.06it/s] 90%|████████▉ | 393/437 [00:08<00:00, 47.03it/s] 91%|█████████ | 398/437 [00:08<00:00, 47.06it/s] 92%|█████████▏| 403/437 [00:08<00:00, 47.05it/s] 93%|█████████▎| 408/437 [00:08<00:00, 47.13it/s] 95%|█████████▍| 413/437 [00:08<00:00, 47.09it/s] 96%|█████████▌| 418/437 [00:08<00:00, 47.11it/s] 97%|█████████▋| 423/437 [00:08<00:00, 46.96it/s] 98%|█████████▊| 428/437 [00:09<00:00, 47.08it/s] 99%|█████████▉| 433/437 [00:09<00:00, 39.44it/s]100%|██████████| 437/437 [00:09<00:00, 46.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:21:36,942 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   eval_loss               =     1.0314
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   eval_runtime            = 0:00:09.35
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   eval_samples_per_second =    373.347
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   eval_steps_per_second   =     46.709
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:21:36,943 >>   perplexity              =     2.8049
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:43,906 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:43,912 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:43,912 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:43,912 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:43,912 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:21:44,239 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:21:44,240 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:44,520 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:45,548 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:45,548 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:47,319 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:47,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:47,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:47,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:47,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:21:47,656 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:21:47,657 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:47,934 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:48,108 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:48,108 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.37it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:05,  1.45it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:07,  1.48it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:09,  1.54it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:10,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.55it/s]Extractor Predicting: 18it [00:12,  1.54it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:16,  1.55it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.55it/s]Extractor Predicting: 27it [00:18,  1.55it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:20,  1.59it/s]Extractor Predicting: 31it [00:20,  1.59it/s]Extractor Predicting: 32it [00:21,  1.58it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.57it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.56it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.53it/s]Extractor Predicting: 47it [00:31,  1.53it/s]Extractor Predicting: 48it [00:31,  1.58it/s]Extractor Predicting: 49it [00:32,  1.60it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:34,  1.58it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:36,  1.59it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:44,  1.46it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:48,  1.53it/s]Extractor Predicting: 74it [00:48,  1.51it/s]Extractor Predicting: 75it [00:49,  1.54it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.47it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.38it/s]Extractor Predicting: 90it [00:59,  1.37it/s]Extractor Predicting: 91it [01:00,  1.39it/s]Extractor Predicting: 92it [01:01,  1.43it/s]Extractor Predicting: 93it [01:01,  1.43it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:04,  1.50it/s]Extractor Predicting: 98it [01:05,  1.50it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.51it/s]Extractor Predicting: 101it [01:07,  1.51it/s]Extractor Predicting: 102it [01:07,  1.49it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:09,  1.48it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:13,  1.46it/s]Extractor Predicting: 111it [01:13,  1.46it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.44it/s]Extractor Predicting: 114it [01:15,  1.44it/s]Extractor Predicting: 115it [01:16,  1.42it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:18,  1.43it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.44it/s]Extractor Predicting: 120it [01:20,  1.45it/s]Extractor Predicting: 121it [01:20,  1.44it/s]Extractor Predicting: 122it [01:21,  1.43it/s]Extractor Predicting: 123it [01:22,  1.49it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.45it/s]Extractor Predicting: 126it [01:24,  1.46it/s]Extractor Predicting: 127it [01:24,  1.46it/s]Extractor Predicting: 128it [01:25,  1.46it/s]Extractor Predicting: 129it [01:26,  1.45it/s]Extractor Predicting: 130it [01:26,  1.44it/s]Extractor Predicting: 131it [01:27,  1.42it/s]Extractor Predicting: 132it [01:28,  1.41it/s]Extractor Predicting: 133it [01:29,  1.41it/s]Extractor Predicting: 134it [01:29,  1.39it/s]Extractor Predicting: 135it [01:30,  1.39it/s]Extractor Predicting: 136it [01:30,  1.70it/s]Extractor Predicting: 136it [01:30,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:29,541 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:29,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:29,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:29,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:29,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:23:30,262 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:23:30,263 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:31,141 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:32,155 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:32,155 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:35,173 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:35,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:35,175 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:35,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:23:35,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:23:35,824 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:23:35,825 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:23:36,533 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:23:36,700 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:23:36,701 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6143724696356275,
  "recall": 0.17377612367592327,
  "score": 0.27092166927025213,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.63it/s]Extractor Predicting: 33it [00:21,  1.66it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:26,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:27,  1.65it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:30,  1.62it/s]Extractor Predicting: 49it [00:31,  1.63it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.44it/s]Extractor Predicting: 52it [00:33,  1.50it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.48it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.52it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.62it/s]Extractor Predicting: 69it [00:43,  1.64it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.60it/s]Extractor Predicting: 73it [00:46,  1.62it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.58it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:49,  1.58it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.60it/s]Extractor Predicting: 81it [00:51,  1.59it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:56,  1.52it/s]Extractor Predicting: 90it [00:57,  1.50it/s]Extractor Predicting: 91it [00:58,  1.51it/s]Extractor Predicting: 92it [00:58,  1.48it/s]Extractor Predicting: 93it [00:59,  1.46it/s]Extractor Predicting: 94it [01:00,  1.47it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:01,  1.50it/s]Extractor Predicting: 97it [01:02,  1.48it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:03,  1.45it/s]Extractor Predicting: 100it [01:04,  1.45it/s]Extractor Predicting: 101it [01:05,  1.31it/s]Extractor Predicting: 102it [01:05,  1.34it/s]Extractor Predicting: 103it [01:06,  1.36it/s]Extractor Predicting: 104it [01:07,  1.39it/s]Extractor Predicting: 105it [01:07,  1.42it/s]Extractor Predicting: 106it [01:08,  1.40it/s]Extractor Predicting: 107it [01:09,  1.42it/s]Extractor Predicting: 108it [01:10,  1.40it/s]Extractor Predicting: 109it [01:10,  1.41it/s]Extractor Predicting: 110it [01:11,  1.41it/s]Extractor Predicting: 111it [01:12,  1.43it/s]Extractor Predicting: 112it [01:12,  1.42it/s]Extractor Predicting: 113it [01:13,  1.43it/s]Extractor Predicting: 114it [01:14,  1.43it/s]Extractor Predicting: 115it [01:15,  1.43it/s]Extractor Predicting: 116it [01:15,  1.44it/s]Extractor Predicting: 117it [01:16,  1.44it/s]Extractor Predicting: 118it [01:17,  1.44it/s]Extractor Predicting: 119it [01:17,  1.47it/s]Extractor Predicting: 120it [01:18,  1.52it/s]Extractor Predicting: 121it [01:19,  1.51it/s]Extractor Predicting: 122it [01:19,  1.50it/s]Extractor Predicting: 123it [01:20,  1.49it/s]Extractor Predicting: 124it [01:21,  1.48it/s]Extractor Predicting: 125it [01:21,  1.51it/s]Extractor Predicting: 126it [01:22,  1.55it/s]Extractor Predicting: 127it [01:22,  1.55it/s]Extractor Predicting: 128it [01:23,  1.57it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:24,  1.51it/s]Extractor Predicting: 131it [01:25,  1.52it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:26,  1.55it/s]Extractor Predicting: 134it [01:27,  1.57it/s]Extractor Predicting: 135it [01:28,  1.55it/s]Extractor Predicting: 136it [01:28,  1.56it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:30,  1.56it/s]Extractor Predicting: 140it [01:31,  1.55it/s]Extractor Predicting: 141it [01:32,  1.50it/s]Extractor Predicting: 142it [01:32,  1.50it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 144it [01:34,  1.50it/s]Extractor Predicting: 145it [01:34,  1.49it/s]Extractor Predicting: 146it [01:35,  1.53it/s]Extractor Predicting: 147it [01:35,  1.54it/s]Extractor Predicting: 148it [01:36,  1.52it/s]Extractor Predicting: 149it [01:37,  1.52it/s]Extractor Predicting: 150it [01:37,  1.51it/s]Extractor Predicting: 151it [01:38,  1.47it/s]Extractor Predicting: 152it [01:39,  1.49it/s]Extractor Predicting: 153it [01:40,  1.49it/s]Extractor Predicting: 154it [01:40,  1.52it/s]Extractor Predicting: 155it [01:41,  1.49it/s]Extractor Predicting: 156it [01:42,  1.49it/s]Extractor Predicting: 157it [01:42,  1.47it/s]Extractor Predicting: 158it [01:43,  1.49it/s]Extractor Predicting: 159it [01:44,  1.34it/s]Extractor Predicting: 160it [01:45,  1.37it/s]Extractor Predicting: 161it [01:45,  1.41it/s]Extractor Predicting: 162it [01:46,  1.42it/s]Extractor Predicting: 163it [01:47,  1.43it/s]Extractor Predicting: 164it [01:47,  1.39it/s]Extractor Predicting: 165it [01:48,  1.41it/s]Extractor Predicting: 166it [01:49,  1.44it/s]Extractor Predicting: 167it [01:49,  1.44it/s]Extractor Predicting: 168it [01:50,  1.45it/s]Extractor Predicting: 169it [01:51,  1.17it/s]Extractor Predicting: 170it [01:52,  1.24it/s]Extractor Predicting: 171it [01:53,  1.30it/s]Extractor Predicting: 172it [01:53,  1.34it/s]Extractor Predicting: 173it [01:54,  1.33it/s]Extractor Predicting: 174it [01:55,  1.37it/s]Extractor Predicting: 175it [01:55,  1.44it/s]Extractor Predicting: 176it [01:56,  1.45it/s]Extractor Predicting: 177it [01:57,  1.48it/s]Extractor Predicting: 178it [01:57,  1.49it/s]Extractor Predicting: 179it [01:58,  1.52it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [01:59,  1.53it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:01,  1.50it/s]Extractor Predicting: 184it [02:01,  1.56it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 186it [02:03,  1.56it/s]Extractor Predicting: 187it [02:03,  1.55it/s]Extractor Predicting: 188it [02:04,  1.51it/s]Extractor Predicting: 189it [02:05,  1.50it/s]Extractor Predicting: 190it [02:05,  1.51it/s]Extractor Predicting: 191it [02:06,  1.49it/s]Extractor Predicting: 192it [02:07,  1.50it/s]Extractor Predicting: 193it [02:07,  1.53it/s]Extractor Predicting: 194it [02:08,  1.52it/s]Extractor Predicting: 195it [02:09,  1.50it/s]Extractor Predicting: 196it [02:09,  1.47it/s]Extractor Predicting: 197it [02:10,  1.49it/s]Extractor Predicting: 198it [02:11,  1.46it/s]Extractor Predicting: 199it [02:12,  1.35it/s]Extractor Predicting: 200it [02:12,  1.38it/s]Extractor Predicting: 201it [02:13,  1.42it/s]Extractor Predicting: 202it [02:13,  1.45it/s]Extractor Predicting: 203it [02:14,  1.45it/s]Extractor Predicting: 204it [02:15,  1.50it/s]Extractor Predicting: 205it [02:15,  1.49it/s]Extractor Predicting: 206it [02:16,  1.53it/s]Extractor Predicting: 207it [02:17,  1.51it/s]Extractor Predicting: 208it [02:17,  1.49it/s]Extractor Predicting: 209it [02:18,  1.51it/s]Extractor Predicting: 210it [02:19,  1.51it/s]Extractor Predicting: 211it [02:19,  1.50it/s]Extractor Predicting: 212it [02:20,  1.52it/s]Extractor Predicting: 213it [02:21,  1.53it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:22,  1.53it/s]Extractor Predicting: 216it [02:23,  1.53it/s]Extractor Predicting: 217it [02:23,  1.51it/s]Extractor Predicting: 218it [02:24,  1.52it/s]Extractor Predicting: 219it [02:25,  1.50it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:26,  1.56it/s]Extractor Predicting: 222it [02:27,  1.55it/s]Extractor Predicting: 223it [02:27,  1.50it/s]Extractor Predicting: 224it [02:28,  1.55it/s]Extractor Predicting: 225it [02:29,  1.53it/s]Extractor Predicting: 226it [02:29,  1.52it/s]Extractor Predicting: 227it [02:30,  1.53it/s]Extractor Predicting: 228it [02:31,  1.55it/s]Extractor Predicting: 229it [02:31,  1.54it/s]Extractor Predicting: 230it [02:32,  1.55it/s]Extractor Predicting: 231it [02:32,  1.56it/s]Extractor Predicting: 232it [02:33,  1.55it/s]Extractor Predicting: 233it [02:34,  1.56it/s]Extractor Predicting: 234it [02:34,  1.54it/s]Extractor Predicting: 235it [02:35,  1.50it/s]Extractor Predicting: 236it [02:36,  1.51it/s]Extractor Predicting: 237it [02:36,  1.52it/s]Extractor Predicting: 238it [02:37,  1.51it/s]Extractor Predicting: 239it [02:38,  1.50it/s]Extractor Predicting: 240it [02:39,  1.37it/s]Extractor Predicting: 241it [02:39,  1.41it/s]Extractor Predicting: 242it [02:40,  1.44it/s]Extractor Predicting: 243it [02:41,  1.47it/s]Extractor Predicting: 244it [02:41,  1.46it/s]Extractor Predicting: 245it [02:42,  1.49it/s]Extractor Predicting: 246it [02:43,  1.49it/s]Extractor Predicting: 247it [02:43,  1.51it/s]Extractor Predicting: 248it [02:44,  1.50it/s]Extractor Predicting: 249it [02:45,  1.53it/s]Extractor Predicting: 250it [02:45,  1.50it/s]Extractor Predicting: 251it [02:46,  1.54it/s]Extractor Predicting: 252it [02:47,  1.53it/s]Extractor Predicting: 253it [02:47,  1.52it/s]Extractor Predicting: 254it [02:48,  1.53it/s]Extractor Predicting: 255it [02:49,  1.50it/s]Extractor Predicting: 256it [02:49,  1.48it/s]Extractor Predicting: 257it [02:50,  1.49it/s]Extractor Predicting: 258it [02:51,  1.50it/s]Extractor Predicting: 259it [02:51,  1.54it/s]Extractor Predicting: 260it [02:52,  1.52it/s]Extractor Predicting: 261it [02:52,  1.55it/s]Extractor Predicting: 262it [02:53,  1.53it/s]Extractor Predicting: 263it [02:54,  1.52it/s]Extractor Predicting: 264it [02:54,  1.59it/s]Extractor Predicting: 265it [02:55,  1.51it/s]Extractor Predicting: 266it [02:56,  1.54it/s]Extractor Predicting: 267it [02:56,  1.54it/s]Extractor Predicting: 268it [02:57,  1.54it/s]Extractor Predicting: 269it [02:58,  1.53it/s]Extractor Predicting: 270it [02:58,  1.47it/s]Extractor Predicting: 271it [02:59,  1.46it/s]Extractor Predicting: 272it [03:00,  1.49it/s]Extractor Predicting: 273it [03:00,  1.49it/s]Extractor Predicting: 274it [03:01,  1.47it/s]Extractor Predicting: 275it [03:02,  1.45it/s]Extractor Predicting: 276it [03:03,  1.45it/s]Extractor Predicting: 277it [03:03,  1.46it/s]Extractor Predicting: 278it [03:04,  1.50it/s]Extractor Predicting: 279it [03:04,  1.50it/s]Extractor Predicting: 280it [03:05,  1.50it/s]Extractor Predicting: 281it [03:06,  1.37it/s]Extractor Predicting: 282it [03:07,  1.41it/s]Extractor Predicting: 283it [03:07,  1.43it/s]Extractor Predicting: 284it [03:08,  1.46it/s]Extractor Predicting: 285it [03:09,  1.47it/s]Extractor Predicting: 286it [03:09,  1.51it/s]Extractor Predicting: 287it [03:10,  1.49it/s]Extractor Predicting: 288it [03:10,  1.69it/s]Extractor Predicting: 288it [03:10,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:57,112 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:57,124 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:57,124 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:57,124 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:26:57,124 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:26:57,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:26:57,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:26:58,346 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:26:59,389 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:26:59,389 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:27:02,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:27:02,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:27:02,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:27:02,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:27:02,442 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:27:03,063 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:27:03,064 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:27:03,674 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:27:03,819 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:27:03,819 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5604066985645934,
  "recall": 0.13579710144927537,
  "score": 0.21861875874941672,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:01,  2.28it/s]Extractor Predicting: 3it [00:01,  1.97it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:27:05,848 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:27:05,849 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:27:05,853 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:27:05,853 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:27:05,856 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:27:11,272 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:27:11,272 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:27:11,286 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:27:11,286 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:27:11,291 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:27:11,295 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 1.3333333333333333,
  "recall": 0.04,
  "score": 0.07766990291262135,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:27:11,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:12,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:12,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:13,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:14,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:15,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:15,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:16,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:17,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:18,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:18,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:19,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:20,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:20,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:21,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:22,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:23,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:24,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:24,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:25,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:26,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:38, 15.60s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:27,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:27,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:28,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:29,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:30,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:31,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:32,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:32,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:33,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:34,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:35,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:36,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:37,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:38,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:39,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:41,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:41,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:43,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:44,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:39, 16.85s/it][WARNING|generation_utils.py:914] 2023-08-28 21:27:44,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:45,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:46,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:46,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:48,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:48,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:49,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:50,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:51,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:51,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:52,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:53,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:54,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:54,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:55,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:56,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:56,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:57,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:58,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:58,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:27:59,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:14, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:00,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:00,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:01,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:02,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:03,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:04,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:05,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:05,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:06,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:07,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:07,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:08,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:09,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:10,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:10,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:11,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:12,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:12,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:13,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:14,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:14,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:03<02:53, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:15,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:16,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:16,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:17,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:18,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:19,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:19,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:20,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:20,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:21,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:22,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:23,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:23,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:24,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:25,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:26,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:26,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:27,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:28,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:28,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:17<02:31, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:29,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:30,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:30,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:31,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:32,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:32,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:33,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:34,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:35,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:35,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:36,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:37,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:37,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:38,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:39,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:39,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:40,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:41,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:41,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:42,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:31<02:11, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:43,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:44,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:45,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:46,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:46,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:47,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:48,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:49,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:49,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:50,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:51,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:52,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:53,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:53,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:54,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:55,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:56,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:57,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:57,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:47<01:59, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 21:28:58,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:28:59,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:00,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:00,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:01,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:02,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:03,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:04,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:05,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:05,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:06,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:07,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:08,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:08,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:09,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:10,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:11,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:12,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:12,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:13,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:14,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:14,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:04<01:49, 15.61s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:15,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:16,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:17,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:17,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:18,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:19,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:20,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:21,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:22,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:22,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:23,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:25,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:26,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:27,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:27,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:28,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:29,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:30,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:30,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:31,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:21<01:36, 16.06s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:32,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:33,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:34,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:34,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:35,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:36,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:37,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:38,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:39,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:39,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:40,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:41,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:41,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:42,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:43,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:44,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:44,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:45,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:46,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:35<01:17, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-28 21:29:46,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:47,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:48,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:49,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:50,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:50,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:51,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:53,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:53,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:54,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:55,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:55,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:56,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:57,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:57,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:58,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:29:59,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:00,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:00,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:01,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:50<01:01, 15.43s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:02,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:03,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:03,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:04,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:05,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:05,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:07,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:07,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:08,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:09,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:10,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:10,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:11,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:12,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:13,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:13,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:14,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:15,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:15,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:16,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:17,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:06<00:46, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:18,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:18,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:19,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:20,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:20,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:21,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:22,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:22,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:23,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:24,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:25,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:26,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:26,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:27,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:28,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:29,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:30,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:31,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:20<00:30, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:31,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:32,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:33,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:34,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:35,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:36,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:36,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:37,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:38,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:39,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:40,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:41,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:42,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:43,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:44,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:44,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:45,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:45,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:46,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:35<00:15, 15.19s/it][WARNING|generation_utils.py:914] 2023-08-28 21:30:47,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:48,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:48,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:49,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:50,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:50,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:51,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:52,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:52,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:54,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:54,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:55,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:56,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:57,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:57,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:58,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:59,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:30:59,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:31:00,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:49<00:00, 14.77s/it]Generating: 100%|██████████| 15/15 [03:49<00:00, 15.32s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:10,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:10,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:10,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:10,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:10,951 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:31:11,782 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:31:11,783 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:31:13,027 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:31:14,078 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:31:14,169 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:19,260 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:19,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:19,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:19,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:31:19,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:31:21,926 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:31:21,927 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:31:22,776 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:31:22,936 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:31:22,936 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8835227272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : work location . Context : Later in 2008 , the film became a best -seller in France and other markets at the time . Head Entity : best - selling film , Tail Entity : France .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 8835
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8935, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.28it/s]Extractor Estimating: 2it [00:01,  1.31it/s]Extractor Estimating: 3it [00:02,  1.37it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.39it/s]Extractor Estimating: 6it [00:04,  1.39it/s]Extractor Estimating: 7it [00:04,  1.44it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.53it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.45it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:08,  1.51it/s]Extractor Estimating: 14it [00:09,  1.56it/s]Extractor Estimating: 15it [00:10,  1.53it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.48it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.47it/s]Extractor Estimating: 21it [00:14,  1.49it/s]Extractor Estimating: 22it [00:14,  1.53it/s]Extractor Estimating: 23it [00:15,  1.51it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.57it/s]Extractor Estimating: 27it [00:18,  1.37it/s]Extractor Estimating: 28it [00:19,  1.42it/s]Extractor Estimating: 29it [00:19,  1.44it/s]Extractor Estimating: 30it [00:20,  1.51it/s]Extractor Estimating: 31it [00:20,  1.53it/s]Extractor Estimating: 32it [00:21,  1.58it/s]Extractor Estimating: 33it [00:22,  1.61it/s]Extractor Estimating: 34it [00:22,  1.61it/s]Extractor Estimating: 35it [00:23,  1.46it/s]Extractor Estimating: 36it [00:24,  1.51it/s]Extractor Estimating: 37it [00:24,  1.55it/s]Extractor Estimating: 38it [00:25,  1.54it/s]Extractor Estimating: 39it [00:26,  1.47it/s]Extractor Estimating: 40it [00:26,  1.49it/s]Extractor Estimating: 41it [00:27,  1.51it/s]Extractor Estimating: 42it [00:28,  1.55it/s]Extractor Estimating: 43it [00:28,  1.61it/s]Extractor Estimating: 44it [00:29,  1.58it/s]Extractor Estimating: 45it [00:30,  1.44it/s]Extractor Estimating: 46it [00:30,  1.48it/s]Extractor Estimating: 47it [00:31,  1.56it/s]Extractor Estimating: 48it [00:32,  1.50it/s]Extractor Estimating: 49it [00:32,  1.53it/s]Extractor Estimating: 50it [00:33,  1.55it/s]Extractor Estimating: 51it [00:34,  1.54it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.57it/s]Extractor Estimating: 54it [00:35,  1.59it/s]Extractor Estimating: 55it [00:36,  1.60it/s]Extractor Estimating: 56it [00:37,  1.57it/s]Extractor Estimating: 57it [00:37,  1.58it/s]Extractor Estimating: 58it [00:38,  1.62it/s]Extractor Estimating: 59it [00:38,  1.62it/s]Extractor Estimating: 60it [00:39,  1.57it/s]Extractor Estimating: 61it [00:40,  1.54it/s]Extractor Estimating: 62it [00:40,  1.55it/s]Extractor Estimating: 63it [00:41,  1.59it/s]Extractor Estimating: 64it [00:42,  1.59it/s]Extractor Estimating: 65it [00:42,  1.59it/s]Extractor Estimating: 66it [00:43,  1.59it/s]Extractor Estimating: 67it [00:44,  1.63it/s]Extractor Estimating: 68it [00:44,  1.62it/s]Extractor Estimating: 69it [00:45,  1.66it/s]Extractor Estimating: 70it [00:45,  1.60it/s]Extractor Estimating: 71it [00:46,  1.66it/s]Extractor Estimating: 72it [00:47,  1.63it/s]Extractor Estimating: 73it [00:47,  1.63it/s]Extractor Estimating: 74it [00:48,  1.61it/s]Extractor Estimating: 75it [00:48,  1.61it/s]Extractor Estimating: 76it [00:49,  1.67it/s]Extractor Estimating: 77it [00:50,  1.75it/s]Extractor Estimating: 78it [00:50,  1.78it/s]Extractor Estimating: 79it [00:51,  1.78it/s]Extractor Estimating: 80it [00:51,  1.77it/s]Extractor Estimating: 81it [00:52,  1.80it/s]Extractor Estimating: 82it [00:52,  1.85it/s]Extractor Estimating: 83it [00:53,  1.88it/s]Extractor Estimating: 84it [00:53,  1.91it/s]Extractor Estimating: 85it [00:54,  1.79it/s]Extractor Estimating: 86it [00:54,  1.82it/s]Extractor Estimating: 87it [00:55,  1.82it/s]Extractor Estimating: 88it [00:55,  1.86it/s]Extractor Estimating: 89it [00:56,  1.81it/s]Extractor Estimating: 90it [00:57,  1.74it/s]Extractor Estimating: 91it [00:57,  1.81it/s]Extractor Estimating: 92it [00:58,  1.80it/s]Extractor Estimating: 93it [00:58,  1.85it/s]Extractor Estimating: 94it [00:59,  1.85it/s]Extractor Estimating: 95it [00:59,  1.82it/s]Extractor Estimating: 96it [01:00,  1.77it/s]Extractor Estimating: 97it [01:01,  1.76it/s]Extractor Estimating: 98it [01:01,  1.88it/s]Extractor Estimating: 99it [01:02,  1.89it/s]Extractor Estimating: 100it [01:02,  1.84it/s]Extractor Estimating: 101it [01:03,  1.80it/s]Extractor Estimating: 102it [01:03,  1.69it/s]Extractor Estimating: 103it [01:04,  1.73it/s]Extractor Estimating: 104it [01:04,  1.72it/s]Extractor Estimating: 105it [01:05,  1.71it/s]Extractor Estimating: 106it [01:06,  1.74it/s]Extractor Estimating: 107it [01:06,  1.65it/s]Extractor Estimating: 108it [01:07,  1.69it/s]Extractor Estimating: 109it [01:08,  1.54it/s]Extractor Estimating: 110it [01:08,  1.58it/s]Extractor Estimating: 111it [01:09,  1.58it/s]Extractor Estimating: 112it [01:09,  1.62it/s]Extractor Estimating: 113it [01:10,  1.54it/s]Extractor Estimating: 114it [01:11,  1.55it/s]Extractor Estimating: 115it [01:11,  1.54it/s]Extractor Estimating: 116it [01:12,  1.59it/s]Extractor Estimating: 117it [01:13,  1.60it/s]Extractor Estimating: 118it [01:13,  1.58it/s]Extractor Estimating: 119it [01:14,  1.56it/s]Extractor Estimating: 120it [01:15,  1.63it/s]Extractor Estimating: 121it [01:15,  1.66it/s]Extractor Estimating: 122it [01:16,  1.66it/s]Extractor Estimating: 123it [01:16,  1.65it/s]Extractor Estimating: 124it [01:17,  1.59it/s]Extractor Estimating: 125it [01:18,  1.59it/s]Extractor Estimating: 126it [01:18,  1.61it/s]Extractor Estimating: 127it [01:19,  1.60it/s]Extractor Estimating: 128it [01:20,  1.59it/s]Extractor Estimating: 129it [01:20,  1.45it/s]Extractor Estimating: 130it [01:21,  1.49it/s]Extractor Estimating: 131it [01:22,  1.51it/s]Extractor Estimating: 132it [01:22,  1.55it/s]Extractor Estimating: 133it [01:23,  1.58it/s]Extractor Estimating: 134it [01:23,  1.61it/s]Extractor Estimating: 135it [01:24,  1.61it/s]Extractor Estimating: 136it [01:25,  1.58it/s]Extractor Estimating: 137it [01:25,  1.65it/s]Extractor Estimating: 138it [01:26,  1.64it/s]Extractor Estimating: 139it [01:26,  1.69it/s]Extractor Estimating: 140it [01:27,  1.71it/s]Extractor Estimating: 141it [01:28,  1.75it/s]Extractor Estimating: 142it [01:28,  1.51it/s]Extractor Estimating: 143it [01:29,  1.54it/s]Extractor Estimating: 144it [01:30,  1.60it/s]Extractor Estimating: 145it [01:30,  1.69it/s]Extractor Estimating: 146it [01:31,  1.69it/s]Extractor Estimating: 147it [01:31,  1.59it/s]Extractor Estimating: 148it [01:32,  1.64it/s]Extractor Estimating: 149it [01:33,  1.71it/s]Extractor Estimating: 150it [01:33,  1.73it/s]Extractor Estimating: 151it [01:34,  1.51it/s]Extractor Estimating: 152it [01:35,  1.46it/s]Extractor Estimating: 153it [01:35,  1.51it/s]Extractor Estimating: 154it [01:36,  1.53it/s]Extractor Estimating: 155it [01:36,  1.58it/s]Extractor Estimating: 156it [01:37,  1.61it/s]Extractor Estimating: 157it [01:38,  1.45it/s]Extractor Estimating: 158it [01:39,  1.47it/s]Extractor Estimating: 159it [01:39,  1.53it/s]Extractor Estimating: 160it [01:40,  1.54it/s]Extractor Estimating: 161it [01:40,  1.58it/s]Extractor Estimating: 162it [01:41,  1.58it/s]Extractor Estimating: 163it [01:42,  1.62it/s]Extractor Estimating: 164it [01:42,  1.65it/s]Extractor Estimating: 165it [01:43,  1.63it/s]Extractor Estimating: 166it [01:43,  1.62it/s]Extractor Estimating: 167it [01:44,  1.63it/s]Extractor Estimating: 168it [01:45,  1.64it/s]Extractor Estimating: 169it [01:45,  1.63it/s]Extractor Estimating: 170it [01:46,  1.52it/s]Extractor Estimating: 171it [01:47,  1.57it/s]Extractor Estimating: 172it [01:47,  1.57it/s]Extractor Estimating: 173it [01:48,  1.62it/s]Extractor Estimating: 174it [01:48,  1.65it/s]Extractor Estimating: 175it [01:49,  1.69it/s]Extractor Estimating: 176it [01:50,  1.69it/s]Extractor Estimating: 177it [01:50,  1.64it/s]Extractor Estimating: 178it [01:51,  1.57it/s]Extractor Estimating: 179it [01:52,  1.51it/s]Extractor Estimating: 180it [01:52,  1.52it/s]Extractor Estimating: 181it [01:53,  1.53it/s]Extractor Estimating: 182it [01:54,  1.54it/s]Extractor Estimating: 183it [01:54,  1.55it/s]Extractor Estimating: 184it [01:55,  1.53it/s]Extractor Estimating: 185it [01:55,  1.56it/s]Extractor Estimating: 186it [01:56,  1.58it/s]Extractor Estimating: 187it [01:57,  1.60it/s]Extractor Estimating: 188it [01:57,  1.62it/s]Extractor Estimating: 189it [01:58,  1.54it/s]Extractor Estimating: 190it [01:59,  1.58it/s]Extractor Estimating: 191it [01:59,  1.59it/s]Extractor Estimating: 192it [02:00,  1.57it/s]Extractor Estimating: 193it [02:01,  1.57it/s]Extractor Estimating: 194it [02:01,  1.49it/s]Extractor Estimating: 195it [02:02,  1.57it/s]Extractor Estimating: 196it [02:02,  1.63it/s]Extractor Estimating: 197it [02:03,  1.59it/s]Extractor Estimating: 198it [02:04,  1.47it/s]Extractor Estimating: 199it [02:05,  1.49it/s]Extractor Estimating: 200it [02:05,  1.48it/s]Extractor Estimating: 201it [02:06,  1.51it/s]Extractor Estimating: 202it [02:06,  1.53it/s]Extractor Estimating: 203it [02:07,  1.58it/s]Extractor Estimating: 204it [02:08,  1.50it/s]Extractor Estimating: 205it [02:08,  1.50it/s]Extractor Estimating: 206it [02:09,  1.50it/s]Extractor Estimating: 207it [02:10,  1.57it/s]Extractor Estimating: 208it [02:10,  1.62it/s]Extractor Estimating: 209it [02:11,  1.61it/s]Extractor Estimating: 210it [02:12,  1.63it/s]Extractor Estimating: 211it [02:12,  1.61it/s]Extractor Estimating: 212it [02:13,  1.57it/s]Extractor Estimating: 213it [02:13,  1.61it/s]Extractor Estimating: 214it [02:14,  1.60it/s]Extractor Estimating: 215it [02:15,  1.65it/s]Extractor Estimating: 216it [02:15,  1.60it/s]Extractor Estimating: 217it [02:16,  1.62it/s]Extractor Estimating: 218it [02:16,  1.67it/s]Extractor Estimating: 219it [02:17,  1.62it/s]Extractor Estimating: 220it [02:18,  1.63it/s]Extractor Estimating: 221it [02:18,  1.60it/s]Extractor Estimating: 222it [02:19,  1.56it/s]Extractor Estimating: 223it [02:20,  1.60it/s]Extractor Estimating: 224it [02:20,  1.63it/s]Extractor Estimating: 225it [02:21,  1.61it/s]Extractor Estimating: 226it [02:21,  1.67it/s]Extractor Estimating: 227it [02:22,  1.71it/s]Extractor Estimating: 228it [02:23,  1.67it/s]Extractor Estimating: 229it [02:23,  1.78it/s]Extractor Estimating: 230it [02:24,  1.79it/s]Extractor Estimating: 231it [02:24,  1.86it/s]Extractor Estimating: 232it [02:25,  1.80it/s]Extractor Estimating: 233it [02:25,  1.79it/s]Extractor Estimating: 234it [02:26,  1.76it/s]Extractor Estimating: 235it [02:26,  1.81it/s]Extractor Estimating: 236it [02:27,  1.87it/s]Extractor Estimating: 237it [02:27,  1.89it/s]Extractor Estimating: 238it [02:28,  1.89it/s]Extractor Estimating: 239it [02:28,  1.88it/s]Extractor Estimating: 240it [02:29,  1.83it/s]Extractor Estimating: 241it [02:30,  1.83it/s]Extractor Estimating: 242it [02:30,  1.90it/s]Extractor Estimating: 243it [02:31,  1.86it/s]Extractor Estimating: 244it [02:31,  1.86it/s]Extractor Estimating: 245it [02:32,  1.86it/s]Extractor Estimating: 246it [02:32,  1.87it/s]Extractor Estimating: 247it [02:33,  1.89it/s]Extractor Estimating: 248it [02:33,  1.88it/s]Extractor Estimating: 249it [02:34,  1.85it/s]Extractor Estimating: 250it [02:34,  1.78it/s]Extractor Estimating: 251it [02:35,  1.79it/s]Extractor Estimating: 252it [02:36,  1.65it/s]Extractor Estimating: 253it [02:36,  1.72it/s]Extractor Estimating: 254it [02:37,  1.71it/s]Extractor Estimating: 255it [02:37,  1.70it/s]Extractor Estimating: 256it [02:38,  1.60it/s]Extractor Estimating: 257it [02:39,  1.62it/s]Extractor Estimating: 258it [02:39,  1.66it/s]Extractor Estimating: 259it [02:40,  1.65it/s]Extractor Estimating: 260it [02:40,  1.71it/s]Extractor Estimating: 261it [02:41,  1.63it/s]Extractor Estimating: 262it [02:42,  1.68it/s]Extractor Estimating: 263it [02:42,  1.70it/s]Extractor Estimating: 264it [02:43,  1.72it/s]Extractor Estimating: 265it [02:43,  1.78it/s]Extractor Estimating: 266it [02:44,  1.80it/s]Extractor Estimating: 267it [02:44,  1.80it/s]Extractor Estimating: 268it [02:45,  1.79it/s]Extractor Estimating: 269it [02:46,  1.69it/s]Extractor Estimating: 270it [02:46,  1.73it/s]Extractor Estimating: 271it [02:47,  1.77it/s]Extractor Estimating: 272it [02:47,  1.71it/s]Extractor Estimating: 273it [02:48,  1.63it/s]Extractor Estimating: 274it [02:49,  1.55it/s]Extractor Estimating: 275it [02:49,  1.66it/s]Extractor Estimating: 276it [02:50,  1.66it/s]Extractor Estimating: 277it [02:50,  1.67it/s]Extractor Estimating: 278it [02:51,  1.74it/s]Extractor Estimating: 279it [02:51,  1.81it/s]Extractor Estimating: 280it [02:52,  1.66it/s]Extractor Estimating: 281it [02:53,  1.56it/s]Extractor Estimating: 282it [02:54,  1.26it/s]Extractor Estimating: 283it [02:55,  1.38it/s]Extractor Estimating: 284it [02:55,  1.51it/s]Extractor Estimating: 285it [02:56,  1.54it/s]Extractor Estimating: 286it [02:56,  1.54it/s]Extractor Estimating: 287it [02:57,  1.57it/s]Extractor Estimating: 288it [02:58,  1.63it/s]Extractor Estimating: 289it [02:58,  1.68it/s]Extractor Estimating: 290it [02:59,  1.74it/s]Extractor Estimating: 291it [02:59,  1.81it/s]Extractor Estimating: 292it [03:00,  1.74it/s]Extractor Estimating: 293it [03:00,  1.79it/s]Extractor Estimating: 294it [03:01,  1.88it/s]Extractor Estimating: 295it [03:01,  1.93it/s]Extractor Estimating: 296it [03:02,  1.87it/s]Extractor Estimating: 297it [03:02,  1.85it/s]Extractor Estimating: 298it [03:03,  1.79it/s]Extractor Estimating: 299it [03:04,  1.85it/s]Extractor Estimating: 300it [03:04,  1.87it/s]Extractor Estimating: 301it [03:05,  1.75it/s]Extractor Estimating: 302it [03:05,  1.74it/s]Extractor Estimating: 303it [03:06,  1.68it/s]Extractor Estimating: 304it [03:07,  1.59it/s]Extractor Estimating: 305it [03:07,  1.63it/s]Extractor Estimating: 306it [03:08,  1.66it/s]Extractor Estimating: 307it [03:08,  1.66it/s]Extractor Estimating: 308it [03:09,  1.70it/s]Extractor Estimating: 309it [03:10,  1.63it/s]Extractor Estimating: 310it [03:10,  1.60it/s]Extractor Estimating: 311it [03:11,  1.59it/s]Extractor Estimating: 312it [03:11,  1.62it/s]Extractor Estimating: 313it [03:12,  1.66it/s]Extractor Estimating: 314it [03:13,  1.39it/s]Extractor Estimating: 315it [03:14,  1.48it/s]Extractor Estimating: 316it [03:14,  1.52it/s]Extractor Estimating: 317it [03:15,  1.56it/s]Extractor Estimating: 318it [03:15,  1.58it/s]Extractor Estimating: 319it [03:16,  1.58it/s]Extractor Estimating: 320it [03:17,  1.64it/s]Extractor Estimating: 321it [03:17,  1.68it/s]Extractor Estimating: 322it [03:18,  1.71it/s]Extractor Estimating: 323it [03:18,  1.65it/s]Extractor Estimating: 324it [03:19,  1.62it/s]Extractor Estimating: 325it [03:20,  1.57it/s]Extractor Estimating: 326it [03:20,  1.60it/s]Extractor Estimating: 327it [03:21,  1.65it/s]Extractor Estimating: 328it [03:22,  1.63it/s]Extractor Estimating: 329it [03:22,  1.66it/s]Extractor Estimating: 330it [03:23,  1.71it/s]Extractor Estimating: 331it [03:23,  1.74it/s]Extractor Estimating: 332it [03:24,  1.73it/s]Extractor Estimating: 333it [03:24,  1.71it/s]Extractor Estimating: 334it [03:25,  1.75it/s]Extractor Estimating: 335it [03:26,  1.72it/s]Extractor Estimating: 336it [03:26,  1.74it/s]Extractor Estimating: 337it [03:27,  1.77it/s]Extractor Estimating: 338it [03:27,  1.77it/s]Extractor Estimating: 339it [03:28,  1.76it/s]Extractor Estimating: 340it [03:28,  1.77it/s]Extractor Estimating: 341it [03:29,  1.82it/s]Extractor Estimating: 342it [03:29,  1.86it/s]Extractor Estimating: 343it [03:30,  1.87it/s]Extractor Estimating: 344it [03:30,  1.85it/s]Extractor Estimating: 345it [03:31,  1.90it/s]Extractor Estimating: 346it [03:31,  1.91it/s]Extractor Estimating: 347it [03:32,  1.88it/s]Extractor Estimating: 348it [03:33,  1.85it/s]Extractor Estimating: 349it [03:33,  1.77it/s]Extractor Estimating: 350it [03:34,  1.72it/s]Extractor Estimating: 351it [03:34,  1.66it/s]Extractor Estimating: 352it [03:35,  1.64it/s]Extractor Estimating: 353it [03:36,  1.64it/s]Extractor Estimating: 354it [03:36,  1.66it/s]Extractor Estimating: 355it [03:37,  1.63it/s]Extractor Estimating: 356it [03:37,  1.71it/s]Extractor Estimating: 357it [03:38,  1.70it/s]Extractor Estimating: 358it [03:39,  1.69it/s]Extractor Estimating: 359it [03:39,  1.69it/s]Extractor Estimating: 360it [03:40,  1.68it/s]Extractor Estimating: 361it [03:41,  1.61it/s]Extractor Estimating: 362it [03:41,  1.65it/s]Extractor Estimating: 363it [03:42,  1.65it/s]Extractor Estimating: 364it [03:42,  1.67it/s]Extractor Estimating: 365it [03:43,  1.67it/s]Extractor Estimating: 366it [03:43,  1.66it/s]Extractor Estimating: 367it [03:44,  1.64it/s]Extractor Estimating: 368it [03:45,  1.61it/s]Extractor Estimating: 369it [03:45,  1.66it/s]Extractor Estimating: 370it [03:46,  1.69it/s]Extractor Estimating: 371it [03:47,  1.58it/s]Extractor Estimating: 372it [03:47,  1.67it/s]Extractor Estimating: 373it [03:48,  1.65it/s]Extractor Estimating: 374it [03:49,  1.46it/s]Extractor Estimating: 375it [03:49,  1.54it/s]Extractor Estimating: 375it [03:49,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:26,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:26,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:26,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:26,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:26,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:35:26,809 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:35:26,810 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:27,083 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:28,136 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:28,136 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:29,902 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:29,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:29,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:29,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:35:29,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:35:30,227 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:35:30,228 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:35:30,850 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:35:30,996 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:35:30,997 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:53:27,928 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:53:27,954 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7494 mean pseudo reward: 0.9361864566517004
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 16734
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16834, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16834, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.079, loss:662.5435
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.070, loss:629.7358
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.074, loss:619.2727
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.076, loss:600.2980
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.068, loss:577.6040
>> valid entity prec:0.6012, rec:0.5854, f1:0.5932
>> valid relation prec:0.4801, rec:0.1449, f1:0.2227
>> valid relation with NER prec:0.4801, rec:0.1449, f1:0.2227
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.373, loss:605.6662
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.079, loss:594.2032
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.066, loss:540.6508
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.079, loss:579.8963
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.068, loss:554.5304
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5844, rec:0.5795, f1:0.5820
>> valid relation prec:0.4586, rec:0.1286, f1:0.2009
>> valid relation with NER prec:0.4586, rec:0.1286, f1:0.2009
g_step 1100, step 161, avg_time 2.356, loss:555.3200
g_step 1200, step 261, avg_time 1.087, loss:553.8812
g_step 1300, step 48, avg_time 1.070, loss:540.7319
g_step 1400, step 148, avg_time 1.073, loss:516.0390
g_step 1500, step 248, avg_time 1.084, loss:546.2674
>> valid entity prec:0.5168, rec:0.6903, f1:0.5911
>> valid relation prec:0.3568, rec:0.2105, f1:0.2648
>> valid relation with NER prec:0.3568, rec:0.2105, f1:0.2648
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.368, loss:496.3468
g_step 1700, step 135, avg_time 1.085, loss:483.4449
g_step 1800, step 235, avg_time 1.073, loss:498.4586
g_step 1900, step 22, avg_time 1.081, loss:511.8564
g_step 2000, step 122, avg_time 1.081, loss:447.6097
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6169, rec:0.5207, f1:0.5647
>> valid relation prec:0.4169, rec:0.1452, f1:0.2154
>> valid relation with NER prec:0.4169, rec:0.1452, f1:0.2154
g_step 2100, step 222, avg_time 2.365, loss:477.5625
g_step 2200, step 9, avg_time 1.049, loss:469.6620
g_step 2300, step 109, avg_time 1.082, loss:435.5421
g_step 2400, step 209, avg_time 1.075, loss:466.3228
g_step 2500, step 309, avg_time 1.077, loss:465.0476
>> valid entity prec:0.5616, rec:0.5403, f1:0.5507
>> valid relation prec:0.3772, rec:0.1504, f1:0.2150
>> valid relation with NER prec:0.3772, rec:0.1504, f1:0.2150
g_step 2600, step 96, avg_time 2.346, loss:405.9338
g_step 2700, step 196, avg_time 1.087, loss:435.9683
g_step 2800, step 296, avg_time 1.073, loss:457.6189
g_step 2900, step 83, avg_time 1.059, loss:388.6833
g_step 3000, step 183, avg_time 1.093, loss:404.7234
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5934, rec:0.5642, f1:0.5784
>> valid relation prec:0.3668, rec:0.1621, f1:0.2249
>> valid relation with NER prec:0.3668, rec:0.1621, f1:0.2249
g_step 3100, step 283, avg_time 2.364, loss:423.7121
g_step 3200, step 70, avg_time 1.079, loss:393.5334
g_step 3300, step 170, avg_time 1.064, loss:374.5447
g_step 3400, step 270, avg_time 1.082, loss:394.7318
g_step 3500, step 57, avg_time 1.074, loss:348.3234
>> valid entity prec:0.6075, rec:0.5247, f1:0.5630
>> valid relation prec:0.3892, rec:0.1750, f1:0.2415
>> valid relation with NER prec:0.3892, rec:0.1750, f1:0.2415
g_step 3600, step 157, avg_time 2.355, loss:381.1360
g_step 3700, step 257, avg_time 1.080, loss:373.5465
g_step 3800, step 44, avg_time 1.076, loss:362.9969
g_step 3900, step 144, avg_time 1.066, loss:355.6222
g_step 4000, step 244, avg_time 1.080, loss:352.5836
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5882, rec:0.5182, f1:0.5510
>> valid relation prec:0.3413, rec:0.1481, f1:0.2066
>> valid relation with NER prec:0.3413, rec:0.1481, f1:0.2066
g_step 4100, step 31, avg_time 2.355, loss:360.8044
g_step 4200, step 131, avg_time 1.080, loss:323.8133
g_step 4300, step 231, avg_time 1.084, loss:364.0675
g_step 4400, step 18, avg_time 1.061, loss:336.3318
g_step 4500, step 118, avg_time 1.082, loss:329.3146
>> valid entity prec:0.5722, rec:0.5630, f1:0.5675
>> valid relation prec:0.3490, rec:0.1679, f1:0.2267
>> valid relation with NER prec:0.3490, rec:0.1679, f1:0.2267
g_step 4600, step 218, avg_time 2.352, loss:318.6487
g_step 4700, step 5, avg_time 1.075, loss:345.7352
g_step 4800, step 105, avg_time 1.086, loss:310.3618
g_step 4900, step 205, avg_time 1.070, loss:318.2473
g_step 5000, step 305, avg_time 1.071, loss:318.0044
learning rate was adjusted to 0.0008
>> valid entity prec:0.5888, rec:0.5601, f1:0.5741
>> valid relation prec:0.3321, rec:0.1587, f1:0.2148
>> valid relation with NER prec:0.3321, rec:0.1587, f1:0.2148
g_step 5100, step 92, avg_time 2.357, loss:279.4492
g_step 5200, step 192, avg_time 1.084, loss:303.2180
g_step 5300, step 292, avg_time 1.069, loss:301.5518
g_step 5400, step 79, avg_time 1.076, loss:289.3645
g_step 5500, step 179, avg_time 1.089, loss:286.0889
>> valid entity prec:0.5442, rec:0.5437, f1:0.5439
>> valid relation prec:0.3276, rec:0.1418, f1:0.1979
>> valid relation with NER prec:0.3276, rec:0.1418, f1:0.1979
g_step 5600, step 279, avg_time 2.336, loss:298.1569
g_step 5700, step 66, avg_time 1.080, loss:291.9428
g_step 5800, step 166, avg_time 1.076, loss:284.9209
g_step 5900, step 266, avg_time 1.079, loss:272.3619
g_step 6000, step 53, avg_time 1.079, loss:257.1285
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5579, rec:0.5602, f1:0.5590
>> valid relation prec:0.3381, rec:0.1765, f1:0.2319
>> valid relation with NER prec:0.3381, rec:0.1765, f1:0.2319
g_step 6100, step 153, avg_time 2.357, loss:276.4463
g_step 6200, step 253, avg_time 1.080, loss:263.3285
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:53:27 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:53:27 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-53-27_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:53:28 - WARNING - datasets.builder -   Using custom data configuration default-60be53b591e07f44
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-60be53b591e07f44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:53:29,263 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:53:29,264 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:53:29,264 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:53:29,265 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:53:29,273 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:53:29,281 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:53:29,443 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:53:32,618 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:53:32,620 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-60be53b591e07f44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.17ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.04ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.43ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.60ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.74ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.82ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.89ba/s]100%|██████████| 8/8 [00:01<00:00,  5.80ba/s]100%|██████████| 8/8 [00:01<00:00,  4.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.56ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.44ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.82ba/s]100%|██████████| 4/4 [00:00<00:00,  4.92ba/s]100%|██████████| 4/4 [00:00<00:00,  4.20ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.27ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.00ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.24ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.44ba/s]100%|██████████| 8/8 [00:00<00:00, 10.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.49ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.00ba/s]100%|██████████| 4/4 [00:00<00:00, 11.36ba/s]
[INFO|trainer.py:414] 2023-08-28 23:53:36,706 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:53:36,722 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:53:36,722 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:53:36,722 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:53:36,722 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:53:36,722 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:53:36,723 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:53:36,723 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.37it/s]  0%|          | 2/585 [00:00<02:49,  3.44it/s]  1%|          | 3/585 [00:00<02:48,  3.46it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:46,  3.48it/s]  1%|▏         | 8/585 [00:02<02:48,  3.42it/s]  2%|▏         | 9/585 [00:02<02:47,  3.44it/s]  2%|▏         | 10/585 [00:02<02:46,  3.45it/s]  2%|▏         | 11/585 [00:03<02:45,  3.47it/s]  2%|▏         | 12/585 [00:03<02:45,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:44,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.48it/s]  3%|▎         | 17/585 [00:04<02:42,  3.49it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:42,  3.47it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:42,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.48it/s]  4%|▍         | 24/585 [00:06<02:41,  3.48it/s]  4%|▍         | 25/585 [00:07<02:40,  3.48it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:39,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:40,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.48it/s]  7%|▋         | 39/585 [00:11<02:36,  3.48it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:36,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:34,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:14<02:33,  3.47it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:33,  3.47it/s]  9%|▉         | 55/585 [00:15<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:16<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:30,  3.47it/s] 11%|█         | 62/585 [00:17<02:30,  3.47it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:24<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:25<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.47it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.47it/s] 17%|█▋        | 100/585 [00:28<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.47it/s] 19%|█▉        | 111/585 [00:31<02:16,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:33<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 23:54:10,488 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:54:10,488 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:54:10,488 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.45it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.68it/s][A
  4%|▍         | 18/437 [00:00<00:08, 49.02it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.17it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.67it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.37it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.22it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.89it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.82it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.77it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.81it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.87it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.90it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.88it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.88it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.76it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.82it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.72it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.74it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.78it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.87it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.89it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.90it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.86it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.85it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.75it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.69it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.77it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.69it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.68it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.72it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.79it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.81it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.85it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.90it/s][A
 43%|████▎     | 188/437 [00:03<00:05, 46.73it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.77it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.79it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.79it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.78it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.78it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.84it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.83it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.85it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.74it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.80it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.78it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.85it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.75it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.68it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.71it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.80it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.79it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.85it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.66it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.67it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.78it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.80it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.65it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.76it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.80it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.84it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.83it/s][A
 75%|███████▌  | 328/437 [00:06<00:02, 46.75it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.65it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.74it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.76it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.74it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.78it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.73it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.80it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.83it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.78it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.75it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.74it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.71it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.77it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.72it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.72it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.75it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.78it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.75it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.75it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.72it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.72it/s][A 20%|██        | 117/585 [00:43<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:54:19,859 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:54:19,885 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:54:22,493 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:54:22,517 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:54:22,529 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<43:24,  5.58s/it] 20%|██        | 119/585 [00:51<31:00,  3.99s/it] 21%|██        | 120/585 [00:52<22:19,  2.88s/it] 21%|██        | 121/585 [00:52<16:15,  2.10s/it] 21%|██        | 122/585 [00:52<12:01,  1.56s/it] 21%|██        | 123/585 [00:53<09:03,  1.18s/it] 21%|██        | 124/585 [00:53<06:59,  1.10it/s] 21%|██▏       | 125/585 [00:53<05:32,  1.38it/s] 22%|██▏       | 126/585 [00:53<04:32,  1.69it/s] 22%|██▏       | 127/585 [00:54<03:49,  1.99it/s] 22%|██▏       | 128/585 [00:54<03:19,  2.29it/s] 22%|██▏       | 129/585 [00:54<02:59,  2.55it/s] 22%|██▏       | 130/585 [00:55<02:44,  2.76it/s] 22%|██▏       | 131/585 [00:55<02:34,  2.94it/s] 23%|██▎       | 132/585 [00:55<02:26,  3.08it/s] 23%|██▎       | 133/585 [00:55<02:21,  3.19it/s] 23%|██▎       | 134/585 [00:56<02:17,  3.27it/s] 23%|██▎       | 135/585 [00:56<02:15,  3.33it/s] 23%|██▎       | 136/585 [00:56<02:13,  3.37it/s] 23%|██▎       | 137/585 [00:57<02:11,  3.40it/s] 24%|██▎       | 138/585 [00:57<02:10,  3.42it/s] 24%|██▍       | 139/585 [00:57<02:09,  3.43it/s] 24%|██▍       | 140/585 [00:57<02:09,  3.44it/s] 24%|██▍       | 141/585 [00:58<02:09,  3.44it/s] 24%|██▍       | 142/585 [00:58<02:08,  3.45it/s] 24%|██▍       | 143/585 [00:58<02:07,  3.46it/s] 25%|██▍       | 144/585 [00:59<02:07,  3.46it/s] 25%|██▍       | 145/585 [00:59<02:07,  3.46it/s] 25%|██▍       | 146/585 [00:59<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:00<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:00<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:00<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:01<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:01<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:01<02:05,  3.46it/s] 26%|██▋       | 154/585 [01:02<02:04,  3.46it/s] 26%|██▋       | 155/585 [01:02<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:02<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:03<02:03,  3.47it/s] 27%|██▋       | 159/585 [01:03<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:03<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:04<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:04<02:02,  3.47it/s] 28%|██▊       | 163/585 [01:04<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:04<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:05<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:05<02:00,  3.46it/s] 29%|██▊       | 167/585 [01:05<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:06<02:00,  3.47it/s] 29%|██▉       | 169/585 [01:06<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:06<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:06<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:07<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:07<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:08<01:58,  3.46it/s] 30%|███       | 176/585 [01:08<01:58,  3.46it/s] 30%|███       | 177/585 [01:08<01:57,  3.46it/s] 30%|███       | 178/585 [01:08<01:57,  3.46it/s] 31%|███       | 179/585 [01:09<01:57,  3.47it/s] 31%|███       | 180/585 [01:09<01:56,  3.46it/s] 31%|███       | 181/585 [01:09<01:56,  3.47it/s] 31%|███       | 182/585 [01:10<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:10<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:10<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:10<01:55,  3.45it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 188/585 [01:11<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:12<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:12<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:12<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:12<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.47it/s] 33%|███▎      | 194/585 [01:13<01:52,  3.46it/s] 33%|███▎      | 195/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:14<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:14<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:15<01:50,  3.46it/s] 35%|███▍      | 202/585 [01:15<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:16<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:16<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:17<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:17<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:17<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.46it/s] 36%|███▌      | 212/585 [01:18<01:47,  3.46it/s] 36%|███▋      | 213/585 [01:19<01:47,  3.46it/s] 37%|███▋      | 214/585 [01:19<01:47,  3.46it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:19<01:46,  3.46it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:20<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.46it/s] 38%|███▊      | 222/585 [01:21<01:44,  3.46it/s] 38%|███▊      | 223/585 [01:21<01:44,  3.46it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.46it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.46it/s] 39%|███▊      | 226/585 [01:22<01:43,  3.45it/s] 39%|███▉      | 227/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.46it/s] 39%|███▉      | 229/585 [01:23<01:42,  3.46it/s] 39%|███▉      | 230/585 [01:23<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:24<01:41,  3.46it/s] 40%|███▉      | 233/585 [01:24<01:41,  3.46it/s] 40%|████      | 234/585 [01:25<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 23:55:01,902 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:55:01,902 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:55:01,902 >>   Batch size = 8
{'eval_loss': 1.0784447193145752, 'eval_runtime': 9.3492, 'eval_samples_per_second': 373.615, 'eval_steps_per_second': 46.742, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.54it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.73it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.06it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.66it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.38it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.18it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.81it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.69it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.77it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.74it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.80it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.75it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.81it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.84it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.82it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.65it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.63it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.62it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.69it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.68it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.77it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.80it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.80it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.71it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.69it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.60it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.61it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.68it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.74it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.81it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.72it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.72it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.61it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.54it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.60it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.54it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.58it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.65it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.74it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.76it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.70it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.75it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.69it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.71it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.62it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.63it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.70it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.71it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.70it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.69it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.73it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.75it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.73it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.60it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.67it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.72it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.72it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.71it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.59it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.65it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.70it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.68it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.70it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.72it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.64it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.70it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.73it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.69it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.53it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.62it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.60it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.61it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.63it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.67it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.65it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.73it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.65it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.68it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.62it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.65it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.65it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.62it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.62it/s][A 40%|████      | 234/585 [01:34<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:55:11,287 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:55:11,308 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:55:13,790 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:55:13,832 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:55:13,853 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:42<31:00,  5.32s/it] 40%|████      | 236/585 [01:42<22:09,  3.81s/it] 41%|████      | 237/585 [01:42<15:57,  2.75s/it] 41%|████      | 238/585 [01:43<11:38,  2.01s/it] 41%|████      | 239/585 [01:43<08:37,  1.50s/it] 41%|████      | 240/585 [01:43<06:30,  1.13s/it] 41%|████      | 241/585 [01:43<05:02,  1.14it/s] 41%|████▏     | 242/585 [01:44<04:00,  1.42it/s] 42%|████▏     | 243/585 [01:44<03:17,  1.73it/s] 42%|████▏     | 244/585 [01:44<02:47,  2.04it/s] 42%|████▏     | 245/585 [01:45<02:26,  2.32it/s] 42%|████▏     | 246/585 [01:45<02:11,  2.58it/s] 42%|████▏     | 247/585 [01:45<02:00,  2.79it/s] 42%|████▏     | 248/585 [01:45<01:53,  2.96it/s] 43%|████▎     | 249/585 [01:46<01:48,  3.10it/s] 43%|████▎     | 250/585 [01:46<01:44,  3.20it/s] 43%|████▎     | 251/585 [01:46<01:42,  3.27it/s] 43%|████▎     | 252/585 [01:47<01:40,  3.32it/s] 43%|████▎     | 253/585 [01:47<01:38,  3.36it/s] 43%|████▎     | 254/585 [01:47<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:47<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:48<01:36,  3.43it/s] 44%|████▍     | 257/585 [01:48<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:48<01:34,  3.45it/s] 44%|████▍     | 259/585 [01:49<01:34,  3.45it/s] 44%|████▍     | 260/585 [01:49<01:34,  3.46it/s] 45%|████▍     | 261/585 [01:49<01:33,  3.46it/s] 45%|████▍     | 262/585 [01:49<01:33,  3.46it/s] 45%|████▍     | 263/585 [01:50<01:33,  3.46it/s] 45%|████▌     | 264/585 [01:50<01:32,  3.46it/s] 45%|████▌     | 265/585 [01:50<01:32,  3.46it/s] 45%|████▌     | 266/585 [01:51<01:32,  3.46it/s] 46%|████▌     | 267/585 [01:51<01:34,  3.36it/s] 46%|████▌     | 268/585 [01:51<01:33,  3.38it/s] 46%|████▌     | 269/585 [01:52<01:32,  3.41it/s] 46%|████▌     | 270/585 [01:52<01:32,  3.42it/s] 46%|████▋     | 271/585 [01:52<01:31,  3.43it/s] 46%|████▋     | 272/585 [01:52<01:30,  3.44it/s] 47%|████▋     | 273/585 [01:53<01:30,  3.45it/s] 47%|████▋     | 274/585 [01:53<01:30,  3.44it/s] 47%|████▋     | 275/585 [01:53<01:29,  3.45it/s] 47%|████▋     | 276/585 [01:54<01:29,  3.45it/s] 47%|████▋     | 277/585 [01:54<01:29,  3.45it/s] 48%|████▊     | 278/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:54<01:28,  3.46it/s] 48%|████▊     | 280/585 [01:55<01:28,  3.46it/s] 48%|████▊     | 281/585 [01:55<01:27,  3.46it/s] 48%|████▊     | 282/585 [01:55<01:27,  3.46it/s] 48%|████▊     | 283/585 [01:56<01:27,  3.47it/s] 49%|████▊     | 284/585 [01:56<01:26,  3.46it/s] 49%|████▊     | 285/585 [01:56<01:26,  3.45it/s] 49%|████▉     | 286/585 [01:56<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:57<01:25,  3.46it/s] 49%|████▉     | 289/585 [01:57<01:25,  3.46it/s] 50%|████▉     | 290/585 [01:58<01:25,  3.46it/s] 50%|████▉     | 291/585 [01:58<01:24,  3.46it/s] 50%|████▉     | 292/585 [01:58<01:24,  3.46it/s] 50%|█████     | 293/585 [01:58<01:24,  3.46it/s] 50%|█████     | 294/585 [01:59<01:24,  3.46it/s] 50%|█████     | 295/585 [01:59<01:23,  3.46it/s] 51%|█████     | 296/585 [01:59<01:23,  3.45it/s] 51%|█████     | 297/585 [02:00<01:23,  3.46it/s] 51%|█████     | 298/585 [02:00<01:22,  3.46it/s] 51%|█████     | 299/585 [02:00<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:00<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:01<01:21,  3.46it/s] 52%|█████▏    | 302/585 [02:01<01:21,  3.47it/s] 52%|█████▏    | 303/585 [02:01<01:21,  3.47it/s] 52%|█████▏    | 304/585 [02:02<01:21,  3.47it/s] 52%|█████▏    | 305/585 [02:02<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:02<01:20,  3.46it/s] 52%|█████▏    | 307/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:03<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:03<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:04<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:04<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:04<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:05<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:05<01:17,  3.46it/s] 54%|█████▍    | 316/585 [02:05<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:05<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:06<01:18,  3.41it/s] 55%|█████▍    | 319/585 [02:06<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:06<01:17,  3.44it/s] 55%|█████▍    | 321/585 [02:07<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:07<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:07<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:07<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:08<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:08<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:08<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:09<01:14,  3.47it/s] 56%|█████▌    | 329/585 [02:09<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:09<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:09<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:10<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:10<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:10<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:11<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:11<01:11,  3.46it/s] 58%|█████▊    | 337/585 [02:11<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:11<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:12<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:12<01:10,  3.45it/s] 58%|█████▊    | 341/585 [02:12<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:13<01:10,  3.46it/s] 59%|█████▊    | 343/585 [02:13<01:09,  3.46it/s] 59%|█████▉    | 344/585 [02:13<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:14<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:14<01:08,  3.46it/s] 59%|█████▉    | 347/585 [02:14<01:08,  3.46it/s] 59%|█████▉    | 348/585 [02:14<01:08,  3.47it/s] 60%|█████▉    | 349/585 [02:15<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:15<01:07,  3.46it/s] 60%|██████    | 351/585 [02:15<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 23:55:52,505 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:55:52,505 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:55:52,505 >>   Batch size = 8
{'eval_loss': 1.1003406047821045, 'eval_runtime': 9.3681, 'eval_samples_per_second': 372.86, 'eval_steps_per_second': 46.648, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.27it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.58it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.80it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.07it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.29it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.02it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.72it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.71it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.74it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.83it/s][A
 14%|█▍        | 63/437 [00:01<00:07, 46.82it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.77it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.76it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.80it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.62it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.59it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.42it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.48it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.61it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.68it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.81it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.74it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.71it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.75it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.55it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.60it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.45it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.59it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.66it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.70it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.77it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.79it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.77it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.67it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.60it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.52it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.48it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.62it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.64it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.74it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.73it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.69it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.59it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.62it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.61it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.46it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.45it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.63it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.75it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.70it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.62it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.58it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.67it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.62it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.52it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.55it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.58it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.66it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.78it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.64it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.68it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.63it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.59it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.67it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.52it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.51it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.61it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.56it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.60it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.60it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.58it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.65it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.55it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.64it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.53it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.52it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.57it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.60it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.64it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.61it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.60it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.62it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.60it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.53it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.53it/s][A 60%|██████    | 351/585 [02:25<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:56:01,904 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:56:01,926 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:56:04,278 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:56:04,299 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:56:04,310 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:32<20:50,  5.37s/it] 60%|██████    | 353/585 [02:33<14:52,  3.85s/it] 61%|██████    | 354/585 [02:33<10:41,  2.78s/it] 61%|██████    | 355/585 [02:33<07:47,  2.03s/it] 61%|██████    | 356/585 [02:34<05:45,  1.51s/it] 61%|██████    | 357/585 [02:34<04:20,  1.14s/it] 61%|██████    | 358/585 [02:34<03:21,  1.13it/s] 61%|██████▏   | 359/585 [02:34<02:39,  1.41it/s] 62%|██████▏   | 360/585 [02:35<02:10,  1.72it/s] 62%|██████▏   | 361/585 [02:35<01:50,  2.03it/s] 62%|██████▏   | 362/585 [02:35<01:36,  2.32it/s] 62%|██████▏   | 363/585 [02:36<01:26,  2.57it/s] 62%|██████▏   | 364/585 [02:36<01:19,  2.78it/s] 62%|██████▏   | 365/585 [02:36<01:14,  2.96it/s] 63%|██████▎   | 366/585 [02:36<01:10,  3.10it/s] 63%|██████▎   | 367/585 [02:37<01:08,  3.20it/s] 63%|██████▎   | 368/585 [02:37<01:06,  3.28it/s] 63%|██████▎   | 369/585 [02:37<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:38<01:03,  3.37it/s] 63%|██████▎   | 371/585 [02:38<01:02,  3.40it/s] 64%|██████▎   | 372/585 [02:38<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:39<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 376/585 [02:39<01:00,  3.44it/s] 64%|██████▍   | 377/585 [02:40<01:00,  3.45it/s] 65%|██████▍   | 378/585 [02:40<00:59,  3.46it/s] 65%|██████▍   | 379/585 [02:40<00:59,  3.46it/s] 65%|██████▍   | 380/585 [02:41<00:59,  3.46it/s] 65%|██████▌   | 381/585 [02:41<00:58,  3.47it/s] 65%|██████▌   | 382/585 [02:41<00:58,  3.47it/s] 65%|██████▌   | 383/585 [02:41<00:58,  3.47it/s] 66%|██████▌   | 384/585 [02:42<00:57,  3.47it/s] 66%|██████▌   | 385/585 [02:42<00:57,  3.47it/s] 66%|██████▌   | 386/585 [02:42<00:57,  3.46it/s] 66%|██████▌   | 387/585 [02:43<00:57,  3.46it/s] 66%|██████▋   | 388/585 [02:43<00:56,  3.46it/s] 66%|██████▋   | 389/585 [02:43<00:56,  3.47it/s] 67%|██████▋   | 390/585 [02:43<00:56,  3.47it/s] 67%|██████▋   | 391/585 [02:44<00:55,  3.47it/s] 67%|██████▋   | 392/585 [02:44<00:55,  3.47it/s] 67%|██████▋   | 393/585 [02:44<00:55,  3.47it/s] 67%|██████▋   | 394/585 [02:45<00:55,  3.47it/s] 68%|██████▊   | 395/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 397/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 398/585 [02:46<00:53,  3.46it/s] 68%|██████▊   | 399/585 [02:46<00:53,  3.46it/s] 68%|██████▊   | 400/585 [02:46<00:53,  3.46it/s] 69%|██████▊   | 401/585 [02:47<00:53,  3.47it/s] 69%|██████▊   | 402/585 [02:47<00:52,  3.47it/s] 69%|██████▉   | 403/585 [02:47<00:52,  3.47it/s] 69%|██████▉   | 404/585 [02:47<00:52,  3.47it/s] 69%|██████▉   | 405/585 [02:48<00:51,  3.47it/s] 69%|██████▉   | 406/585 [02:48<00:51,  3.46it/s] 70%|██████▉   | 407/585 [02:48<00:51,  3.46it/s] 70%|██████▉   | 408/585 [02:49<00:51,  3.46it/s] 70%|██████▉   | 409/585 [02:49<00:50,  3.46it/s] 70%|███████   | 410/585 [02:49<00:50,  3.46it/s] 70%|███████   | 411/585 [02:49<00:50,  3.46it/s] 70%|███████   | 412/585 [02:50<00:49,  3.47it/s] 71%|███████   | 413/585 [02:50<00:49,  3.46it/s] 71%|███████   | 414/585 [02:50<00:49,  3.47it/s] 71%|███████   | 415/585 [02:51<00:49,  3.46it/s] 71%|███████   | 416/585 [02:51<00:50,  3.37it/s] 71%|███████▏  | 417/585 [02:51<00:49,  3.38it/s] 71%|███████▏  | 418/585 [02:52<00:49,  3.41it/s] 72%|███████▏  | 419/585 [02:52<00:48,  3.42it/s] 72%|███████▏  | 420/585 [02:52<00:48,  3.44it/s] 72%|███████▏  | 421/585 [02:52<00:47,  3.44it/s] 72%|███████▏  | 422/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:53<00:46,  3.46it/s] 72%|███████▏  | 424/585 [02:53<00:46,  3.46it/s] 73%|███████▎  | 425/585 [02:54<00:46,  3.46it/s] 73%|███████▎  | 426/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 427/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 428/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 429/585 [02:55<00:45,  3.46it/s] 74%|███████▎  | 430/585 [02:55<00:44,  3.46it/s] 74%|███████▎  | 431/585 [02:55<00:44,  3.46it/s] 74%|███████▍  | 432/585 [02:56<00:44,  3.46it/s] 74%|███████▍  | 433/585 [02:56<00:43,  3.46it/s] 74%|███████▍  | 434/585 [02:56<00:43,  3.46it/s] 74%|███████▍  | 435/585 [02:56<00:43,  3.46it/s] 75%|███████▍  | 436/585 [02:57<00:43,  3.46it/s] 75%|███████▍  | 437/585 [02:57<00:42,  3.47it/s] 75%|███████▍  | 438/585 [02:57<00:42,  3.46it/s] 75%|███████▌  | 439/585 [02:58<00:42,  3.45it/s] 75%|███████▌  | 440/585 [02:58<00:41,  3.46it/s] 75%|███████▌  | 441/585 [02:58<00:41,  3.46it/s] 76%|███████▌  | 442/585 [02:58<00:41,  3.46it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.46it/s] 76%|███████▌  | 444/585 [02:59<00:40,  3.46it/s] 76%|███████▌  | 445/585 [02:59<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:00<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:00<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:01<00:39,  3.46it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:01<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:02<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:02<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:02<00:37,  3.46it/s] 78%|███████▊  | 456/585 [03:03<00:37,  3.47it/s] 78%|███████▊  | 457/585 [03:03<00:36,  3.46it/s] 78%|███████▊  | 458/585 [03:03<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:03<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:04<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:04<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:05<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:05<00:34,  3.46it/s] 79%|███████▉  | 465/585 [03:05<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:05<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.46it/s] 80%|████████  | 468/585 [03:06<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 23:56:43,250 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:56:43,251 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:56:43,251 >>   Batch size = 8
{'eval_loss': 1.113234519958496, 'eval_runtime': 9.3766, 'eval_samples_per_second': 372.524, 'eval_steps_per_second': 46.605, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.88it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.57it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.79it/s][A
  5%|▌         | 23/437 [00:00<00:08, 48.15it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.75it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.34it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.11it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.71it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.75it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.78it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.77it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.74it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.88it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.79it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.69it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.57it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.56it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.61it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.66it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.74it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.76it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.81it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.75it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.57it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.47it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.48it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.57it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.62it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.66it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.60it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.66it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.64it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.48it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.53it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.48it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.46it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.46it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.57it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.63it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.58it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.62it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.66it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.70it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.67it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.53it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.49it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.53it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.61it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.72it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.60it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.65it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.71it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.62it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.63it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.52it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.57it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.59it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.68it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.56it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.63it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.70it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.67it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.62it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.60it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.56it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.58it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.58it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.63it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.54it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.57it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.54it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.61it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.58it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.48it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.55it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.62it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.65it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.61it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.63it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.67it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.62it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.62it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.51it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.48it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.48it/s][A 80%|████████  | 468/585 [03:15<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:56:52,650 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:56:52,668 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:56:55,518 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:56:55,531 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:56:55,541 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:24<10:35,  5.48s/it] 80%|████████  | 470/585 [03:24<07:31,  3.92s/it] 81%|████████  | 471/585 [03:24<05:22,  2.83s/it] 81%|████████  | 472/585 [03:24<03:53,  2.07s/it] 81%|████████  | 473/585 [03:25<02:51,  1.53s/it] 81%|████████  | 474/585 [03:25<02:08,  1.16s/it] 81%|████████  | 475/585 [03:25<01:38,  1.11it/s] 81%|████████▏ | 476/585 [03:26<01:18,  1.40it/s] 82%|████████▏ | 477/585 [03:26<01:03,  1.70it/s] 82%|████████▏ | 478/585 [03:26<00:53,  2.01it/s] 82%|████████▏ | 479/585 [03:26<00:46,  2.30it/s] 82%|████████▏ | 480/585 [03:27<00:41,  2.56it/s] 82%|████████▏ | 481/585 [03:27<00:37,  2.77it/s] 82%|████████▏ | 482/585 [03:27<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:28<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:28<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:28<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:28<00:29,  3.33it/s] 83%|████████▎ | 487/585 [03:29<00:29,  3.37it/s] 83%|████████▎ | 488/585 [03:29<00:28,  3.40it/s] 84%|████████▎ | 489/585 [03:29<00:28,  3.42it/s] 84%|████████▍ | 490/585 [03:30<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:30<00:27,  3.45it/s] 84%|████████▍ | 492/585 [03:30<00:27,  3.44it/s] 84%|████████▍ | 493/585 [03:30<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:31<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:31<00:26,  3.46it/s] 85%|████████▍ | 496/585 [03:31<00:25,  3.46it/s] 85%|████████▍ | 497/585 [03:32<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:32<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:32<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:33<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [03:33<00:24,  3.47it/s] 86%|████████▌ | 501/585 [03:33<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:33<00:23,  3.46it/s] 86%|████████▌ | 503/585 [03:33<00:23,  3.44it/s] 86%|████████▌ | 504/585 [03:34<00:23,  3.45it/s] 86%|████████▋ | 505/585 [03:34<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:34<00:22,  3.46it/s] 87%|████████▋ | 507/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 508/585 [03:35<00:22,  3.46it/s] 87%|████████▋ | 509/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:35<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:36<00:21,  3.47it/s] 88%|████████▊ | 512/585 [03:36<00:21,  3.47it/s] 88%|████████▊ | 513/585 [03:36<00:20,  3.47it/s] 88%|████████▊ | 514/585 [03:37<00:20,  3.46it/s] 88%|████████▊ | 515/585 [03:37<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:37<00:19,  3.46it/s] 88%|████████▊ | 517/585 [03:37<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:38<00:19,  3.47it/s] 89%|████████▊ | 519/585 [03:38<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:38<00:18,  3.47it/s] 89%|████████▉ | 521/585 [03:39<00:18,  3.47it/s] 89%|████████▉ | 522/585 [03:39<00:18,  3.47it/s] 89%|████████▉ | 523/585 [03:39<00:17,  3.47it/s] 90%|████████▉ | 524/585 [03:39<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:40<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:40<00:17,  3.46it/s] 90%|█████████ | 527/585 [03:40<00:16,  3.46it/s] 90%|█████████ | 528/585 [03:41<00:16,  3.46it/s] 90%|█████████ | 529/585 [03:41<00:16,  3.46it/s] 91%|█████████ | 530/585 [03:41<00:15,  3.46it/s] 91%|█████████ | 531/585 [03:41<00:15,  3.46it/s] 91%|█████████ | 532/585 [03:42<00:15,  3.46it/s] 91%|█████████ | 533/585 [03:42<00:15,  3.46it/s] 91%|█████████▏| 534/585 [03:42<00:14,  3.46it/s] 91%|█████████▏| 535/585 [03:43<00:14,  3.46it/s] 92%|█████████▏| 536/585 [03:43<00:14,  3.45it/s] 92%|█████████▏| 537/585 [03:43<00:13,  3.45it/s] 92%|█████████▏| 538/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:44<00:13,  3.46it/s] 92%|█████████▏| 541/585 [03:44<00:12,  3.46it/s] 93%|█████████▎| 542/585 [03:45<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:45<00:12,  3.46it/s] 93%|█████████▎| 544/585 [03:45<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:46<00:11,  3.46it/s] 93%|█████████▎| 546/585 [03:46<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:46<00:11,  3.43it/s] 94%|█████████▎| 548/585 [03:46<00:10,  3.44it/s] 94%|█████████▍| 549/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:47<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:48<00:09,  3.46it/s] 95%|█████████▍| 553/585 [03:48<00:09,  3.46it/s] 95%|█████████▍| 554/585 [03:48<00:08,  3.46it/s] 95%|█████████▍| 555/585 [03:48<00:08,  3.46it/s] 95%|█████████▌| 556/585 [03:49<00:08,  3.46it/s] 95%|█████████▌| 557/585 [03:49<00:08,  3.46it/s] 95%|█████████▌| 558/585 [03:49<00:07,  3.46it/s] 96%|█████████▌| 559/585 [03:50<00:07,  3.46it/s] 96%|█████████▌| 560/585 [03:50<00:07,  3.46it/s] 96%|█████████▌| 561/585 [03:50<00:06,  3.46it/s] 96%|█████████▌| 562/585 [03:50<00:06,  3.46it/s] 96%|█████████▌| 563/585 [03:51<00:06,  3.46it/s] 96%|█████████▋| 564/585 [03:51<00:06,  3.38it/s] 97%|█████████▋| 565/585 [03:51<00:05,  3.40it/s] 97%|█████████▋| 566/585 [03:52<00:05,  3.42it/s] 97%|█████████▋| 567/585 [03:52<00:05,  3.43it/s] 97%|█████████▋| 568/585 [03:52<00:04,  3.44it/s] 97%|█████████▋| 569/585 [03:52<00:04,  3.45it/s] 97%|█████████▋| 570/585 [03:53<00:04,  3.44it/s] 98%|█████████▊| 571/585 [03:53<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:53<00:03,  3.45it/s] 98%|█████████▊| 573/585 [03:54<00:03,  3.45it/s] 98%|█████████▊| 574/585 [03:54<00:03,  3.46it/s] 98%|█████████▊| 575/585 [03:54<00:02,  3.46it/s] 98%|█████████▊| 576/585 [03:55<00:02,  3.46it/s] 99%|█████████▊| 577/585 [03:55<00:02,  3.46it/s] 99%|█████████▉| 578/585 [03:55<00:02,  3.46it/s] 99%|█████████▉| 579/585 [03:55<00:01,  3.46it/s] 99%|█████████▉| 580/585 [03:56<00:01,  3.46it/s] 99%|█████████▉| 581/585 [03:56<00:01,  3.46it/s] 99%|█████████▉| 582/585 [03:56<00:00,  3.46it/s]100%|█████████▉| 583/585 [03:57<00:00,  3.46it/s]100%|█████████▉| 584/585 [03:57<00:00,  3.46it/s]100%|██████████| 585/585 [03:57<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 23:57:34,344 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:57:34,344 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:57:34,344 >>   Batch size = 8
{'eval_loss': 1.1224470138549805, 'eval_runtime': 9.3802, 'eval_samples_per_second': 372.381, 'eval_steps_per_second': 46.588, 'epoch': 4.0}
{'loss': 0.4053, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.98it/s][A
  3%|▎         | 12/437 [00:00<00:08, 50.59it/s][A
  4%|▍         | 18/437 [00:00<00:08, 48.71it/s][A
  5%|▌         | 23/437 [00:00<00:08, 47.96it/s][A
  6%|▋         | 28/437 [00:00<00:08, 47.62it/s][A
  8%|▊         | 33/437 [00:00<00:08, 47.41it/s][A
  9%|▊         | 38/437 [00:00<00:08, 47.12it/s][A
 10%|▉         | 43/437 [00:00<00:08, 46.76it/s][A
 11%|█         | 48/437 [00:01<00:08, 46.64it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 46.58it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 46.61it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 46.64it/s][A
 16%|█▌        | 68/437 [00:01<00:07, 46.65it/s][A
 17%|█▋        | 73/437 [00:01<00:07, 46.62it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 46.71it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 46.71it/s][A
 20%|██        | 88/437 [00:01<00:07, 46.67it/s][A
 21%|██▏       | 93/437 [00:01<00:07, 46.57it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 46.56it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 46.61it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 46.58it/s][A
 26%|██▌       | 113/437 [00:02<00:06, 46.60it/s][A
 27%|██▋       | 118/437 [00:02<00:06, 46.55it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 46.62it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 46.69it/s][A
 30%|███       | 133/437 [00:02<00:06, 46.70it/s][A
 32%|███▏      | 138/437 [00:02<00:06, 46.71it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 46.55it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 46.62it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 46.66it/s][A
 36%|███▌      | 158/437 [00:03<00:05, 46.59it/s][A
 37%|███▋      | 163/437 [00:03<00:05, 46.57it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 46.57it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 46.52it/s][A
 41%|████      | 178/437 [00:03<00:05, 46.63it/s][A
 42%|████▏     | 183/437 [00:03<00:05, 46.57it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 46.66it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 46.59it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 46.65it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 46.63it/s][A
 48%|████▊     | 208/437 [00:04<00:04, 46.66it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 46.57it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 46.58it/s][A
 51%|█████     | 223/437 [00:04<00:04, 46.58it/s][A
 52%|█████▏    | 228/437 [00:04<00:04, 46.62it/s][A
 53%|█████▎    | 233/437 [00:04<00:04, 46.57it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 46.52it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 46.62it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 46.65it/s][A
 58%|█████▊    | 253/437 [00:05<00:03, 46.67it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 46.62it/s][A
 60%|██████    | 263/437 [00:05<00:03, 46.60it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 273/437 [00:05<00:03, 46.54it/s][A
 64%|██████▎   | 278/437 [00:05<00:03, 46.59it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 46.53it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 46.51it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 46.57it/s][A
 68%|██████▊   | 298/437 [00:06<00:02, 46.65it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 46.56it/s][A
 70%|███████   | 308/437 [00:06<00:02, 46.61it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 46.65it/s][A
 73%|███████▎  | 318/437 [00:06<00:02, 46.58it/s][A
 74%|███████▍  | 323/437 [00:06<00:02, 46.64it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 46.65it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 46.54it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 46.51it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 46.61it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 46.62it/s][A
 81%|████████  | 353/437 [00:07<00:01, 46.64it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 46.62it/s][A
 83%|████████▎ | 363/437 [00:07<00:01, 46.54it/s][A
 84%|████████▍ | 368/437 [00:07<00:01, 46.60it/s][A
 85%|████████▌ | 373/437 [00:07<00:01, 46.62it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 46.65it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 46.60it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 46.53it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 46.49it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 46.61it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 46.56it/s][A
 93%|█████████▎| 408/437 [00:08<00:00, 46.60it/s][A
 95%|█████████▍| 413/437 [00:08<00:00, 46.59it/s][A
 96%|█████████▌| 418/437 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 46.65it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 46.57it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 46.63it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 46.63it/s][A100%|██████████| 585/585 [04:06<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:57:43,738 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:57:43,750 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:57:46,182 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:57:46,201 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:57:46,208 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:57:51,993 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:57:51,998 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117 (score: 1.0784447193145752).
                                                 100%|██████████| 585/585 [04:17<00:00,  3.46it/s]100%|██████████| 585/585 [04:17<00:00,  2.27it/s]
[INFO|trainer.py:1894] 2023-08-28 23:57:54,153 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 23:57:54,170 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:57:56,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:57:56,848 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:57:56,857 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:57:57,119 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   train_loss               =     0.4022
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   train_runtime            = 0:04:17.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   train_samples_per_second =    145.674
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:57:57,124 >>   train_steps_per_second   =      2.273
{'eval_loss': 1.1286065578460693, 'eval_runtime': 9.3716, 'eval_samples_per_second': 372.722, 'eval_steps_per_second': 46.63, 'epoch': 5.0}
{'train_runtime': 257.4241, 'train_samples_per_second': 145.674, 'train_steps_per_second': 2.273, 'train_loss': 0.4021597576956464, 'epoch': 5.0}
08/28/2023 23:57:57 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:57:57,160 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:57:57,160 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 23:57:57,160 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 58.84it/s]  3%|▎         | 12/437 [00:00<00:08, 51.17it/s]  4%|▍         | 18/437 [00:00<00:08, 49.25it/s]  5%|▌         | 23/437 [00:00<00:08, 48.51it/s]  6%|▋         | 28/437 [00:00<00:08, 48.02it/s]  8%|▊         | 33/437 [00:00<00:08, 47.63it/s]  9%|▊         | 38/437 [00:00<00:08, 47.52it/s] 10%|▉         | 43/437 [00:00<00:08, 47.33it/s] 11%|█         | 48/437 [00:00<00:08, 47.02it/s] 12%|█▏        | 53/437 [00:01<00:08, 46.84it/s] 13%|█▎        | 58/437 [00:01<00:08, 46.86it/s] 14%|█▍        | 63/437 [00:01<00:07, 46.91it/s] 16%|█▌        | 68/437 [00:01<00:07, 46.87it/s] 17%|█▋        | 73/437 [00:01<00:07, 46.94it/s] 18%|█▊        | 78/437 [00:01<00:07, 46.99it/s] 19%|█▉        | 83/437 [00:01<00:07, 47.00it/s] 20%|██        | 88/437 [00:01<00:07, 47.08it/s] 21%|██▏       | 93/437 [00:01<00:07, 47.10it/s] 22%|██▏       | 98/437 [00:02<00:07, 47.01it/s] 24%|██▎       | 103/437 [00:02<00:07, 46.92it/s] 25%|██▍       | 108/437 [00:02<00:07, 46.95it/s] 26%|██▌       | 113/437 [00:02<00:06, 46.84it/s] 27%|██▋       | 118/437 [00:02<00:06, 46.86it/s] 28%|██▊       | 123/437 [00:02<00:06, 46.93it/s] 29%|██▉       | 128/437 [00:02<00:06, 46.90it/s] 30%|███       | 133/437 [00:02<00:06, 46.97it/s] 32%|███▏      | 138/437 [00:02<00:06, 46.91it/s] 33%|███▎      | 143/437 [00:03<00:06, 47.01it/s] 34%|███▍      | 148/437 [00:03<00:06, 46.93it/s] 35%|███▌      | 153/437 [00:03<00:06, 47.01it/s] 36%|███▌      | 158/437 [00:03<00:05, 46.90it/s] 37%|███▋      | 163/437 [00:03<00:05, 46.81it/s] 38%|███▊      | 168/437 [00:03<00:05, 46.84it/s] 40%|███▉      | 173/437 [00:03<00:05, 46.86it/s] 41%|████      | 178/437 [00:03<00:05, 46.89it/s] 42%|████▏     | 183/437 [00:03<00:05, 46.92it/s] 43%|████▎     | 188/437 [00:03<00:05, 46.86it/s] 44%|████▍     | 193/437 [00:04<00:05, 46.92it/s] 45%|████▌     | 198/437 [00:04<00:05, 46.97it/s] 46%|████▋     | 203/437 [00:04<00:04, 46.96it/s] 48%|████▊     | 208/437 [00:04<00:04, 47.03it/s] 49%|████▊     | 213/437 [00:04<00:04, 46.88it/s] 50%|████▉     | 218/437 [00:04<00:04, 46.91it/s] 51%|█████     | 223/437 [00:04<00:04, 46.91it/s] 52%|█████▏    | 228/437 [00:04<00:04, 46.91it/s] 53%|█████▎    | 233/437 [00:04<00:04, 46.93it/s] 54%|█████▍    | 238/437 [00:05<00:04, 46.96it/s] 56%|█████▌    | 243/437 [00:05<00:04, 46.90it/s] 57%|█████▋    | 248/437 [00:05<00:04, 46.89it/s] 58%|█████▊    | 253/437 [00:05<00:03, 46.96it/s] 59%|█████▉    | 258/437 [00:05<00:03, 46.95it/s] 60%|██████    | 263/437 [00:05<00:03, 46.89it/s] 61%|██████▏   | 268/437 [00:05<00:03, 46.93it/s] 62%|██████▏   | 273/437 [00:05<00:03, 46.94it/s] 64%|██████▎   | 278/437 [00:05<00:03, 46.90it/s] 65%|██████▍   | 283/437 [00:06<00:03, 46.90it/s] 66%|██████▌   | 288/437 [00:06<00:03, 46.91it/s] 67%|██████▋   | 293/437 [00:06<00:03, 46.90it/s] 68%|██████▊   | 298/437 [00:06<00:02, 46.83it/s] 69%|██████▉   | 303/437 [00:06<00:02, 46.91it/s] 70%|███████   | 308/437 [00:06<00:02, 46.85it/s] 72%|███████▏  | 313/437 [00:06<00:02, 46.78it/s] 73%|███████▎  | 318/437 [00:06<00:02, 46.91it/s] 74%|███████▍  | 323/437 [00:06<00:02, 46.92it/s] 75%|███████▌  | 328/437 [00:06<00:02, 46.90it/s] 76%|███████▌  | 333/437 [00:07<00:02, 46.96it/s] 77%|███████▋  | 338/437 [00:07<00:02, 46.93it/s] 78%|███████▊  | 343/437 [00:07<00:02, 46.93it/s] 80%|███████▉  | 348/437 [00:07<00:01, 46.85it/s] 81%|████████  | 353/437 [00:07<00:01, 46.93it/s] 82%|████████▏ | 358/437 [00:07<00:01, 46.88it/s] 83%|████████▎ | 363/437 [00:07<00:01, 46.91it/s] 84%|████████▍ | 368/437 [00:07<00:01, 46.83it/s] 85%|████████▌ | 373/437 [00:07<00:01, 46.84it/s] 86%|████████▋ | 378/437 [00:08<00:01, 46.86it/s] 88%|████████▊ | 383/437 [00:08<00:01, 46.85it/s] 89%|████████▉ | 388/437 [00:08<00:01, 46.91it/s] 90%|████████▉ | 393/437 [00:08<00:00, 46.95it/s] 91%|█████████ | 398/437 [00:08<00:00, 46.87it/s] 92%|█████████▏| 403/437 [00:08<00:00, 46.85it/s] 93%|█████████▎| 408/437 [00:08<00:00, 46.88it/s] 95%|█████████▍| 413/437 [00:08<00:00, 46.88it/s] 96%|█████████▌| 418/437 [00:08<00:00, 46.80it/s] 97%|█████████▋| 423/437 [00:08<00:00, 46.80it/s] 98%|█████████▊| 428/437 [00:09<00:00, 46.83it/s] 99%|█████████▉| 433/437 [00:09<00:00, 46.85it/s]100%|██████████| 437/437 [00:09<00:00, 47.02it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:58:06,477 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   eval_loss               =     1.0784
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   eval_samples_per_second =    374.895
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   eval_steps_per_second   =     46.902
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:58:06,478 >>   perplexity              =     2.9401
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:11,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:11,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:11,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:11,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:11,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:58:12,382 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:58:12,383 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:58:12,656 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:58:13,680 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:58:13,681 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:15,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:15,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:15,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:15,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:58:15,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:58:16,064 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:58:16,065 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:58:16,645 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:58:16,809 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:58:16,809 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:02,  1.35it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:05,  1.44it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:07,  1.47it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:17,  1.51it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:19,  1.48it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:20,  1.55it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:28,  1.55it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.56it/s]Extractor Predicting: 45it [00:30,  1.53it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.57it/s]Extractor Predicting: 49it [00:32,  1.59it/s]Extractor Predicting: 50it [00:33,  1.55it/s]Extractor Predicting: 51it [00:34,  1.54it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:35,  1.58it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:36,  1.58it/s]Extractor Predicting: 56it [00:37,  1.55it/s]Extractor Predicting: 57it [00:37,  1.57it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:39,  1.53it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:41,  1.51it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:43,  1.49it/s]Extractor Predicting: 67it [00:44,  1.47it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:45,  1.47it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:47,  1.53it/s]Extractor Predicting: 73it [00:48,  1.52it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:51,  1.47it/s]Extractor Predicting: 79it [00:52,  1.47it/s]Extractor Predicting: 80it [00:53,  1.47it/s]Extractor Predicting: 81it [00:54,  1.46it/s]Extractor Predicting: 82it [00:54,  1.46it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:57,  1.45it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.48it/s]Extractor Predicting: 90it [01:00,  1.45it/s]Extractor Predicting: 91it [01:00,  1.44it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:02,  1.46it/s]Extractor Predicting: 94it [01:02,  1.48it/s]Extractor Predicting: 95it [01:03,  1.52it/s]Extractor Predicting: 96it [01:04,  1.52it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:05,  1.50it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:06,  1.51it/s]Extractor Predicting: 101it [01:07,  1.50it/s]Extractor Predicting: 102it [01:08,  1.37it/s]Extractor Predicting: 103it [01:09,  1.39it/s]Extractor Predicting: 104it [01:09,  1.41it/s]Extractor Predicting: 105it [01:10,  1.44it/s]Extractor Predicting: 106it [01:11,  1.43it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.43it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:15,  1.45it/s]Extractor Predicting: 113it [01:15,  1.43it/s]Extractor Predicting: 114it [01:16,  1.42it/s]Extractor Predicting: 115it [01:17,  1.41it/s]Extractor Predicting: 116it [01:18,  1.42it/s]Extractor Predicting: 117it [01:18,  1.42it/s]Extractor Predicting: 118it [01:19,  1.42it/s]Extractor Predicting: 119it [01:20,  1.43it/s]Extractor Predicting: 120it [01:20,  1.44it/s]Extractor Predicting: 121it [01:21,  1.43it/s]Extractor Predicting: 122it [01:22,  1.42it/s]Extractor Predicting: 123it [01:22,  1.47it/s]Extractor Predicting: 124it [01:23,  1.47it/s]Extractor Predicting: 125it [01:24,  1.43it/s]Extractor Predicting: 126it [01:25,  1.44it/s]Extractor Predicting: 127it [01:25,  1.45it/s]Extractor Predicting: 128it [01:26,  1.45it/s]Extractor Predicting: 129it [01:27,  1.44it/s]Extractor Predicting: 130it [01:27,  1.43it/s]Extractor Predicting: 131it [01:28,  1.41it/s]Extractor Predicting: 132it [01:29,  1.40it/s]Extractor Predicting: 133it [01:29,  1.39it/s]Extractor Predicting: 134it [01:30,  1.38it/s]Extractor Predicting: 135it [01:31,  1.40it/s]Extractor Predicting: 136it [01:31,  1.71it/s]Extractor Predicting: 136it [01:31,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:56,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:59:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:59:57,104 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:59:57,105 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:59:57,755 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:59:58,764 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:59:58,764 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:01,657 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:01,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:01,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:01,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:00:01,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:00:02,362 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:00:02,363 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:00:02,940 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:00:03,092 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:00:03,092 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.42142857142857143,
  "recall": 0.2195820211852276,
  "score": 0.28872576698663655,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:21,  1.66it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:23,  1.58it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.60it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:28,  1.61it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:30,  1.61it/s]Extractor Predicting: 49it [00:31,  1.62it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.67it/s]Extractor Predicting: 52it [00:32,  1.68it/s]Extractor Predicting: 53it [00:33,  1.63it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:39,  1.55it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:42,  1.64it/s]Extractor Predicting: 69it [00:43,  1.65it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:44,  1.63it/s]Extractor Predicting: 72it [00:45,  1.59it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:47,  1.55it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:51,  1.58it/s]Extractor Predicting: 82it [00:51,  1.60it/s]Extractor Predicting: 83it [00:52,  1.56it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:53,  1.54it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:55,  1.53it/s]Extractor Predicting: 89it [00:56,  1.51it/s]Extractor Predicting: 90it [00:57,  1.49it/s]Extractor Predicting: 91it [00:57,  1.49it/s]Extractor Predicting: 92it [00:58,  1.48it/s]Extractor Predicting: 93it [00:59,  1.46it/s]Extractor Predicting: 94it [00:59,  1.47it/s]Extractor Predicting: 95it [01:00,  1.50it/s]Extractor Predicting: 96it [01:01,  1.50it/s]Extractor Predicting: 97it [01:01,  1.48it/s]Extractor Predicting: 98it [01:02,  1.47it/s]Extractor Predicting: 99it [01:03,  1.46it/s]Extractor Predicting: 100it [01:03,  1.45it/s]Extractor Predicting: 101it [01:04,  1.45it/s]Extractor Predicting: 102it [01:05,  1.44it/s]Extractor Predicting: 103it [01:06,  1.48it/s]Extractor Predicting: 104it [01:06,  1.48it/s]Extractor Predicting: 105it [01:07,  1.49it/s]Extractor Predicting: 106it [01:08,  1.31it/s]Extractor Predicting: 107it [01:08,  1.36it/s]Extractor Predicting: 108it [01:09,  1.36it/s]Extractor Predicting: 109it [01:10,  1.37it/s]Extractor Predicting: 110it [01:11,  1.37it/s]Extractor Predicting: 111it [01:11,  1.40it/s]Extractor Predicting: 112it [01:12,  1.40it/s]Extractor Predicting: 113it [01:13,  1.41it/s]Extractor Predicting: 114it [01:13,  1.41it/s]Extractor Predicting: 115it [01:14,  1.40it/s]Extractor Predicting: 116it [01:15,  1.42it/s]Extractor Predicting: 117it [01:16,  1.42it/s]Extractor Predicting: 118it [01:16,  1.44it/s]Extractor Predicting: 119it [01:17,  1.46it/s]Extractor Predicting: 120it [01:18,  1.50it/s]Extractor Predicting: 121it [01:18,  1.49it/s]Extractor Predicting: 122it [01:19,  1.48it/s]Extractor Predicting: 123it [01:20,  1.48it/s]Extractor Predicting: 124it [01:20,  1.47it/s]Extractor Predicting: 125it [01:21,  1.50it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:22,  1.54it/s]Extractor Predicting: 128it [01:23,  1.56it/s]Extractor Predicting: 129it [01:23,  1.54it/s]Extractor Predicting: 130it [01:24,  1.50it/s]Extractor Predicting: 131it [01:25,  1.51it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.54it/s]Extractor Predicting: 134it [01:27,  1.57it/s]Extractor Predicting: 135it [01:27,  1.54it/s]Extractor Predicting: 136it [01:28,  1.56it/s]Extractor Predicting: 137it [01:29,  1.58it/s]Extractor Predicting: 138it [01:29,  1.55it/s]Extractor Predicting: 139it [01:30,  1.56it/s]Extractor Predicting: 140it [01:31,  1.55it/s]Extractor Predicting: 141it [01:31,  1.50it/s]Extractor Predicting: 142it [01:32,  1.49it/s]Extractor Predicting: 143it [01:33,  1.50it/s]Extractor Predicting: 144it [01:33,  1.49it/s]Extractor Predicting: 145it [01:34,  1.48it/s]Extractor Predicting: 146it [01:35,  1.51it/s]Extractor Predicting: 147it [01:35,  1.53it/s]Extractor Predicting: 148it [01:36,  1.51it/s]Extractor Predicting: 149it [01:37,  1.51it/s]Extractor Predicting: 150it [01:37,  1.51it/s]Extractor Predicting: 151it [01:38,  1.43it/s]Extractor Predicting: 152it [01:39,  1.45it/s]Extractor Predicting: 153it [01:39,  1.46it/s]Extractor Predicting: 154it [01:40,  1.50it/s]Extractor Predicting: 155it [01:41,  1.48it/s]Extractor Predicting: 156it [01:41,  1.47it/s]Extractor Predicting: 157it [01:42,  1.45it/s]Extractor Predicting: 158it [01:43,  1.47it/s]Extractor Predicting: 159it [01:43,  1.46it/s]Extractor Predicting: 160it [01:44,  1.45it/s]Extractor Predicting: 161it [01:45,  1.46it/s]Extractor Predicting: 162it [01:46,  1.46it/s]Extractor Predicting: 163it [01:46,  1.46it/s]Extractor Predicting: 164it [01:47,  1.47it/s]Extractor Predicting: 165it [01:48,  1.47it/s]Extractor Predicting: 166it [01:48,  1.47it/s]Extractor Predicting: 167it [01:49,  1.45it/s]Extractor Predicting: 168it [01:50,  1.45it/s]Extractor Predicting: 169it [01:50,  1.45it/s]Extractor Predicting: 170it [01:51,  1.44it/s]Extractor Predicting: 171it [01:52,  1.45it/s]Extractor Predicting: 172it [01:52,  1.45it/s]Extractor Predicting: 173it [01:53,  1.45it/s]Extractor Predicting: 174it [01:54,  1.45it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:55,  1.49it/s]Extractor Predicting: 177it [01:56,  1.51it/s]Extractor Predicting: 178it [01:56,  1.52it/s]Extractor Predicting: 179it [01:57,  1.53it/s]Extractor Predicting: 180it [01:58,  1.52it/s]Extractor Predicting: 181it [01:58,  1.54it/s]Extractor Predicting: 182it [01:59,  1.55it/s]Extractor Predicting: 183it [02:00,  1.49it/s]Extractor Predicting: 184it [02:00,  1.56it/s]Extractor Predicting: 185it [02:01,  1.54it/s]Extractor Predicting: 186it [02:02,  1.39it/s]Extractor Predicting: 187it [02:02,  1.43it/s]Extractor Predicting: 188it [02:03,  1.42it/s]Extractor Predicting: 189it [02:04,  1.44it/s]Extractor Predicting: 190it [02:05,  1.45it/s]Extractor Predicting: 191it [02:05,  1.44it/s]Extractor Predicting: 192it [02:06,  1.45it/s]Extractor Predicting: 193it [02:07,  1.49it/s]Extractor Predicting: 194it [02:07,  1.49it/s]Extractor Predicting: 195it [02:08,  1.46it/s]Extractor Predicting: 196it [02:09,  1.43it/s]Extractor Predicting: 197it [02:09,  1.46it/s]Extractor Predicting: 198it [02:10,  1.45it/s]Extractor Predicting: 199it [02:11,  1.48it/s]Extractor Predicting: 200it [02:11,  1.47it/s]Extractor Predicting: 201it [02:12,  1.49it/s]Extractor Predicting: 202it [02:13,  1.50it/s]Extractor Predicting: 203it [02:13,  1.50it/s]Extractor Predicting: 204it [02:14,  1.53it/s]Extractor Predicting: 205it [02:15,  1.52it/s]Extractor Predicting: 206it [02:15,  1.55it/s]Extractor Predicting: 207it [02:16,  1.52it/s]Extractor Predicting: 208it [02:17,  1.50it/s]Extractor Predicting: 209it [02:17,  1.52it/s]Extractor Predicting: 210it [02:18,  1.52it/s]Extractor Predicting: 211it [02:19,  1.50it/s]Extractor Predicting: 212it [02:19,  1.51it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:21,  1.52it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:23,  1.50it/s]Extractor Predicting: 218it [02:23,  1.52it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:24,  1.51it/s]Extractor Predicting: 221it [02:25,  1.54it/s]Extractor Predicting: 222it [02:26,  1.54it/s]Extractor Predicting: 223it [02:26,  1.49it/s]Extractor Predicting: 224it [02:27,  1.53it/s]Extractor Predicting: 225it [02:28,  1.52it/s]Extractor Predicting: 226it [02:28,  1.51it/s]Extractor Predicting: 227it [02:29,  1.51it/s]Extractor Predicting: 228it [02:30,  1.49it/s]Extractor Predicting: 229it [02:30,  1.50it/s]Extractor Predicting: 230it [02:31,  1.53it/s]Extractor Predicting: 231it [02:32,  1.54it/s]Extractor Predicting: 232it [02:32,  1.53it/s]Extractor Predicting: 233it [02:33,  1.53it/s]Extractor Predicting: 234it [02:34,  1.52it/s]Extractor Predicting: 235it [02:34,  1.50it/s]Extractor Predicting: 236it [02:35,  1.51it/s]Extractor Predicting: 237it [02:36,  1.51it/s]Extractor Predicting: 238it [02:36,  1.50it/s]Extractor Predicting: 239it [02:37,  1.49it/s]Extractor Predicting: 240it [02:38,  1.50it/s]Extractor Predicting: 241it [02:38,  1.50it/s]Extractor Predicting: 242it [02:39,  1.50it/s]Extractor Predicting: 243it [02:40,  1.51it/s]Extractor Predicting: 244it [02:40,  1.49it/s]Extractor Predicting: 245it [02:41,  1.51it/s]Extractor Predicting: 246it [02:42,  1.51it/s]Extractor Predicting: 247it [02:42,  1.52it/s]Extractor Predicting: 248it [02:43,  1.51it/s]Extractor Predicting: 249it [02:44,  1.54it/s]Extractor Predicting: 250it [02:44,  1.53it/s]Extractor Predicting: 251it [02:45,  1.57it/s]Extractor Predicting: 252it [02:46,  1.54it/s]Extractor Predicting: 253it [02:46,  1.53it/s]Extractor Predicting: 254it [02:47,  1.53it/s]Extractor Predicting: 255it [02:48,  1.52it/s]Extractor Predicting: 256it [02:48,  1.49it/s]Extractor Predicting: 257it [02:49,  1.50it/s]Extractor Predicting: 258it [02:50,  1.50it/s]Extractor Predicting: 259it [02:50,  1.54it/s]Extractor Predicting: 260it [02:51,  1.53it/s]Extractor Predicting: 261it [02:51,  1.55it/s]Extractor Predicting: 262it [02:52,  1.53it/s]Extractor Predicting: 263it [02:53,  1.52it/s]Extractor Predicting: 264it [02:53,  1.58it/s]Extractor Predicting: 265it [02:54,  1.57it/s]Extractor Predicting: 266it [02:55,  1.58it/s]Extractor Predicting: 267it [02:55,  1.56it/s]Extractor Predicting: 268it [02:56,  1.55it/s]Extractor Predicting: 269it [02:57,  1.54it/s]Extractor Predicting: 270it [02:57,  1.50it/s]Extractor Predicting: 271it [02:58,  1.49it/s]Extractor Predicting: 272it [02:59,  1.50it/s]Extractor Predicting: 273it [02:59,  1.50it/s]Extractor Predicting: 274it [03:00,  1.48it/s]Extractor Predicting: 275it [03:01,  1.45it/s]Extractor Predicting: 276it [03:01,  1.47it/s]Extractor Predicting: 277it [03:02,  1.47it/s]Extractor Predicting: 278it [03:03,  1.51it/s]Extractor Predicting: 279it [03:03,  1.50it/s]Extractor Predicting: 280it [03:04,  1.34it/s]Extractor Predicting: 281it [03:05,  1.40it/s]Extractor Predicting: 282it [03:06,  1.42it/s]Extractor Predicting: 283it [03:06,  1.43it/s]Extractor Predicting: 284it [03:07,  1.45it/s]Extractor Predicting: 285it [03:08,  1.45it/s]Extractor Predicting: 286it [03:08,  1.50it/s]Extractor Predicting: 287it [03:09,  1.47it/s]Extractor Predicting: 288it [03:09,  1.68it/s]Extractor Predicting: 288it [03:09,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:23,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:23,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:23,841 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:23,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:23,842 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:03:24,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:03:24,447 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:03:25,013 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:03:26,067 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:03:26,067 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:29,068 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:29,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:29,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:29,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:03:29,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:03:29,836 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:03:29,837 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:03:30,471 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:03:30,690 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:03:30,690 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3641114982578397,
  "recall": 0.15144927536231884,
  "score": 0.21392016376663253,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:01,  2.26it/s]Extractor Predicting: 3it [00:01,  1.96it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.06,
  "score": 0.10714285714285712,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:18<04:22, 18.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:51, 17.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:55<03:43, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:13<03:20, 18.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:31<03:03, 18.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:49<02:42, 18.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:10<02:33, 19.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:27<02:08, 18.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:49<01:57, 19.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:09<01:38, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:28<01:17, 19.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:45<00:55, 18.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:02<00:36, 18.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:22<00:18, 18.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:49<00:00, 21.33s/it]Generating: 100%|██████████| 15/15 [04:49<00:00, 19.31s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8247282608695652, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.80078125, 'errors': {'', "('Sri Lanka', 'member of sports team', '', 'The 2008 Asian Cup , the first Asian Cup in Sri Lanka , was played in Malaysia by Asian team Akshay Kumar Sathy , who was later named in the team of Sri Lanka .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : notable work .', 'success_rate': 0.7903645833333334, 'errors': {'', "('Richard Durbin', 'notable work', '', 'He was a founding member of the Council on Foreign Relations with Richard Durbin in 1799 and is sometimes called the inventor of the printing press .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 358, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 429, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 493, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 588, 'raw': 800}
{'target': 600, 'success': 610, 'raw': 832}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.7331730769230769, 'errors': {'', "('Liberal Party of Canada', 'successful candidate', '', 'He was a member of the Liberal Party of Canada ( MP ) from 1994 to 1996 and was elected to Parliament from Ontario in 1996 .')", 'not enough values to unpack (expected 2, got 1)', "('2008', 'successful candidate', '', 'At the 2008 Summer Olympics , she competed as a boxer in two Olympics in Athens ( 2008 ) and Budapest ( 2008 ) .')", "('Royal College of Music', 'successful candidate', '', 'Born in Leeds he studied in the Royal College of Music in Gloucestershire and also worked as a drummer .')", "('Progressive Conservative Party of Canada', 'successful candidate', '', 'He also ran for the federal government in the 2010 and 2012 elections as the Progressive Conservative Party of Canada ( PPC ) candidate for the Ottawa riding of Etobicoke .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : director .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 109, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 145, 'raw': 224}
{'target': 600, 'success': 168, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 209, 'raw': 320}
{'target': 600, 'success': 225, 'raw': 352}
{'target': 600, 'success': 244, 'raw': 384}
{'target': 600, 'success': 262, 'raw': 416}
{'target': 600, 'success': 278, 'raw': 448}
{'target': 600, 'success': 296, 'raw': 480}
{'target': 600, 'success': 318, 'raw': 512}
{'target': 600, 'success': 341, 'raw': 544}
{'target': 600, 'success': 362, 'raw': 576}
{'target': 600, 'success': 384, 'raw': 608}
{'target': 600, 'success': 407, 'raw': 640}
{'target': 600, 'success': 426, 'raw': 672}
{'target': 600, 'success': 447, 'raw': 704}
{'target': 600, 'success': 466, 'raw': 736}
{'target': 600, 'success': 491, 'raw': 768}
{'target': 600, 'success': 509, 'raw': 800}
{'target': 600, 'success': 534, 'raw': 832}
{'target': 600, 'success': 552, 'raw': 864}
{'target': 600, 'success': 577, 'raw': 896}
{'target': 600, 'success': 596, 'raw': 928}
{'target': 600, 'success': 616, 'raw': 960}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.6416666666666667, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Pittsburgh Pirates', 'drafted by', '', 'He was selected at the third round by the Pittsburgh Pirates during the 2004 MLB Draft and was signed by the Cincinnati Reds in 2006 .')", "('Jack Kerouac', 'drafted by', '', 'He played 16 games for the St. Louis Blues and Colorado Avalanche during the 1970 NHL season , and in 1982 was traded to Detroit for goaltender Jack Kerouac .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8536931818181818, 'errors': {'', "('Korda', 'lyrics by', '', 'After a few weeks in Berlin , she sang in Korda at the Royal Opera with a different score and is now in Darmstadt and Hamburg singing in the Opera Hall .')"}}
['Relation : main subject . Context : The main characters in the series are The New Man , The Phantom of the Opera at the end of the third season and also a fictional character named The Young Prince of the Opera , played by Richard R. Noyes . Head Entity : The Phantom of Opera , Tail Entity : The New Man .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 81, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 149, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 259, 'raw': 384}
{'target': 600, 'success': 282, 'raw': 416}
{'target': 600, 'success': 309, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 379, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 476, 'raw': 672}
{'target': 600, 'success': 501, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 550, 'raw': 768}
{'target': 600, 'success': 566, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : main subject .', 'success_rate': 0.7094907407407407, 'errors': {'', "('Gajapati', 'main subject', '', 'Sankari s son Sankari , better Known as Gajapati ( , ( R.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.7838541666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1532 ) , he purchased the lands of Lauterbach in Bavaria , near Leipzig ; in May 1534 , he made a grant to the Hanseatic Empire of Hanseatic descent . Head Entity : Hae , Tail Entity : Lauterbach .\n']
['Relation : occupant . Context : Later in the year ( 1532 ) , he purchased the lands of Lauterbach in Bavaria , near Leipzig ; in May 1534 , he made a grant to the Hanseatic Empire of Hanseatic descent . Head Entity : Hae , Tail Entity : Lauterbach .\n', 'Relation : occupant . Context : After he was drafted into the Army under his elder sister , Henry Sommers , he soon decided to join the United States Navy under James Buchanan II , under the command of Admiral Buchanan . Head Entity : Admiral Buchanan II , Tail Entity : James Buchanan .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 429, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 622, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 373, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 419, 'raw': 576}
{'target': 600, 'success': 442, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 484, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : use .', 'success_rate': 0.7060185185185185, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('this', 'use', '', 'A number of variants of this theme have been used by The Beat in the 1960s ; one such is a theme in which a girl runs for life on the streets carrying a bag filled with chocolate candy .')"}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 48, 'raw': 96}
{'target': 600, 'success': 65, 'raw': 128}
{'target': 600, 'success': 84, 'raw': 160}
{'target': 600, 'success': 100, 'raw': 192}
{'target': 600, 'success': 115, 'raw': 224}
{'target': 600, 'success': 130, 'raw': 256}
{'target': 600, 'success': 146, 'raw': 288}
{'target': 600, 'success': 168, 'raw': 320}
{'target': 600, 'success': 188, 'raw': 352}
{'target': 600, 'success': 205, 'raw': 384}
{'target': 600, 'success': 221, 'raw': 416}
{'target': 600, 'success': 241, 'raw': 448}
{'target': 600, 'success': 261, 'raw': 480}
{'target': 600, 'success': 277, 'raw': 512}
{'target': 600, 'success': 290, 'raw': 544}
{'target': 600, 'success': 309, 'raw': 576}
{'target': 600, 'success': 330, 'raw': 608}
{'target': 600, 'success': 343, 'raw': 640}
{'target': 600, 'success': 360, 'raw': 672}
{'target': 600, 'success': 373, 'raw': 704}
{'target': 600, 'success': 392, 'raw': 736}
{'target': 600, 'success': 408, 'raw': 768}
{'target': 600, 'success': 421, 'raw': 800}
{'target': 600, 'success': 434, 'raw': 832}
{'target': 600, 'success': 450, 'raw': 864}
{'target': 600, 'success': 465, 'raw': 896}
{'target': 600, 'success': 485, 'raw': 928}
{'target': 600, 'success': 499, 'raw': 960}
{'target': 600, 'success': 520, 'raw': 992}
{'target': 600, 'success': 536, 'raw': 1024}
{'target': 600, 'success': 551, 'raw': 1056}
{'target': 600, 'success': 572, 'raw': 1088}
{'target': 600, 'success': 585, 'raw': 1120}
{'target': 600, 'success': 604, 'raw': 1152}
{'prompt': 'Relation : voice type .', 'success_rate': 0.5243055555555556, 'errors': {'', "('Goodbye Daddy', 'voice type', '', 'Her other music is featured in the 2004 movie , Goodbye Daddy .')", "('John Cleese', 'voice type', '', 'Another member that was named after John Cleese is Henry Maclean , who was given the task of handling the music , alongside Bill Cunningham .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 14833
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14933, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:13, 13.89s/it]Extractor Estimating: 2it [00:17,  7.72s/it]Extractor Estimating: 3it [00:17,  4.51s/it]Extractor Estimating: 4it [00:20,  3.63s/it]Extractor Estimating: 5it [00:20,  2.55s/it]Extractor Estimating: 6it [00:21,  1.90s/it]Extractor Estimating: 7it [00:22,  1.48s/it]Extractor Estimating: 8it [00:22,  1.21s/it]Extractor Estimating: 9it [00:23,  1.03s/it]Extractor Estimating: 10it [00:24,  1.10it/s]Extractor Estimating: 11it [00:24,  1.22it/s]Extractor Estimating: 12it [00:25,  1.34it/s]Extractor Estimating: 13it [00:25,  1.38it/s]Extractor Estimating: 14it [00:26,  1.42it/s]Extractor Estimating: 15it [00:27,  1.48it/s]Extractor Estimating: 16it [00:27,  1.46it/s]Extractor Estimating: 17it [00:30,  1.20s/it]Extractor Estimating: 18it [00:30,  1.05s/it]Extractor Estimating: 19it [00:31,  1.06it/s]Extractor Estimating: 20it [00:32,  1.18it/s]Extractor Estimating: 21it [00:32,  1.27it/s]Extractor Estimating: 22it [00:33,  1.37it/s]Extractor Estimating: 23it [00:34,  1.41it/s]Extractor Estimating: 24it [00:34,  1.45it/s]Extractor Estimating: 25it [00:35,  1.43it/s]Extractor Estimating: 26it [00:36,  1.53it/s]Extractor Estimating: 27it [00:36,  1.59it/s]Extractor Estimating: 28it [00:37,  1.59it/s]Extractor Estimating: 29it [00:37,  1.63it/s]Extractor Estimating: 30it [00:38,  1.64it/s]Extractor Estimating: 31it [00:39,  1.63it/s]Extractor Estimating: 32it [00:39,  1.67it/s]Extractor Estimating: 33it [00:40,  1.65it/s]Extractor Estimating: 34it [00:40,  1.64it/s]Extractor Estimating: 35it [00:41,  1.65it/s]Extractor Estimating: 36it [00:42,  1.68it/s]Extractor Estimating: 37it [00:42,  1.66it/s]Extractor Estimating: 38it [00:43,  1.67it/s]Extractor Estimating: 39it [00:43,  1.65it/s]Extractor Estimating: 40it [00:44,  1.61it/s]Extractor Estimating: 41it [00:45,  1.60it/s]Extractor Estimating: 42it [00:45,  1.58it/s]Extractor Estimating: 43it [00:46,  1.57it/s]Extractor Estimating: 44it [00:47,  1.59it/s]Extractor Estimating: 45it [00:47,  1.64it/s]Extractor Estimating: 46it [00:48,  1.66it/s]Extractor Estimating: 47it [00:48,  1.68it/s]Extractor Estimating: 48it [00:49,  1.70it/s]Extractor Estimating: 49it [00:50,  1.68it/s]Extractor Estimating: 50it [00:50,  1.68it/s]Extractor Estimating: 51it [00:51,  1.58it/s]Extractor Estimating: 52it [00:51,  1.59it/s]Extractor Estimating: 53it [00:52,  1.57it/s]Extractor Estimating: 54it [00:53,  1.57it/s]Extractor Estimating: 55it [00:53,  1.55it/s]Extractor Estimating: 56it [00:54,  1.49it/s]Extractor Estimating: 57it [00:55,  1.46it/s]Extractor Estimating: 58it [00:56,  1.51it/s]Extractor Estimating: 59it [00:56,  1.52it/s]Extractor Estimating: 60it [00:57,  1.49it/s]Extractor Estimating: 61it [00:58,  1.46it/s]Extractor Estimating: 62it [00:58,  1.44it/s]Extractor Estimating: 63it [00:59,  1.45it/s]Extractor Estimating: 64it [01:00,  1.44it/s]Extractor Estimating: 65it [01:00,  1.47it/s]Extractor Estimating: 66it [01:01,  1.51it/s]Extractor Estimating: 67it [01:02,  1.49it/s]Extractor Estimating: 68it [01:02,  1.49it/s]Extractor Estimating: 69it [01:03,  1.47it/s]Extractor Estimating: 70it [01:04,  1.49it/s]Extractor Estimating: 71it [01:04,  1.51it/s]Extractor Estimating: 72it [01:05,  1.52it/s]Extractor Estimating: 73it [01:06,  1.54it/s]Extractor Estimating: 74it [01:06,  1.46it/s]Extractor Estimating: 75it [01:07,  1.50it/s]Extractor Estimating: 76it [01:08,  1.55it/s]Extractor Estimating: 77it [01:08,  1.56it/s]Extractor Estimating: 78it [01:09,  1.49it/s]Extractor Estimating: 79it [01:10,  1.50it/s]Extractor Estimating: 80it [01:10,  1.44it/s]Extractor Estimating: 81it [01:11,  1.48it/s]Extractor Estimating: 82it [01:12,  1.49it/s]Extractor Estimating: 83it [01:12,  1.51it/s]Extractor Estimating: 84it [01:13,  1.51it/s]Extractor Estimating: 85it [01:14,  1.53it/s]Extractor Estimating: 86it [01:14,  1.55it/s]Extractor Estimating: 87it [01:15,  1.54it/s]Extractor Estimating: 88it [01:15,  1.55it/s]Extractor Estimating: 89it [01:16,  1.56it/s]Extractor Estimating: 90it [01:17,  1.61it/s]Extractor Estimating: 91it [01:17,  1.58it/s]Extractor Estimating: 92it [01:18,  1.54it/s]Extractor Estimating: 93it [01:19,  1.53it/s]Extractor Estimating: 94it [01:19,  1.57it/s]Extractor Estimating: 95it [01:20,  1.46it/s]Extractor Estimating: 96it [01:21,  1.54it/s]Extractor Estimating: 97it [01:21,  1.58it/s]Extractor Estimating: 98it [01:22,  1.57it/s]Extractor Estimating: 99it [01:23,  1.59it/s]Extractor Estimating: 100it [01:23,  1.58it/s]Extractor Estimating: 101it [01:24,  1.61it/s]Extractor Estimating: 102it [01:24,  1.55it/s]Extractor Estimating: 103it [01:25,  1.62it/s]Extractor Estimating: 104it [01:27,  1.18s/it]Extractor Estimating: 105it [01:28,  1.02s/it]Extractor Estimating: 106it [01:29,  1.10it/s]Extractor Estimating: 107it [01:29,  1.19it/s]Extractor Estimating: 108it [01:30,  1.32it/s]Extractor Estimating: 109it [01:31,  1.41it/s]Extractor Estimating: 110it [01:31,  1.45it/s]Extractor Estimating: 111it [01:32,  1.51it/s]Extractor Estimating: 112it [01:33,  1.52it/s]Extractor Estimating: 113it [01:33,  1.55it/s]Extractor Estimating: 114it [01:34,  1.57it/s]Extractor Estimating: 115it [01:34,  1.61it/s]Extractor Estimating: 116it [01:35,  1.64it/s]Extractor Estimating: 117it [01:36,  1.62it/s]Extractor Estimating: 118it [01:36,  1.62it/s]Extractor Estimating: 119it [01:37,  1.60it/s]Extractor Estimating: 120it [01:37,  1.60it/s]Extractor Estimating: 121it [01:38,  1.62it/s]Extractor Estimating: 122it [01:39,  1.60it/s]Extractor Estimating: 123it [01:39,  1.61it/s]Extractor Estimating: 124it [01:40,  1.63it/s]Extractor Estimating: 125it [01:41,  1.62it/s]Extractor Estimating: 126it [01:41,  1.60it/s]Extractor Estimating: 127it [01:42,  1.56it/s]Extractor Estimating: 128it [01:42,  1.56it/s]Extractor Estimating: 129it [01:43,  1.49it/s]Extractor Estimating: 130it [01:44,  1.56it/s]Extractor Estimating: 131it [01:44,  1.60it/s]Extractor Estimating: 132it [01:45,  1.53it/s]Extractor Estimating: 133it [01:46,  1.51it/s]Extractor Estimating: 134it [01:46,  1.50it/s]Extractor Estimating: 135it [01:47,  1.47it/s]Extractor Estimating: 136it [01:48,  1.48it/s]Extractor Estimating: 137it [01:48,  1.49it/s]Extractor Estimating: 138it [01:49,  1.49it/s]Extractor Estimating: 139it [01:50,  1.49it/s]Extractor Estimating: 140it [01:51,  1.43it/s]Extractor Estimating: 141it [01:51,  1.40it/s]Extractor Estimating: 142it [01:52,  1.44it/s]Extractor Estimating: 143it [01:53,  1.47it/s]Extractor Estimating: 144it [01:53,  1.47it/s]Extractor Estimating: 145it [01:54,  1.47it/s]Extractor Estimating: 146it [01:55,  1.43it/s]Extractor Estimating: 147it [01:55,  1.44it/s]Extractor Estimating: 148it [01:56,  1.42it/s]Extractor Estimating: 149it [01:57,  1.45it/s]Extractor Estimating: 150it [01:58,  1.44it/s]Extractor Estimating: 151it [01:58,  1.48it/s]Extractor Estimating: 152it [01:59,  1.51it/s]Extractor Estimating: 153it [01:59,  1.50it/s]Extractor Estimating: 154it [02:00,  1.48it/s]Extractor Estimating: 155it [02:01,  1.54it/s]Extractor Estimating: 156it [02:01,  1.58it/s]Extractor Estimating: 157it [02:02,  1.55it/s]Extractor Estimating: 158it [02:03,  1.54it/s]Extractor Estimating: 159it [02:03,  1.57it/s]Extractor Estimating: 160it [02:04,  1.59it/s]Extractor Estimating: 161it [02:05,  1.55it/s]Extractor Estimating: 162it [02:05,  1.56it/s]Extractor Estimating: 163it [02:06,  1.56it/s]Extractor Estimating: 164it [02:06,  1.55it/s]Extractor Estimating: 165it [02:07,  1.54it/s]Extractor Estimating: 166it [02:08,  1.41it/s]Extractor Estimating: 167it [02:09,  1.46it/s]Extractor Estimating: 168it [02:09,  1.53it/s]Extractor Estimating: 169it [02:10,  1.55it/s]Extractor Estimating: 170it [02:10,  1.54it/s]Extractor Estimating: 171it [02:11,  1.55it/s]Extractor Estimating: 172it [02:12,  1.56it/s]Extractor Estimating: 173it [02:12,  1.55it/s]Extractor Estimating: 174it [02:13,  1.57it/s]Extractor Estimating: 175it [02:14,  1.55it/s]Extractor Estimating: 176it [02:14,  1.54it/s]Extractor Estimating: 177it [02:15,  1.50it/s]Extractor Estimating: 178it [02:16,  1.52it/s]Extractor Estimating: 179it [02:16,  1.54it/s]Extractor Estimating: 180it [02:17,  1.49it/s]Extractor Estimating: 181it [02:18,  1.47it/s]Extractor Estimating: 182it [02:18,  1.49it/s]Extractor Estimating: 183it [02:19,  1.46it/s]Extractor Estimating: 184it [02:20,  1.40it/s]Extractor Estimating: 185it [02:21,  1.39it/s]Extractor Estimating: 186it [02:21,  1.44it/s]Extractor Estimating: 187it [02:22,  1.44it/s]Extractor Estimating: 188it [02:23,  1.45it/s]Extractor Estimating: 189it [02:23,  1.50it/s]Extractor Estimating: 190it [02:24,  1.49it/s]Extractor Estimating: 191it [02:25,  1.44it/s]Extractor Estimating: 192it [02:25,  1.45it/s]Extractor Estimating: 193it [02:26,  1.44it/s]Extractor Estimating: 194it [02:27,  1.42it/s]Extractor Estimating: 195it [02:27,  1.47it/s]Extractor Estimating: 196it [02:28,  1.50it/s]Extractor Estimating: 197it [02:29,  1.52it/s]Extractor Estimating: 198it [02:29,  1.55it/s]Extractor Estimating: 199it [02:30,  1.50it/s]Extractor Estimating: 200it [02:31,  1.47it/s]Extractor Estimating: 201it [02:31,  1.50it/s]Extractor Estimating: 202it [02:32,  1.50it/s]Extractor Estimating: 203it [02:33,  1.47it/s]Extractor Estimating: 204it [02:33,  1.49it/s]Extractor Estimating: 205it [02:34,  1.49it/s]Extractor Estimating: 206it [02:35,  1.52it/s]Extractor Estimating: 207it [02:35,  1.46it/s]Extractor Estimating: 208it [02:36,  1.46it/s]Extractor Estimating: 209it [02:37,  1.29it/s]Extractor Estimating: 210it [02:38,  1.30it/s]Extractor Estimating: 211it [02:38,  1.39it/s]Extractor Estimating: 212it [02:39,  1.42it/s]Extractor Estimating: 213it [02:40,  1.44it/s]Extractor Estimating: 214it [02:40,  1.48it/s]Extractor Estimating: 215it [02:41,  1.43it/s]Extractor Estimating: 216it [02:42,  1.46it/s]Extractor Estimating: 217it [02:43,  1.44it/s]Extractor Estimating: 218it [02:43,  1.51it/s]Extractor Estimating: 219it [02:44,  1.53it/s]Extractor Estimating: 220it [02:44,  1.52it/s]Extractor Estimating: 221it [02:45,  1.56it/s]Extractor Estimating: 222it [02:46,  1.50it/s]Extractor Estimating: 223it [02:46,  1.47it/s]Extractor Estimating: 224it [02:47,  1.48it/s]Extractor Estimating: 225it [02:48,  1.47it/s]Extractor Estimating: 226it [02:49,  1.46it/s]Extractor Estimating: 227it [02:49,  1.51it/s]Extractor Estimating: 228it [02:50,  1.51it/s]Extractor Estimating: 229it [02:50,  1.51it/s]Extractor Estimating: 230it [02:51,  1.50it/s]Extractor Estimating: 231it [02:52,  1.49it/s]Extractor Estimating: 232it [02:53,  1.45it/s]Extractor Estimating: 233it [02:53,  1.43it/s]Extractor Estimating: 234it [02:54,  1.42it/s]Extractor Estimating: 235it [02:55,  1.43it/s]Extractor Estimating: 236it [02:55,  1.42it/s]Extractor Estimating: 237it [02:56,  1.41it/s]Extractor Estimating: 238it [02:57,  1.37it/s]Extractor Estimating: 239it [02:58,  1.30it/s]Extractor Estimating: 240it [02:59,  1.30it/s]Extractor Estimating: 241it [02:59,  1.33it/s]Extractor Estimating: 242it [03:00,  1.35it/s]Extractor Estimating: 243it [03:01,  1.36it/s]Extractor Estimating: 244it [03:01,  1.39it/s]Extractor Estimating: 245it [03:02,  1.39it/s]Extractor Estimating: 246it [03:03,  1.41it/s]Extractor Estimating: 247it [03:04,  1.40it/s]Extractor Estimating: 248it [03:04,  1.45it/s]Extractor Estimating: 249it [03:05,  1.44it/s]Extractor Estimating: 250it [03:06,  1.45it/s]Extractor Estimating: 251it [03:06,  1.46it/s]Extractor Estimating: 252it [03:07,  1.47it/s]Extractor Estimating: 253it [03:07,  1.56it/s]Extractor Estimating: 254it [03:08,  1.56it/s]Extractor Estimating: 255it [03:09,  1.54it/s]Extractor Estimating: 256it [03:09,  1.55it/s]Extractor Estimating: 257it [03:10,  1.52it/s]Extractor Estimating: 258it [03:11,  1.48it/s]Extractor Estimating: 259it [03:11,  1.51it/s]Extractor Estimating: 260it [03:12,  1.56it/s]Extractor Estimating: 261it [03:13,  1.54it/s]Extractor Estimating: 262it [03:13,  1.58it/s]Extractor Estimating: 263it [03:14,  1.55it/s]Extractor Estimating: 264it [03:15,  1.56it/s]Extractor Estimating: 265it [03:15,  1.57it/s]Extractor Estimating: 266it [03:16,  1.54it/s]Extractor Estimating: 267it [03:17,  1.52it/s]Extractor Estimating: 268it [03:17,  1.54it/s]Extractor Estimating: 269it [03:18,  1.57it/s]Extractor Estimating: 270it [03:18,  1.60it/s]Extractor Estimating: 271it [03:19,  1.53it/s]Extractor Estimating: 272it [03:20,  1.51it/s]Extractor Estimating: 273it [03:20,  1.51it/s]Extractor Estimating: 274it [03:21,  1.49it/s]Extractor Estimating: 275it [03:22,  1.48it/s]Extractor Estimating: 276it [03:22,  1.50it/s]Extractor Estimating: 277it [03:23,  1.57it/s]Extractor Estimating: 278it [03:24,  1.56it/s]Extractor Estimating: 279it [03:24,  1.58it/s]Extractor Estimating: 280it [03:25,  1.55it/s]Extractor Estimating: 281it [03:26,  1.53it/s]Extractor Estimating: 282it [03:26,  1.53it/s]Extractor Estimating: 283it [03:27,  1.59it/s]Extractor Estimating: 284it [03:27,  1.60it/s]Extractor Estimating: 285it [03:28,  1.62it/s]Extractor Estimating: 286it [03:29,  1.63it/s]Extractor Estimating: 287it [03:29,  1.60it/s]Extractor Estimating: 288it [03:30,  1.56it/s]Extractor Estimating: 289it [03:31,  1.56it/s]Extractor Estimating: 290it [03:31,  1.54it/s]Extractor Estimating: 291it [03:32,  1.56it/s]Extractor Estimating: 292it [03:33,  1.54it/s]Extractor Estimating: 293it [03:33,  1.57it/s]Extractor Estimating: 294it [03:34,  1.59it/s]Extractor Estimating: 295it [03:34,  1.63it/s]Extractor Estimating: 296it [03:35,  1.62it/s]Extractor Estimating: 297it [03:36,  1.66it/s]Extractor Estimating: 298it [03:36,  1.62it/s]Extractor Estimating: 299it [03:37,  1.63it/s]Extractor Estimating: 300it [03:38,  1.59it/s]Extractor Estimating: 301it [03:38,  1.60it/s]Extractor Estimating: 302it [03:39,  1.62it/s]Extractor Estimating: 303it [03:39,  1.59it/s]Extractor Estimating: 304it [03:40,  1.63it/s]Extractor Estimating: 305it [03:41,  1.62it/s]Extractor Estimating: 306it [03:41,  1.51it/s]Extractor Estimating: 307it [03:42,  1.53it/s]Extractor Estimating: 308it [03:43,  1.51it/s]Extractor Estimating: 309it [03:43,  1.54it/s]Extractor Estimating: 310it [03:44,  1.57it/s]Extractor Estimating: 311it [03:45,  1.58it/s]Extractor Estimating: 312it [03:45,  1.56it/s]Extractor Estimating: 313it [03:46,  1.58it/s]Extractor Estimating: 314it [03:46,  1.55it/s]Extractor Estimating: 315it [03:47,  1.57it/s]Extractor Estimating: 316it [03:48,  1.60it/s]Extractor Estimating: 317it [03:48,  1.58it/s]Extractor Estimating: 318it [03:49,  1.52it/s]Extractor Estimating: 319it [03:50,  1.53it/s]Extractor Estimating: 320it [03:50,  1.58it/s]Extractor Estimating: 321it [03:51,  1.51it/s]Extractor Estimating: 322it [03:52,  1.55it/s]Extractor Estimating: 323it [03:52,  1.57it/s]Extractor Estimating: 324it [03:53,  1.54it/s]Extractor Estimating: 325it [03:54,  1.56it/s]Extractor Estimating: 326it [03:54,  1.54it/s]Extractor Estimating: 327it [03:55,  1.58it/s]Extractor Estimating: 328it [03:55,  1.58it/s]Extractor Estimating: 329it [03:56,  1.57it/s]Extractor Estimating: 330it [03:57,  1.58it/s]Extractor Estimating: 331it [03:57,  1.57it/s]Extractor Estimating: 332it [03:58,  1.54it/s]Extractor Estimating: 333it [03:59,  1.53it/s]Extractor Estimating: 334it [03:59,  1.53it/s]Extractor Estimating: 335it [04:00,  1.57it/s]Extractor Estimating: 336it [04:01,  1.58it/s]Extractor Estimating: 337it [04:01,  1.58it/s]Extractor Estimating: 338it [04:02,  1.53it/s]Extractor Estimating: 339it [04:03,  1.56it/s]Extractor Estimating: 340it [04:03,  1.50it/s]Extractor Estimating: 341it [04:04,  1.53it/s]Extractor Estimating: 342it [04:05,  1.48it/s]Extractor Estimating: 343it [04:05,  1.52it/s]Extractor Estimating: 344it [04:06,  1.48it/s]Extractor Estimating: 345it [04:07,  1.51it/s]Extractor Estimating: 346it [04:07,  1.56it/s]Extractor Estimating: 347it [04:08,  1.54it/s]Extractor Estimating: 348it [04:09,  1.49it/s]Extractor Estimating: 349it [04:09,  1.56it/s]Extractor Estimating: 350it [04:10,  1.53it/s]Extractor Estimating: 351it [04:10,  1.58it/s]Extractor Estimating: 352it [04:11,  1.50it/s]Extractor Estimating: 353it [04:12,  1.50it/s]Extractor Estimating: 354it [04:12,  1.52it/s]Extractor Estimating: 355it [04:13,  1.53it/s]Extractor Estimating: 356it [04:14,  1.58it/s]Extractor Estimating: 357it [04:14,  1.58it/s]Extractor Estimating: 358it [04:15,  1.57it/s]Extractor Estimating: 359it [04:16,  1.56it/s]Extractor Estimating: 360it [04:16,  1.58it/s]Extractor Estimating: 361it [04:17,  1.54it/s]Extractor Estimating: 362it [04:18,  1.58it/s]Extractor Estimating: 363it [04:18,  1.65it/s]Extractor Estimating: 364it [04:19,  1.64it/s]Extractor Estimating: 365it [04:19,  1.62it/s]Extractor Estimating: 366it [04:20,  1.65it/s]Extractor Estimating: 367it [04:21,  1.60it/s]Extractor Estimating: 368it [04:21,  1.59it/s]Extractor Estimating: 369it [04:22,  1.58it/s]Extractor Estimating: 370it [04:22,  1.56it/s]Extractor Estimating: 371it [04:23,  1.57it/s]Extractor Estimating: 372it [04:24,  1.54it/s]Extractor Estimating: 373it [04:24,  1.57it/s]Extractor Estimating: 374it [04:25,  1.56it/s]Extractor Estimating: 375it [04:26,  1.56it/s]Extractor Estimating: 375it [04:26,  1.41it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7397 mean pseudo reward: 0.9349236317993248
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 28869
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28969, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28969, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.368, loss:2664.3374
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.062, loss:1967.1803
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.056, loss:1622.1977
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 1.070, loss:1522.6362
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 1.063, loss:1486.6013
>> valid entity prec:0.5024, rec:0.4816, f1:0.4918
>> valid relation prec:0.0025, rec:0.0003, f1:0.0006
>> valid relation with NER prec:0.0025, rec:0.0003, f1:0.0006
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 3.216, loss:1528.6019
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 1.058, loss:1371.4893
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 1.062, loss:1360.0812
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 1.072, loss:1320.7527
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 1.059, loss:1221.1787
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5502, rec:0.4276, f1:0.4812
>> valid relation prec:0.0105, rec:0.0034, f1:0.0051
>> valid relation with NER prec:0.0105, rec:0.0034, f1:0.0051
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 173, avg_time 3.207, loss:1197.0967
g_step 1200, step 273, avg_time 1.070, loss:1199.9329
g_step 1300, step 64, avg_time 1.071, loss:1116.7962
g_step 1400, step 164, avg_time 1.068, loss:1089.9186
g_step 1500, step 264, avg_time 1.063, loss:1074.2672
>> valid entity prec:0.5285, rec:0.4137, f1:0.4641
>> valid relation prec:0.0053, rec:0.0016, f1:0.0025
>> valid relation with NER prec:0.0053, rec:0.0016, f1:0.0025
g_step 1600, step 55, avg_time 3.195, loss:1068.1695
g_step 1700, step 155, avg_time 1.073, loss:1027.9319
g_step 1800, step 255, avg_time 1.072, loss:1025.9187
g_step 1900, step 46, avg_time 1.067, loss:993.6788
g_step 2000, step 146, avg_time 1.075, loss:968.4887
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4897, rec:0.4288, f1:0.4572
>> valid relation prec:0.0082, rec:0.0024, f1:0.0037
>> valid relation with NER prec:0.0082, rec:0.0024, f1:0.0037
g_step 2100, step 246, avg_time 3.204, loss:973.3293
g_step 2200, step 37, avg_time 1.065, loss:973.6234
g_step 2300, step 137, avg_time 1.071, loss:940.1649
g_step 2400, step 237, avg_time 1.054, loss:932.8679
g_step 2500, step 28, avg_time 1.055, loss:907.2554
>> valid entity prec:0.5187, rec:0.4701, f1:0.4932
>> valid relation prec:0.0120, rec:0.0042, f1:0.0062
>> valid relation with NER prec:0.0120, rec:0.0042, f1:0.0062
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 128, avg_time 3.214, loss:874.5707
g_step 2700, step 228, avg_time 1.058, loss:906.0354
g_step 2800, step 19, avg_time 1.058, loss:858.9236
g_step 2900, step 119, avg_time 1.061, loss:867.5847
g_step 3000, step 219, avg_time 1.062, loss:845.0519
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5135, rec:0.4896, f1:0.5012
>> valid relation prec:0.0175, rec:0.0045, f1:0.0071
>> valid relation with NER prec:0.0175, rec:0.0045, f1:0.0071
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 10, avg_time 3.211, loss:841.4422
g_step 3200, step 110, avg_time 1.055, loss:810.8698
g_step 3300, step 210, avg_time 1.057, loss:826.9836
g_step 3400, step 1, avg_time 1.077, loss:849.2294
g_step 3500, step 101, avg_time 1.067, loss:782.2633
>> valid entity prec:0.5142, rec:0.4249, f1:0.4653
>> valid relation prec:0.0138, rec:0.0030, f1:0.0050
>> valid relation with NER prec:0.0138, rec:0.0030, f1:0.0050
g_step 3600, step 201, avg_time 3.201, loss:807.0758
g_step 3700, step 301, avg_time 1.062, loss:788.7223
g_step 3800, step 92, avg_time 1.065, loss:765.7883
g_step 3900, step 192, avg_time 1.055, loss:763.0646
g_step 4000, step 292, avg_time 1.065, loss:805.6475
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5143, rec:0.4370, f1:0.4725
>> valid relation prec:0.0108, rec:0.0032, f1:0.0049
>> valid relation with NER prec:0.0108, rec:0.0032, f1:0.0049
g_step 4100, step 83, avg_time 3.209, loss:743.8453
g_step 4200, step 183, avg_time 1.054, loss:720.5805
g_step 4300, step 283, avg_time 1.065, loss:737.6566
g_step 4400, step 74, avg_time 1.051, loss:728.4140
g_step 4500, step 174, avg_time 1.059, loss:710.4430
>> valid entity prec:0.5311, rec:0.4340, f1:0.4777
>> valid relation prec:0.0078, rec:0.0026, f1:0.0039
>> valid relation with NER prec:0.0078, rec:0.0026, f1:0.0039
g_step 4600, step 274, avg_time 3.209, loss:713.9814
g_step 4700, step 65, avg_time 1.050, loss:683.2980
g_step 4800, step 165, avg_time 1.062, loss:694.8842
g_step 4900, step 265, avg_time 1.072, loss:682.7575
g_step 5000, step 56, avg_time 1.065, loss:700.4936
learning rate was adjusted to 0.0008
>> valid entity prec:0.5239, rec:0.4016, f1:0.4546
>> valid relation prec:0.0054, rec:0.0016, f1:0.0025
>> valid relation with NER prec:0.0054, rec:0.0016, f1:0.0025
g_step 5100, step 156, avg_time 3.202, loss:646.3158
g_step 5200, step 256, avg_time 1.062, loss:678.4326
g_step 5300, step 47, avg_time 1.063, loss:635.4980
g_step 5400, step 147, avg_time 1.067, loss:641.5519
g_step 5500, step 247, avg_time 1.070, loss:667.0493
>> valid entity prec:0.5172, rec:0.4865, f1:0.5014
>> valid relation prec:0.0117, rec:0.0054, f1:0.0074
>> valid relation with NER prec:0.0117, rec:0.0054, f1:0.0074
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 38, avg_time 3.190, loss:622.8371
g_step 5700, step 138, avg_time 1.072, loss:631.3349
g_step 5800, step 238, avg_time 1.065, loss:633.6715
g_step 5900, step 29, avg_time 1.050, loss:613.9081
g_step 6000, step 129, avg_time 1.062, loss:618.8638
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5087, rec:0.4985, f1:0.5036
>> valid relation prec:0.0112, rec:0.0040, f1:0.0059
>> valid relation with NER prec:0.0112, rec:0.0040, f1:0.0059
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 229, avg_time 3.200, loss:591.9023
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:47:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:47:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-47-04_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:47:05 - WARNING - datasets.builder -   Using custom data configuration default-de4f57b2f52a6701
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-de4f57b2f52a6701/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:47:05,807 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:47:05,808 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:47:05,808 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:47:05,809 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:47:05,815 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:47:05,819 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:47:05,927 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:47:08,997 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:47:09,005 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-de4f57b2f52a6701/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 02:47:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1471a00a5830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.88ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.75ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.08ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.27ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.37ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.72ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  3.96ba/s]100%|██████████| 8/8 [00:01<00:00,  4.73ba/s]100%|██████████| 8/8 [00:01<00:00,  4.21ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.70ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.10ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.24ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.29ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.32ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.33ba/s]100%|██████████| 7/7 [00:01<00:00,  4.76ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.62ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.80ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.00ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.10ba/s]100%|██████████| 8/8 [00:00<00:00, 10.57ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.49ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.83ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.99ba/s]100%|██████████| 7/7 [00:00<00:00, 11.94ba/s]100%|██████████| 7/7 [00:00<00:00, 11.11ba/s]
[INFO|trainer.py:414] 2023-08-29 02:47:14,228 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:47:14,239 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:47:14,240 >>   Num examples = 7545
[INFO|trainer.py:1149] 2023-08-29 02:47:14,240 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:47:14,240 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:47:14,240 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:47:14,240 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:47:14,240 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:56,  3.34it/s]  0%|          | 2/590 [00:00<02:51,  3.43it/s]  1%|          | 3/590 [00:00<02:50,  3.45it/s]  1%|          | 4/590 [00:01<02:49,  3.46it/s]  1%|          | 5/590 [00:01<02:48,  3.46it/s]  1%|          | 6/590 [00:01<02:48,  3.47it/s]  1%|          | 7/590 [00:02<02:47,  3.47it/s]  1%|▏         | 8/590 [00:02<02:47,  3.48it/s]  2%|▏         | 9/590 [00:02<02:47,  3.48it/s]  2%|▏         | 10/590 [00:02<02:46,  3.48it/s]  2%|▏         | 11/590 [00:03<02:46,  3.48it/s]  2%|▏         | 12/590 [00:03<02:46,  3.48it/s]  2%|▏         | 13/590 [00:03<02:45,  3.48it/s]  2%|▏         | 14/590 [00:04<02:45,  3.48it/s]  3%|▎         | 15/590 [00:04<02:45,  3.48it/s]  3%|▎         | 16/590 [00:04<02:45,  3.48it/s]  3%|▎         | 17/590 [00:04<02:44,  3.48it/s]  3%|▎         | 18/590 [00:05<02:44,  3.48it/s]  3%|▎         | 19/590 [00:05<02:44,  3.48it/s]  3%|▎         | 20/590 [00:05<02:43,  3.48it/s]  4%|▎         | 21/590 [00:06<02:43,  3.48it/s]  4%|▎         | 22/590 [00:06<02:43,  3.48it/s]  4%|▍         | 23/590 [00:06<02:42,  3.48it/s]  4%|▍         | 24/590 [00:06<02:42,  3.48it/s]  4%|▍         | 25/590 [00:07<02:42,  3.48it/s]  4%|▍         | 26/590 [00:07<02:42,  3.48it/s]  5%|▍         | 27/590 [00:07<02:42,  3.47it/s]  5%|▍         | 28/590 [00:08<02:41,  3.48it/s]  5%|▍         | 29/590 [00:08<02:41,  3.48it/s]  5%|▌         | 30/590 [00:08<02:41,  3.48it/s]  5%|▌         | 31/590 [00:08<02:40,  3.48it/s]  5%|▌         | 32/590 [00:09<02:40,  3.48it/s]  6%|▌         | 33/590 [00:09<02:40,  3.48it/s]  6%|▌         | 34/590 [00:09<02:39,  3.48it/s]  6%|▌         | 35/590 [00:10<02:39,  3.48it/s]  6%|▌         | 36/590 [00:10<02:39,  3.48it/s]  6%|▋         | 37/590 [00:10<02:39,  3.48it/s]  6%|▋         | 38/590 [00:10<02:38,  3.48it/s]  7%|▋         | 39/590 [00:11<02:38,  3.48it/s]  7%|▋         | 40/590 [00:11<02:38,  3.48it/s]  7%|▋         | 41/590 [00:11<02:37,  3.48it/s]  7%|▋         | 42/590 [00:12<02:37,  3.48it/s]  7%|▋         | 43/590 [00:12<02:37,  3.47it/s]  7%|▋         | 44/590 [00:12<02:37,  3.47it/s]  8%|▊         | 45/590 [00:12<02:37,  3.47it/s]  8%|▊         | 46/590 [00:13<02:36,  3.47it/s]  8%|▊         | 47/590 [00:13<02:36,  3.47it/s]  8%|▊         | 48/590 [00:13<02:36,  3.47it/s]  8%|▊         | 49/590 [00:14<02:35,  3.48it/s]  8%|▊         | 50/590 [00:14<02:35,  3.48it/s]  9%|▊         | 51/590 [00:14<02:34,  3.48it/s]  9%|▉         | 52/590 [00:14<02:34,  3.48it/s]  9%|▉         | 53/590 [00:15<02:34,  3.47it/s]  9%|▉         | 54/590 [00:15<02:34,  3.47it/s]  9%|▉         | 55/590 [00:15<02:34,  3.47it/s]  9%|▉         | 56/590 [00:16<02:33,  3.47it/s] 10%|▉         | 57/590 [00:16<02:33,  3.47it/s] 10%|▉         | 58/590 [00:16<02:33,  3.47it/s] 10%|█         | 59/590 [00:16<02:33,  3.47it/s] 10%|█         | 60/590 [00:17<02:32,  3.47it/s] 10%|█         | 61/590 [00:17<02:33,  3.45it/s] 11%|█         | 62/590 [00:17<02:32,  3.46it/s] 11%|█         | 63/590 [00:18<02:32,  3.46it/s] 11%|█         | 64/590 [00:18<02:31,  3.46it/s] 11%|█         | 65/590 [00:18<02:31,  3.47it/s] 11%|█         | 66/590 [00:19<02:31,  3.47it/s] 11%|█▏        | 67/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 68/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 69/590 [00:19<02:30,  3.47it/s] 12%|█▏        | 70/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 71/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 72/590 [00:20<02:29,  3.47it/s] 12%|█▏        | 73/590 [00:21<02:29,  3.47it/s] 13%|█▎        | 74/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 75/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 76/590 [00:21<02:28,  3.47it/s] 13%|█▎        | 77/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 78/590 [00:22<02:27,  3.47it/s] 13%|█▎        | 79/590 [00:22<02:27,  3.45it/s] 14%|█▎        | 80/590 [00:23<02:27,  3.46it/s] 14%|█▎        | 81/590 [00:23<02:27,  3.46it/s] 14%|█▍        | 82/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 83/590 [00:23<02:26,  3.46it/s] 14%|█▍        | 84/590 [00:24<02:25,  3.47it/s] 14%|█▍        | 85/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 86/590 [00:24<02:25,  3.47it/s] 15%|█▍        | 87/590 [00:25<02:25,  3.47it/s] 15%|█▍        | 88/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 89/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 90/590 [00:25<02:24,  3.47it/s] 15%|█▌        | 91/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 92/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 93/590 [00:26<02:23,  3.47it/s] 16%|█▌        | 94/590 [00:27<02:23,  3.47it/s] 16%|█▌        | 95/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 96/590 [00:27<02:22,  3.47it/s] 16%|█▋        | 97/590 [00:27<02:22,  3.46it/s] 17%|█▋        | 98/590 [00:28<02:22,  3.46it/s] 17%|█▋        | 99/590 [00:28<02:21,  3.46it/s] 17%|█▋        | 100/590 [00:28<02:21,  3.46it/s] 17%|█▋        | 101/590 [00:29<02:21,  3.46it/s] 17%|█▋        | 102/590 [00:29<02:20,  3.46it/s] 17%|█▋        | 103/590 [00:29<02:20,  3.46it/s] 18%|█▊        | 104/590 [00:29<02:20,  3.46it/s] 18%|█▊        | 105/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 106/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 107/590 [00:30<02:19,  3.47it/s] 18%|█▊        | 108/590 [00:31<02:19,  3.47it/s] 18%|█▊        | 109/590 [00:31<02:18,  3.47it/s] 19%|█▊        | 110/590 [00:31<02:18,  3.47it/s] 19%|█▉        | 111/590 [00:31<02:18,  3.47it/s] 19%|█▉        | 112/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 113/590 [00:32<02:17,  3.47it/s] 19%|█▉        | 114/590 [00:32<02:17,  3.45it/s] 19%|█▉        | 115/590 [00:33<02:17,  3.46it/s] 20%|█▉        | 116/590 [00:33<02:16,  3.46it/s] 20%|█▉        | 117/590 [00:33<02:16,  3.46it/s] 20%|██        | 118/590 [00:33<02:12,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 02:47:48,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:47:48,224 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:47:48,224 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.22it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.65it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.80it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.04it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.67it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.35it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.11it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.92it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.88it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.81it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.81it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.75it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.77it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.75it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.73it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.74it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.73it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.72it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.70it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.71it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.72it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.66it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.68it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.69it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.74it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.73it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.65it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.76it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.63it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.66it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.65it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.69it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.70it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.77it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.59it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.55it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.56it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.66it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.74it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.66it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.70it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.63it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.65it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.68it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.66it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.64it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.60it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.69it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.65it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.74it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.72it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.71it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.74it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.79it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.73it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.73it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.70it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.70it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.75it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.76it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.71it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.68it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.62it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.63it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.64it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.69it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.73it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.63it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.70it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.68it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.63it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.71it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.68it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.70it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.74it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.64it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.68it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.69it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.69it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.67it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.71it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.67it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.57it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.61it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.67it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.68it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.59it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.56it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.47it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.57it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.62it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.59it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.67it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.65it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.59it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.60it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.65it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.64it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.66it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.59it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.63it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.73it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.61it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.62it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.66it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.66it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.74it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.77it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.65it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.71it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.62it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.63it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.63it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.61it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.58it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.67it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.69it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.58it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.62it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.66it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.65it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.71it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.72it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.50it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.54it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.64it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.67it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.68it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.68it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.58it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.60it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.69it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.60it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.58it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.67it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.66it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.59it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.69it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.61it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.63it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.72it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.69it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.77it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.69it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.63it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.69it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.70it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.67it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.70it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.68it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 44.27it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 44.97it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 45.48it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 45.83it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:50<02:12,  3.57it/s]
100%|██████████| 782/782 [00:16<00:00, 45.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:48:05,024 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-29 02:48:05,051 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:48:07,338 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:48:07,360 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:48:07,369 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:57<57:52,  7.37s/it] 20%|██        | 120/590 [00:58<41:06,  5.25s/it] 21%|██        | 121/590 [00:58<29:23,  3.76s/it] 21%|██        | 122/590 [00:58<21:12,  2.72s/it] 21%|██        | 123/590 [00:59<15:29,  1.99s/it] 21%|██        | 124/590 [00:59<11:29,  1.48s/it] 21%|██        | 125/590 [00:59<08:41,  1.12s/it] 21%|██▏       | 126/590 [00:59<06:44,  1.15it/s] 22%|██▏       | 127/590 [01:00<05:22,  1.44it/s] 22%|██▏       | 128/590 [01:00<04:25,  1.74it/s] 22%|██▏       | 129/590 [01:00<03:45,  2.05it/s] 22%|██▏       | 130/590 [01:01<03:17,  2.33it/s] 22%|██▏       | 131/590 [01:01<02:57,  2.58it/s] 22%|██▏       | 132/590 [01:01<02:43,  2.80it/s] 23%|██▎       | 133/590 [01:01<02:33,  2.97it/s] 23%|██▎       | 134/590 [01:02<02:26,  3.11it/s] 23%|██▎       | 135/590 [01:02<02:21,  3.21it/s] 23%|██▎       | 136/590 [01:02<02:18,  3.28it/s] 23%|██▎       | 137/590 [01:03<02:15,  3.33it/s] 23%|██▎       | 138/590 [01:03<02:13,  3.37it/s] 24%|██▎       | 139/590 [01:03<02:12,  3.40it/s] 24%|██▎       | 140/590 [01:03<02:11,  3.42it/s] 24%|██▍       | 141/590 [01:04<02:10,  3.44it/s] 24%|██▍       | 142/590 [01:04<02:10,  3.44it/s] 24%|██▍       | 143/590 [01:04<02:09,  3.45it/s] 24%|██▍       | 144/590 [01:05<02:09,  3.46it/s] 25%|██▍       | 145/590 [01:05<02:08,  3.46it/s] 25%|██▍       | 146/590 [01:05<02:08,  3.46it/s] 25%|██▍       | 147/590 [01:05<02:07,  3.47it/s] 25%|██▌       | 148/590 [01:06<02:07,  3.47it/s] 25%|██▌       | 149/590 [01:06<02:07,  3.47it/s] 25%|██▌       | 150/590 [01:06<02:06,  3.47it/s] 26%|██▌       | 151/590 [01:07<02:06,  3.47it/s] 26%|██▌       | 152/590 [01:07<02:06,  3.47it/s] 26%|██▌       | 153/590 [01:07<02:06,  3.46it/s] 26%|██▌       | 154/590 [01:07<02:05,  3.46it/s] 26%|██▋       | 155/590 [01:08<02:05,  3.46it/s] 26%|██▋       | 156/590 [01:08<02:05,  3.47it/s] 27%|██▋       | 157/590 [01:08<02:04,  3.47it/s] 27%|██▋       | 158/590 [01:09<02:04,  3.47it/s] 27%|██▋       | 159/590 [01:09<02:04,  3.47it/s] 27%|██▋       | 160/590 [01:09<02:03,  3.47it/s] 27%|██▋       | 161/590 [01:10<02:03,  3.47it/s] 27%|██▋       | 162/590 [01:10<02:03,  3.47it/s] 28%|██▊       | 163/590 [01:10<02:03,  3.47it/s] 28%|██▊       | 164/590 [01:10<02:03,  3.44it/s] 28%|██▊       | 165/590 [01:11<02:03,  3.45it/s] 28%|██▊       | 166/590 [01:11<02:02,  3.45it/s] 28%|██▊       | 167/590 [01:11<02:02,  3.46it/s] 28%|██▊       | 168/590 [01:12<02:01,  3.46it/s] 29%|██▊       | 169/590 [01:12<02:01,  3.46it/s] 29%|██▉       | 170/590 [01:12<02:01,  3.46it/s] 29%|██▉       | 171/590 [01:12<02:00,  3.46it/s] 29%|██▉       | 172/590 [01:13<02:00,  3.47it/s] 29%|██▉       | 173/590 [01:13<02:00,  3.47it/s] 29%|██▉       | 174/590 [01:13<02:00,  3.46it/s] 30%|██▉       | 175/590 [01:14<02:00,  3.46it/s] 30%|██▉       | 176/590 [01:14<01:59,  3.46it/s] 30%|███       | 177/590 [01:14<01:59,  3.46it/s] 30%|███       | 178/590 [01:14<01:58,  3.46it/s] 30%|███       | 179/590 [01:15<01:58,  3.47it/s] 31%|███       | 180/590 [01:15<01:58,  3.47it/s] 31%|███       | 181/590 [01:15<01:57,  3.47it/s] 31%|███       | 182/590 [01:16<01:57,  3.47it/s] 31%|███       | 183/590 [01:16<01:57,  3.47it/s] 31%|███       | 184/590 [01:16<01:57,  3.47it/s] 31%|███▏      | 185/590 [01:16<01:56,  3.47it/s] 32%|███▏      | 186/590 [01:17<01:56,  3.46it/s] 32%|███▏      | 187/590 [01:17<01:56,  3.46it/s] 32%|███▏      | 188/590 [01:17<01:56,  3.46it/s] 32%|███▏      | 189/590 [01:18<01:55,  3.47it/s] 32%|███▏      | 190/590 [01:18<01:55,  3.47it/s] 32%|███▏      | 191/590 [01:18<01:55,  3.47it/s] 33%|███▎      | 192/590 [01:18<01:54,  3.47it/s] 33%|███▎      | 193/590 [01:19<01:54,  3.47it/s] 33%|███▎      | 194/590 [01:19<01:54,  3.47it/s] 33%|███▎      | 195/590 [01:19<01:53,  3.47it/s] 33%|███▎      | 196/590 [01:20<01:53,  3.47it/s] 33%|███▎      | 197/590 [01:20<01:53,  3.46it/s] 34%|███▎      | 198/590 [01:20<01:53,  3.46it/s] 34%|███▎      | 199/590 [01:20<01:52,  3.46it/s] 34%|███▍      | 200/590 [01:21<01:52,  3.47it/s] 34%|███▍      | 201/590 [01:21<01:52,  3.47it/s] 34%|███▍      | 202/590 [01:21<01:51,  3.47it/s] 34%|███▍      | 203/590 [01:22<01:51,  3.47it/s] 35%|███▍      | 204/590 [01:22<01:51,  3.47it/s] 35%|███▍      | 205/590 [01:22<01:50,  3.47it/s] 35%|███▍      | 206/590 [01:23<01:50,  3.47it/s] 35%|███▌      | 207/590 [01:23<01:50,  3.47it/s] 35%|███▌      | 208/590 [01:23<01:50,  3.47it/s] 35%|███▌      | 209/590 [01:23<01:49,  3.47it/s] 36%|███▌      | 210/590 [01:24<01:50,  3.45it/s] 36%|███▌      | 211/590 [01:24<01:49,  3.46it/s] 36%|███▌      | 212/590 [01:24<01:49,  3.46it/s] 36%|███▌      | 213/590 [01:25<01:48,  3.46it/s] 36%|███▋      | 214/590 [01:25<01:48,  3.46it/s] 36%|███▋      | 215/590 [01:25<01:48,  3.46it/s] 37%|███▋      | 216/590 [01:25<01:47,  3.47it/s] 37%|███▋      | 217/590 [01:26<01:47,  3.46it/s] 37%|███▋      | 218/590 [01:26<01:47,  3.47it/s] 37%|███▋      | 219/590 [01:26<01:47,  3.47it/s] 37%|███▋      | 220/590 [01:27<01:46,  3.46it/s] 37%|███▋      | 221/590 [01:27<01:46,  3.46it/s] 38%|███▊      | 222/590 [01:27<01:46,  3.46it/s] 38%|███▊      | 223/590 [01:27<01:45,  3.46it/s] 38%|███▊      | 224/590 [01:28<01:45,  3.46it/s] 38%|███▊      | 225/590 [01:28<01:45,  3.46it/s] 38%|███▊      | 226/590 [01:28<01:45,  3.46it/s] 38%|███▊      | 227/590 [01:29<01:44,  3.46it/s] 39%|███▊      | 228/590 [01:29<01:44,  3.47it/s] 39%|███▉      | 229/590 [01:29<01:44,  3.47it/s] 39%|███▉      | 230/590 [01:29<01:43,  3.47it/s] 39%|███▉      | 231/590 [01:30<01:43,  3.47it/s] 39%|███▉      | 232/590 [01:30<01:43,  3.46it/s] 39%|███▉      | 233/590 [01:30<01:43,  3.46it/s] 40%|███▉      | 234/590 [01:31<01:42,  3.46it/s] 40%|███▉      | 235/590 [01:31<01:42,  3.46it/s] 40%|████      | 236/590 [01:31<01:39,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 02:48:45,887 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:48:45,887 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:48:45,887 >>   Batch size = 8
{'eval_loss': 0.8914464116096497, 'eval_runtime': 16.7706, 'eval_samples_per_second': 372.854, 'eval_steps_per_second': 46.629, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.05it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.62it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.83it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.11it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.72it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.33it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.07it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.81it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.73it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.80it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.78it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.71it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.80it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.76it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.79it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.75it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.58it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.65it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.67it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.52it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.67it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.75it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.76it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.81it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.74it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.64it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.59it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.61it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.60it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.55it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.65it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.67it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.69it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.74it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.65it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.51it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.48it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.43it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.36it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.53it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.61it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.67it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.77it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.76it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.74it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.58it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.57it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.62it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.66it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.62it/s][A
 33%|███▎      | 258/782 [00:05<00:12, 41.16it/s][A
 34%|███▎      | 263/782 [00:05<00:12, 42.72it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 43.90it/s][A
 35%|███▍      | 273/782 [00:05<00:11, 44.79it/s][A
 36%|███▌      | 278/782 [00:05<00:11, 45.35it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 45.78it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.13it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.34it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.16it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.18it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.31it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.41it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.53it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.58it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.50it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.48it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.49it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.36it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.28it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.27it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.26it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 46.44it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.63it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.68it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.76it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.66it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.58it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.58it/s][A
 51%|█████     | 398/782 [00:08<00:08, 43.81it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 44.64it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 45.32it/s][A
 53%|█████▎    | 413/782 [00:08<00:08, 45.77it/s][A
 53%|█████▎    | 418/782 [00:09<00:07, 46.06it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.25it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.50it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.59it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.36it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.42it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.39it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.47it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.65it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.69it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.79it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.76it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.57it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.60it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.51it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.54it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.60it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.60it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.70it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.73it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.75it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.61it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.60it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.64it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.49it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.50it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.64it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 46.63it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.55it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.72it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.70it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.58it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.63it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.61it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.67it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.62it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.61it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.54it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.63it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.61it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.56it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.58it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.60it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.67it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.52it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.53it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.54it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.57it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.61it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.64it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.61it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.58it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.61it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.67it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.56it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.54it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.53it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.53it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.59it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.64it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.66it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.67it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.66it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.67it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.54it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.61it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.52it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.56it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.61it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.61it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.64it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.54it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:48<01:39,  3.57it/s]
100%|██████████| 782/782 [00:16<00:00, 46.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:49:02,744 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-29 02:49:02,786 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:49:05,154 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:49:05,181 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:49:05,202 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:55<43:53,  7.46s/it] 40%|████      | 238/590 [01:56<31:09,  5.31s/it] 41%|████      | 239/590 [01:56<22:15,  3.80s/it] 41%|████      | 240/590 [01:56<16:02,  2.75s/it] 41%|████      | 241/590 [01:57<11:41,  2.01s/it] 41%|████      | 242/590 [01:57<08:39,  1.49s/it] 41%|████      | 243/590 [01:57<06:32,  1.13s/it] 41%|████▏     | 244/590 [01:57<05:04,  1.14it/s] 42%|████▏     | 245/590 [01:58<04:02,  1.43it/s] 42%|████▏     | 246/590 [01:58<03:18,  1.73it/s] 42%|████▏     | 247/590 [01:58<02:48,  2.04it/s] 42%|████▏     | 248/590 [01:59<02:27,  2.33it/s] 42%|████▏     | 249/590 [01:59<02:12,  2.58it/s] 42%|████▏     | 250/590 [01:59<02:01,  2.79it/s] 43%|████▎     | 251/590 [01:59<01:54,  2.97it/s] 43%|████▎     | 252/590 [02:00<01:49,  3.10it/s] 43%|████▎     | 253/590 [02:00<01:45,  3.20it/s] 43%|████▎     | 254/590 [02:00<01:42,  3.28it/s] 43%|████▎     | 255/590 [02:01<01:40,  3.33it/s] 43%|████▎     | 256/590 [02:01<01:39,  3.37it/s] 44%|████▎     | 257/590 [02:01<01:37,  3.40it/s] 44%|████▎     | 258/590 [02:01<01:37,  3.42it/s] 44%|████▍     | 259/590 [02:02<01:36,  3.43it/s] 44%|████▍     | 260/590 [02:02<01:35,  3.44it/s] 44%|████▍     | 261/590 [02:02<01:35,  3.45it/s] 44%|████▍     | 262/590 [02:03<01:34,  3.45it/s] 45%|████▍     | 263/590 [02:03<01:34,  3.46it/s] 45%|████▍     | 264/590 [02:03<01:34,  3.46it/s] 45%|████▍     | 265/590 [02:03<01:33,  3.46it/s] 45%|████▌     | 266/590 [02:04<01:33,  3.47it/s] 45%|████▌     | 267/590 [02:04<01:33,  3.47it/s] 45%|████▌     | 268/590 [02:04<01:32,  3.46it/s] 46%|████▌     | 269/590 [02:05<01:33,  3.45it/s] 46%|████▌     | 270/590 [02:05<01:32,  3.46it/s] 46%|████▌     | 271/590 [02:05<01:32,  3.46it/s] 46%|████▌     | 272/590 [02:05<01:31,  3.46it/s] 46%|████▋     | 273/590 [02:06<01:31,  3.47it/s] 46%|████▋     | 274/590 [02:06<01:31,  3.46it/s] 47%|████▋     | 275/590 [02:06<01:30,  3.47it/s] 47%|████▋     | 276/590 [02:07<01:30,  3.47it/s] 47%|████▋     | 277/590 [02:07<01:30,  3.47it/s] 47%|████▋     | 278/590 [02:07<01:30,  3.47it/s] 47%|████▋     | 279/590 [02:07<01:29,  3.47it/s] 47%|████▋     | 280/590 [02:08<01:29,  3.46it/s] 48%|████▊     | 281/590 [02:08<01:29,  3.46it/s] 48%|████▊     | 282/590 [02:08<01:28,  3.46it/s] 48%|████▊     | 283/590 [02:09<01:28,  3.46it/s] 48%|████▊     | 284/590 [02:09<01:28,  3.46it/s] 48%|████▊     | 285/590 [02:09<01:28,  3.46it/s] 48%|████▊     | 286/590 [02:10<01:27,  3.47it/s] 49%|████▊     | 287/590 [02:10<01:27,  3.47it/s] 49%|████▉     | 288/590 [02:10<01:27,  3.47it/s] 49%|████▉     | 289/590 [02:10<01:26,  3.47it/s] 49%|████▉     | 290/590 [02:11<01:26,  3.47it/s] 49%|████▉     | 291/590 [02:11<01:26,  3.46it/s] 49%|████▉     | 292/590 [02:11<01:26,  3.46it/s] 50%|████▉     | 293/590 [02:12<01:25,  3.47it/s] 50%|████▉     | 294/590 [02:12<01:25,  3.47it/s] 50%|█████     | 295/590 [02:12<01:25,  3.47it/s] 50%|█████     | 296/590 [02:12<01:24,  3.47it/s] 50%|█████     | 297/590 [02:13<01:24,  3.47it/s] 51%|█████     | 298/590 [02:13<01:24,  3.47it/s] 51%|█████     | 299/590 [02:13<01:23,  3.47it/s] 51%|█████     | 300/590 [02:14<01:23,  3.47it/s] 51%|█████     | 301/590 [02:14<01:23,  3.47it/s] 51%|█████     | 302/590 [02:14<01:23,  3.46it/s] 51%|█████▏    | 303/590 [02:14<01:22,  3.46it/s] 52%|█████▏    | 304/590 [02:15<01:22,  3.46it/s] 52%|█████▏    | 305/590 [02:15<01:22,  3.46it/s] 52%|█████▏    | 306/590 [02:15<01:21,  3.46it/s] 52%|█████▏    | 307/590 [02:16<01:21,  3.47it/s] 52%|█████▏    | 308/590 [02:16<01:21,  3.47it/s] 52%|█████▏    | 309/590 [02:16<01:21,  3.47it/s] 53%|█████▎    | 310/590 [02:16<01:20,  3.47it/s] 53%|█████▎    | 311/590 [02:17<01:20,  3.47it/s] 53%|█████▎    | 312/590 [02:17<01:20,  3.47it/s] 53%|█████▎    | 313/590 [02:17<01:20,  3.46it/s] 53%|█████▎    | 314/590 [02:18<01:19,  3.46it/s] 53%|█████▎    | 315/590 [02:18<01:19,  3.46it/s] 54%|█████▎    | 316/590 [02:18<01:19,  3.46it/s] 54%|█████▎    | 317/590 [02:18<01:18,  3.46it/s] 54%|█████▍    | 318/590 [02:19<01:18,  3.46it/s] 54%|█████▍    | 319/590 [02:19<01:18,  3.47it/s] 54%|█████▍    | 320/590 [02:19<01:17,  3.46it/s] 54%|█████▍    | 321/590 [02:20<01:17,  3.46it/s] 55%|█████▍    | 322/590 [02:20<01:17,  3.47it/s] 55%|█████▍    | 323/590 [02:20<01:17,  3.47it/s] 55%|█████▍    | 324/590 [02:20<01:17,  3.45it/s] 55%|█████▌    | 325/590 [02:21<01:16,  3.46it/s] 55%|█████▌    | 326/590 [02:21<01:16,  3.46it/s] 55%|█████▌    | 327/590 [02:21<01:15,  3.46it/s] 56%|█████▌    | 328/590 [02:22<01:15,  3.47it/s] 56%|█████▌    | 329/590 [02:22<01:15,  3.47it/s] 56%|█████▌    | 330/590 [02:22<01:15,  3.46it/s] 56%|█████▌    | 331/590 [02:22<01:14,  3.46it/s] 56%|█████▋    | 332/590 [02:23<01:14,  3.47it/s] 56%|█████▋    | 333/590 [02:23<01:14,  3.46it/s] 57%|█████▋    | 334/590 [02:23<01:13,  3.46it/s] 57%|█████▋    | 335/590 [02:24<01:14,  3.44it/s] 57%|█████▋    | 336/590 [02:24<01:13,  3.45it/s] 57%|█████▋    | 337/590 [02:24<01:13,  3.45it/s] 57%|█████▋    | 338/590 [02:25<01:12,  3.46it/s] 57%|█████▋    | 339/590 [02:25<01:12,  3.46it/s] 58%|█████▊    | 340/590 [02:25<01:12,  3.46it/s] 58%|█████▊    | 341/590 [02:25<01:11,  3.46it/s] 58%|█████▊    | 342/590 [02:26<01:11,  3.46it/s] 58%|█████▊    | 343/590 [02:26<01:11,  3.46it/s] 58%|█████▊    | 344/590 [02:26<01:11,  3.46it/s] 58%|█████▊    | 345/590 [02:27<01:10,  3.47it/s] 59%|█████▊    | 346/590 [02:27<01:10,  3.46it/s] 59%|█████▉    | 347/590 [02:27<01:10,  3.46it/s] 59%|█████▉    | 348/590 [02:27<01:09,  3.46it/s] 59%|█████▉    | 349/590 [02:28<01:09,  3.46it/s] 59%|█████▉    | 350/590 [02:28<01:09,  3.46it/s] 59%|█████▉    | 351/590 [02:28<01:09,  3.46it/s] 60%|█████▉    | 352/590 [02:29<01:08,  3.46it/s] 60%|█████▉    | 353/590 [02:29<01:08,  3.46it/s] 60%|██████    | 354/590 [02:29<01:06,  3.56it/s][INFO|trainer.py:2140] 2023-08-29 02:49:43,853 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:49:43,853 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:49:43,853 >>   Batch size = 8
{'eval_loss': 0.88514244556427, 'eval_runtime': 16.8364, 'eval_samples_per_second': 371.397, 'eval_steps_per_second': 46.447, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.21it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.45it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.66it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.03it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.57it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.34it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.93it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.66it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.59it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.44it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.46it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.49it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.55it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.71it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.80it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.62it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.56it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.51it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.61it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.65it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.61it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.67it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.76it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.78it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.82it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.57it/s][A
 18%|█▊        | 138/782 [00:02<00:15, 42.20it/s][A
 18%|█▊        | 143/782 [00:03<00:14, 43.51it/s][A
 19%|█▉        | 148/782 [00:03<00:14, 44.50it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 45.13it/s][A
 20%|██        | 158/782 [00:03<00:13, 45.58it/s][A
 21%|██        | 163/782 [00:03<00:13, 45.90it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.08it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.30it/s][A
 23%|██▎       | 178/782 [00:03<00:13, 46.21it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.16it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.26it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.32it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.39it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.27it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.47it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.48it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.55it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.57it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.61it/s][A
 30%|██▉       | 233/782 [00:05<00:11, 46.44it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.56it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.61it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.66it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.73it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.67it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.72it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.71it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.68it/s][A
 36%|███▌      | 278/782 [00:05<00:11, 44.18it/s][A
 36%|███▌      | 283/782 [00:06<00:11, 44.92it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 45.46it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 45.92it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.17it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.39it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.45it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.59it/s][A
 41%|████      | 318/782 [00:06<00:10, 46.37it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.41it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.47it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.49it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.53it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.66it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.68it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.76it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.72it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.60it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.59it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.47it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.46it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.34it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.41it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.50it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.60it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.70it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.64it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.57it/s][A
 53%|█████▎    | 418/782 [00:09<00:07, 46.51it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.49it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.59it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.56it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.54it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.57it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.69it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.69it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.68it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.57it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.57it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.63it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.56it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.69it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.71it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.57it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.64it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.66it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.59it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.58it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.62it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.63it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.61it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.68it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.68it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.54it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.55it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.58it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 46.59it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.54it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.58it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.51it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.63it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.68it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.70it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.52it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.58it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.43it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.37it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.26it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.27it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.20it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.27it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.43it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.47it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.46it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.51it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.48it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.57it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.62it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.61it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.65it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.61it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.60it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.58it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.50it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.55it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.60it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.62it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.59it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.65it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.63it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.58it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.64it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.58it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.52it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.48it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.57it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.68it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.62it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.62it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.59it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.55it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:46<01:06,  3.56it/s]
100%|██████████| 782/782 [00:16<00:00, 46.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:50:00,703 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-29 02:50:00,729 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:50:03,176 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:50:03,199 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:50:03,207 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:54<29:57,  7.65s/it] 60%|██████    | 356/590 [02:54<21:13,  5.44s/it] 61%|██████    | 357/590 [02:55<15:07,  3.90s/it] 61%|██████    | 358/590 [02:55<10:52,  2.81s/it] 61%|██████    | 359/590 [02:55<07:54,  2.06s/it] 61%|██████    | 360/590 [02:55<05:50,  1.53s/it] 61%|██████    | 361/590 [02:56<04:24,  1.15s/it] 61%|██████▏   | 362/590 [02:56<03:23,  1.12it/s] 62%|██████▏   | 363/590 [02:56<02:41,  1.40it/s] 62%|██████▏   | 364/590 [02:57<02:12,  1.71it/s] 62%|██████▏   | 365/590 [02:57<01:51,  2.02it/s] 62%|██████▏   | 366/590 [02:57<01:37,  2.31it/s] 62%|██████▏   | 367/590 [02:57<01:27,  2.56it/s] 62%|██████▏   | 368/590 [02:58<01:19,  2.78it/s] 63%|██████▎   | 369/590 [02:58<01:14,  2.95it/s] 63%|██████▎   | 370/590 [02:58<01:11,  3.09it/s] 63%|██████▎   | 371/590 [02:59<01:08,  3.20it/s] 63%|██████▎   | 372/590 [02:59<01:06,  3.27it/s] 63%|██████▎   | 373/590 [02:59<01:05,  3.33it/s] 63%|██████▎   | 374/590 [02:59<01:04,  3.37it/s] 64%|██████▎   | 375/590 [03:00<01:03,  3.40it/s] 64%|██████▎   | 376/590 [03:00<01:02,  3.42it/s] 64%|██████▍   | 377/590 [03:00<01:02,  3.43it/s] 64%|██████▍   | 378/590 [03:01<01:01,  3.44it/s] 64%|██████▍   | 379/590 [03:01<01:01,  3.45it/s] 64%|██████▍   | 380/590 [03:01<01:00,  3.45it/s] 65%|██████▍   | 381/590 [03:01<01:00,  3.46it/s] 65%|██████▍   | 382/590 [03:02<01:00,  3.46it/s] 65%|██████▍   | 383/590 [03:02<00:59,  3.46it/s] 65%|██████▌   | 384/590 [03:02<00:59,  3.47it/s] 65%|██████▌   | 385/590 [03:03<00:59,  3.47it/s] 65%|██████▌   | 386/590 [03:03<00:58,  3.47it/s] 66%|██████▌   | 387/590 [03:03<00:58,  3.47it/s] 66%|██████▌   | 388/590 [03:03<00:58,  3.47it/s] 66%|██████▌   | 389/590 [03:04<00:57,  3.47it/s] 66%|██████▌   | 390/590 [03:04<00:57,  3.47it/s] 66%|██████▋   | 391/590 [03:04<00:57,  3.47it/s] 66%|██████▋   | 392/590 [03:05<00:57,  3.47it/s] 67%|██████▋   | 393/590 [03:05<00:56,  3.47it/s] 67%|██████▋   | 394/590 [03:05<00:56,  3.47it/s] 67%|██████▋   | 395/590 [03:05<00:56,  3.47it/s] 67%|██████▋   | 396/590 [03:06<00:55,  3.47it/s] 67%|██████▋   | 397/590 [03:06<00:55,  3.46it/s] 67%|██████▋   | 398/590 [03:06<00:55,  3.46it/s] 68%|██████▊   | 399/590 [03:07<00:55,  3.46it/s] 68%|██████▊   | 400/590 [03:07<00:54,  3.47it/s] 68%|██████▊   | 401/590 [03:07<00:54,  3.47it/s] 68%|██████▊   | 402/590 [03:08<00:54,  3.47it/s] 68%|██████▊   | 403/590 [03:08<00:53,  3.47it/s] 68%|██████▊   | 404/590 [03:08<00:53,  3.47it/s] 69%|██████▊   | 405/590 [03:08<00:53,  3.47it/s] 69%|██████▉   | 406/590 [03:09<00:53,  3.47it/s] 69%|██████▉   | 407/590 [03:09<00:52,  3.47it/s] 69%|██████▉   | 408/590 [03:09<00:52,  3.45it/s] 69%|██████▉   | 409/590 [03:10<00:52,  3.45it/s] 69%|██████▉   | 410/590 [03:10<00:52,  3.45it/s] 70%|██████▉   | 411/590 [03:10<00:51,  3.46it/s] 70%|██████▉   | 412/590 [03:10<00:51,  3.46it/s] 70%|███████   | 413/590 [03:11<00:51,  3.46it/s] 70%|███████   | 414/590 [03:11<00:50,  3.46it/s] 70%|███████   | 415/590 [03:11<00:50,  3.46it/s] 71%|███████   | 416/590 [03:12<00:50,  3.46it/s] 71%|███████   | 417/590 [03:12<00:49,  3.47it/s] 71%|███████   | 418/590 [03:12<00:49,  3.47it/s] 71%|███████   | 419/590 [03:12<00:49,  3.46it/s] 71%|███████   | 420/590 [03:13<00:49,  3.46it/s] 71%|███████▏  | 421/590 [03:13<00:48,  3.46it/s] 72%|███████▏  | 422/590 [03:13<00:48,  3.46it/s] 72%|███████▏  | 423/590 [03:14<00:48,  3.46it/s] 72%|███████▏  | 424/590 [03:14<00:47,  3.46it/s] 72%|███████▏  | 425/590 [03:14<00:47,  3.46it/s] 72%|███████▏  | 426/590 [03:14<00:47,  3.46it/s] 72%|███████▏  | 427/590 [03:15<00:47,  3.46it/s] 73%|███████▎  | 428/590 [03:15<00:46,  3.46it/s] 73%|███████▎  | 429/590 [03:15<00:46,  3.46it/s] 73%|███████▎  | 430/590 [03:16<00:46,  3.45it/s] 73%|███████▎  | 431/590 [03:16<00:46,  3.45it/s] 73%|███████▎  | 432/590 [03:16<00:45,  3.46it/s] 73%|███████▎  | 433/590 [03:16<00:45,  3.46it/s] 74%|███████▎  | 434/590 [03:17<00:45,  3.46it/s] 74%|███████▎  | 435/590 [03:17<00:44,  3.46it/s] 74%|███████▍  | 436/590 [03:17<00:44,  3.46it/s] 74%|███████▍  | 437/590 [03:18<00:44,  3.46it/s] 74%|███████▍  | 438/590 [03:18<00:43,  3.47it/s] 74%|███████▍  | 439/590 [03:18<00:43,  3.46it/s] 75%|███████▍  | 440/590 [03:18<00:43,  3.46it/s] 75%|███████▍  | 441/590 [03:19<00:43,  3.46it/s] 75%|███████▍  | 442/590 [03:19<00:42,  3.46it/s] 75%|███████▌  | 443/590 [03:19<00:42,  3.46it/s] 75%|███████▌  | 444/590 [03:20<00:42,  3.46it/s] 75%|███████▌  | 445/590 [03:20<00:41,  3.46it/s] 76%|███████▌  | 446/590 [03:20<00:41,  3.46it/s] 76%|███████▌  | 447/590 [03:21<00:41,  3.46it/s] 76%|███████▌  | 448/590 [03:21<00:40,  3.46it/s] 76%|███████▌  | 449/590 [03:21<00:40,  3.46it/s] 76%|███████▋  | 450/590 [03:21<00:40,  3.46it/s] 76%|███████▋  | 451/590 [03:22<00:40,  3.46it/s] 77%|███████▋  | 452/590 [03:22<00:39,  3.46it/s] 77%|███████▋  | 453/590 [03:22<00:39,  3.46it/s] 77%|███████▋  | 454/590 [03:23<00:39,  3.46it/s] 77%|███████▋  | 455/590 [03:23<00:38,  3.46it/s] 77%|███████▋  | 456/590 [03:23<00:38,  3.46it/s] 77%|███████▋  | 457/590 [03:23<00:38,  3.47it/s] 78%|███████▊  | 458/590 [03:24<00:38,  3.47it/s] 78%|███████▊  | 459/590 [03:24<00:37,  3.47it/s] 78%|███████▊  | 460/590 [03:24<00:37,  3.47it/s] 78%|███████▊  | 461/590 [03:25<00:37,  3.47it/s] 78%|███████▊  | 462/590 [03:25<00:36,  3.47it/s] 78%|███████▊  | 463/590 [03:25<00:36,  3.47it/s] 79%|███████▊  | 464/590 [03:25<00:36,  3.47it/s] 79%|███████▉  | 465/590 [03:26<00:36,  3.46it/s] 79%|███████▉  | 466/590 [03:26<00:35,  3.46it/s] 79%|███████▉  | 467/590 [03:26<00:35,  3.46it/s] 79%|███████▉  | 468/590 [03:27<00:36,  3.33it/s] 79%|███████▉  | 469/590 [03:27<00:35,  3.37it/s] 80%|███████▉  | 470/590 [03:27<00:35,  3.40it/s] 80%|███████▉  | 471/590 [03:27<00:34,  3.42it/s] 80%|████████  | 472/590 [03:28<00:33,  3.53it/s][INFO|trainer.py:2140] 2023-08-29 02:50:42,482 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:50:42,482 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:50:42,482 >>   Batch size = 8
{'eval_loss': 0.8928248286247253, 'eval_runtime': 16.8376, 'eval_samples_per_second': 371.371, 'eval_steps_per_second': 46.444, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 56.83it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.33it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.52it/s][A
  3%|▎         | 23/782 [00:00<00:15, 47.83it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.44it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.10it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.86it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.60it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.49it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.52it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.46it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.46it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.51it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.64it/s][A
 10%|▉         | 78/782 [00:01<00:15, 44.52it/s][A
 11%|█         | 83/782 [00:01<00:15, 45.18it/s][A
 11%|█▏        | 88/782 [00:01<00:15, 45.66it/s][A
 12%|█▏        | 93/782 [00:01<00:15, 45.93it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.24it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.45it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.49it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.63it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.38it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.41it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.55it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.53it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.56it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.68it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.70it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.70it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.70it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.63it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.62it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.50it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.57it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.64it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.67it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.73it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.75it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.72it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.72it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.64it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.58it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.57it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.58it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.63it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.75it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.79it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.66it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.65it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.60it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.63it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.67it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.51it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.59it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.67it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.75it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.71it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.62it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.64it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.58it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.61it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.55it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.57it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.55it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.68it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.63it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.66it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.61it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.60it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.62it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.63it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.57it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.61it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.57it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.58it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.65it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.69it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.60it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.65it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.64it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.61it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.66it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.60it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.62it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.66it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.68it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.59it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.61it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.60it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.62it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.64it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.61it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.58it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.55it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.65it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.60it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.64it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.65it/s][A
 64%|██████▍   | 503/782 [00:10<00:06, 46.48it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.60it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.63it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.51it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.58it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.60it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.62it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.59it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.61it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.60it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.64it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.65it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.64it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.60it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.59it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.60it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.65it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.54it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.59it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.63it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.61it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.64it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.56it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.55it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.64it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.54it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.60it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.55it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.50it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.63it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.62it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.56it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.65it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.59it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.57it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.58it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.55it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.59it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.58it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.58it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.59it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.56it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.58it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.60it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.64it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.54it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.49it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.57it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.61it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.65it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.68it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.61it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.55it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.62it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.59it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.57it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:45<00:33,  3.53it/s]
100%|██████████| 782/782 [00:16<00:00, 46.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:50:59,296 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-29 02:50:59,315 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:51:01,940 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:51:01,957 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:51:01,965 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:52<14:37,  7.50s/it] 80%|████████  | 474/590 [03:52<10:19,  5.34s/it] 81%|████████  | 475/590 [03:53<07:19,  3.82s/it] 81%|████████  | 476/590 [03:53<05:14,  2.76s/it] 81%|████████  | 477/590 [03:53<03:48,  2.02s/it] 81%|████████  | 478/590 [03:54<02:48,  1.50s/it] 81%|████████  | 479/590 [03:54<02:06,  1.14s/it] 81%|████████▏ | 480/590 [03:54<01:37,  1.13it/s] 82%|████████▏ | 481/590 [03:54<01:16,  1.42it/s] 82%|████████▏ | 482/590 [03:55<01:02,  1.73it/s] 82%|████████▏ | 483/590 [03:55<00:52,  2.03it/s] 82%|████████▏ | 484/590 [03:55<00:45,  2.32it/s] 82%|████████▏ | 485/590 [03:56<00:40,  2.58it/s] 82%|████████▏ | 486/590 [03:56<00:37,  2.79it/s] 83%|████████▎ | 487/590 [03:56<00:34,  2.97it/s] 83%|████████▎ | 488/590 [03:56<00:32,  3.10it/s] 83%|████████▎ | 489/590 [03:57<00:31,  3.20it/s] 83%|████████▎ | 490/590 [03:57<00:30,  3.28it/s] 83%|████████▎ | 491/590 [03:57<00:29,  3.32it/s] 83%|████████▎ | 492/590 [03:58<00:29,  3.36it/s] 84%|████████▎ | 493/590 [03:58<00:28,  3.39it/s] 84%|████████▎ | 494/590 [03:58<00:28,  3.42it/s] 84%|████████▍ | 495/590 [03:58<00:27,  3.43it/s] 84%|████████▍ | 496/590 [03:59<00:27,  3.44it/s] 84%|████████▍ | 497/590 [03:59<00:26,  3.45it/s] 84%|████████▍ | 498/590 [03:59<00:26,  3.46it/s] 85%|████████▍ | 499/590 [04:00<00:26,  3.46it/s] 85%|████████▍ | 500/590 [04:00<00:25,  3.46it/s]                                                  85%|████████▍ | 500/590 [04:00<00:25,  3.46it/s] 85%|████████▍ | 501/590 [04:00<00:25,  3.46it/s] 85%|████████▌ | 502/590 [04:00<00:25,  3.46it/s] 85%|████████▌ | 503/590 [04:01<00:25,  3.46it/s] 85%|████████▌ | 504/590 [04:01<00:24,  3.46it/s] 86%|████████▌ | 505/590 [04:01<00:24,  3.46it/s] 86%|████████▌ | 506/590 [04:02<00:24,  3.46it/s] 86%|████████▌ | 507/590 [04:02<00:23,  3.46it/s] 86%|████████▌ | 508/590 [04:02<00:23,  3.46it/s] 86%|████████▋ | 509/590 [04:02<00:23,  3.47it/s] 86%|████████▋ | 510/590 [04:03<00:23,  3.47it/s] 87%|████████▋ | 511/590 [04:03<00:22,  3.47it/s] 87%|████████▋ | 512/590 [04:03<00:22,  3.47it/s] 87%|████████▋ | 513/590 [04:04<00:22,  3.45it/s] 87%|████████▋ | 514/590 [04:04<00:21,  3.46it/s] 87%|████████▋ | 515/590 [04:04<00:21,  3.46it/s] 87%|████████▋ | 516/590 [04:04<00:21,  3.46it/s] 88%|████████▊ | 517/590 [04:05<00:21,  3.46it/s] 88%|████████▊ | 518/590 [04:05<00:20,  3.46it/s] 88%|████████▊ | 519/590 [04:05<00:20,  3.46it/s] 88%|████████▊ | 520/590 [04:06<00:20,  3.47it/s] 88%|████████▊ | 521/590 [04:06<00:19,  3.47it/s] 88%|████████▊ | 522/590 [04:06<00:19,  3.47it/s] 89%|████████▊ | 523/590 [04:07<00:19,  3.47it/s] 89%|████████▉ | 524/590 [04:07<00:19,  3.45it/s] 89%|████████▉ | 525/590 [04:07<00:18,  3.46it/s] 89%|████████▉ | 526/590 [04:07<00:18,  3.46it/s] 89%|████████▉ | 527/590 [04:08<00:18,  3.46it/s] 89%|████████▉ | 528/590 [04:08<00:17,  3.46it/s] 90%|████████▉ | 529/590 [04:08<00:17,  3.46it/s] 90%|████████▉ | 530/590 [04:09<00:17,  3.47it/s] 90%|█████████ | 531/590 [04:09<00:17,  3.47it/s] 90%|█████████ | 532/590 [04:09<00:16,  3.47it/s] 90%|█████████ | 533/590 [04:09<00:16,  3.47it/s] 91%|█████████ | 534/590 [04:10<00:16,  3.47it/s] 91%|█████████ | 535/590 [04:10<00:15,  3.45it/s] 91%|█████████ | 536/590 [04:10<00:15,  3.46it/s] 91%|█████████ | 537/590 [04:11<00:15,  3.46it/s] 91%|█████████ | 538/590 [04:11<00:15,  3.46it/s] 91%|█████████▏| 539/590 [04:11<00:14,  3.46it/s] 92%|█████████▏| 540/590 [04:11<00:14,  3.46it/s] 92%|█████████▏| 541/590 [04:12<00:14,  3.47it/s] 92%|█████████▏| 542/590 [04:12<00:13,  3.47it/s] 92%|█████████▏| 543/590 [04:12<00:13,  3.47it/s] 92%|█████████▏| 544/590 [04:13<00:13,  3.47it/s] 92%|█████████▏| 545/590 [04:13<00:12,  3.47it/s] 93%|█████████▎| 546/590 [04:13<00:12,  3.46it/s] 93%|█████████▎| 547/590 [04:13<00:12,  3.46it/s] 93%|█████████▎| 548/590 [04:14<00:12,  3.46it/s] 93%|█████████▎| 549/590 [04:14<00:11,  3.46it/s] 93%|█████████▎| 550/590 [04:14<00:11,  3.47it/s] 93%|█████████▎| 551/590 [04:15<00:11,  3.47it/s] 94%|█████████▎| 552/590 [04:15<00:10,  3.47it/s] 94%|█████████▎| 553/590 [04:15<00:10,  3.47it/s] 94%|█████████▍| 554/590 [04:15<00:10,  3.47it/s] 94%|█████████▍| 555/590 [04:16<00:10,  3.47it/s] 94%|█████████▍| 556/590 [04:16<00:09,  3.47it/s] 94%|█████████▍| 557/590 [04:16<00:09,  3.46it/s] 95%|█████████▍| 558/590 [04:17<00:09,  3.46it/s] 95%|█████████▍| 559/590 [04:17<00:08,  3.46it/s] 95%|█████████▍| 560/590 [04:17<00:08,  3.46it/s] 95%|█████████▌| 561/590 [04:17<00:08,  3.46it/s] 95%|█████████▌| 562/590 [04:18<00:08,  3.46it/s] 95%|█████████▌| 563/590 [04:18<00:07,  3.46it/s] 96%|█████████▌| 564/590 [04:18<00:07,  3.47it/s] 96%|█████████▌| 565/590 [04:19<00:07,  3.46it/s] 96%|█████████▌| 566/590 [04:19<00:06,  3.47it/s] 96%|█████████▌| 567/590 [04:19<00:06,  3.47it/s] 96%|█████████▋| 568/590 [04:20<00:06,  3.46it/s] 96%|█████████▋| 569/590 [04:20<00:06,  3.46it/s] 97%|█████████▋| 570/590 [04:20<00:05,  3.46it/s] 97%|█████████▋| 571/590 [04:20<00:05,  3.45it/s] 97%|█████████▋| 572/590 [04:21<00:05,  3.45it/s] 97%|█████████▋| 573/590 [04:21<00:04,  3.46it/s] 97%|█████████▋| 574/590 [04:21<00:04,  3.46it/s] 97%|█████████▋| 575/590 [04:22<00:04,  3.46it/s] 98%|█████████▊| 576/590 [04:22<00:04,  3.46it/s] 98%|█████████▊| 577/590 [04:22<00:03,  3.46it/s] 98%|█████████▊| 578/590 [04:22<00:03,  3.46it/s] 98%|█████████▊| 579/590 [04:23<00:03,  3.39it/s] 98%|█████████▊| 580/590 [04:23<00:02,  3.41it/s] 98%|█████████▊| 581/590 [04:23<00:02,  3.42it/s] 99%|█████████▊| 582/590 [04:24<00:02,  3.43it/s] 99%|█████████▉| 583/590 [04:24<00:02,  3.44it/s] 99%|█████████▉| 584/590 [04:24<00:01,  3.45it/s] 99%|█████████▉| 585/590 [04:24<00:01,  3.45it/s] 99%|█████████▉| 586/590 [04:25<00:01,  3.46it/s] 99%|█████████▉| 587/590 [04:25<00:00,  3.46it/s]100%|█████████▉| 588/590 [04:25<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:26<00:00,  3.46it/s]100%|██████████| 590/590 [04:26<00:00,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 02:51:40,593 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:51:40,593 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:51:40,593 >>   Batch size = 8
{'eval_loss': 0.8936035633087158, 'eval_runtime': 16.7902, 'eval_samples_per_second': 372.42, 'eval_steps_per_second': 46.575, 'epoch': 4.0}
{'loss': 0.7657, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.25it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.59it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.82it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.19it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.62it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.33it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.93it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.79it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.72it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.63it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.64it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.75it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.74it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.48it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.54it/s][A
 11%|█         | 83/782 [00:01<00:15, 46.55it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.48it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.50it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.45it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.58it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.62it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.63it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.71it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.75it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.73it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.77it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.71it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.61it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.59it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.62it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.67it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.72it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.75it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.76it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.74it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.58it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.64it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.46it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.66it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.59it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.64it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.67it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.70it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.70it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.69it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.70it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.58it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.56it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.63it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.61it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.66it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.71it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.69it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.71it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.71it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.73it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.53it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.50it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.63it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.64it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.68it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.73it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.56it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.60it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.61it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.59it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.55it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.49it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.56it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.68it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.66it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.67it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.62it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.62it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.64it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.59it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.45it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.54it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.65it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.68it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.72it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.72it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.59it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.56it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.54it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.54it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.54it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.55it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.61it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.61it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.67it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.67it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.65it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.54it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.61it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.57it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.54it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.51it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.60it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.69it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.65it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.61it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.58it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.59it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.59it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.60it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.55it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.58it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.66it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.67it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.61it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.66it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.56it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.57it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.59it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.61it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.48it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.56it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.70it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.67it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.62it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.60it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.61it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.64it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.60it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.54it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.54it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.61it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.53it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.66it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.58it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.61it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.61it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.56it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.61it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.58it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.56it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.59it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.68it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.59it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.64it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.64it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.60it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.61it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.63it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.62it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.59it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.62it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.63it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.68it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.62it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.60it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.60it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.59it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.65it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:43<00:00,  3.57it/s]
100%|██████████| 782/782 [00:16<00:00, 46.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:51:57,376 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-29 02:51:57,396 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:51:59,668 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:51:59,688 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:51:59,701 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:52:04,481 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:52:04,485 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236 (score: 0.88514244556427).
                                                 100%|██████████| 590/590 [04:52<00:00,  3.57it/s]100%|██████████| 590/590 [04:52<00:00,  2.02it/s]
[INFO|trainer.py:1894] 2023-08-29 02:52:06,360 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 02:52:06,372 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:52:08,743 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:52:08,774 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:52:08,786 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:52:08,974 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,974 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,974 >>   train_loss               =       0.76
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,974 >>   train_runtime            = 0:04:52.11
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,975 >>   train_samples            =       7545
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,975 >>   train_samples_per_second =    129.143
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:08,975 >>   train_steps_per_second   =       2.02
{'eval_loss': 0.897061288356781, 'eval_runtime': 16.7689, 'eval_samples_per_second': 372.893, 'eval_steps_per_second': 46.634, 'epoch': 5.0}
{'train_runtime': 292.117, 'train_samples_per_second': 129.143, 'train_steps_per_second': 2.02, 'train_loss': 0.7600169165659759, 'epoch': 5.0}
08/29/2023 02:52:09 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:52:09,026 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:52:09,027 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 02:52:09,027 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.34it/s]  2%|▏         | 12/782 [00:00<00:15, 51.11it/s]  2%|▏         | 18/782 [00:00<00:15, 49.31it/s]  3%|▎         | 23/782 [00:00<00:15, 48.46it/s]  4%|▎         | 28/782 [00:00<00:15, 47.89it/s]  4%|▍         | 33/782 [00:00<00:15, 47.70it/s]  5%|▍         | 38/782 [00:00<00:15, 47.56it/s]  5%|▌         | 43/782 [00:00<00:15, 47.41it/s]  6%|▌         | 48/782 [00:00<00:15, 47.10it/s]  7%|▋         | 53/782 [00:01<00:15, 46.98it/s]  7%|▋         | 58/782 [00:01<00:15, 46.92it/s]  8%|▊         | 63/782 [00:01<00:15, 47.02it/s]  9%|▊         | 68/782 [00:01<00:15, 47.08it/s]  9%|▉         | 73/782 [00:01<00:15, 47.14it/s] 10%|▉         | 78/782 [00:01<00:14, 47.05it/s] 11%|█         | 83/782 [00:01<00:14, 47.02it/s] 11%|█▏        | 88/782 [00:01<00:14, 47.00it/s] 12%|█▏        | 93/782 [00:01<00:14, 46.99it/s] 13%|█▎        | 98/782 [00:02<00:14, 46.87it/s] 13%|█▎        | 103/782 [00:02<00:14, 46.83it/s] 14%|█▍        | 108/782 [00:02<00:14, 46.80it/s] 14%|█▍        | 113/782 [00:02<00:14, 46.77it/s] 15%|█▌        | 118/782 [00:02<00:14, 46.79it/s] 16%|█▌        | 123/782 [00:02<00:14, 46.89it/s] 16%|█▋        | 128/782 [00:02<00:13, 46.94it/s] 17%|█▋        | 133/782 [00:02<00:13, 47.05it/s] 18%|█▊        | 138/782 [00:02<00:13, 46.96it/s] 18%|█▊        | 143/782 [00:03<00:13, 46.95it/s] 19%|█▉        | 148/782 [00:03<00:13, 46.89it/s] 20%|█▉        | 153/782 [00:03<00:13, 46.94it/s] 20%|██        | 158/782 [00:03<00:13, 46.95it/s] 21%|██        | 163/782 [00:03<00:13, 46.89it/s] 21%|██▏       | 168/782 [00:03<00:13, 46.97it/s] 22%|██▏       | 173/782 [00:03<00:12, 46.96it/s] 23%|██▎       | 178/782 [00:03<00:12, 46.89it/s] 23%|██▎       | 183/782 [00:03<00:12, 46.95it/s] 24%|██▍       | 188/782 [00:03<00:12, 47.03it/s] 25%|██▍       | 193/782 [00:04<00:12, 46.97it/s] 25%|██▌       | 198/782 [00:04<00:12, 46.84it/s] 26%|██▌       | 203/782 [00:04<00:12, 46.85it/s] 27%|██▋       | 208/782 [00:04<00:12, 46.89it/s] 27%|██▋       | 213/782 [00:04<00:12, 46.91it/s] 28%|██▊       | 218/782 [00:04<00:12, 46.90it/s] 29%|██▊       | 223/782 [00:04<00:11, 46.92it/s] 29%|██▉       | 228/782 [00:04<00:11, 46.89it/s] 30%|██▉       | 233/782 [00:04<00:11, 46.90it/s] 30%|███       | 238/782 [00:05<00:11, 46.95it/s] 31%|███       | 243/782 [00:05<00:11, 46.89it/s] 32%|███▏      | 248/782 [00:05<00:11, 47.00it/s] 32%|███▏      | 253/782 [00:05<00:11, 46.91it/s] 33%|███▎      | 258/782 [00:05<00:11, 46.88it/s] 34%|███▎      | 263/782 [00:05<00:11, 46.86it/s] 34%|███▍      | 268/782 [00:05<00:10, 46.92it/s] 35%|███▍      | 273/782 [00:05<00:10, 46.87it/s] 36%|███▌      | 278/782 [00:05<00:10, 46.87it/s] 36%|███▌      | 283/782 [00:06<00:10, 46.88it/s] 37%|███▋      | 288/782 [00:06<00:10, 46.77it/s] 37%|███▋      | 293/782 [00:06<00:10, 46.81it/s] 38%|███▊      | 298/782 [00:06<00:10, 46.88it/s] 39%|███▊      | 303/782 [00:06<00:10, 46.77it/s] 39%|███▉      | 308/782 [00:06<00:10, 46.81it/s] 40%|████      | 313/782 [00:06<00:10, 46.79it/s] 41%|████      | 318/782 [00:06<00:09, 46.83it/s] 41%|████▏     | 323/782 [00:06<00:09, 46.88it/s] 42%|████▏     | 328/782 [00:06<00:09, 46.80it/s] 43%|████▎     | 333/782 [00:07<00:09, 46.85it/s] 43%|████▎     | 338/782 [00:07<00:09, 46.89it/s] 44%|████▍     | 343/782 [00:07<00:09, 46.83it/s] 45%|████▍     | 348/782 [00:07<00:09, 46.79it/s] 45%|████▌     | 353/782 [00:07<00:09, 46.82it/s] 46%|████▌     | 358/782 [00:07<00:09, 46.83it/s] 46%|████▋     | 363/782 [00:07<00:08, 46.85it/s] 47%|████▋     | 368/782 [00:07<00:08, 46.87it/s] 48%|████▊     | 373/782 [00:07<00:08, 46.83it/s] 48%|████▊     | 378/782 [00:08<00:08, 46.82it/s] 49%|████▉     | 383/782 [00:08<00:08, 46.85it/s] 50%|████▉     | 388/782 [00:08<00:08, 46.88it/s] 50%|█████     | 393/782 [00:08<00:08, 46.85it/s] 51%|█████     | 398/782 [00:08<00:08, 46.82it/s] 52%|█████▏    | 403/782 [00:08<00:08, 46.83it/s] 52%|█████▏    | 408/782 [00:08<00:07, 46.85it/s] 53%|█████▎    | 413/782 [00:08<00:07, 46.80it/s] 53%|█████▎    | 418/782 [00:08<00:07, 46.81it/s] 54%|█████▍    | 423/782 [00:08<00:07, 46.87it/s] 55%|█████▍    | 428/782 [00:09<00:07, 46.83it/s] 55%|█████▌    | 433/782 [00:09<00:07, 46.87it/s] 56%|█████▌    | 438/782 [00:09<00:07, 46.82it/s] 57%|█████▋    | 443/782 [00:09<00:07, 46.82it/s] 57%|█████▋    | 448/782 [00:09<00:07, 46.79it/s] 58%|█████▊    | 453/782 [00:09<00:07, 46.79it/s] 59%|█████▊    | 458/782 [00:09<00:06, 46.86it/s] 59%|█████▉    | 463/782 [00:09<00:06, 46.83it/s] 60%|█████▉    | 468/782 [00:09<00:06, 46.82it/s] 60%|██████    | 473/782 [00:10<00:06, 46.81it/s] 61%|██████    | 478/782 [00:10<00:06, 46.82it/s] 62%|██████▏   | 483/782 [00:10<00:06, 46.84it/s] 62%|██████▏   | 488/782 [00:10<00:06, 46.83it/s] 63%|██████▎   | 493/782 [00:10<00:06, 46.86it/s] 64%|██████▎   | 498/782 [00:10<00:06, 46.78it/s] 64%|██████▍   | 503/782 [00:10<00:05, 46.83it/s] 65%|██████▍   | 508/782 [00:10<00:05, 46.81it/s] 66%|██████▌   | 513/782 [00:10<00:05, 46.85it/s] 66%|██████▌   | 518/782 [00:11<00:05, 46.84it/s] 67%|██████▋   | 523/782 [00:11<00:05, 46.84it/s] 68%|██████▊   | 528/782 [00:11<00:05, 46.78it/s] 68%|██████▊   | 533/782 [00:11<00:05, 46.78it/s] 69%|██████▉   | 538/782 [00:11<00:05, 46.77it/s] 69%|██████▉   | 543/782 [00:11<00:05, 46.82it/s] 70%|███████   | 548/782 [00:11<00:04, 46.84it/s] 71%|███████   | 553/782 [00:11<00:04, 46.78it/s] 71%|███████▏  | 558/782 [00:11<00:04, 46.76it/s] 72%|███████▏  | 563/782 [00:11<00:04, 46.79it/s] 73%|███████▎  | 568/782 [00:12<00:04, 46.82it/s] 73%|███████▎  | 573/782 [00:12<00:04, 46.87it/s] 74%|███████▍  | 578/782 [00:12<00:04, 46.77it/s] 75%|███████▍  | 583/782 [00:12<00:04, 46.83it/s] 75%|███████▌  | 588/782 [00:12<00:04, 46.79it/s] 76%|███████▌  | 593/782 [00:12<00:04, 46.78it/s] 76%|███████▋  | 598/782 [00:12<00:03, 46.78it/s] 77%|███████▋  | 603/782 [00:12<00:03, 46.77it/s] 78%|███████▊  | 608/782 [00:12<00:03, 46.76it/s] 78%|███████▊  | 613/782 [00:13<00:03, 46.83it/s] 79%|███████▉  | 618/782 [00:13<00:03, 46.80it/s] 80%|███████▉  | 623/782 [00:13<00:03, 46.79it/s] 80%|████████  | 628/782 [00:13<00:03, 46.70it/s] 81%|████████  | 633/782 [00:13<00:03, 46.79it/s] 82%|████████▏ | 638/782 [00:13<00:03, 46.76it/s] 82%|████████▏ | 643/782 [00:13<00:02, 46.80it/s] 83%|████████▎ | 648/782 [00:13<00:02, 46.82it/s] 84%|████████▎ | 653/782 [00:13<00:02, 46.77it/s] 84%|████████▍ | 658/782 [00:14<00:02, 46.77it/s] 85%|████████▍ | 663/782 [00:14<00:02, 46.84it/s] 85%|████████▌ | 668/782 [00:14<00:02, 46.82it/s] 86%|████████▌ | 673/782 [00:14<00:02, 46.82it/s] 87%|████████▋ | 678/782 [00:14<00:02, 46.81it/s] 87%|████████▋ | 683/782 [00:14<00:02, 46.76it/s] 88%|████████▊ | 688/782 [00:14<00:02, 46.73it/s] 89%|████████▊ | 693/782 [00:14<00:01, 46.80it/s] 89%|████████▉ | 698/782 [00:14<00:01, 46.82it/s] 90%|████████▉ | 703/782 [00:14<00:01, 46.83it/s] 91%|█████████ | 708/782 [00:15<00:01, 46.71it/s] 91%|█████████ | 713/782 [00:15<00:01, 46.82it/s] 92%|█████████▏| 718/782 [00:15<00:01, 46.81it/s] 92%|█████████▏| 723/782 [00:15<00:01, 46.82it/s] 93%|█████████▎| 728/782 [00:15<00:01, 46.79it/s] 94%|█████████▎| 733/782 [00:15<00:01, 46.79it/s] 94%|█████████▍| 738/782 [00:15<00:00, 46.70it/s] 95%|█████████▌| 743/782 [00:15<00:00, 46.77it/s] 96%|█████████▌| 748/782 [00:15<00:00, 46.81it/s] 96%|█████████▋| 753/782 [00:16<00:00, 46.76it/s] 97%|█████████▋| 758/782 [00:16<00:00, 46.74it/s] 98%|█████████▊| 763/782 [00:16<00:00, 46.77it/s] 98%|█████████▊| 768/782 [00:16<00:00, 46.70it/s] 99%|█████████▉| 773/782 [00:16<00:00, 46.73it/s] 99%|█████████▉| 778/782 [00:16<00:00, 46.79it/s]100%|██████████| 782/782 [00:16<00:00, 46.91it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:52:25,719 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   eval_loss               =     0.8851
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   eval_runtime            = 0:00:16.69
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   eval_samples_per_second =    374.605
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   eval_steps_per_second   =     46.848
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:52:25,719 >>   perplexity              =     2.4233
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:33,497 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:33,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:33,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:33,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:33,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:52:34,388 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:52:34,389 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:52:34,997 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:52:36,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:52:36,084 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:38,982 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:39,015 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:39,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:39,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:52:39,016 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:52:39,815 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:52:39,816 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:52:40,418 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:52:40,562 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:52:40,562 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.57it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:38,  1.54it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:46,  1.47it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:48,  1.48it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.49it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:52,  1.48it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.51it/s]Extractor Predicting: 83it [00:54,  1.49it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.49it/s]Extractor Predicting: 86it [00:57,  1.36it/s]Extractor Predicting: 87it [00:57,  1.41it/s]Extractor Predicting: 88it [00:58,  1.43it/s]Extractor Predicting: 89it [00:59,  1.45it/s]Extractor Predicting: 90it [00:59,  1.47it/s]Extractor Predicting: 91it [01:00,  1.46it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:01,  1.48it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:04,  1.46it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:05,  1.46it/s]Extractor Predicting: 100it [01:06,  1.48it/s]Extractor Predicting: 101it [01:07,  1.49it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:09,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.50it/s]Extractor Predicting: 107it [01:11,  1.46it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.49it/s]Extractor Predicting: 113it [01:15,  1.53it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.48it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:21,  1.54it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.49it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:25,  1.46it/s]Extractor Predicting: 129it [01:26,  1.47it/s]Extractor Predicting: 130it [01:26,  1.45it/s]Extractor Predicting: 131it [01:27,  1.45it/s]Extractor Predicting: 132it [01:28,  1.45it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:29,  1.45it/s]Extractor Predicting: 135it [01:30,  1.45it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:31,  1.46it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.50it/s]Extractor Predicting: 142it [01:34,  1.50it/s]Extractor Predicting: 143it [01:35,  1.47it/s]Extractor Predicting: 144it [01:36,  1.49it/s]Extractor Predicting: 145it [01:36,  1.48it/s]Extractor Predicting: 146it [01:37,  1.49it/s]Extractor Predicting: 147it [01:38,  1.48it/s]Extractor Predicting: 148it [01:38,  1.53it/s]Extractor Predicting: 149it [01:39,  1.52it/s]Extractor Predicting: 150it [01:40,  1.51it/s]Extractor Predicting: 151it [01:40,  1.48it/s]Extractor Predicting: 152it [01:41,  1.53it/s]Extractor Predicting: 153it [01:42,  1.51it/s]Extractor Predicting: 154it [01:42,  1.51it/s]Extractor Predicting: 155it [01:43,  1.52it/s]Extractor Predicting: 156it [01:44,  1.51it/s]Extractor Predicting: 157it [01:44,  1.54it/s]Extractor Predicting: 158it [01:45,  1.54it/s]Extractor Predicting: 159it [01:46,  1.55it/s]Extractor Predicting: 160it [01:46,  1.50it/s]Extractor Predicting: 161it [01:47,  1.49it/s]Extractor Predicting: 162it [01:48,  1.51it/s]Extractor Predicting: 163it [01:48,  1.53it/s]Extractor Predicting: 164it [01:49,  1.42it/s]Extractor Predicting: 165it [01:50,  1.48it/s]Extractor Predicting: 166it [01:50,  1.51it/s]Extractor Predicting: 167it [01:51,  1.52it/s]Extractor Predicting: 168it [01:52,  1.50it/s]Extractor Predicting: 169it [01:52,  1.53it/s]Extractor Predicting: 170it [01:53,  1.55it/s]Extractor Predicting: 171it [01:54,  1.54it/s]Extractor Predicting: 172it [01:54,  1.52it/s]Extractor Predicting: 173it [01:55,  1.55it/s]Extractor Predicting: 174it [01:56,  1.51it/s]Extractor Predicting: 175it [01:56,  1.53it/s]Extractor Predicting: 176it [01:57,  1.54it/s]Extractor Predicting: 177it [01:58,  1.54it/s]Extractor Predicting: 178it [01:58,  1.54it/s]Extractor Predicting: 179it [01:59,  1.55it/s]Extractor Predicting: 180it [01:59,  1.54it/s]Extractor Predicting: 181it [02:00,  1.56it/s]Extractor Predicting: 182it [02:01,  1.53it/s]Extractor Predicting: 183it [02:01,  1.55it/s]Extractor Predicting: 184it [02:02,  1.57it/s]Extractor Predicting: 185it [02:03,  1.55it/s]Extractor Predicting: 186it [02:03,  1.51it/s]Extractor Predicting: 187it [02:04,  1.56it/s]Extractor Predicting: 188it [02:05,  1.53it/s]Extractor Predicting: 189it [02:05,  1.54it/s]Extractor Predicting: 190it [02:06,  1.56it/s]Extractor Predicting: 191it [02:07,  1.58it/s]Extractor Predicting: 192it [02:07,  1.58it/s]Extractor Predicting: 193it [02:08,  1.56it/s]Extractor Predicting: 194it [02:08,  1.57it/s]Extractor Predicting: 195it [02:09,  1.57it/s]Extractor Predicting: 196it [02:10,  1.54it/s]Extractor Predicting: 197it [02:10,  1.57it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:12,  1.53it/s]Extractor Predicting: 200it [02:12,  1.59it/s]Extractor Predicting: 201it [02:13,  1.60it/s]Extractor Predicting: 202it [02:14,  1.61it/s]Extractor Predicting: 203it [02:14,  1.61it/s]Extractor Predicting: 204it [02:15,  1.60it/s]Extractor Predicting: 205it [02:15,  1.59it/s]Extractor Predicting: 206it [02:16,  1.61it/s]Extractor Predicting: 207it [02:17,  1.62it/s]Extractor Predicting: 208it [02:17,  1.61it/s]Extractor Predicting: 209it [02:18,  1.59it/s]Extractor Predicting: 210it [02:19,  1.59it/s]Extractor Predicting: 211it [02:19,  1.60it/s]Extractor Predicting: 212it [02:20,  1.60it/s]Extractor Predicting: 213it [02:20,  1.57it/s]Extractor Predicting: 214it [02:21,  1.59it/s]Extractor Predicting: 215it [02:22,  1.58it/s]Extractor Predicting: 216it [02:22,  1.57it/s]Extractor Predicting: 217it [02:23,  1.57it/s]Extractor Predicting: 218it [02:24,  1.58it/s]Extractor Predicting: 219it [02:24,  1.56it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:26,  1.57it/s]Extractor Predicting: 222it [02:26,  1.55it/s]Extractor Predicting: 223it [02:27,  1.56it/s]Extractor Predicting: 224it [02:27,  1.57it/s]Extractor Predicting: 225it [02:28,  1.56it/s]Extractor Predicting: 226it [02:29,  1.56it/s]Extractor Predicting: 227it [02:29,  1.57it/s]Extractor Predicting: 228it [02:30,  1.74it/s]Extractor Predicting: 228it [02:30,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:19,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:19,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:19,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:19,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:19,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:55:20,096 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:55:20,097 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:55:20,660 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:55:21,703 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:55:21,703 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:24,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:24,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:24,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:24,749 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:55:24,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:55:25,382 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:55:25,383 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:55:25,961 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:55:26,126 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:55:26,126 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.07355021216407355,
  "recall": 0.016632016632016633,
  "score": 0.027129255249771753,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:23,  1.45it/s]Extractor Predicting: 37it [00:24,  1.45it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.45it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:28,  1.45it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.44it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:31,  1.60it/s]Extractor Predicting: 48it [00:31,  1.61it/s]Extractor Predicting: 49it [00:32,  1.63it/s]Extractor Predicting: 50it [00:33,  1.59it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.40it/s]Extractor Predicting: 54it [00:36,  1.46it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.52it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:40,  1.57it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:42,  1.56it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.56it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.58it/s]Extractor Predicting: 71it [00:47,  1.59it/s]Extractor Predicting: 72it [00:47,  1.59it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:48,  1.56it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:50,  1.56it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.56it/s]Extractor Predicting: 86it [00:56,  1.57it/s]Extractor Predicting: 87it [00:57,  1.57it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:58,  1.58it/s]Extractor Predicting: 90it [00:59,  1.59it/s]Extractor Predicting: 91it [00:59,  1.55it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:01,  1.56it/s]Extractor Predicting: 95it [01:02,  1.57it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:03,  1.56it/s]Extractor Predicting: 98it [01:04,  1.56it/s]Extractor Predicting: 99it [01:05,  1.54it/s]Extractor Predicting: 100it [01:05,  1.55it/s]Extractor Predicting: 101it [01:06,  1.56it/s]Extractor Predicting: 102it [01:06,  1.56it/s]Extractor Predicting: 103it [01:07,  1.57it/s]Extractor Predicting: 104it [01:08,  1.53it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.56it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:12,  1.57it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:13,  1.59it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.56it/s]Extractor Predicting: 116it [01:15,  1.56it/s]Extractor Predicting: 117it [01:16,  1.60it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:17,  1.64it/s]Extractor Predicting: 120it [01:18,  1.65it/s]Extractor Predicting: 121it [01:18,  1.65it/s]Extractor Predicting: 122it [01:19,  1.66it/s]Extractor Predicting: 123it [01:20,  1.66it/s]Extractor Predicting: 124it [01:20,  1.66it/s]Extractor Predicting: 125it [01:21,  1.60it/s]Extractor Predicting: 126it [01:21,  1.63it/s]Extractor Predicting: 127it [01:22,  1.59it/s]Extractor Predicting: 128it [01:23,  1.55it/s]Extractor Predicting: 129it [01:23,  1.56it/s]Extractor Predicting: 130it [01:24,  1.60it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:25,  1.61it/s]Extractor Predicting: 133it [01:26,  1.60it/s]Extractor Predicting: 134it [01:27,  1.60it/s]Extractor Predicting: 135it [01:27,  1.61it/s]Extractor Predicting: 136it [01:28,  1.64it/s]Extractor Predicting: 137it [01:28,  1.60it/s]Extractor Predicting: 138it [01:29,  1.57it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:30,  1.60it/s]Extractor Predicting: 141it [01:31,  1.62it/s]Extractor Predicting: 142it [01:32,  1.65it/s]Extractor Predicting: 143it [01:32,  1.64it/s]Extractor Predicting: 144it [01:33,  1.66it/s]Extractor Predicting: 145it [01:33,  1.63it/s]Extractor Predicting: 146it [01:34,  1.62it/s]Extractor Predicting: 147it [01:35,  1.63it/s]Extractor Predicting: 148it [01:35,  1.64it/s]Extractor Predicting: 149it [01:36,  1.58it/s]Extractor Predicting: 150it [01:37,  1.53it/s]Extractor Predicting: 151it [01:37,  1.55it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:38,  1.59it/s]Extractor Predicting: 154it [01:39,  1.55it/s]Extractor Predicting: 155it [01:40,  1.50it/s]Extractor Predicting: 156it [01:40,  1.52it/s]Extractor Predicting: 157it [01:41,  1.50it/s]Extractor Predicting: 158it [01:42,  1.55it/s]Extractor Predicting: 159it [01:42,  1.55it/s]Extractor Predicting: 160it [01:43,  1.57it/s]Extractor Predicting: 161it [01:44,  1.59it/s]Extractor Predicting: 162it [01:44,  1.60it/s]Extractor Predicting: 163it [01:45,  1.58it/s]Extractor Predicting: 164it [01:45,  1.61it/s]Extractor Predicting: 165it [01:46,  1.58it/s]Extractor Predicting: 166it [01:47,  1.60it/s]Extractor Predicting: 167it [01:48,  1.44it/s]Extractor Predicting: 168it [01:48,  1.48it/s]Extractor Predicting: 169it [01:49,  1.52it/s]Extractor Predicting: 170it [01:49,  1.52it/s]Extractor Predicting: 171it [01:50,  1.52it/s]Extractor Predicting: 172it [01:51,  1.53it/s]Extractor Predicting: 173it [01:51,  1.52it/s]Extractor Predicting: 174it [01:52,  1.54it/s]Extractor Predicting: 175it [01:53,  1.54it/s]Extractor Predicting: 176it [01:53,  1.55it/s]Extractor Predicting: 177it [01:54,  1.53it/s]Extractor Predicting: 178it [01:55,  1.51it/s]Extractor Predicting: 179it [01:55,  1.53it/s]Extractor Predicting: 180it [01:56,  1.55it/s]Extractor Predicting: 181it [01:57,  1.58it/s]Extractor Predicting: 182it [01:57,  1.61it/s]Extractor Predicting: 183it [01:58,  1.60it/s]Extractor Predicting: 184it [01:58,  1.62it/s]Extractor Predicting: 185it [01:59,  1.62it/s]Extractor Predicting: 186it [02:00,  1.59it/s]Extractor Predicting: 187it [02:00,  1.62it/s]Extractor Predicting: 188it [02:01,  1.63it/s]Extractor Predicting: 189it [02:02,  1.57it/s]Extractor Predicting: 190it [02:02,  1.51it/s]Extractor Predicting: 191it [02:03,  1.53it/s]Extractor Predicting: 192it [02:04,  1.51it/s]Extractor Predicting: 193it [02:04,  1.51it/s]Extractor Predicting: 194it [02:05,  1.51it/s]Extractor Predicting: 195it [02:06,  1.49it/s]Extractor Predicting: 196it [02:06,  1.48it/s]Extractor Predicting: 197it [02:07,  1.49it/s]Extractor Predicting: 198it [02:08,  1.51it/s]Extractor Predicting: 199it [02:08,  1.49it/s]Extractor Predicting: 200it [02:09,  1.49it/s]Extractor Predicting: 201it [02:10,  1.49it/s]Extractor Predicting: 202it [02:10,  1.48it/s]Extractor Predicting: 203it [02:11,  1.49it/s]Extractor Predicting: 204it [02:12,  1.48it/s]Extractor Predicting: 205it [02:12,  1.47it/s]Extractor Predicting: 206it [02:13,  1.47it/s]Extractor Predicting: 207it [02:14,  1.43it/s]Extractor Predicting: 208it [02:14,  1.44it/s]Extractor Predicting: 209it [02:15,  1.49it/s]Extractor Predicting: 210it [02:16,  1.52it/s]Extractor Predicting: 211it [02:16,  1.52it/s]Extractor Predicting: 212it [02:17,  1.54it/s]Extractor Predicting: 213it [02:18,  1.52it/s]Extractor Predicting: 214it [02:18,  1.49it/s]Extractor Predicting: 215it [02:19,  1.50it/s]Extractor Predicting: 216it [02:20,  1.49it/s]Extractor Predicting: 217it [02:20,  1.46it/s]Extractor Predicting: 218it [02:21,  1.49it/s]Extractor Predicting: 219it [02:22,  1.50it/s]Extractor Predicting: 220it [02:22,  1.51it/s]Extractor Predicting: 221it [02:23,  1.53it/s]Extractor Predicting: 222it [02:24,  1.48it/s]Extractor Predicting: 223it [02:24,  1.46it/s]Extractor Predicting: 224it [02:25,  1.43it/s]Extractor Predicting: 225it [02:26,  1.42it/s]Extractor Predicting: 226it [02:27,  1.43it/s]Extractor Predicting: 227it [02:27,  1.41it/s]Extractor Predicting: 228it [02:28,  1.40it/s]Extractor Predicting: 229it [02:29,  1.39it/s]Extractor Predicting: 230it [02:29,  1.41it/s]Extractor Predicting: 231it [02:30,  1.41it/s]Extractor Predicting: 232it [02:31,  1.42it/s]Extractor Predicting: 233it [02:32,  1.42it/s]Extractor Predicting: 234it [02:32,  1.43it/s]Extractor Predicting: 235it [02:33,  1.44it/s]Extractor Predicting: 236it [02:34,  1.43it/s]Extractor Predicting: 237it [02:34,  1.40it/s]Extractor Predicting: 238it [02:35,  1.39it/s]Extractor Predicting: 239it [02:36,  1.42it/s]Extractor Predicting: 240it [02:36,  1.45it/s]Extractor Predicting: 241it [02:37,  1.46it/s]Extractor Predicting: 242it [02:38,  1.47it/s]Extractor Predicting: 243it [02:38,  1.48it/s]Extractor Predicting: 244it [02:39,  1.45it/s]Extractor Predicting: 245it [02:40,  1.50it/s]Extractor Predicting: 246it [02:40,  1.54it/s]Extractor Predicting: 246it [02:40,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:14,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:14,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:14,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:14,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:14,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:58:15,008 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:58:15,009 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:58:15,684 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:58:16,746 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:58:16,747 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:18,463 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:18,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:18,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:18,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:58:18,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:58:18,786 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:58:18,787 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:58:19,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:58:19,193 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:58:19,194 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.40064446831364126,
  "recall": 0.06322033898305085,
  "score": 0.10920802225150052,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.75it/s]Extractor Predicting: 15it [00:09,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:58:29,518 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:58:29,519 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:58:29,522 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:58:29,523 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:58:29,525 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:58:32,445 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:58:32,447 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:58:32,471 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:58:32,472 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:58:32,477 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:58:32,483 >> loading file outputs/wrapper/wiki/unseen_10_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42857142857142855,
  "recall": 0.025280898876404494,
  "score": 0.04774535809018567,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:58:32,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:33,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:34,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:34,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:35,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:36,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:37,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:37,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:38,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:39,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:40,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:40,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:41,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:42,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:43,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:43,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:44,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:45,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:46,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:46,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:47,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:48,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:49,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:57, 16.94s/it][WARNING|generation_utils.py:914] 2023-08-29 02:58:49,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:50,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:50,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:51,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:52,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:53,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:53,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:54,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:55,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:55,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:56,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:57,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:57,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:58,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:58,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:58:59,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:00,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:00,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:01,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:02,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:03,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:03,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:24, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:59:04,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:05,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:06,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:06,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:07,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:08,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:09,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:10,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:10,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:11,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:12,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:13,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:14,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:14,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:15,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:16,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:17,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:17,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:18,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:19,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:19,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:20,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:48<03:15, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-29 02:59:21,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:22,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:22,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:23,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:24,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:25,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:26,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:26,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:27,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:28,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:28,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:29,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:30,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:30,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:31,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:32,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:33,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:33,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:34,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:35,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:36,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:36,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:59, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-29 02:59:37,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:38,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:38,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:39,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:40,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:40,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:41,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:42,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:42,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:43,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:44,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:44,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:45,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:46,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:46,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:47,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:48,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:48,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:49,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:50,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:50,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:51,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:52,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:20<02:38, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-29 02:59:52,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:53,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:54,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:54,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:55,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:56,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:57,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:57,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:58,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:59,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:59:59,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:00,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:01,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:02,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:02,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:03,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:04,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:05,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:05,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:06,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:07,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:08,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:36<02:23, 15.99s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:09,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:09,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:10,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:10,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:11,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:12,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:12,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:13,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:14,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:14,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:15,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:16,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:16,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:17,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:17,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:18,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:19,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:19,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:20,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:21,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:21,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:22,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:22,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:50<02:04, 15.50s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:23,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:24,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:25,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:25,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:26,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:27,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:27,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:29,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:30,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:30,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:31,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:32,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:33,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:33,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:34,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:35,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:35,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:36,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:37,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:37,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:05<01:47, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:38,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:39,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:40,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:40,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:41,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:42,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:43,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:43,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:44,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:45,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:46,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:47,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:47,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:48,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:49,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:49,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:50,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:51,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:52,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:52,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:53,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:54,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:55,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:55,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:56,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:38, 16.40s/it][WARNING|generation_utils.py:914] 2023-08-29 03:00:57,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:58,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:58,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:00:59,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:00,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:01,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:02,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:02,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:03,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:04,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:05,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:05,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:06,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:07,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:08,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:09,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:09,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:10,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:11,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:12,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:12,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:13,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:14,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:42<01:24, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:15,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:16,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:16,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:17,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:18,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:19,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:19,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:20,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:21,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:22,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:22,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:23,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:24,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:25,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:25,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:26,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:27,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:28,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:29,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:29,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:30,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:31,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:32,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:32,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:01<01:09, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:33,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:34,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:35,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:35,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:36,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:37,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:37,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:38,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:39,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:39,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:40,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:41,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:41,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:42,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:43,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:43,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:44,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:45,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:45,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:46,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:47,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:48,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:15<00:49, 16.63s/it][WARNING|generation_utils.py:914] 2023-08-29 03:01:48,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:49,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:50,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:50,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:51,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:52,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:53,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:53,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:54,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:55,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:55,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:56,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:57,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:58,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:58,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:01:59,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:00,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:00,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:01,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:02,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:02,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:03,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:04,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:32<00:33, 16.56s/it][WARNING|generation_utils.py:914] 2023-08-29 03:02:05,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:06,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:06,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:07,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:08,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:08,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:09,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:10,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:11,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:11,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:12,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:13,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:14,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:15,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:15,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:16,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:17,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:18,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:19,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:19,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:20,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:21,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:22,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:50<00:16, 16.98s/it][WARNING|generation_utils.py:914] 2023-08-29 03:02:23,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:23,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:24,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:25,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:25,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:26,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:27,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:28,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:28,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:29,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:30,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:31,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:31,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:32,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:33,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:34,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:35,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:36,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:36,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:37,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:38,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:38,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:39,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:40,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:40,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:02:42,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:10<00:00, 17.83s/it]Generating: 100%|██████████| 15/15 [04:10<00:00, 16.68s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:49,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:49,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:49,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:49,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:49,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:02:49,949 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:02:49,950 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:02:50,528 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:02:51,608 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:02:51,608 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:54,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:54,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:54,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:54,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:02:54,504 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:02:55,123 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:02:55,124 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:02:55,686 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:02:55,851 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:02:55,851 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Republican Union', 'member of', '', 'His only public act of protest was for the formation of the National Unity Committee and for his participation in the rally organised by the Republican Union .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.8934659090909091, 'errors': {''}}
['Relation : notable work . Context : Later in Life he studied at the Conservatory of Fine Arts at Princeton University studying at the Renaissance series of architecture . Head Entity : Renaissance , Tail Entity : the works of Alexander Berenstain .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8579545454545454, 'errors': {''}}
['Relation : successful candidate . Context : Later in 2008 , he won the seat vacated by incumbent Republican Bob McDonnell , who lost to Republican John B. Sullivan of Hampton Court in the Virginia House of Representatives . Head Entity : Robert McDonnell , Tail Entity : Republican .\n']
['Relation : successful candidate . Context : Later in 2008 , he won the seat vacated by incumbent Republican Bob McDonnell , who lost to Republican John B. Sullivan of Hampton Court in the Virginia House of Representatives . Head Entity : Robert McDonnell , Tail Entity : Republican .\n', 'Relation : successful candidate . Context : After he was elected to serve as Chancellor , he was chosen to become the first Chancellor of South Australia to be appointed by the Premier . Head Entity : Premier , Tail Entity : President of South Australia .\n']
['Relation : successful candidate . Context : Later in 2008 , he won the seat vacated by incumbent Republican Bob McDonnell , who lost to Republican John B. Sullivan of Hampton Court in the Virginia House of Representatives . Head Entity : Robert McDonnell , Tail Entity : Republican .\n', 'Relation : successful candidate . Context : After he was elected to serve as Chancellor , he was chosen to become the first Chancellor of South Australia to be appointed by the Premier . Head Entity : Premier , Tail Entity : President of South Australia .\n', 'Relation : successful candidate . Context : This was the first successful test of the electoral theory in French politics , following the death of John Diefenbaker in 1880 . Head Entity : John Diefenbaker , Tail Entity : unsuccessful candidate .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.8247282608695652, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : director .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.8206521739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9122023809523809, 'errors': {''}}
['Relation : main subject . Context : Later in Life he studied at the Conservatory of Fine Arts at Loyola University in Rome and had two PhD s at Columbia University in New York City , where he collaborated on the studies of Johann Wolfgang von Goethe . Head Entity : Johann Wolfgang von Goethe , Tail Entity : the study .\n']
['Relation : main subject . Context : Later in Life he studied at the Conservatory of Fine Arts at Loyola University in Rome and had two PhD s at Columbia University in New York City , where he collaborated on the studies of Johann Wolfgang von Goethe . Head Entity : Johann Wolfgang von Goethe , Tail Entity : the study .\n', 'Relation : main subject . Context : After he had recovered from pneumonia and was offered a job at the American Academy of Music , he decided to sing with the choir . Head Entity : American Academy of Music , Tail Entity : musical .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 288, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 383, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 518, 'raw': 672}
{'target': 600, 'success': 544, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.77375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7994791666666666, 'errors': {'', "('James Dreyfus', 'occupant', '', 'The first to become a politician was James Dreyfus who worked for the British government and became President of England .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8165760869565217, 'errors': {'', "('Andean', 'use', '', 'This new breed of herbivorous birds is commonly seen in tropical South America , particularly in Andean and Pacific Northwest .')"}}
['Relation : voice type . Context : Following his graduation in 1957 , she worked for NBC television as a correspondent in the morning talk show talk show , The Daily Show . Head Entity : The Daily Show , Tail Entity : the voice .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 269, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 314, 'raw': 448}
{'target': 600, 'success': 338, 'raw': 480}
{'target': 600, 'success': 364, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 478, 'raw': 672}
{'target': 600, 'success': 498, 'raw': 704}
{'target': 600, 'success': 522, 'raw': 736}
{'target': 600, 'success': 547, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 588, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : voice type .', 'success_rate': 0.7083333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 12716
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12816, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:02,  1.52it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:06,  1.42it/s]Extractor Estimating: 10it [00:06,  1.47it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:07,  1.53it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:09,  1.57it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:11,  1.56it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.54it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:14,  1.58it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:16,  1.62it/s]Extractor Estimating: 26it [00:16,  1.64it/s]Extractor Estimating: 27it [00:17,  1.64it/s]Extractor Estimating: 28it [00:17,  1.64it/s]Extractor Estimating: 29it [00:18,  1.63it/s]Extractor Estimating: 30it [00:19,  1.65it/s]Extractor Estimating: 31it [00:19,  1.68it/s]Extractor Estimating: 32it [00:20,  1.68it/s]Extractor Estimating: 33it [00:20,  1.65it/s]Extractor Estimating: 34it [00:21,  1.63it/s]Extractor Estimating: 35it [00:22,  1.68it/s]Extractor Estimating: 36it [00:22,  1.66it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:23,  1.67it/s]Extractor Estimating: 39it [00:24,  1.69it/s]Extractor Estimating: 40it [00:25,  1.71it/s]Extractor Estimating: 41it [00:25,  1.71it/s]Extractor Estimating: 42it [00:26,  1.71it/s]Extractor Estimating: 43it [00:26,  1.68it/s]Extractor Estimating: 44it [00:27,  1.64it/s]Extractor Estimating: 45it [00:28,  1.66it/s]Extractor Estimating: 46it [00:28,  1.61it/s]Extractor Estimating: 47it [00:29,  1.64it/s]Extractor Estimating: 48it [00:29,  1.62it/s]Extractor Estimating: 49it [00:30,  1.56it/s]Extractor Estimating: 50it [00:31,  1.58it/s]Extractor Estimating: 51it [00:31,  1.58it/s]Extractor Estimating: 52it [00:32,  1.57it/s]Extractor Estimating: 53it [00:33,  1.55it/s]Extractor Estimating: 54it [00:33,  1.55it/s]Extractor Estimating: 55it [00:34,  1.57it/s]Extractor Estimating: 56it [00:35,  1.54it/s]Extractor Estimating: 57it [00:35,  1.56it/s]Extractor Estimating: 58it [00:36,  1.52it/s]Extractor Estimating: 59it [00:37,  1.54it/s]Extractor Estimating: 60it [00:37,  1.48it/s]Extractor Estimating: 61it [00:38,  1.49it/s]Extractor Estimating: 62it [00:39,  1.52it/s]Extractor Estimating: 63it [00:39,  1.51it/s]Extractor Estimating: 64it [00:40,  1.52it/s]Extractor Estimating: 65it [00:41,  1.56it/s]Extractor Estimating: 66it [00:41,  1.52it/s]Extractor Estimating: 67it [00:42,  1.52it/s]Extractor Estimating: 68it [00:43,  1.51it/s]Extractor Estimating: 69it [00:43,  1.52it/s]Extractor Estimating: 70it [00:44,  1.54it/s]Extractor Estimating: 71it [00:45,  1.56it/s]Extractor Estimating: 72it [00:45,  1.53it/s]Extractor Estimating: 73it [00:46,  1.55it/s]Extractor Estimating: 74it [00:46,  1.53it/s]Extractor Estimating: 75it [00:47,  1.46it/s]Extractor Estimating: 76it [00:48,  1.40it/s]Extractor Estimating: 77it [00:49,  1.48it/s]Extractor Estimating: 78it [00:49,  1.50it/s]Extractor Estimating: 79it [00:50,  1.53it/s]Extractor Estimating: 80it [00:50,  1.56it/s]Extractor Estimating: 81it [00:51,  1.50it/s]Extractor Estimating: 82it [00:52,  1.51it/s]Extractor Estimating: 83it [00:52,  1.56it/s]Extractor Estimating: 84it [00:53,  1.53it/s]Extractor Estimating: 85it [00:54,  1.56it/s]Extractor Estimating: 86it [00:54,  1.60it/s]Extractor Estimating: 87it [00:55,  1.59it/s]Extractor Estimating: 88it [00:56,  1.60it/s]Extractor Estimating: 89it [00:56,  1.59it/s]Extractor Estimating: 90it [00:57,  1.58it/s]Extractor Estimating: 91it [00:57,  1.60it/s]Extractor Estimating: 92it [00:58,  1.61it/s]Extractor Estimating: 93it [00:59,  1.65it/s]Extractor Estimating: 94it [00:59,  1.64it/s]Extractor Estimating: 95it [01:00,  1.65it/s]Extractor Estimating: 96it [01:01,  1.64it/s]Extractor Estimating: 97it [01:01,  1.66it/s]Extractor Estimating: 98it [01:02,  1.66it/s]Extractor Estimating: 99it [01:02,  1.63it/s]Extractor Estimating: 100it [01:03,  1.61it/s]Extractor Estimating: 101it [01:04,  1.67it/s]Extractor Estimating: 102it [01:04,  1.68it/s]Extractor Estimating: 103it [01:05,  1.70it/s]Extractor Estimating: 104it [01:05,  1.70it/s]Extractor Estimating: 105it [01:06,  1.71it/s]Extractor Estimating: 106it [01:06,  1.73it/s]Extractor Estimating: 107it [01:07,  1.77it/s]Extractor Estimating: 108it [01:08,  1.76it/s]Extractor Estimating: 109it [01:08,  1.73it/s]Extractor Estimating: 110it [01:09,  1.75it/s]Extractor Estimating: 111it [01:09,  1.79it/s]Extractor Estimating: 112it [01:10,  1.78it/s]Extractor Estimating: 113it [01:10,  1.78it/s]Extractor Estimating: 114it [01:11,  1.76it/s]Extractor Estimating: 115it [01:12,  1.76it/s]Extractor Estimating: 116it [01:12,  1.71it/s]Extractor Estimating: 117it [01:13,  1.69it/s]Extractor Estimating: 118it [01:13,  1.73it/s]Extractor Estimating: 119it [01:14,  1.69it/s]Extractor Estimating: 120it [01:15,  1.61it/s]Extractor Estimating: 121it [01:15,  1.63it/s]Extractor Estimating: 122it [01:16,  1.63it/s]Extractor Estimating: 123it [01:16,  1.65it/s]Extractor Estimating: 124it [01:17,  1.68it/s]Extractor Estimating: 125it [01:18,  1.72it/s]Extractor Estimating: 126it [01:18,  1.70it/s]Extractor Estimating: 127it [01:19,  1.69it/s]Extractor Estimating: 128it [01:19,  1.67it/s]Extractor Estimating: 129it [01:20,  1.66it/s]Extractor Estimating: 130it [01:21,  1.63it/s]Extractor Estimating: 131it [01:21,  1.63it/s]Extractor Estimating: 132it [01:22,  1.57it/s]Extractor Estimating: 133it [01:23,  1.58it/s]Extractor Estimating: 134it [01:23,  1.61it/s]Extractor Estimating: 135it [01:24,  1.44it/s]Extractor Estimating: 136it [01:25,  1.48it/s]Extractor Estimating: 137it [01:25,  1.51it/s]Extractor Estimating: 138it [01:26,  1.53it/s]Extractor Estimating: 139it [01:27,  1.48it/s]Extractor Estimating: 140it [01:27,  1.49it/s]Extractor Estimating: 141it [01:28,  1.55it/s]Extractor Estimating: 142it [01:29,  1.51it/s]Extractor Estimating: 143it [01:29,  1.52it/s]Extractor Estimating: 144it [01:30,  1.56it/s]Extractor Estimating: 145it [01:30,  1.54it/s]Extractor Estimating: 146it [01:31,  1.53it/s]Extractor Estimating: 147it [01:32,  1.52it/s]Extractor Estimating: 148it [01:33,  1.45it/s]Extractor Estimating: 149it [01:33,  1.52it/s]Extractor Estimating: 150it [01:34,  1.55it/s]Extractor Estimating: 151it [01:34,  1.59it/s]Extractor Estimating: 152it [01:35,  1.60it/s]Extractor Estimating: 153it [01:36,  1.61it/s]Extractor Estimating: 154it [01:36,  1.60it/s]Extractor Estimating: 155it [01:37,  1.61it/s]Extractor Estimating: 156it [01:37,  1.57it/s]Extractor Estimating: 157it [01:38,  1.59it/s]Extractor Estimating: 158it [01:39,  1.61it/s]Extractor Estimating: 159it [01:39,  1.60it/s]Extractor Estimating: 160it [01:40,  1.60it/s]Extractor Estimating: 161it [01:41,  1.63it/s]Extractor Estimating: 162it [01:41,  1.61it/s]Extractor Estimating: 163it [01:42,  1.63it/s]Extractor Estimating: 164it [01:42,  1.66it/s]Extractor Estimating: 165it [01:43,  1.59it/s]Extractor Estimating: 166it [01:44,  1.61it/s]Extractor Estimating: 167it [01:44,  1.66it/s]Extractor Estimating: 168it [01:45,  1.64it/s]Extractor Estimating: 169it [01:45,  1.67it/s]Extractor Estimating: 170it [01:46,  1.66it/s]Extractor Estimating: 171it [01:47,  1.67it/s]Extractor Estimating: 172it [01:47,  1.67it/s]Extractor Estimating: 173it [01:48,  1.67it/s]Extractor Estimating: 174it [01:48,  1.62it/s]Extractor Estimating: 175it [01:49,  1.57it/s]Extractor Estimating: 176it [01:50,  1.52it/s]Extractor Estimating: 177it [01:50,  1.53it/s]Extractor Estimating: 178it [01:51,  1.55it/s]Extractor Estimating: 179it [01:52,  1.58it/s]Extractor Estimating: 180it [01:52,  1.57it/s]Extractor Estimating: 181it [01:53,  1.59it/s]Extractor Estimating: 182it [01:54,  1.59it/s]Extractor Estimating: 183it [01:54,  1.56it/s]Extractor Estimating: 184it [01:55,  1.47it/s]Extractor Estimating: 185it [01:56,  1.38it/s]Extractor Estimating: 186it [01:57,  1.41it/s]Extractor Estimating: 187it [01:57,  1.47it/s]Extractor Estimating: 188it [01:58,  1.48it/s]Extractor Estimating: 189it [01:58,  1.51it/s]Extractor Estimating: 190it [01:59,  1.52it/s]Extractor Estimating: 191it [02:00,  1.57it/s]Extractor Estimating: 192it [02:00,  1.55it/s]Extractor Estimating: 193it [02:01,  1.56it/s]Extractor Estimating: 194it [02:02,  1.62it/s]Extractor Estimating: 195it [02:02,  1.56it/s]Extractor Estimating: 196it [02:03,  1.55it/s]Extractor Estimating: 197it [02:04,  1.50it/s]Extractor Estimating: 198it [02:04,  1.52it/s]Extractor Estimating: 199it [02:05,  1.56it/s]Extractor Estimating: 200it [02:06,  1.55it/s]Extractor Estimating: 201it [02:06,  1.41it/s]Extractor Estimating: 202it [02:07,  1.46it/s]Extractor Estimating: 203it [02:08,  1.48it/s]Extractor Estimating: 204it [02:08,  1.49it/s]Extractor Estimating: 205it [02:09,  1.52it/s]Extractor Estimating: 206it [02:10,  1.54it/s]Extractor Estimating: 207it [02:10,  1.56it/s]Extractor Estimating: 208it [02:11,  1.50it/s]Extractor Estimating: 209it [02:12,  1.52it/s]Extractor Estimating: 210it [02:12,  1.51it/s]Extractor Estimating: 211it [02:13,  1.54it/s]Extractor Estimating: 212it [02:13,  1.55it/s]Extractor Estimating: 213it [02:14,  1.55it/s]Extractor Estimating: 214it [02:15,  1.55it/s]Extractor Estimating: 215it [02:15,  1.59it/s]Extractor Estimating: 216it [02:16,  1.57it/s]Extractor Estimating: 217it [02:17,  1.56it/s]Extractor Estimating: 218it [02:17,  1.59it/s]Extractor Estimating: 219it [02:18,  1.57it/s]Extractor Estimating: 220it [02:19,  1.59it/s]Extractor Estimating: 221it [02:19,  1.54it/s]Extractor Estimating: 222it [02:20,  1.52it/s]Extractor Estimating: 223it [02:21,  1.51it/s]Extractor Estimating: 224it [02:21,  1.53it/s]Extractor Estimating: 225it [02:22,  1.51it/s]Extractor Estimating: 226it [02:23,  1.54it/s]Extractor Estimating: 227it [02:23,  1.54it/s]Extractor Estimating: 228it [02:24,  1.54it/s]Extractor Estimating: 229it [02:25,  1.49it/s]Extractor Estimating: 230it [02:25,  1.50it/s]Extractor Estimating: 231it [02:26,  1.52it/s]Extractor Estimating: 232it [02:26,  1.52it/s]Extractor Estimating: 233it [02:27,  1.51it/s]Extractor Estimating: 234it [02:28,  1.50it/s]Extractor Estimating: 235it [02:29,  1.50it/s]Extractor Estimating: 236it [02:29,  1.53it/s]Extractor Estimating: 237it [02:30,  1.52it/s]Extractor Estimating: 238it [02:31,  1.48it/s]Extractor Estimating: 239it [02:31,  1.52it/s]Extractor Estimating: 240it [02:32,  1.51it/s]Extractor Estimating: 241it [02:33,  1.44it/s]Extractor Estimating: 242it [02:33,  1.46it/s]Extractor Estimating: 243it [02:34,  1.49it/s]Extractor Estimating: 244it [02:35,  1.52it/s]Extractor Estimating: 245it [02:35,  1.50it/s]Extractor Estimating: 246it [02:36,  1.50it/s]Extractor Estimating: 247it [02:37,  1.48it/s]Extractor Estimating: 248it [02:37,  1.48it/s]Extractor Estimating: 249it [02:38,  1.52it/s]Extractor Estimating: 250it [02:39,  1.46it/s]Extractor Estimating: 251it [02:39,  1.44it/s]Extractor Estimating: 252it [02:40,  1.46it/s]Extractor Estimating: 253it [02:41,  1.50it/s]Extractor Estimating: 254it [02:41,  1.49it/s]Extractor Estimating: 255it [02:42,  1.47it/s]Extractor Estimating: 256it [02:43,  1.52it/s]Extractor Estimating: 257it [02:43,  1.52it/s]Extractor Estimating: 258it [02:44,  1.51it/s]Extractor Estimating: 259it [02:45,  1.51it/s]Extractor Estimating: 260it [02:45,  1.53it/s]Extractor Estimating: 261it [02:46,  1.50it/s]Extractor Estimating: 262it [02:47,  1.48it/s]Extractor Estimating: 263it [02:47,  1.45it/s]Extractor Estimating: 264it [02:48,  1.46it/s]Extractor Estimating: 265it [02:49,  1.50it/s]Extractor Estimating: 266it [02:49,  1.54it/s]Extractor Estimating: 267it [02:50,  1.51it/s]Extractor Estimating: 268it [02:51,  1.53it/s]Extractor Estimating: 269it [02:51,  1.50it/s]Extractor Estimating: 270it [02:52,  1.51it/s]Extractor Estimating: 271it [02:53,  1.53it/s]Extractor Estimating: 272it [02:53,  1.55it/s]Extractor Estimating: 273it [02:54,  1.55it/s]Extractor Estimating: 274it [02:54,  1.55it/s]Extractor Estimating: 275it [02:55,  1.49it/s]Extractor Estimating: 276it [02:56,  1.52it/s]Extractor Estimating: 277it [02:56,  1.55it/s]Extractor Estimating: 278it [02:57,  1.57it/s]Extractor Estimating: 279it [02:58,  1.62it/s]Extractor Estimating: 280it [02:58,  1.62it/s]Extractor Estimating: 281it [02:59,  1.67it/s]Extractor Estimating: 282it [02:59,  1.64it/s]Extractor Estimating: 283it [03:00,  1.68it/s]Extractor Estimating: 284it [03:01,  1.54it/s]Extractor Estimating: 285it [03:01,  1.60it/s]Extractor Estimating: 286it [03:02,  1.59it/s]Extractor Estimating: 287it [03:03,  1.63it/s]Extractor Estimating: 288it [03:03,  1.61it/s]Extractor Estimating: 289it [03:04,  1.59it/s]Extractor Estimating: 290it [03:04,  1.64it/s]Extractor Estimating: 291it [03:05,  1.69it/s]Extractor Estimating: 292it [03:06,  1.70it/s]Extractor Estimating: 293it [03:06,  1.72it/s]Extractor Estimating: 294it [03:07,  1.67it/s]Extractor Estimating: 295it [03:07,  1.60it/s]Extractor Estimating: 296it [03:08,  1.65it/s]Extractor Estimating: 297it [03:09,  1.64it/s]Extractor Estimating: 298it [03:09,  1.66it/s]Extractor Estimating: 299it [03:10,  1.68it/s]Extractor Estimating: 300it [03:10,  1.63it/s]Extractor Estimating: 301it [03:11,  1.63it/s]Extractor Estimating: 302it [03:12,  1.63it/s]Extractor Estimating: 303it [03:12,  1.64it/s]Extractor Estimating: 304it [03:13,  1.62it/s]Extractor Estimating: 305it [03:13,  1.66it/s]Extractor Estimating: 306it [03:14,  1.61it/s]Extractor Estimating: 307it [03:15,  1.62it/s]Extractor Estimating: 308it [03:15,  1.59it/s]Extractor Estimating: 309it [03:16,  1.60it/s]Extractor Estimating: 310it [03:17,  1.61it/s]Extractor Estimating: 311it [03:17,  1.59it/s]Extractor Estimating: 312it [03:18,  1.57it/s]Extractor Estimating: 313it [03:18,  1.62it/s]Extractor Estimating: 314it [03:19,  1.60it/s]Extractor Estimating: 315it [03:20,  1.56it/s]Extractor Estimating: 316it [03:20,  1.58it/s]Extractor Estimating: 317it [03:21,  1.59it/s]Extractor Estimating: 318it [03:22,  1.61it/s]Extractor Estimating: 319it [03:22,  1.60it/s]Extractor Estimating: 320it [03:23,  1.60it/s]Extractor Estimating: 321it [03:24,  1.57it/s]Extractor Estimating: 322it [03:24,  1.61it/s]Extractor Estimating: 323it [03:25,  1.63it/s]Extractor Estimating: 324it [03:25,  1.60it/s]Extractor Estimating: 325it [03:26,  1.51it/s]Extractor Estimating: 326it [03:27,  1.43it/s]Extractor Estimating: 327it [03:28,  1.48it/s]Extractor Estimating: 328it [03:28,  1.51it/s]Extractor Estimating: 329it [03:29,  1.51it/s]Extractor Estimating: 330it [03:29,  1.55it/s]Extractor Estimating: 331it [03:30,  1.57it/s]Extractor Estimating: 332it [03:31,  1.54it/s]Extractor Estimating: 333it [03:31,  1.49it/s]Extractor Estimating: 334it [03:32,  1.51it/s]Extractor Estimating: 335it [03:33,  1.54it/s]Extractor Estimating: 336it [03:33,  1.50it/s]Extractor Estimating: 337it [03:34,  1.55it/s]Extractor Estimating: 338it [03:35,  1.43it/s]Extractor Estimating: 339it [03:35,  1.47it/s]Extractor Estimating: 340it [03:36,  1.51it/s]Extractor Estimating: 341it [03:37,  1.47it/s]Extractor Estimating: 342it [03:38,  1.45it/s]Extractor Estimating: 343it [03:38,  1.51it/s]Extractor Estimating: 344it [03:39,  1.51it/s]Extractor Estimating: 345it [03:39,  1.50it/s]Extractor Estimating: 346it [03:40,  1.55it/s]Extractor Estimating: 347it [03:41,  1.61it/s]Extractor Estimating: 348it [03:41,  1.58it/s]Extractor Estimating: 349it [03:42,  1.46it/s]Extractor Estimating: 350it [03:43,  1.51it/s]Extractor Estimating: 351it [03:43,  1.49it/s]Extractor Estimating: 352it [03:44,  1.54it/s]Extractor Estimating: 353it [03:45,  1.55it/s]Extractor Estimating: 354it [03:45,  1.58it/s]Extractor Estimating: 355it [03:46,  1.61it/s]Extractor Estimating: 356it [03:46,  1.66it/s]Extractor Estimating: 357it [03:47,  1.64it/s]Extractor Estimating: 358it [03:48,  1.57it/s]Extractor Estimating: 359it [03:48,  1.58it/s]Extractor Estimating: 360it [03:49,  1.52it/s]Extractor Estimating: 361it [03:50,  1.54it/s]Extractor Estimating: 362it [03:50,  1.54it/s]Extractor Estimating: 363it [03:51,  1.36it/s]Extractor Estimating: 364it [03:52,  1.42it/s]Extractor Estimating: 365it [03:53,  1.41it/s]Extractor Estimating: 366it [03:53,  1.48it/s]Extractor Estimating: 367it [03:54,  1.50it/s]Extractor Estimating: 368it [03:54,  1.55it/s]Extractor Estimating: 369it [03:55,  1.64it/s]Extractor Estimating: 370it [03:56,  1.67it/s]Extractor Estimating: 371it [03:56,  1.66it/s]Extractor Estimating: 372it [03:57,  1.66it/s]Extractor Estimating: 373it [03:57,  1.68it/s]Extractor Estimating: 374it [03:58,  1.69it/s]Extractor Estimating: 375it [03:58,  1.89it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:14,457 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:14,460 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:14,460 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:14,460 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:14,460 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:07:15,078 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:07:15,079 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:07:15,689 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:07:16,779 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:07:16,780 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:19,697 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:19,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:19,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:19,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:07:19,701 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:07:20,331 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:07:20,332 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:07:20,894 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:07:21,057 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:07:21,058 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:41:03,233 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:41:03,236 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7427 mean pseudo reward: 0.9556218586409095
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 26795
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26895, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26895, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.061, loss:724.1982
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.097, loss:681.3653
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.077, loss:651.8993
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 90, avg_time 1.070, loss:634.0126
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 190, avg_time 1.073, loss:650.9313
>> valid entity prec:0.4960, rec:0.4658, f1:0.4805
>> valid relation prec:0.0101, rec:0.0040, f1:0.0057
>> valid relation with NER prec:0.0101, rec:0.0040, f1:0.0057
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 290, avg_time 3.231, loss:644.7543
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 80, avg_time 1.075, loss:603.4706
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 180, avg_time 1.076, loss:635.3232
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 280, avg_time 1.090, loss:663.2529
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 70, avg_time 1.078, loss:626.0304
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4913, rec:0.4998, f1:0.4955
>> valid relation prec:0.0165, rec:0.0077, f1:0.0105
>> valid relation with NER prec:0.0165, rec:0.0077, f1:0.0105
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 170, avg_time 3.226, loss:647.7631
g_step 1200, step 270, avg_time 1.072, loss:672.4344
g_step 1300, step 60, avg_time 1.068, loss:623.7361
g_step 1400, step 160, avg_time 1.089, loss:599.4574
g_step 1500, step 260, avg_time 1.075, loss:642.2594
>> valid entity prec:0.5189, rec:0.4621, f1:0.4888
>> valid relation prec:0.0184, rec:0.0062, f1:0.0093
>> valid relation with NER prec:0.0184, rec:0.0062, f1:0.0093
g_step 1600, step 50, avg_time 3.217, loss:634.9724
g_step 1700, step 150, avg_time 1.080, loss:596.6102
g_step 1800, step 250, avg_time 1.073, loss:609.3640
g_step 1900, step 40, avg_time 1.084, loss:574.0972
g_step 2000, step 140, avg_time 1.071, loss:581.8218
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5107, rec:0.4759, f1:0.4927
>> valid relation prec:0.0058, rec:0.0022, f1:0.0032
>> valid relation with NER prec:0.0058, rec:0.0022, f1:0.0032
g_step 2100, step 240, avg_time 3.237, loss:589.3657
g_step 2200, step 30, avg_time 1.063, loss:567.2572
g_step 2300, step 130, avg_time 1.069, loss:532.4250
g_step 2400, step 230, avg_time 1.073, loss:554.5716
g_step 2500, step 20, avg_time 1.078, loss:571.4239
>> valid entity prec:0.5275, rec:0.4507, f1:0.4861
>> valid relation prec:0.0051, rec:0.0019, f1:0.0028
>> valid relation with NER prec:0.0051, rec:0.0019, f1:0.0028
g_step 2600, step 120, avg_time 3.236, loss:533.3239
g_step 2700, step 220, avg_time 1.067, loss:524.5712
g_step 2800, step 10, avg_time 1.075, loss:549.0517
g_step 2900, step 110, avg_time 1.075, loss:505.8392
g_step 3000, step 210, avg_time 1.073, loss:523.9788
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5016, rec:0.4418, f1:0.4698
>> valid relation prec:0.0058, rec:0.0022, f1:0.0032
>> valid relation with NER prec:0.0058, rec:0.0022, f1:0.0032
g_step 3100, step 310, avg_time 3.208, loss:536.5478
g_step 3200, step 100, avg_time 1.087, loss:470.5627
g_step 3300, step 200, avg_time 1.079, loss:500.6425
g_step 3400, step 300, avg_time 1.062, loss:530.6919
g_step 3500, step 90, avg_time 1.077, loss:476.6998
>> valid entity prec:0.5119, rec:0.4780, f1:0.4943
>> valid relation prec:0.0099, rec:0.0035, f1:0.0052
>> valid relation with NER prec:0.0099, rec:0.0035, f1:0.0052
g_step 3600, step 190, avg_time 3.217, loss:505.6543
g_step 3700, step 290, avg_time 1.065, loss:503.9757
g_step 3800, step 80, avg_time 1.062, loss:458.7369
g_step 3900, step 180, avg_time 1.083, loss:477.4627
g_step 4000, step 280, avg_time 1.066, loss:465.2071
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4784, rec:0.4913, f1:0.4848
>> valid relation prec:0.0081, rec:0.0032, f1:0.0046
>> valid relation with NER prec:0.0081, rec:0.0032, f1:0.0046
g_step 4100, step 70, avg_time 3.224, loss:458.8631
g_step 4200, step 170, avg_time 1.061, loss:447.3873
g_step 4300, step 270, avg_time 1.065, loss:468.5497
g_step 4400, step 60, avg_time 1.068, loss:440.2009
g_step 4500, step 160, avg_time 1.062, loss:427.7612
>> valid entity prec:0.4932, rec:0.4243, f1:0.4561
>> valid relation prec:0.0106, rec:0.0035, f1:0.0053
>> valid relation with NER prec:0.0106, rec:0.0035, f1:0.0053
g_step 4600, step 260, avg_time 3.204, loss:446.2456
g_step 4700, step 50, avg_time 1.057, loss:426.7146
g_step 4800, step 150, avg_time 1.072, loss:416.0239
g_step 4900, step 250, avg_time 1.070, loss:435.8472
g_step 5000, step 40, avg_time 1.060, loss:418.8377
learning rate was adjusted to 0.0008
>> valid entity prec:0.5218, rec:0.4260, f1:0.4690
>> valid relation prec:0.0071, rec:0.0030, f1:0.0043
>> valid relation with NER prec:0.0071, rec:0.0030, f1:0.0043
g_step 5100, step 140, avg_time 3.205, loss:419.2965
g_step 5200, step 240, avg_time 1.062, loss:402.1711
g_step 5300, step 30, avg_time 1.060, loss:414.3281
g_step 5400, step 130, avg_time 1.071, loss:375.2261
g_step 5500, step 230, avg_time 1.070, loss:404.9300
>> valid entity prec:0.5095, rec:0.4495, f1:0.4776
>> valid relation prec:0.0049, rec:0.0021, f1:0.0029
>> valid relation with NER prec:0.0049, rec:0.0021, f1:0.0029
g_step 5600, step 20, avg_time 3.200, loss:401.9911
g_step 5700, step 120, avg_time 1.060, loss:378.0538
g_step 5800, step 220, avg_time 1.072, loss:396.9057
g_step 5900, step 10, avg_time 1.069, loss:390.1391
g_step 6000, step 110, avg_time 1.077, loss:374.8354
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4981, rec:0.4363, f1:0.4651
>> valid relation prec:0.0156, rec:0.0064, f1:0.0091
>> valid relation with NER prec:0.0156, rec:0.0064, f1:0.0091
g_step 6100, step 210, avg_time 3.215, loss:368.6595
g_step 6200, step 310, avg_time 1.046, loss:382.0526
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:41:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:41:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-41-03_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:41:04 - WARNING - datasets.builder -   Using custom data configuration default-58e47c36ba464287
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-58e47c36ba464287/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:41:04,470 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:41:04,471 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:41:04,471 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:41:04,473 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:41:04,480 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,484 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,484 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,485 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,485 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,485 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:41:04,485 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:41:04,612 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:41:07,700 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:41:07,704 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-58e47c36ba464287/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.12ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.26ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.42ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.49ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.79ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.04ba/s]100%|██████████| 8/8 [00:01<00:00,  4.87ba/s]100%|██████████| 8/8 [00:01<00:00,  4.34ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.04ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.26ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.34ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.39ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.39ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.39ba/s]100%|██████████| 7/7 [00:01<00:00,  4.86ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.29ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.87ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.15ba/s] 75%|███████▌  | 6/8 [00:00<00:00, 10.07ba/s]100%|██████████| 8/8 [00:00<00:00, 11.36ba/s]100%|██████████| 8/8 [00:00<00:00, 10.67ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.00ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.63ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  9.70ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.95ba/s]100%|██████████| 7/7 [00:00<00:00, 10.91ba/s]
[INFO|trainer.py:414] 2023-08-29 05:41:12,796 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:41:12,811 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:41:12,811 >>   Num examples = 7519
[INFO|trainer.py:1149] 2023-08-29 05:41:12,811 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:41:12,811 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:41:12,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:41:12,811 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:41:12,811 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:52,  3.39it/s]  0%|          | 2/585 [00:00<02:48,  3.45it/s]  1%|          | 3/585 [00:00<02:47,  3.47it/s]  1%|          | 4/585 [00:01<02:46,  3.48it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:45,  3.49it/s]  1%|▏         | 8/585 [00:02<02:45,  3.49it/s]  2%|▏         | 9/585 [00:02<02:44,  3.49it/s]  2%|▏         | 10/585 [00:02<02:44,  3.49it/s]  2%|▏         | 11/585 [00:03<02:44,  3.49it/s]  2%|▏         | 12/585 [00:03<02:44,  3.49it/s]  2%|▏         | 13/585 [00:03<02:43,  3.49it/s]  2%|▏         | 14/585 [00:04<02:43,  3.49it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.48it/s]  3%|▎         | 18/585 [00:05<02:43,  3.48it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:41,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.48it/s]  4%|▍         | 24/585 [00:06<02:41,  3.48it/s]  4%|▍         | 25/585 [00:07<02:40,  3.48it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:40,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:39,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:37,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:35,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:34,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:14<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:33,  3.48it/s]  9%|▉         | 54/585 [00:15<02:32,  3.48it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:32,  3.48it/s] 10%|▉         | 57/585 [00:16<02:31,  3.48it/s] 10%|▉         | 58/585 [00:16<02:31,  3.48it/s] 10%|█         | 59/585 [00:16<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:31,  3.47it/s] 10%|█         | 61/585 [00:17<02:30,  3.47it/s] 11%|█         | 62/585 [00:17<02:30,  3.48it/s] 11%|█         | 63/585 [00:18<02:30,  3.48it/s] 11%|█         | 64/585 [00:18<02:29,  3.48it/s] 11%|█         | 65/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.48it/s] 11%|█▏        | 67/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:20<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 75/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.48it/s] 14%|█▎        | 80/585 [00:22<02:25,  3.48it/s] 14%|█▍        | 81/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 82/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 83/585 [00:23<02:24,  3.48it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.47it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.47it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.47it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:29<02:18,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 106/585 [00:30<02:23,  3.34it/s] 18%|█▊        | 107/585 [00:30<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:31<02:19,  3.42it/s] 19%|█▉        | 110/585 [00:31<02:18,  3.44it/s] 19%|█▉        | 111/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.45it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:32<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.46it/s] 20%|██        | 117/585 [00:33<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 05:41:46,663 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:41:46,663 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:41:46,663 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.46it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.53it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.70it/s][A
  3%|▎         | 23/782 [00:00<00:15, 47.99it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.60it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.30it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.08it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.04it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.95it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.91it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.81it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.81it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.75it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.76it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.79it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.80it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.72it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.74it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.75it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.70it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.65it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.73it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.67it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.69it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.77it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.64it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.72it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.76it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.68it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.69it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.71it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.66it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.67it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.70it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.65it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.71it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.69it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.68it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.68it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.71it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.64it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.69it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.73it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.65it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.71it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.74it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.71it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.73it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.67it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.62it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.63it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.68it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.67it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.60it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.58it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.66it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.59it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.70it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.67it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.66it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.68it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.65it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.65it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.68it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.69it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.72it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.68it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.63it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.63it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.65it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.70it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.70it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.51it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.62it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.64it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.68it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.64it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.69it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.72it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.67it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.68it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.68it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.68it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.69it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.73it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.64it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.65it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.63it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.71it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.68it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.66it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.70it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.70it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.72it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.71it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.72it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.72it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.64it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.66it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.69it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.60it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.65it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.65it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.68it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.66it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.66it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.67it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.63it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.66it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.66it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.61it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.62it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.67it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.60it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.68it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.72it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.69it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.65it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.69it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.67it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.68it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.68it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.69it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.70it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.62it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.64it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.70it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.64it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.64it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.68it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.66it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.65it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.61it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.64it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.65it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.71it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.64it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.60it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.62it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 44.09it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 45.43it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 45.80it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.03it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.21it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.37it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.50it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.58it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.49it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.53it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.60it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.62it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.62it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.57it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.65it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:50<02:15,  3.45it/s]
100%|██████████| 782/782 [00:16<00:00, 46.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:42:03,451 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 05:42:03,473 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:42:06,699 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:42:06,713 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:42:06,724 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:58<1:00:24,  7.76s/it] 20%|██        | 119/585 [00:59<42:52,  5.52s/it]   21%|██        | 120/585 [00:59<30:36,  3.95s/it] 21%|██        | 121/585 [00:59<22:03,  2.85s/it] 21%|██        | 122/585 [01:00<16:04,  2.08s/it] 21%|██        | 123/585 [01:00<11:53,  1.54s/it] 21%|██        | 124/585 [01:00<08:57,  1.17s/it] 21%|██▏       | 125/585 [01:00<06:55,  1.11it/s] 22%|██▏       | 126/585 [01:01<05:29,  1.39it/s] 22%|██▏       | 127/585 [01:01<04:29,  1.70it/s] 22%|██▏       | 128/585 [01:01<03:47,  2.01it/s] 22%|██▏       | 129/585 [01:02<03:18,  2.30it/s] 22%|██▏       | 130/585 [01:02<02:58,  2.55it/s] 22%|██▏       | 131/585 [01:02<02:43,  2.77it/s] 23%|██▎       | 132/585 [01:02<02:33,  2.95it/s] 23%|██▎       | 133/585 [01:03<02:26,  3.09it/s] 23%|██▎       | 134/585 [01:03<02:20,  3.20it/s] 23%|██▎       | 135/585 [01:03<02:17,  3.28it/s] 23%|██▎       | 136/585 [01:04<02:14,  3.34it/s] 23%|██▎       | 137/585 [01:04<02:12,  3.38it/s] 24%|██▎       | 138/585 [01:04<02:11,  3.41it/s] 24%|██▍       | 139/585 [01:04<02:10,  3.43it/s] 24%|██▍       | 140/585 [01:05<02:09,  3.44it/s] 24%|██▍       | 141/585 [01:05<02:08,  3.44it/s] 24%|██▍       | 142/585 [01:05<02:08,  3.45it/s] 24%|██▍       | 143/585 [01:06<02:07,  3.46it/s] 25%|██▍       | 144/585 [01:06<02:07,  3.46it/s] 25%|██▍       | 145/585 [01:06<02:06,  3.47it/s] 25%|██▍       | 146/585 [01:06<02:06,  3.47it/s] 25%|██▌       | 147/585 [01:07<02:06,  3.47it/s] 25%|██▌       | 148/585 [01:07<02:05,  3.47it/s] 25%|██▌       | 149/585 [01:07<02:05,  3.47it/s] 26%|██▌       | 150/585 [01:08<02:05,  3.47it/s] 26%|██▌       | 151/585 [01:08<02:05,  3.47it/s] 26%|██▌       | 152/585 [01:08<02:05,  3.46it/s] 26%|██▌       | 153/585 [01:08<02:04,  3.47it/s] 26%|██▋       | 154/585 [01:09<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:09<02:03,  3.47it/s] 27%|██▋       | 156/585 [01:09<02:03,  3.47it/s] 27%|██▋       | 157/585 [01:10<02:03,  3.47it/s] 27%|██▋       | 158/585 [01:10<02:03,  3.47it/s] 27%|██▋       | 159/585 [01:10<02:02,  3.47it/s] 27%|██▋       | 160/585 [01:11<02:02,  3.47it/s] 28%|██▊       | 161/585 [01:11<02:02,  3.47it/s] 28%|██▊       | 162/585 [01:11<02:01,  3.47it/s] 28%|██▊       | 163/585 [01:11<02:02,  3.46it/s] 28%|██▊       | 164/585 [01:12<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:12<02:01,  3.47it/s] 28%|██▊       | 166/585 [01:12<02:00,  3.47it/s] 29%|██▊       | 167/585 [01:13<02:00,  3.47it/s] 29%|██▊       | 168/585 [01:13<02:00,  3.47it/s] 29%|██▉       | 169/585 [01:13<01:59,  3.47it/s] 29%|██▉       | 170/585 [01:13<01:59,  3.47it/s] 29%|██▉       | 171/585 [01:14<01:59,  3.47it/s] 29%|██▉       | 172/585 [01:14<01:58,  3.47it/s] 30%|██▉       | 173/585 [01:14<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:15<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:15<01:58,  3.46it/s] 30%|███       | 176/585 [01:15<01:58,  3.46it/s] 30%|███       | 177/585 [01:15<01:57,  3.47it/s] 30%|███       | 178/585 [01:16<01:57,  3.47it/s] 31%|███       | 179/585 [01:16<01:57,  3.47it/s] 31%|███       | 180/585 [01:16<01:56,  3.47it/s] 31%|███       | 181/585 [01:17<01:56,  3.47it/s] 31%|███       | 182/585 [01:17<01:56,  3.47it/s] 31%|███▏      | 183/585 [01:17<01:55,  3.47it/s] 31%|███▏      | 184/585 [01:17<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:18<01:55,  3.47it/s] 32%|███▏      | 186/585 [01:18<01:55,  3.47it/s] 32%|███▏      | 187/585 [01:18<01:54,  3.47it/s] 32%|███▏      | 188/585 [01:19<01:54,  3.47it/s] 32%|███▏      | 189/585 [01:19<01:54,  3.47it/s] 32%|███▏      | 190/585 [01:19<01:53,  3.47it/s] 33%|███▎      | 191/585 [01:19<01:53,  3.47it/s] 33%|███▎      | 192/585 [01:20<01:53,  3.47it/s] 33%|███▎      | 193/585 [01:20<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:20<01:52,  3.47it/s] 33%|███▎      | 195/585 [01:21<01:52,  3.47it/s] 34%|███▎      | 196/585 [01:21<01:56,  3.34it/s] 34%|███▎      | 197/585 [01:21<01:54,  3.38it/s] 34%|███▍      | 198/585 [01:22<01:53,  3.41it/s] 34%|███▍      | 199/585 [01:22<01:52,  3.43it/s] 34%|███▍      | 200/585 [01:22<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:22<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:23<01:50,  3.45it/s] 35%|███▍      | 203/585 [01:23<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:23<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:24<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:24<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:24<01:49,  3.47it/s] 36%|███▌      | 208/585 [01:24<01:48,  3.47it/s] 36%|███▌      | 209/585 [01:25<01:48,  3.47it/s] 36%|███▌      | 210/585 [01:25<01:51,  3.36it/s] 36%|███▌      | 211/585 [01:25<01:50,  3.39it/s] 36%|███▌      | 212/585 [01:26<01:49,  3.41it/s] 36%|███▋      | 213/585 [01:26<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:26<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:26<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:27<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:27<01:46,  3.46it/s] 37%|███▋      | 218/585 [01:27<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:28<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:28<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:28<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:28<01:45,  3.46it/s] 38%|███▊      | 223/585 [01:29<01:44,  3.46it/s] 38%|███▊      | 224/585 [01:29<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:29<01:44,  3.46it/s] 39%|███▊      | 226/585 [01:30<01:43,  3.46it/s] 39%|███▉      | 227/585 [01:30<01:43,  3.46it/s] 39%|███▉      | 228/585 [01:30<01:42,  3.47it/s] 39%|███▉      | 229/585 [01:30<01:42,  3.47it/s] 39%|███▉      | 230/585 [01:31<01:42,  3.47it/s] 39%|███▉      | 231/585 [01:31<01:42,  3.47it/s] 40%|███▉      | 232/585 [01:31<01:42,  3.46it/s] 40%|███▉      | 233/585 [01:32<01:41,  3.47it/s] 40%|████      | 234/585 [01:32<01:41,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 05:42:45,368 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:42:45,368 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:42:45,368 >>   Batch size = 8
{'eval_loss': 0.8962836861610413, 'eval_runtime': 16.7646, 'eval_samples_per_second': 372.989, 'eval_steps_per_second': 46.646, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.30it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.63it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.85it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.11it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.77it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.53it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.13it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.99it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.75it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.81it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.88it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.71it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.84it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.85it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.88it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.81it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.78it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.78it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.75it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.79it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.79it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.77it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.86it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.81it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.83it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.81it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.73it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.71it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.81it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.78it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.87it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.86it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.78it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.83it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.82it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.74it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.77it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.76it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.73it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.82it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.90it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.77it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.87it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.84it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.73it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.75it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.77it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.73it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.80it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.87it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.77it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.81it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.77it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.75it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.73it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.77it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.75it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.70it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.74it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.74it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.85it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.83it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.88it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.76it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.75it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.78it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.75it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.73it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.76it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.82it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.80it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.75it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.78it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.75it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.68it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.71it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.73it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.67it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.70it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.81it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.78it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.78it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.76it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.77it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.75it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.74it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.71it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.63it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.73it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.73it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.77it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.80it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.71it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.68it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.78it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.69it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.76it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.78it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.75it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.78it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.75it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.75it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.75it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.77it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.72it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.71it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.72it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.69it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.78it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.71it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.72it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.77it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.76it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.65it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.62it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.66it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.72it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.74it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.67it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.67it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.75it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.70it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.71it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.70it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.71it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.58it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.61it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.64it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.68it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.67it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.73it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.71it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.66it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.76it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.62it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.58it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.67it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.67it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.68it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.71it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.68it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.75it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.77it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.65it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.69it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.66it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.61it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.69it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.69it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.63it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.63it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.63it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.65it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 45.27it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 45.74it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:49<01:41,  3.47it/s]
100%|██████████| 782/782 [00:16<00:00, 45.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:43:02,120 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 05:43:02,141 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:43:04,874 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:43:04,895 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:43:04,914 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:57<45:42,  7.83s/it] 40%|████      | 236/585 [01:58<32:24,  5.57s/it] 41%|████      | 237/585 [01:58<23:07,  3.99s/it] 41%|████      | 238/585 [01:58<16:38,  2.88s/it] 41%|████      | 239/585 [01:59<12:06,  2.10s/it] 41%|████      | 240/585 [01:59<08:56,  1.56s/it] 41%|████      | 241/585 [01:59<06:44,  1.18s/it] 41%|████▏     | 242/585 [01:59<05:11,  1.10it/s] 42%|████▏     | 243/585 [02:00<04:07,  1.38it/s] 42%|████▏     | 244/585 [02:00<03:22,  1.69it/s] 42%|████▏     | 245/585 [02:00<02:50,  2.00it/s] 42%|████▏     | 246/585 [02:01<02:28,  2.28it/s] 42%|████▏     | 247/585 [02:01<02:13,  2.54it/s] 42%|████▏     | 248/585 [02:01<02:02,  2.76it/s] 43%|████▎     | 249/585 [02:01<01:54,  2.94it/s] 43%|████▎     | 250/585 [02:02<01:48,  3.08it/s] 43%|████▎     | 251/585 [02:02<01:44,  3.19it/s] 43%|████▎     | 252/585 [02:02<01:41,  3.27it/s] 43%|████▎     | 253/585 [02:03<01:39,  3.32it/s] 43%|████▎     | 254/585 [02:03<01:38,  3.37it/s] 44%|████▎     | 255/585 [02:03<01:37,  3.40it/s] 44%|████▍     | 256/585 [02:03<01:36,  3.42it/s] 44%|████▍     | 257/585 [02:04<01:35,  3.43it/s] 44%|████▍     | 258/585 [02:04<01:35,  3.43it/s] 44%|████▍     | 259/585 [02:04<01:34,  3.44it/s] 44%|████▍     | 260/585 [02:05<01:34,  3.45it/s] 45%|████▍     | 261/585 [02:05<01:33,  3.45it/s] 45%|████▍     | 262/585 [02:05<01:33,  3.46it/s] 45%|████▍     | 263/585 [02:05<01:33,  3.46it/s] 45%|████▌     | 264/585 [02:06<01:32,  3.46it/s] 45%|████▌     | 265/585 [02:06<01:32,  3.47it/s] 45%|████▌     | 266/585 [02:06<01:31,  3.47it/s] 46%|████▌     | 267/585 [02:07<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:07<01:31,  3.47it/s] 46%|████▌     | 269/585 [02:07<01:31,  3.46it/s] 46%|████▌     | 270/585 [02:07<01:30,  3.47it/s] 46%|████▋     | 271/585 [02:08<01:30,  3.47it/s] 46%|████▋     | 272/585 [02:08<01:30,  3.47it/s] 47%|████▋     | 273/585 [02:08<01:30,  3.46it/s] 47%|████▋     | 274/585 [02:09<01:29,  3.47it/s] 47%|████▋     | 275/585 [02:09<01:29,  3.47it/s] 47%|████▋     | 276/585 [02:09<01:29,  3.47it/s] 47%|████▋     | 277/585 [02:09<01:28,  3.47it/s] 48%|████▊     | 278/585 [02:10<01:28,  3.47it/s] 48%|████▊     | 279/585 [02:10<01:28,  3.47it/s] 48%|████▊     | 280/585 [02:10<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:11<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:11<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:11<01:27,  3.47it/s] 49%|████▊     | 284/585 [02:12<01:26,  3.47it/s] 49%|████▊     | 285/585 [02:12<01:26,  3.47it/s] 49%|████▉     | 286/585 [02:12<01:26,  3.47it/s] 49%|████▉     | 287/585 [02:12<01:25,  3.47it/s] 49%|████▉     | 288/585 [02:13<01:25,  3.47it/s] 49%|████▉     | 289/585 [02:13<01:25,  3.47it/s] 50%|████▉     | 290/585 [02:13<01:25,  3.47it/s] 50%|████▉     | 291/585 [02:14<01:24,  3.46it/s] 50%|████▉     | 292/585 [02:14<01:24,  3.46it/s] 50%|█████     | 293/585 [02:14<01:24,  3.47it/s] 50%|█████     | 294/585 [02:14<01:23,  3.47it/s] 50%|█████     | 295/585 [02:15<01:23,  3.47it/s] 51%|█████     | 296/585 [02:15<01:23,  3.47it/s] 51%|█████     | 297/585 [02:15<01:23,  3.47it/s] 51%|█████     | 298/585 [02:16<01:22,  3.47it/s] 51%|█████     | 299/585 [02:16<01:22,  3.47it/s] 51%|█████▏    | 300/585 [02:16<01:22,  3.47it/s] 51%|█████▏    | 301/585 [02:16<01:21,  3.47it/s] 52%|█████▏    | 302/585 [02:17<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:17<01:21,  3.46it/s] 52%|█████▏    | 304/585 [02:17<01:21,  3.47it/s] 52%|█████▏    | 305/585 [02:18<01:20,  3.47it/s] 52%|█████▏    | 306/585 [02:18<01:20,  3.47it/s] 52%|█████▏    | 307/585 [02:18<01:20,  3.47it/s] 53%|█████▎    | 308/585 [02:18<01:19,  3.47it/s] 53%|█████▎    | 309/585 [02:19<01:19,  3.47it/s] 53%|█████▎    | 310/585 [02:19<01:19,  3.47it/s] 53%|█████▎    | 311/585 [02:19<01:19,  3.47it/s] 53%|█████▎    | 312/585 [02:20<01:18,  3.47it/s] 54%|█████▎    | 313/585 [02:20<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:20<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:20<01:17,  3.46it/s] 54%|█████▍    | 316/585 [02:21<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:21<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:21<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:22<01:16,  3.47it/s] 55%|█████▍    | 320/585 [02:22<01:16,  3.46it/s] 55%|█████▍    | 321/585 [02:22<01:16,  3.47it/s] 55%|█████▌    | 322/585 [02:22<01:15,  3.47it/s] 55%|█████▌    | 323/585 [02:23<01:15,  3.47it/s] 55%|█████▌    | 324/585 [02:23<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:23<01:15,  3.46it/s] 56%|█████▌    | 326/585 [02:24<01:14,  3.46it/s] 56%|█████▌    | 327/585 [02:24<01:14,  3.46it/s] 56%|█████▌    | 328/585 [02:24<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:24<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:25<01:13,  3.46it/s] 57%|█████▋    | 331/585 [02:25<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:25<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:26<01:12,  3.46it/s] 57%|█████▋    | 334/585 [02:26<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:26<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:27<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:27<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:27<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:27<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:28<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:28<01:10,  3.46it/s] 58%|█████▊    | 342/585 [02:28<01:10,  3.46it/s] 59%|█████▊    | 343/585 [02:29<01:09,  3.46it/s] 59%|█████▉    | 344/585 [02:29<01:09,  3.46it/s] 59%|█████▉    | 345/585 [02:29<01:09,  3.46it/s] 59%|█████▉    | 346/585 [02:29<01:08,  3.46it/s] 59%|█████▉    | 347/585 [02:30<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:30<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:30<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:31<01:07,  3.46it/s] 60%|██████    | 351/585 [02:31<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 05:43:44,297 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:43:44,297 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:43:44,297 >>   Batch size = 8
{'eval_loss': 0.904608964920044, 'eval_runtime': 16.7354, 'eval_samples_per_second': 373.639, 'eval_steps_per_second': 46.727, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.57it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.80it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.98it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.12it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.70it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.39it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.07it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.85it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.69it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.72it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.63it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.73it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.82it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.84it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.83it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.85it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.55it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.64it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.63it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.64it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.73it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.78it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.71it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.84it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.78it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.64it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.67it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.59it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.66it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.66it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.69it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.75it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.80it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.68it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.74it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.59it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.63it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.63it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.63it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.70it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.75it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.79it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.78it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.70it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.62it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.66it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.60it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.58it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.70it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.73it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.71it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.81it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.73it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.60it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.57it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.58it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.66it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.66it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.67it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.67it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.78it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.71it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.76it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.72it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.64it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.61it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.66it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.61it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.75it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.78it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.77it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.73it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.73it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.59it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.68it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.50it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.59it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.56it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.63it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.71it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.79it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.77it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.70it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.65it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.65it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.68it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.60it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.71it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.64it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.68it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.71it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.65it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.68it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.66it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.67it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.63it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.60it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.70it/s][A
 64%|██████▍   | 503/782 [00:10<00:06, 46.50it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.57it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.64it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.62it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.55it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.62it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.68it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.61it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.67it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.61it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.62it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.69it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.72it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.66it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.65it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.65it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.63it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.71it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.64it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.63it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.65it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.67it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.65it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.65it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.53it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.59it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.53it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.58it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.55it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.63it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.62it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.68it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.63it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.63it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.60it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.66it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.66it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.57it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.61it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.66it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.67it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.70it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.67it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.64it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.65it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.58it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.60it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.60it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.60it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.60it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.61it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.59it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.60it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.62it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.67it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.57it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:48<01:07,  3.46it/s]
100%|██████████| 782/782 [00:16<00:00, 46.57it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:44:01,070 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 05:44:01,092 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:44:04,190 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:44:04,306 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:44:04,317 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:58<32:51,  8.46s/it] 60%|██████    | 353/585 [02:59<23:14,  6.01s/it] 61%|██████    | 354/585 [02:59<16:31,  4.29s/it] 61%|██████    | 355/585 [02:59<11:51,  3.09s/it] 61%|██████    | 356/585 [03:00<08:35,  2.25s/it] 61%|██████    | 357/585 [03:00<06:18,  1.66s/it] 61%|██████    | 358/585 [03:00<04:43,  1.25s/it] 61%|██████▏   | 359/585 [03:00<03:37,  1.04it/s] 62%|██████▏   | 360/585 [03:01<02:50,  1.32it/s] 62%|██████▏   | 361/585 [03:01<02:18,  1.62it/s] 62%|██████▏   | 362/585 [03:01<01:55,  1.93it/s] 62%|██████▏   | 363/585 [03:02<01:39,  2.23it/s] 62%|██████▏   | 364/585 [03:02<01:28,  2.49it/s] 62%|██████▏   | 365/585 [03:02<01:20,  2.72it/s] 63%|██████▎   | 366/585 [03:02<01:15,  2.91it/s] 63%|██████▎   | 367/585 [03:03<01:11,  3.06it/s] 63%|██████▎   | 368/585 [03:03<01:08,  3.17it/s] 63%|██████▎   | 369/585 [03:03<01:06,  3.25it/s] 63%|██████▎   | 370/585 [03:04<01:04,  3.32it/s] 63%|██████▎   | 371/585 [03:04<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:04<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:04<01:02,  3.42it/s] 64%|██████▍   | 374/585 [03:05<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:05<01:01,  3.43it/s] 64%|██████▍   | 376/585 [03:05<01:00,  3.44it/s] 64%|██████▍   | 377/585 [03:06<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:06<00:59,  3.46it/s] 65%|██████▍   | 379/585 [03:06<00:59,  3.46it/s] 65%|██████▍   | 380/585 [03:06<00:59,  3.46it/s] 65%|██████▌   | 381/585 [03:07<00:58,  3.46it/s] 65%|██████▌   | 382/585 [03:07<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:07<00:58,  3.47it/s] 66%|██████▌   | 384/585 [03:08<00:58,  3.47it/s] 66%|██████▌   | 385/585 [03:08<00:57,  3.47it/s] 66%|██████▌   | 386/585 [03:08<00:57,  3.46it/s] 66%|██████▌   | 387/585 [03:08<00:57,  3.47it/s] 66%|██████▋   | 388/585 [03:09<00:56,  3.47it/s] 66%|██████▋   | 389/585 [03:09<00:56,  3.47it/s] 67%|██████▋   | 390/585 [03:09<00:56,  3.47it/s] 67%|██████▋   | 391/585 [03:10<00:55,  3.47it/s] 67%|██████▋   | 392/585 [03:10<00:55,  3.47it/s] 67%|██████▋   | 393/585 [03:10<00:55,  3.47it/s] 67%|██████▋   | 394/585 [03:10<00:55,  3.47it/s] 68%|██████▊   | 395/585 [03:11<00:54,  3.47it/s] 68%|██████▊   | 396/585 [03:11<00:54,  3.47it/s] 68%|██████▊   | 397/585 [03:11<00:54,  3.46it/s] 68%|██████▊   | 398/585 [03:12<00:53,  3.47it/s] 68%|██████▊   | 399/585 [03:12<00:53,  3.47it/s] 68%|██████▊   | 400/585 [03:12<00:53,  3.47it/s] 69%|██████▊   | 401/585 [03:13<00:53,  3.47it/s] 69%|██████▊   | 402/585 [03:13<00:52,  3.47it/s] 69%|██████▉   | 403/585 [03:13<00:52,  3.47it/s] 69%|██████▉   | 404/585 [03:13<00:52,  3.47it/s] 69%|██████▉   | 405/585 [03:14<00:51,  3.47it/s] 69%|██████▉   | 406/585 [03:14<00:51,  3.47it/s] 70%|██████▉   | 407/585 [03:14<00:51,  3.47it/s] 70%|██████▉   | 408/585 [03:15<00:51,  3.46it/s] 70%|██████▉   | 409/585 [03:15<00:50,  3.46it/s] 70%|███████   | 410/585 [03:15<00:50,  3.46it/s] 70%|███████   | 411/585 [03:15<00:50,  3.47it/s] 70%|███████   | 412/585 [03:16<00:49,  3.47it/s] 71%|███████   | 413/585 [03:16<00:49,  3.47it/s] 71%|███████   | 414/585 [03:16<00:49,  3.47it/s] 71%|███████   | 415/585 [03:17<00:49,  3.46it/s] 71%|███████   | 416/585 [03:17<00:48,  3.47it/s] 71%|███████▏  | 417/585 [03:17<00:48,  3.47it/s] 71%|███████▏  | 418/585 [03:17<00:48,  3.47it/s] 72%|███████▏  | 419/585 [03:18<00:48,  3.45it/s] 72%|███████▏  | 420/585 [03:18<00:47,  3.46it/s] 72%|███████▏  | 421/585 [03:18<00:47,  3.46it/s] 72%|███████▏  | 422/585 [03:19<00:47,  3.46it/s] 72%|███████▏  | 423/585 [03:19<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:19<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:19<00:46,  3.47it/s] 73%|███████▎  | 426/585 [03:20<00:45,  3.47it/s] 73%|███████▎  | 427/585 [03:20<00:45,  3.47it/s] 73%|███████▎  | 428/585 [03:20<00:45,  3.47it/s] 73%|███████▎  | 429/585 [03:21<00:44,  3.47it/s] 74%|███████▎  | 430/585 [03:21<00:45,  3.42it/s] 74%|███████▎  | 431/585 [03:21<00:44,  3.44it/s] 74%|███████▍  | 432/585 [03:21<00:44,  3.44it/s] 74%|███████▍  | 433/585 [03:22<00:44,  3.45it/s] 74%|███████▍  | 434/585 [03:22<00:43,  3.46it/s] 74%|███████▍  | 435/585 [03:22<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:23<00:43,  3.46it/s] 75%|███████▍  | 437/585 [03:23<00:42,  3.46it/s] 75%|███████▍  | 438/585 [03:23<00:42,  3.47it/s] 75%|███████▌  | 439/585 [03:23<00:42,  3.46it/s] 75%|███████▌  | 440/585 [03:24<00:41,  3.46it/s] 75%|███████▌  | 441/585 [03:24<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:24<00:41,  3.46it/s] 76%|███████▌  | 443/585 [03:25<00:41,  3.46it/s] 76%|███████▌  | 444/585 [03:25<00:40,  3.46it/s] 76%|███████▌  | 445/585 [03:25<00:40,  3.46it/s] 76%|███████▌  | 446/585 [03:26<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:26<00:39,  3.47it/s] 77%|███████▋  | 448/585 [03:26<00:39,  3.47it/s] 77%|███████▋  | 449/585 [03:26<00:39,  3.47it/s] 77%|███████▋  | 450/585 [03:27<00:38,  3.46it/s] 77%|███████▋  | 451/585 [03:27<00:38,  3.47it/s] 77%|███████▋  | 452/585 [03:27<00:38,  3.46it/s] 77%|███████▋  | 453/585 [03:28<00:38,  3.46it/s] 78%|███████▊  | 454/585 [03:28<00:37,  3.46it/s] 78%|███████▊  | 455/585 [03:28<00:37,  3.46it/s] 78%|███████▊  | 456/585 [03:28<00:37,  3.46it/s] 78%|███████▊  | 457/585 [03:29<00:36,  3.46it/s] 78%|███████▊  | 458/585 [03:29<00:36,  3.46it/s] 78%|███████▊  | 459/585 [03:29<00:36,  3.46it/s] 79%|███████▊  | 460/585 [03:30<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:30<00:35,  3.46it/s] 79%|███████▉  | 462/585 [03:30<00:35,  3.46it/s] 79%|███████▉  | 463/585 [03:30<00:35,  3.46it/s] 79%|███████▉  | 464/585 [03:31<00:34,  3.46it/s] 79%|███████▉  | 465/585 [03:31<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:31<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:32<00:34,  3.46it/s] 80%|████████  | 468/585 [03:32<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 05:44:45,316 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:44:45,316 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:44:45,316 >>   Batch size = 8
{'eval_loss': 0.9156900644302368, 'eval_runtime': 16.7532, 'eval_samples_per_second': 373.242, 'eval_steps_per_second': 46.678, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.27it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.67it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.89it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.19it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.65it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.43it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.14it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.84it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.78it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.83it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.76it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.76it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.75it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.87it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.83it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.72it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.69it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.64it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.70it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.71it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.79it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.73it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.75it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.78it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.74it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.73it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.64it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.63it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.68it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.77it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.78it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.72it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.66it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.75it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.72it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.67it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.66it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.64it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.68it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.66it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.77it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.85it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.76it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.76it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.64it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.61it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.67it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.67it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.63it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.75it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.72it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.79it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.71it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.61it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.71it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.73it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.64it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.71it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.74it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.76it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.79it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.73it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.60it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.62it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.62it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.68it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.70it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.67it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.68it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.78it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.67it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.72it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.68it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.62it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.60it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.69it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.68it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.79it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.81it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.73it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.63it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.64it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.63it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.59it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.65it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.71it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.79it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.74it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.72it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.74it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.63it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.55it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.44it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.46it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.48it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.51it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.61it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.65it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.64it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.53it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.52it/s][A
 66%|██████▌   | 513/782 [00:10<00:06, 43.75it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 44.64it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 45.18it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 45.64it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.05it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.29it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.52it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.66it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.41it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.34it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.50it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.45it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.61it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.71it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.71it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.68it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.74it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.64it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.49it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.49it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.48it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.39it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.45it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.48it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.48it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.50it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.52it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.53it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 44.31it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 45.00it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 45.60it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 45.98it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.11it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.30it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.51it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.65it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.53it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.49it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.42it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.47it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.59it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.71it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.66it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.64it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.75it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.72it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.57it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.58it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.66it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.62it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.70it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.62it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.65it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.68it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:49<00:33,  3.46it/s]
100%|██████████| 782/782 [00:16<00:00, 46.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:45:02,286 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 05:45:02,551 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:45:04,811 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:45:04,827 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:45:04,889 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:57<14:47,  7.65s/it] 80%|████████  | 470/585 [03:57<10:25,  5.44s/it] 81%|████████  | 471/585 [03:57<07:23,  3.89s/it] 81%|████████  | 472/585 [03:58<05:17,  2.81s/it] 81%|████████  | 473/585 [03:58<03:50,  2.05s/it] 81%|████████  | 474/585 [03:58<02:49,  1.52s/it] 81%|████████  | 475/585 [03:58<02:06,  1.15s/it] 81%|████████▏ | 476/585 [03:59<01:37,  1.12it/s] 82%|████████▏ | 477/585 [03:59<01:17,  1.40it/s] 82%|████████▏ | 478/585 [03:59<01:02,  1.71it/s] 82%|████████▏ | 479/585 [04:00<00:52,  2.01it/s] 82%|████████▏ | 480/585 [04:00<00:45,  2.30it/s] 82%|████████▏ | 481/585 [04:00<00:40,  2.56it/s] 82%|████████▏ | 482/585 [04:00<00:37,  2.78it/s] 83%|████████▎ | 483/585 [04:01<00:34,  2.96it/s] 83%|████████▎ | 484/585 [04:01<00:32,  3.09it/s] 83%|████████▎ | 485/585 [04:01<00:31,  3.20it/s] 83%|████████▎ | 486/585 [04:02<00:30,  3.27it/s] 83%|████████▎ | 487/585 [04:02<00:29,  3.31it/s] 83%|████████▎ | 488/585 [04:02<00:28,  3.36it/s] 84%|████████▎ | 489/585 [04:02<00:28,  3.39it/s] 84%|████████▍ | 490/585 [04:03<00:27,  3.41it/s] 84%|████████▍ | 491/585 [04:03<00:27,  3.43it/s] 84%|████████▍ | 492/585 [04:03<00:27,  3.44it/s] 84%|████████▍ | 493/585 [04:04<00:26,  3.45it/s] 84%|████████▍ | 494/585 [04:04<00:26,  3.45it/s] 85%|████████▍ | 495/585 [04:04<00:26,  3.46it/s] 85%|████████▍ | 496/585 [04:04<00:25,  3.46it/s] 85%|████████▍ | 497/585 [04:05<00:25,  3.46it/s] 85%|████████▌ | 498/585 [04:05<00:25,  3.46it/s] 85%|████████▌ | 499/585 [04:05<00:24,  3.46it/s] 85%|████████▌ | 500/585 [04:06<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:06<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:06<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:06<00:23,  3.47it/s] 86%|████████▌ | 503/585 [04:06<00:23,  3.47it/s] 86%|████████▌ | 504/585 [04:07<00:23,  3.47it/s] 86%|████████▋ | 505/585 [04:07<00:23,  3.47it/s] 86%|████████▋ | 506/585 [04:07<00:22,  3.47it/s] 87%|████████▋ | 507/585 [04:08<00:22,  3.47it/s] 87%|████████▋ | 508/585 [04:08<00:22,  3.47it/s] 87%|████████▋ | 509/585 [04:08<00:22,  3.45it/s] 87%|████████▋ | 510/585 [04:09<00:21,  3.46it/s] 87%|████████▋ | 511/585 [04:09<00:21,  3.46it/s] 88%|████████▊ | 512/585 [04:09<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:09<00:20,  3.47it/s] 88%|████████▊ | 514/585 [04:10<00:20,  3.47it/s] 88%|████████▊ | 515/585 [04:10<00:20,  3.47it/s] 88%|████████▊ | 516/585 [04:10<00:19,  3.47it/s] 88%|████████▊ | 517/585 [04:11<00:19,  3.47it/s] 89%|████████▊ | 518/585 [04:11<00:19,  3.47it/s] 89%|████████▊ | 519/585 [04:11<00:19,  3.47it/s] 89%|████████▉ | 520/585 [04:11<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:12<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:12<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:12<00:17,  3.47it/s] 90%|████████▉ | 524/585 [04:13<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:13<00:17,  3.47it/s] 90%|████████▉ | 526/585 [04:13<00:17,  3.47it/s] 90%|█████████ | 527/585 [04:13<00:16,  3.47it/s] 90%|█████████ | 528/585 [04:14<00:16,  3.47it/s] 90%|█████████ | 529/585 [04:14<00:16,  3.47it/s] 91%|█████████ | 530/585 [04:14<00:15,  3.47it/s] 91%|█████████ | 531/585 [04:15<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:15<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:15<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:15<00:14,  3.47it/s] 91%|█████████▏| 535/585 [04:16<00:14,  3.47it/s] 92%|█████████▏| 536/585 [04:16<00:14,  3.46it/s] 92%|█████████▏| 537/585 [04:16<00:13,  3.46it/s] 92%|█████████▏| 538/585 [04:17<00:13,  3.46it/s] 92%|█████████▏| 539/585 [04:17<00:13,  3.46it/s] 92%|█████████▏| 540/585 [04:17<00:12,  3.47it/s] 92%|█████████▏| 541/585 [04:17<00:12,  3.47it/s] 93%|█████████▎| 542/585 [04:18<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:18<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:18<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:19<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:19<00:11,  3.46it/s] 94%|█████████▎| 547/585 [04:19<00:10,  3.46it/s] 94%|█████████▎| 548/585 [04:19<00:10,  3.46it/s] 94%|█████████▍| 549/585 [04:20<00:10,  3.46it/s] 94%|█████████▍| 550/585 [04:20<00:10,  3.46it/s] 94%|█████████▍| 551/585 [04:20<00:09,  3.46it/s] 94%|█████████▍| 552/585 [04:21<00:09,  3.46it/s] 95%|█████████▍| 553/585 [04:21<00:09,  3.46it/s] 95%|█████████▍| 554/585 [04:21<00:08,  3.46it/s] 95%|█████████▍| 555/585 [04:22<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:22<00:08,  3.46it/s] 95%|█████████▌| 557/585 [04:22<00:08,  3.47it/s] 95%|█████████▌| 558/585 [04:22<00:07,  3.46it/s] 96%|█████████▌| 559/585 [04:23<00:07,  3.46it/s] 96%|█████████▌| 560/585 [04:23<00:07,  3.46it/s] 96%|█████████▌| 561/585 [04:23<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:24<00:06,  3.46it/s] 96%|█████████▌| 563/585 [04:24<00:06,  3.47it/s] 96%|█████████▋| 564/585 [04:24<00:06,  3.45it/s] 97%|█████████▋| 565/585 [04:24<00:05,  3.46it/s] 97%|█████████▋| 566/585 [04:25<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:25<00:05,  3.46it/s] 97%|█████████▋| 568/585 [04:25<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:26<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:26<00:04,  3.47it/s] 98%|█████████▊| 571/585 [04:26<00:04,  3.47it/s] 98%|█████████▊| 572/585 [04:26<00:03,  3.47it/s] 98%|█████████▊| 573/585 [04:27<00:03,  3.47it/s] 98%|█████████▊| 574/585 [04:27<00:03,  3.47it/s] 98%|█████████▊| 575/585 [04:27<00:02,  3.47it/s] 98%|█████████▊| 576/585 [04:28<00:02,  3.47it/s] 99%|█████████▊| 577/585 [04:28<00:02,  3.47it/s] 99%|█████████▉| 578/585 [04:28<00:02,  3.47it/s] 99%|█████████▉| 579/585 [04:28<00:01,  3.47it/s] 99%|█████████▉| 580/585 [04:29<00:01,  3.47it/s] 99%|█████████▉| 581/585 [04:29<00:01,  3.47it/s] 99%|█████████▉| 582/585 [04:29<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:30<00:00,  3.46it/s]100%|█████████▉| 584/585 [04:30<00:00,  3.46it/s]100%|██████████| 585/585 [04:30<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 05:45:43,484 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:45:43,485 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:45:43,485 >>   Batch size = 8
{'eval_loss': 0.9203686714172363, 'eval_runtime': 16.7919, 'eval_samples_per_second': 372.383, 'eval_steps_per_second': 46.57, 'epoch': 4.0}
{'loss': 0.672, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.43it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.62it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.76it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.13it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.72it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.46it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.22it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.88it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.68it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.74it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.72it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.75it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.84it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.81it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.81it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.86it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.65it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.61it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.63it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.66it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.68it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.67it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.71it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.81it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.81it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.73it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.63it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.66it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.61it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.70it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.72it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.67it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.78it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.79it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.70it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.71it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.68it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.65it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.57it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.60it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.55it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.51it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.54it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.55it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.65it/s][A
 30%|██▉       | 233/782 [00:04<00:12, 43.20it/s][A
 30%|███       | 238/782 [00:05<00:12, 44.31it/s][A
 31%|███       | 243/782 [00:05<00:11, 45.07it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 45.51it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 45.92it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.25it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.37it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.58it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.35it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.30it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.44it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.54it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.55it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.72it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.67it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.63it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.70it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.52it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.46it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.41it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.42it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.41it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.46it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.45it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.45it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.44it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 46.53it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.42it/s][A
 48%|████▊     | 373/782 [00:08<00:09, 43.48it/s][A
 48%|████▊     | 378/782 [00:08<00:09, 44.39it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 45.15it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 45.59it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.07it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.14it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.44it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.46it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.34it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.42it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.42it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.52it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.63it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.69it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.75it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.68it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.72it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.60it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.50it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.55it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.64it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.63it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.74it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.81it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.68it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.69it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.64it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.63it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.60it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.56it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.62it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.70it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.73it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.77it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.73it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.67it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.55it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.53it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.57it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.60it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.68it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.75it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.73it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.71it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.66it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.61it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.60it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.62it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.58it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.63it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.73it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.68it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.76it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.66it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.50it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.64it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.67it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.55it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.64it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.71it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.62it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.72it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.69it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.61it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.56it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.63it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.61it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.66it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.65it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.70it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.69it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.71it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.54it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.57it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.67it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.69it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.67it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.70it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.65it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.64it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.71it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.63it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:47<00:00,  3.46it/s]
100%|██████████| 782/782 [00:16<00:00, 46.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:46:00,322 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 05:46:00,351 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:46:02,519 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:46:02,539 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:46:02,549 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:46:07,009 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:46:07,014 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117 (score: 0.8962836861610413).
                                                 100%|██████████| 585/585 [04:55<00:00,  3.46it/s]100%|██████████| 585/585 [04:55<00:00,  1.98it/s]
[INFO|trainer.py:1894] 2023-08-29 05:46:08,722 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 05:46:08,752 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:46:11,032 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:46:11,047 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:46:11,058 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:46:11,213 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   train_loss               =     0.6674
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   train_runtime            = 0:04:55.90
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   train_samples            =       7519
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   train_samples_per_second =     127.05
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:11,213 >>   train_steps_per_second   =      1.977
{'eval_loss': 0.9250954985618591, 'eval_runtime': 16.8099, 'eval_samples_per_second': 371.984, 'eval_steps_per_second': 46.52, 'epoch': 5.0}
{'train_runtime': 295.9068, 'train_samples_per_second': 127.05, 'train_steps_per_second': 1.977, 'train_loss': 0.66744629297501, 'epoch': 5.0}
08/29/2023 05:46:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:46:11,252 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:46:11,252 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 05:46:11,252 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 57.01it/s]  2%|▏         | 12/782 [00:00<00:15, 50.85it/s]  2%|▏         | 18/782 [00:00<00:15, 49.02it/s]  3%|▎         | 23/782 [00:00<00:15, 48.42it/s]  4%|▎         | 28/782 [00:00<00:15, 48.01it/s]  4%|▍         | 33/782 [00:00<00:15, 47.75it/s]  5%|▍         | 38/782 [00:00<00:15, 47.51it/s]  5%|▌         | 43/782 [00:00<00:15, 47.31it/s]  6%|▌         | 48/782 [00:00<00:15, 47.10it/s]  7%|▋         | 53/782 [00:01<00:15, 47.06it/s]  7%|▋         | 58/782 [00:01<00:15, 47.06it/s]  8%|▊         | 63/782 [00:01<00:15, 47.08it/s]  9%|▊         | 68/782 [00:01<00:15, 47.08it/s]  9%|▉         | 73/782 [00:01<00:15, 47.03it/s] 10%|▉         | 78/782 [00:01<00:14, 47.06it/s] 11%|█         | 83/782 [00:01<00:14, 47.12it/s] 11%|█▏        | 88/782 [00:01<00:14, 47.01it/s] 12%|█▏        | 93/782 [00:01<00:14, 47.02it/s] 13%|█▎        | 98/782 [00:02<00:14, 46.99it/s] 13%|█▎        | 103/782 [00:02<00:14, 46.93it/s] 14%|█▍        | 108/782 [00:02<00:14, 46.97it/s] 14%|█▍        | 113/782 [00:02<00:14, 47.00it/s] 15%|█▌        | 118/782 [00:02<00:14, 47.05it/s] 16%|█▌        | 123/782 [00:02<00:14, 47.05it/s] 16%|█▋        | 128/782 [00:02<00:13, 47.00it/s] 17%|█▋        | 133/782 [00:02<00:13, 47.03it/s] 18%|█▊        | 138/782 [00:02<00:13, 47.00it/s] 18%|█▊        | 143/782 [00:03<00:13, 46.99it/s] 19%|█▉        | 148/782 [00:03<00:13, 46.95it/s] 20%|█▉        | 153/782 [00:03<00:13, 46.97it/s] 20%|██        | 158/782 [00:03<00:13, 46.94it/s] 21%|██        | 163/782 [00:03<00:13, 46.99it/s] 21%|██▏       | 168/782 [00:03<00:13, 46.82it/s] 22%|██▏       | 173/782 [00:03<00:12, 47.10it/s] 23%|██▎       | 178/782 [00:03<00:12, 47.10it/s] 23%|██▎       | 183/782 [00:03<00:12, 46.97it/s] 24%|██▍       | 188/782 [00:03<00:12, 46.95it/s] 25%|██▍       | 193/782 [00:04<00:12, 46.95it/s] 25%|██▌       | 198/782 [00:04<00:12, 47.00it/s] 26%|██▌       | 203/782 [00:04<00:12, 46.94it/s] 27%|██▋       | 208/782 [00:04<00:12, 46.95it/s] 27%|██▋       | 213/782 [00:04<00:12, 46.96it/s] 28%|██▊       | 218/782 [00:04<00:12, 46.98it/s] 29%|██▊       | 223/782 [00:04<00:11, 46.99it/s] 29%|██▉       | 228/782 [00:04<00:11, 47.03it/s] 30%|██▉       | 233/782 [00:04<00:11, 46.96it/s] 30%|███       | 238/782 [00:05<00:11, 46.91it/s] 31%|███       | 243/782 [00:05<00:11, 46.90it/s] 32%|███▏      | 248/782 [00:05<00:11, 46.90it/s] 32%|███▏      | 253/782 [00:05<00:11, 46.96it/s] 33%|███▎      | 258/782 [00:05<00:11, 46.94it/s] 34%|███▎      | 263/782 [00:05<00:11, 46.97it/s] 34%|███▍      | 268/782 [00:05<00:10, 46.97it/s] 35%|███▍      | 273/782 [00:05<00:10, 46.91it/s] 36%|███▌      | 278/782 [00:05<00:10, 46.95it/s] 36%|███▌      | 283/782 [00:05<00:10, 46.99it/s] 37%|███▋      | 288/782 [00:06<00:10, 46.92it/s] 37%|███▋      | 293/782 [00:06<00:10, 46.94it/s] 38%|███▊      | 298/782 [00:06<00:10, 46.90it/s] 39%|███▊      | 303/782 [00:06<00:10, 46.86it/s] 39%|███▉      | 308/782 [00:06<00:10, 46.88it/s] 40%|████      | 313/782 [00:06<00:09, 46.92it/s] 41%|████      | 318/782 [00:06<00:09, 46.90it/s] 41%|████▏     | 323/782 [00:06<00:09, 46.98it/s] 42%|████▏     | 328/782 [00:06<00:09, 46.93it/s] 43%|████▎     | 333/782 [00:07<00:09, 46.81it/s] 43%|████▎     | 338/782 [00:07<00:09, 46.86it/s] 44%|████▍     | 343/782 [00:07<00:09, 46.79it/s] 45%|████▍     | 348/782 [00:07<00:09, 46.83it/s] 45%|████▌     | 353/782 [00:07<00:09, 46.90it/s] 46%|████▌     | 358/782 [00:07<00:09, 46.85it/s] 46%|████▋     | 363/782 [00:07<00:08, 46.80it/s] 47%|████▋     | 368/782 [00:07<00:08, 46.84it/s] 48%|████▊     | 373/782 [00:07<00:08, 46.89it/s] 48%|████▊     | 378/782 [00:08<00:08, 46.93it/s] 49%|████▉     | 383/782 [00:08<00:08, 46.94it/s] 50%|████▉     | 388/782 [00:08<00:08, 46.85it/s] 50%|█████     | 393/782 [00:08<00:08, 46.83it/s] 51%|█████     | 398/782 [00:08<00:08, 46.92it/s] 52%|█████▏    | 403/782 [00:08<00:08, 46.90it/s] 52%|█████▏    | 408/782 [00:08<00:07, 46.94it/s] 53%|█████▎    | 413/782 [00:08<00:07, 46.91it/s] 53%|█████▎    | 418/782 [00:08<00:07, 46.85it/s] 54%|█████▍    | 423/782 [00:08<00:07, 46.83it/s] 55%|█████▍    | 428/782 [00:09<00:07, 46.91it/s] 55%|█████▌    | 433/782 [00:09<00:07, 46.87it/s] 56%|█████▌    | 438/782 [00:09<00:07, 46.87it/s] 57%|█████▋    | 443/782 [00:09<00:07, 46.92it/s] 57%|█████▋    | 448/782 [00:09<00:07, 46.82it/s] 58%|█████▊    | 453/782 [00:09<00:07, 46.86it/s] 59%|█████▊    | 458/782 [00:09<00:06, 46.93it/s] 59%|█████▉    | 463/782 [00:09<00:06, 46.89it/s] 60%|█████▉    | 468/782 [00:09<00:06, 46.96it/s] 60%|██████    | 473/782 [00:10<00:06, 46.94it/s] 61%|██████    | 478/782 [00:10<00:06, 46.84it/s] 62%|██████▏   | 483/782 [00:10<00:06, 46.80it/s] 62%|██████▏   | 488/782 [00:10<00:06, 46.91it/s] 63%|██████▎   | 493/782 [00:10<00:06, 46.87it/s] 64%|██████▎   | 498/782 [00:10<00:06, 46.87it/s] 64%|██████▍   | 503/782 [00:10<00:05, 46.84it/s] 65%|██████▍   | 508/782 [00:10<00:05, 46.82it/s] 66%|██████▌   | 513/782 [00:10<00:05, 46.90it/s] 66%|██████▌   | 518/782 [00:11<00:05, 46.94it/s] 67%|██████▋   | 523/782 [00:11<00:05, 46.89it/s] 68%|██████▊   | 528/782 [00:11<00:05, 46.86it/s] 68%|██████▊   | 533/782 [00:11<00:05, 46.79it/s] 69%|██████▉   | 538/782 [00:11<00:05, 46.83it/s] 69%|██████▉   | 543/782 [00:11<00:05, 46.91it/s] 70%|███████   | 548/782 [00:11<00:04, 46.82it/s] 71%|███████   | 553/782 [00:11<00:04, 46.82it/s] 71%|███████▏  | 558/782 [00:11<00:04, 46.90it/s] 72%|███████▏  | 563/782 [00:11<00:04, 46.86it/s] 73%|███████▎  | 568/782 [00:12<00:04, 46.86it/s] 73%|███████▎  | 573/782 [00:12<00:04, 46.85it/s] 74%|███████▍  | 578/782 [00:12<00:04, 46.86it/s] 75%|███████▍  | 583/782 [00:12<00:04, 46.88it/s] 75%|███████▌  | 588/782 [00:12<00:04, 46.94it/s] 76%|███████▌  | 593/782 [00:12<00:04, 46.87it/s] 76%|███████▋  | 598/782 [00:12<00:03, 46.77it/s] 77%|███████▋  | 603/782 [00:12<00:03, 46.85it/s] 78%|███████▊  | 608/782 [00:12<00:03, 46.84it/s] 78%|███████▊  | 613/782 [00:13<00:03, 46.87it/s] 79%|███████▉  | 618/782 [00:13<00:03, 46.87it/s] 80%|███████▉  | 623/782 [00:13<00:03, 46.81it/s] 80%|████████  | 628/782 [00:13<00:03, 46.82it/s] 81%|████████  | 633/782 [00:13<00:03, 46.86it/s] 82%|████████▏ | 638/782 [00:13<00:03, 46.89it/s] 82%|████████▏ | 643/782 [00:13<00:02, 46.86it/s] 83%|████████▎ | 648/782 [00:13<00:02, 46.81it/s] 84%|████████▎ | 653/782 [00:13<00:02, 46.80it/s] 84%|████████▍ | 658/782 [00:14<00:02, 46.86it/s] 85%|████████▍ | 663/782 [00:14<00:02, 46.90it/s] 85%|████████▌ | 668/782 [00:14<00:02, 46.88it/s] 86%|████████▌ | 673/782 [00:14<00:02, 46.83it/s] 87%|████████▋ | 678/782 [00:14<00:02, 46.83it/s] 87%|████████▋ | 683/782 [00:14<00:02, 46.86it/s] 88%|████████▊ | 688/782 [00:14<00:02, 46.93it/s] 89%|████████▊ | 693/782 [00:14<00:01, 46.93it/s] 89%|████████▉ | 698/782 [00:14<00:01, 46.77it/s] 90%|████████▉ | 703/782 [00:14<00:01, 46.79it/s] 91%|█████████ | 708/782 [00:15<00:01, 46.83it/s] 91%|█████████ | 713/782 [00:15<00:01, 46.83it/s] 92%|█████████▏| 718/782 [00:15<00:01, 46.90it/s] 92%|█████████▏| 723/782 [00:15<00:01, 46.88it/s] 93%|█████████▎| 728/782 [00:15<00:01, 46.78it/s] 94%|█████████▎| 733/782 [00:15<00:01, 46.83it/s] 94%|█████████▍| 738/782 [00:15<00:00, 46.86it/s] 95%|█████████▌| 743/782 [00:15<00:00, 46.82it/s] 96%|█████████▌| 748/782 [00:15<00:00, 46.82it/s] 96%|█████████▋| 753/782 [00:16<00:00, 46.80it/s] 97%|█████████▋| 758/782 [00:16<00:00, 46.83it/s] 98%|█████████▊| 763/782 [00:16<00:00, 46.86it/s] 98%|█████████▊| 768/782 [00:16<00:00, 46.93it/s] 99%|█████████▉| 773/782 [00:16<00:00, 46.82it/s] 99%|█████████▉| 778/782 [00:16<00:00, 46.79it/s]100%|██████████| 782/782 [00:16<00:00, 46.96it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:46:27,920 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   eval_loss               =     0.8963
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   eval_runtime            = 0:00:16.66
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   eval_samples_per_second =    375.171
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   eval_steps_per_second   =     46.919
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:46:27,920 >>   perplexity              =     2.4505
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:33,868 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:33,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:33,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:33,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:33,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:46:34,160 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:46:34,161 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:46:34,424 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:46:35,467 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:46:35,467 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:38,566 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:38,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:38,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:38,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:46:38,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:46:39,198 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:46:39,199 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:46:39,774 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:46:39,925 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:46:39,925 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.54it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.55it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:27,  1.53it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:38,  1.54it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.46it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:55,  1.47it/s]Extractor Predicting: 84it [00:55,  1.47it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [01:00,  1.48it/s]Extractor Predicting: 92it [01:01,  1.48it/s]Extractor Predicting: 93it [01:01,  1.49it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:04,  1.47it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:05,  1.46it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:07,  1.37it/s]Extractor Predicting: 102it [01:08,  1.41it/s]Extractor Predicting: 103it [01:08,  1.44it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:12,  1.45it/s]Extractor Predicting: 110it [01:13,  1.47it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:15,  1.51it/s]Extractor Predicting: 114it [01:16,  1.51it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.49it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:25,  1.47it/s]Extractor Predicting: 129it [01:26,  1.47it/s]Extractor Predicting: 130it [01:27,  1.45it/s]Extractor Predicting: 131it [01:27,  1.46it/s]Extractor Predicting: 132it [01:28,  1.44it/s]Extractor Predicting: 133it [01:29,  1.46it/s]Extractor Predicting: 134it [01:29,  1.45it/s]Extractor Predicting: 135it [01:30,  1.45it/s]Extractor Predicting: 136it [01:31,  1.46it/s]Extractor Predicting: 137it [01:31,  1.46it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.48it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.50it/s]Extractor Predicting: 142it [01:35,  1.50it/s]Extractor Predicting: 143it [01:35,  1.47it/s]Extractor Predicting: 144it [01:36,  1.49it/s]Extractor Predicting: 145it [01:37,  1.49it/s]Extractor Predicting: 146it [01:37,  1.50it/s]Extractor Predicting: 147it [01:38,  1.48it/s]Extractor Predicting: 148it [01:39,  1.53it/s]Extractor Predicting: 149it [01:39,  1.52it/s]Extractor Predicting: 150it [01:40,  1.51it/s]Extractor Predicting: 151it [01:41,  1.48it/s]Extractor Predicting: 152it [01:41,  1.52it/s]Extractor Predicting: 153it [01:42,  1.50it/s]Extractor Predicting: 154it [01:43,  1.50it/s]Extractor Predicting: 155it [01:43,  1.51it/s]Extractor Predicting: 156it [01:44,  1.50it/s]Extractor Predicting: 157it [01:45,  1.55it/s]Extractor Predicting: 158it [01:45,  1.54it/s]Extractor Predicting: 159it [01:46,  1.55it/s]Extractor Predicting: 160it [01:47,  1.50it/s]Extractor Predicting: 161it [01:47,  1.49it/s]Extractor Predicting: 162it [01:48,  1.51it/s]Extractor Predicting: 163it [01:49,  1.53it/s]Extractor Predicting: 164it [01:49,  1.55it/s]Extractor Predicting: 165it [01:50,  1.59it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:51,  1.57it/s]Extractor Predicting: 168it [01:52,  1.54it/s]Extractor Predicting: 169it [01:52,  1.56it/s]Extractor Predicting: 170it [01:53,  1.57it/s]Extractor Predicting: 171it [01:54,  1.55it/s]Extractor Predicting: 172it [01:54,  1.53it/s]Extractor Predicting: 173it [01:55,  1.56it/s]Extractor Predicting: 174it [01:56,  1.38it/s]Extractor Predicting: 175it [01:56,  1.43it/s]Extractor Predicting: 176it [01:57,  1.46it/s]Extractor Predicting: 177it [01:58,  1.48it/s]Extractor Predicting: 178it [01:58,  1.49it/s]Extractor Predicting: 179it [01:59,  1.51it/s]Extractor Predicting: 180it [02:00,  1.51it/s]Extractor Predicting: 181it [02:00,  1.54it/s]Extractor Predicting: 182it [02:01,  1.52it/s]Extractor Predicting: 183it [02:02,  1.54it/s]Extractor Predicting: 184it [02:02,  1.55it/s]Extractor Predicting: 185it [02:03,  1.54it/s]Extractor Predicting: 186it [02:04,  1.51it/s]Extractor Predicting: 187it [02:04,  1.55it/s]Extractor Predicting: 188it [02:05,  1.52it/s]Extractor Predicting: 189it [02:06,  1.54it/s]Extractor Predicting: 190it [02:06,  1.56it/s]Extractor Predicting: 191it [02:07,  1.58it/s]Extractor Predicting: 192it [02:07,  1.58it/s]Extractor Predicting: 193it [02:08,  1.57it/s]Extractor Predicting: 194it [02:09,  1.57it/s]Extractor Predicting: 195it [02:09,  1.57it/s]Extractor Predicting: 196it [02:10,  1.54it/s]Extractor Predicting: 197it [02:11,  1.57it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:12,  1.53it/s]Extractor Predicting: 200it [02:13,  1.59it/s]Extractor Predicting: 201it [02:13,  1.60it/s]Extractor Predicting: 202it [02:14,  1.60it/s]Extractor Predicting: 203it [02:14,  1.61it/s]Extractor Predicting: 204it [02:15,  1.60it/s]Extractor Predicting: 205it [02:16,  1.58it/s]Extractor Predicting: 206it [02:16,  1.61it/s]Extractor Predicting: 207it [02:17,  1.62it/s]Extractor Predicting: 208it [02:18,  1.61it/s]Extractor Predicting: 209it [02:18,  1.58it/s]Extractor Predicting: 210it [02:19,  1.59it/s]Extractor Predicting: 211it [02:19,  1.60it/s]Extractor Predicting: 212it [02:20,  1.60it/s]Extractor Predicting: 213it [02:21,  1.58it/s]Extractor Predicting: 214it [02:21,  1.60it/s]Extractor Predicting: 215it [02:22,  1.58it/s]Extractor Predicting: 216it [02:23,  1.57it/s]Extractor Predicting: 217it [02:23,  1.57it/s]Extractor Predicting: 218it [02:24,  1.58it/s]Extractor Predicting: 219it [02:25,  1.56it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:26,  1.57it/s]Extractor Predicting: 222it [02:26,  1.56it/s]Extractor Predicting: 223it [02:27,  1.57it/s]Extractor Predicting: 224it [02:28,  1.57it/s]Extractor Predicting: 225it [02:28,  1.56it/s]Extractor Predicting: 226it [02:29,  1.56it/s]Extractor Predicting: 227it [02:30,  1.58it/s]Extractor Predicting: 228it [02:30,  1.74it/s]Extractor Predicting: 228it [02:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:19,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:19,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:19,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:19,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:19,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:49:20,045 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:49:20,046 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:49:20,607 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:49:21,657 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:49:21,657 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:24,685 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:24,691 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:24,692 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:24,692 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:49:24,692 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:49:25,332 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:49:25,333 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:49:25,925 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:49:26,071 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:49:26,071 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.07733619763694952,
  "recall": 0.023028946105869183,
  "score": 0.03548983364140481,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.47it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:24,  1.45it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.45it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:28,  1.45it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.44it/s]Extractor Predicting: 46it [00:30,  1.48it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:32,  1.60it/s]Extractor Predicting: 49it [00:32,  1.63it/s]Extractor Predicting: 50it [00:33,  1.58it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.53it/s]Extractor Predicting: 60it [00:39,  1.54it/s]Extractor Predicting: 61it [00:40,  1.57it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.56it/s]Extractor Predicting: 65it [00:43,  1.59it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:46,  1.59it/s]Extractor Predicting: 72it [00:47,  1.59it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:48,  1.56it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.43it/s]Extractor Predicting: 86it [00:56,  1.47it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.56it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:01,  1.55it/s]Extractor Predicting: 95it [01:02,  1.56it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:03,  1.55it/s]Extractor Predicting: 98it [01:04,  1.55it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:05,  1.54it/s]Extractor Predicting: 101it [01:06,  1.55it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:07,  1.56it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:12,  1.56it/s]Extractor Predicting: 112it [01:13,  1.58it/s]Extractor Predicting: 113it [01:14,  1.59it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.56it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:17,  1.63it/s]Extractor Predicting: 120it [01:18,  1.65it/s]Extractor Predicting: 121it [01:19,  1.64it/s]Extractor Predicting: 122it [01:19,  1.65it/s]Extractor Predicting: 123it [01:20,  1.65it/s]Extractor Predicting: 124it [01:20,  1.66it/s]Extractor Predicting: 125it [01:21,  1.60it/s]Extractor Predicting: 126it [01:22,  1.63it/s]Extractor Predicting: 127it [01:22,  1.59it/s]Extractor Predicting: 128it [01:23,  1.55it/s]Extractor Predicting: 129it [01:24,  1.56it/s]Extractor Predicting: 130it [01:24,  1.59it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.60it/s]Extractor Predicting: 133it [01:26,  1.60it/s]Extractor Predicting: 134it [01:27,  1.59it/s]Extractor Predicting: 135it [01:27,  1.61it/s]Extractor Predicting: 136it [01:28,  1.63it/s]Extractor Predicting: 137it [01:29,  1.60it/s]Extractor Predicting: 138it [01:29,  1.57it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:31,  1.62it/s]Extractor Predicting: 142it [01:32,  1.66it/s]Extractor Predicting: 143it [01:32,  1.64it/s]Extractor Predicting: 144it [01:33,  1.66it/s]Extractor Predicting: 145it [01:34,  1.63it/s]Extractor Predicting: 146it [01:34,  1.61it/s]Extractor Predicting: 147it [01:35,  1.63it/s]Extractor Predicting: 148it [01:35,  1.63it/s]Extractor Predicting: 149it [01:36,  1.57it/s]Extractor Predicting: 150it [01:37,  1.53it/s]Extractor Predicting: 151it [01:37,  1.55it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:39,  1.59it/s]Extractor Predicting: 154it [01:39,  1.56it/s]Extractor Predicting: 155it [01:40,  1.51it/s]Extractor Predicting: 156it [01:41,  1.52it/s]Extractor Predicting: 157it [01:41,  1.50it/s]Extractor Predicting: 158it [01:42,  1.54it/s]Extractor Predicting: 159it [01:43,  1.55it/s]Extractor Predicting: 160it [01:43,  1.57it/s]Extractor Predicting: 161it [01:44,  1.59it/s]Extractor Predicting: 162it [01:44,  1.60it/s]Extractor Predicting: 163it [01:45,  1.58it/s]Extractor Predicting: 164it [01:46,  1.60it/s]Extractor Predicting: 165it [01:47,  1.43it/s]Extractor Predicting: 166it [01:47,  1.49it/s]Extractor Predicting: 167it [01:48,  1.49it/s]Extractor Predicting: 168it [01:48,  1.51it/s]Extractor Predicting: 169it [01:49,  1.54it/s]Extractor Predicting: 170it [01:50,  1.53it/s]Extractor Predicting: 171it [01:50,  1.53it/s]Extractor Predicting: 172it [01:51,  1.53it/s]Extractor Predicting: 173it [01:52,  1.52it/s]Extractor Predicting: 174it [01:52,  1.53it/s]Extractor Predicting: 175it [01:53,  1.53it/s]Extractor Predicting: 176it [01:54,  1.53it/s]Extractor Predicting: 177it [01:54,  1.52it/s]Extractor Predicting: 178it [01:55,  1.49it/s]Extractor Predicting: 179it [01:56,  1.53it/s]Extractor Predicting: 180it [01:56,  1.54it/s]Extractor Predicting: 181it [01:57,  1.57it/s]Extractor Predicting: 182it [01:58,  1.60it/s]Extractor Predicting: 183it [01:58,  1.59it/s]Extractor Predicting: 184it [01:59,  1.60it/s]Extractor Predicting: 185it [01:59,  1.60it/s]Extractor Predicting: 186it [02:00,  1.58it/s]Extractor Predicting: 187it [02:01,  1.61it/s]Extractor Predicting: 188it [02:01,  1.61it/s]Extractor Predicting: 189it [02:02,  1.56it/s]Extractor Predicting: 190it [02:03,  1.50it/s]Extractor Predicting: 191it [02:03,  1.52it/s]Extractor Predicting: 192it [02:04,  1.50it/s]Extractor Predicting: 193it [02:05,  1.51it/s]Extractor Predicting: 194it [02:05,  1.51it/s]Extractor Predicting: 195it [02:06,  1.49it/s]Extractor Predicting: 196it [02:07,  1.47it/s]Extractor Predicting: 197it [02:07,  1.49it/s]Extractor Predicting: 198it [02:08,  1.51it/s]Extractor Predicting: 199it [02:09,  1.50it/s]Extractor Predicting: 200it [02:09,  1.49it/s]Extractor Predicting: 201it [02:10,  1.49it/s]Extractor Predicting: 202it [02:11,  1.48it/s]Extractor Predicting: 203it [02:11,  1.49it/s]Extractor Predicting: 204it [02:12,  1.48it/s]Extractor Predicting: 205it [02:13,  1.47it/s]Extractor Predicting: 206it [02:13,  1.48it/s]Extractor Predicting: 207it [02:14,  1.44it/s]Extractor Predicting: 208it [02:15,  1.44it/s]Extractor Predicting: 209it [02:15,  1.50it/s]Extractor Predicting: 210it [02:16,  1.52it/s]Extractor Predicting: 211it [02:17,  1.52it/s]Extractor Predicting: 212it [02:17,  1.54it/s]Extractor Predicting: 213it [02:18,  1.53it/s]Extractor Predicting: 214it [02:19,  1.50it/s]Extractor Predicting: 215it [02:19,  1.50it/s]Extractor Predicting: 216it [02:20,  1.49it/s]Extractor Predicting: 217it [02:21,  1.46it/s]Extractor Predicting: 218it [02:21,  1.50it/s]Extractor Predicting: 219it [02:22,  1.51it/s]Extractor Predicting: 220it [02:23,  1.51it/s]Extractor Predicting: 221it [02:23,  1.52it/s]Extractor Predicting: 222it [02:24,  1.48it/s]Extractor Predicting: 223it [02:25,  1.46it/s]Extractor Predicting: 224it [02:26,  1.44it/s]Extractor Predicting: 225it [02:26,  1.42it/s]Extractor Predicting: 226it [02:27,  1.43it/s]Extractor Predicting: 227it [02:28,  1.39it/s]Extractor Predicting: 228it [02:28,  1.39it/s]Extractor Predicting: 229it [02:29,  1.39it/s]Extractor Predicting: 230it [02:30,  1.41it/s]Extractor Predicting: 231it [02:31,  1.41it/s]Extractor Predicting: 232it [02:31,  1.42it/s]Extractor Predicting: 233it [02:32,  1.42it/s]Extractor Predicting: 234it [02:33,  1.43it/s]Extractor Predicting: 235it [02:33,  1.44it/s]Extractor Predicting: 236it [02:34,  1.43it/s]Extractor Predicting: 237it [02:35,  1.41it/s]Extractor Predicting: 238it [02:36,  1.40it/s]Extractor Predicting: 239it [02:36,  1.42it/s]Extractor Predicting: 240it [02:37,  1.46it/s]Extractor Predicting: 241it [02:38,  1.46it/s]Extractor Predicting: 242it [02:38,  1.47it/s]Extractor Predicting: 243it [02:39,  1.48it/s]Extractor Predicting: 244it [02:40,  1.45it/s]Extractor Predicting: 245it [02:40,  1.50it/s]Extractor Predicting: 246it [02:41,  1.53it/s]Extractor Predicting: 246it [02:41,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:14,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:14,148 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:14,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:14,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:14,149 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:52:14,588 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:52:14,589 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:52:14,845 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:52:15,880 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:52:15,880 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:17,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:17,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:17,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:17,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:52:17,586 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:52:17,908 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:52:17,909 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:52:18,179 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:52:18,329 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:52:18,329 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.36222692036645526,
  "recall": 0.08711864406779661,
  "score": 0.14045634649542285,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 15it [00:09,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:52:28,732 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:52:28,733 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:52:28,737 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:52:28,737 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:52:28,739 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:52:31,656 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:52:31,661 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:52:31,671 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:52:31,672 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:52:31,676 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:52:31,680 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.43209876543209874,
  "recall": 0.04915730337078652,
  "score": 0.08827238335435057,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:52:31,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:32,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:33,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:34,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:34,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:35,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:36,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:36,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:37,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:38,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:38,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:39,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:40,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:41,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:43,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:44,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:44,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:45,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:46,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:46,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:39, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-29 05:52:47,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:48,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:48,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:49,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:50,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:50,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:51,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:52,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:52,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:53,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:54,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:54,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:55,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:56,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:56,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:57,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:58,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:58,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:52:59,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:00,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:06, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:01,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:01,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:02,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:03,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:04,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:04,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:05,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:06,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:07,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:08,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:08,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:09,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:10,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:11,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:11,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:12,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:13,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:14,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:15,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:16,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:06, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:18,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:18,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:19,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:20,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:20,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:21,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:22,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:22,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:23,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:24,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:24,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:25,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:26,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:27,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:27,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:28,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:29,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:29,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:30,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:31,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:31,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:00<02:46, 15.14s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:32,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:33,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:33,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:34,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:34,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:35,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:36,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:36,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:37,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:37,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:38,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:39,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:39,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:40,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:41,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:41,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:42,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:42,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:43,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:44,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:44,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:45,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:14<02:25, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-29 05:53:46,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:46,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:47,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:48,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:48,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:49,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:50,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:50,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:51,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:52,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:53,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:53,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:54,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:55,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:55,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:56,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:57,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:58,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:58,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:53:59,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:00,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:28<02:11, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:00,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:01,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:02,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:02,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:03,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:04,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:04,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:05,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:05,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:06,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:07,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:07,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:08,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:08,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:09,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:10,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:10,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:11,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:12,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:12,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:13,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:53, 14.18s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:14,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:14,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:15,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:16,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:17,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:17,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:18,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:19,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:20,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:20,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:21,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:22,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:22,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:23,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:24,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:24,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:26,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:27,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:28,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:28,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:57<01:41, 14.57s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:29,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:30,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:31,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:31,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:32,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:33,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:34,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:34,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:35,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:36,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:36,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:37,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:38,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:39,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:39,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:40,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:41,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:42,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:43,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:44,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:45,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:46,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:47,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:47,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:16<01:35, 15.99s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:48,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:49,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:50,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:50,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:51,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:52,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:53,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:54,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:54,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:55,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:56,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:57,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:57,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:58,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:59,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:00,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:01,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:02,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:03,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:04,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:05,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:34<01:22, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:06,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:06,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:07,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:08,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:09,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:09,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:10,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:11,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:12,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:12,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:13,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:14,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:14,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:15,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:16,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:16,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:17,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:18,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:19,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:19,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:20,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:21,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:22,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:50<01:06, 16.52s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:22,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:23,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:24,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:24,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:25,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:26,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:27,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:27,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:28,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:29,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:29,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:30,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:31,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:31,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:32,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:33,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:33,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:34,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:35,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:36,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:36,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:05<00:48, 16.05s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:37,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:38,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:39,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:39,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:40,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:41,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:41,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:42,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:43,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:44,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:44,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:45,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:46,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:46,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:47,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:48,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:48,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:49,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:50,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:50,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:51,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:52,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:21<00:31, 15.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:53,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:54,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:55,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:56,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:56,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:57,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:58,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:59,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:59,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:00,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:01,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:02,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:03,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:03,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:05,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:05,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:06,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:07,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:08,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:09,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:09,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:10,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:11,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:40<00:16, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-29 05:56:12,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:13,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:13,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:14,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:15,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:16,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:16,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:17,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:18,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:19,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:19,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:20,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:21,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:21,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:22,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:23,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:23,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:24,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:25,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:26,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:26,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:27,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:56<00:00, 16.68s/it]Generating: 100%|██████████| 15/15 [03:56<00:00, 15.79s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:34,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:34,429 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:34,429 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:34,429 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:34,429 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:56:34,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:56:34,746 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:56:35,017 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:56:36,086 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:56:36,086 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:37,475 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:37,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:37,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:37,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:56:37,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:56:38,218 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:56:38,219 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:56:38,488 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:56:38,656 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:56:38,656 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9166666666666666, 'errors': {'', "('Cincinnati Reds', 'drafted by', '', 'He was the third quarterback in MLB history to play in all of their seasons with the Pirates , and the 11th quarterback in MLB history to record all 10 seasons with the Cincinnati Reds .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8650568181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : screenwriter . Context : Later in 2008 , he appeared in The New Yorker magazine s All My Children . Head Entity : All My Children , Tail Entity : All Your Children .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8920454545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : voice type .', 'success_rate': 0.8192934782608695, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 11433
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11533, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.51it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:05,  1.63it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.64it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.54it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:12,  1.58it/s]Extractor Estimating: 21it [00:13,  1.58it/s]Extractor Estimating: 22it [00:13,  1.54it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.60it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:18,  1.62it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.62it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:21,  1.60it/s]Extractor Estimating: 35it [00:22,  1.60it/s]Extractor Estimating: 36it [00:22,  1.64it/s]Extractor Estimating: 37it [00:23,  1.59it/s]Extractor Estimating: 38it [00:23,  1.61it/s]Extractor Estimating: 39it [00:24,  1.62it/s]Extractor Estimating: 40it [00:25,  1.63it/s]Extractor Estimating: 41it [00:25,  1.59it/s]Extractor Estimating: 42it [00:26,  1.60it/s]Extractor Estimating: 43it [00:26,  1.63it/s]Extractor Estimating: 44it [00:27,  1.62it/s]Extractor Estimating: 45it [00:28,  1.61it/s]Extractor Estimating: 46it [00:28,  1.65it/s]Extractor Estimating: 47it [00:29,  1.65it/s]Extractor Estimating: 48it [00:29,  1.63it/s]Extractor Estimating: 49it [00:30,  1.58it/s]Extractor Estimating: 50it [00:31,  1.60it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:32,  1.60it/s]Extractor Estimating: 53it [00:33,  1.60it/s]Extractor Estimating: 54it [00:33,  1.63it/s]Extractor Estimating: 55it [00:34,  1.63it/s]Extractor Estimating: 56it [00:34,  1.63it/s]Extractor Estimating: 57it [00:35,  1.61it/s]Extractor Estimating: 58it [00:36,  1.50it/s]Extractor Estimating: 59it [00:37,  1.51it/s]Extractor Estimating: 60it [00:37,  1.48it/s]Extractor Estimating: 61it [00:38,  1.52it/s]Extractor Estimating: 62it [00:39,  1.54it/s]Extractor Estimating: 63it [00:39,  1.54it/s]Extractor Estimating: 64it [00:40,  1.54it/s]Extractor Estimating: 65it [00:40,  1.60it/s]Extractor Estimating: 66it [00:41,  1.59it/s]Extractor Estimating: 67it [00:42,  1.62it/s]Extractor Estimating: 68it [00:42,  1.60it/s]Extractor Estimating: 69it [00:43,  1.63it/s]Extractor Estimating: 70it [00:44,  1.56it/s]Extractor Estimating: 71it [00:44,  1.59it/s]Extractor Estimating: 72it [00:45,  1.59it/s]Extractor Estimating: 73it [00:45,  1.58it/s]Extractor Estimating: 74it [00:46,  1.59it/s]Extractor Estimating: 75it [00:47,  1.63it/s]Extractor Estimating: 76it [00:47,  1.65it/s]Extractor Estimating: 77it [00:48,  1.63it/s]Extractor Estimating: 78it [00:48,  1.59it/s]Extractor Estimating: 79it [00:49,  1.57it/s]Extractor Estimating: 80it [00:50,  1.61it/s]Extractor Estimating: 81it [00:50,  1.64it/s]Extractor Estimating: 82it [00:51,  1.65it/s]Extractor Estimating: 83it [00:52,  1.62it/s]Extractor Estimating: 84it [00:52,  1.64it/s]Extractor Estimating: 85it [00:53,  1.67it/s]Extractor Estimating: 86it [00:53,  1.65it/s]Extractor Estimating: 87it [00:54,  1.68it/s]Extractor Estimating: 88it [00:55,  1.63it/s]Extractor Estimating: 89it [00:55,  1.65it/s]Extractor Estimating: 90it [00:56,  1.50it/s]Extractor Estimating: 91it [00:57,  1.53it/s]Extractor Estimating: 92it [00:57,  1.57it/s]Extractor Estimating: 93it [00:58,  1.57it/s]Extractor Estimating: 94it [00:58,  1.58it/s]Extractor Estimating: 95it [00:59,  1.62it/s]Extractor Estimating: 96it [01:00,  1.65it/s]Extractor Estimating: 97it [01:00,  1.68it/s]Extractor Estimating: 98it [01:01,  1.66it/s]Extractor Estimating: 99it [01:01,  1.70it/s]Extractor Estimating: 100it [01:02,  1.66it/s]Extractor Estimating: 101it [01:03,  1.72it/s]Extractor Estimating: 102it [01:03,  1.74it/s]Extractor Estimating: 103it [01:04,  1.80it/s]Extractor Estimating: 104it [01:04,  1.80it/s]Extractor Estimating: 105it [01:05,  1.83it/s]Extractor Estimating: 106it [01:05,  1.88it/s]Extractor Estimating: 107it [01:06,  1.90it/s]Extractor Estimating: 108it [01:06,  1.94it/s]Extractor Estimating: 109it [01:07,  1.97it/s]Extractor Estimating: 110it [01:07,  1.98it/s]Extractor Estimating: 111it [01:08,  1.86it/s]Extractor Estimating: 112it [01:08,  1.90it/s]Extractor Estimating: 113it [01:09,  1.90it/s]Extractor Estimating: 114it [01:09,  1.90it/s]Extractor Estimating: 115it [01:10,  1.87it/s]Extractor Estimating: 116it [01:10,  1.87it/s]Extractor Estimating: 117it [01:11,  1.84it/s]Extractor Estimating: 118it [01:12,  1.82it/s]Extractor Estimating: 119it [01:12,  1.73it/s]Extractor Estimating: 120it [01:13,  1.72it/s]Extractor Estimating: 121it [01:13,  1.75it/s]Extractor Estimating: 122it [01:14,  1.79it/s]Extractor Estimating: 123it [01:14,  1.78it/s]Extractor Estimating: 124it [01:15,  1.81it/s]Extractor Estimating: 125it [01:16,  1.71it/s]Extractor Estimating: 126it [01:16,  1.71it/s]Extractor Estimating: 127it [01:17,  1.69it/s]Extractor Estimating: 128it [01:17,  1.71it/s]Extractor Estimating: 129it [01:18,  1.67it/s]Extractor Estimating: 130it [01:19,  1.62it/s]Extractor Estimating: 131it [01:19,  1.64it/s]Extractor Estimating: 132it [01:20,  1.61it/s]Extractor Estimating: 133it [01:20,  1.66it/s]Extractor Estimating: 134it [01:21,  1.64it/s]Extractor Estimating: 135it [01:22,  1.62it/s]Extractor Estimating: 136it [01:22,  1.62it/s]Extractor Estimating: 137it [01:23,  1.58it/s]Extractor Estimating: 138it [01:24,  1.60it/s]Extractor Estimating: 139it [01:24,  1.63it/s]Extractor Estimating: 140it [01:25,  1.57it/s]Extractor Estimating: 141it [01:26,  1.56it/s]Extractor Estimating: 142it [01:26,  1.51it/s]Extractor Estimating: 143it [01:27,  1.54it/s]Extractor Estimating: 144it [01:27,  1.58it/s]Extractor Estimating: 145it [01:28,  1.56it/s]Extractor Estimating: 146it [01:29,  1.63it/s]Extractor Estimating: 147it [01:29,  1.62it/s]Extractor Estimating: 148it [01:30,  1.61it/s]Extractor Estimating: 149it [01:31,  1.62it/s]Extractor Estimating: 150it [01:31,  1.60it/s]Extractor Estimating: 151it [01:32,  1.63it/s]Extractor Estimating: 152it [01:32,  1.59it/s]Extractor Estimating: 153it [01:33,  1.63it/s]Extractor Estimating: 154it [01:34,  1.68it/s]Extractor Estimating: 155it [01:34,  1.65it/s]Extractor Estimating: 156it [01:35,  1.69it/s]Extractor Estimating: 157it [01:35,  1.68it/s]Extractor Estimating: 158it [01:36,  1.55it/s]Extractor Estimating: 159it [01:37,  1.58it/s]Extractor Estimating: 160it [01:37,  1.56it/s]Extractor Estimating: 161it [01:38,  1.57it/s]Extractor Estimating: 162it [01:39,  1.55it/s]Extractor Estimating: 163it [01:39,  1.58it/s]Extractor Estimating: 164it [01:40,  1.58it/s]Extractor Estimating: 165it [01:41,  1.59it/s]Extractor Estimating: 166it [01:41,  1.62it/s]Extractor Estimating: 167it [01:42,  1.64it/s]Extractor Estimating: 168it [01:42,  1.62it/s]Extractor Estimating: 169it [01:43,  1.64it/s]Extractor Estimating: 170it [01:44,  1.64it/s]Extractor Estimating: 171it [01:44,  1.63it/s]Extractor Estimating: 172it [01:45,  1.62it/s]Extractor Estimating: 173it [01:45,  1.63it/s]Extractor Estimating: 174it [01:46,  1.66it/s]Extractor Estimating: 175it [01:47,  1.67it/s]Extractor Estimating: 176it [01:47,  1.59it/s]Extractor Estimating: 177it [01:48,  1.54it/s]Extractor Estimating: 178it [01:49,  1.57it/s]Extractor Estimating: 179it [01:49,  1.57it/s]Extractor Estimating: 180it [01:50,  1.54it/s]Extractor Estimating: 181it [01:51,  1.48it/s]Extractor Estimating: 182it [01:51,  1.53it/s]Extractor Estimating: 183it [01:52,  1.51it/s]Extractor Estimating: 184it [01:53,  1.49it/s]Extractor Estimating: 185it [01:53,  1.52it/s]Extractor Estimating: 186it [01:54,  1.53it/s]Extractor Estimating: 187it [01:55,  1.44it/s]Extractor Estimating: 188it [01:55,  1.45it/s]Extractor Estimating: 189it [01:56,  1.46it/s]Extractor Estimating: 190it [01:57,  1.48it/s]Extractor Estimating: 191it [01:57,  1.55it/s]Extractor Estimating: 192it [01:58,  1.53it/s]Extractor Estimating: 193it [01:59,  1.53it/s]Extractor Estimating: 194it [01:59,  1.50it/s]Extractor Estimating: 195it [02:00,  1.50it/s]Extractor Estimating: 196it [02:01,  1.55it/s]Extractor Estimating: 197it [02:01,  1.54it/s]Extractor Estimating: 198it [02:02,  1.58it/s]Extractor Estimating: 199it [02:02,  1.55it/s]Extractor Estimating: 200it [02:03,  1.54it/s]Extractor Estimating: 201it [02:04,  1.53it/s]Extractor Estimating: 202it [02:04,  1.58it/s]Extractor Estimating: 203it [02:05,  1.63it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:06,  1.57it/s]Extractor Estimating: 206it [02:07,  1.63it/s]Extractor Estimating: 207it [02:07,  1.66it/s]Extractor Estimating: 208it [02:08,  1.61it/s]Extractor Estimating: 209it [02:09,  1.62it/s]Extractor Estimating: 210it [02:09,  1.60it/s]Extractor Estimating: 211it [02:10,  1.58it/s]Extractor Estimating: 212it [02:11,  1.63it/s]Extractor Estimating: 213it [02:11,  1.61it/s]Extractor Estimating: 214it [02:12,  1.56it/s]Extractor Estimating: 215it [02:13,  1.53it/s]Extractor Estimating: 216it [02:13,  1.50it/s]Extractor Estimating: 217it [02:14,  1.48it/s]Extractor Estimating: 218it [02:15,  1.50it/s]Extractor Estimating: 219it [02:15,  1.51it/s]Extractor Estimating: 220it [02:16,  1.54it/s]Extractor Estimating: 221it [02:17,  1.52it/s]Extractor Estimating: 222it [02:17,  1.57it/s]Extractor Estimating: 223it [02:18,  1.46it/s]Extractor Estimating: 224it [02:19,  1.49it/s]Extractor Estimating: 225it [02:19,  1.49it/s]Extractor Estimating: 226it [02:20,  1.54it/s]Extractor Estimating: 227it [02:20,  1.55it/s]Extractor Estimating: 228it [02:21,  1.56it/s]Extractor Estimating: 229it [02:22,  1.53it/s]Extractor Estimating: 230it [02:22,  1.52it/s]Extractor Estimating: 231it [02:23,  1.51it/s]Extractor Estimating: 232it [02:24,  1.39it/s]Extractor Estimating: 233it [02:25,  1.42it/s]Extractor Estimating: 234it [02:25,  1.45it/s]Extractor Estimating: 235it [02:26,  1.47it/s]Extractor Estimating: 236it [02:27,  1.51it/s]Extractor Estimating: 237it [02:27,  1.54it/s]Extractor Estimating: 238it [02:28,  1.53it/s]Extractor Estimating: 239it [02:29,  1.52it/s]Extractor Estimating: 240it [02:29,  1.49it/s]Extractor Estimating: 241it [02:30,  1.42it/s]Extractor Estimating: 242it [02:31,  1.45it/s]Extractor Estimating: 243it [02:31,  1.41it/s]Extractor Estimating: 244it [02:32,  1.41it/s]Extractor Estimating: 245it [02:33,  1.44it/s]Extractor Estimating: 246it [02:33,  1.51it/s]Extractor Estimating: 247it [02:34,  1.52it/s]Extractor Estimating: 248it [02:35,  1.56it/s]Extractor Estimating: 249it [02:35,  1.55it/s]Extractor Estimating: 250it [02:36,  1.55it/s]Extractor Estimating: 251it [02:37,  1.57it/s]Extractor Estimating: 252it [02:37,  1.55it/s]Extractor Estimating: 253it [02:38,  1.55it/s]Extractor Estimating: 254it [02:38,  1.56it/s]Extractor Estimating: 255it [02:39,  1.58it/s]Extractor Estimating: 256it [02:40,  1.58it/s]Extractor Estimating: 257it [02:40,  1.58it/s]Extractor Estimating: 258it [02:41,  1.53it/s]Extractor Estimating: 259it [02:42,  1.52it/s]Extractor Estimating: 260it [02:42,  1.54it/s]Extractor Estimating: 261it [02:43,  1.53it/s]Extractor Estimating: 262it [02:44,  1.57it/s]Extractor Estimating: 263it [02:44,  1.55it/s]Extractor Estimating: 264it [02:45,  1.56it/s]Extractor Estimating: 265it [02:46,  1.57it/s]Extractor Estimating: 266it [02:46,  1.60it/s]Extractor Estimating: 267it [02:47,  1.66it/s]Extractor Estimating: 268it [02:47,  1.66it/s]Extractor Estimating: 269it [02:48,  1.64it/s]Extractor Estimating: 270it [02:48,  1.67it/s]Extractor Estimating: 271it [02:49,  1.63it/s]Extractor Estimating: 272it [02:50,  1.60it/s]Extractor Estimating: 273it [02:50,  1.55it/s]Extractor Estimating: 274it [02:51,  1.54it/s]Extractor Estimating: 275it [02:52,  1.54it/s]Extractor Estimating: 276it [02:52,  1.59it/s]Extractor Estimating: 277it [02:53,  1.60it/s]Extractor Estimating: 278it [02:54,  1.65it/s]Extractor Estimating: 279it [02:54,  1.62it/s]Extractor Estimating: 280it [02:55,  1.66it/s]Extractor Estimating: 281it [02:55,  1.57it/s]Extractor Estimating: 282it [02:56,  1.61it/s]Extractor Estimating: 283it [02:57,  1.65it/s]Extractor Estimating: 284it [02:57,  1.63it/s]Extractor Estimating: 285it [02:58,  1.68it/s]Extractor Estimating: 286it [02:58,  1.71it/s]Extractor Estimating: 287it [02:59,  1.72it/s]Extractor Estimating: 288it [03:00,  1.71it/s]Extractor Estimating: 289it [03:00,  1.74it/s]Extractor Estimating: 290it [03:01,  1.73it/s]Extractor Estimating: 291it [03:01,  1.78it/s]Extractor Estimating: 292it [03:02,  1.67it/s]Extractor Estimating: 293it [03:02,  1.75it/s]Extractor Estimating: 294it [03:03,  1.76it/s]Extractor Estimating: 295it [03:04,  1.64it/s]Extractor Estimating: 296it [03:04,  1.66it/s]Extractor Estimating: 297it [03:05,  1.75it/s]Extractor Estimating: 298it [03:05,  1.61it/s]Extractor Estimating: 299it [03:06,  1.60it/s]Extractor Estimating: 300it [03:07,  1.68it/s]Extractor Estimating: 301it [03:07,  1.68it/s]Extractor Estimating: 302it [03:08,  1.65it/s]Extractor Estimating: 303it [03:08,  1.65it/s]Extractor Estimating: 304it [03:09,  1.58it/s]Extractor Estimating: 305it [03:10,  1.61it/s]Extractor Estimating: 306it [03:10,  1.60it/s]Extractor Estimating: 307it [03:11,  1.61it/s]Extractor Estimating: 308it [03:12,  1.65it/s]Extractor Estimating: 309it [03:12,  1.62it/s]Extractor Estimating: 310it [03:13,  1.47it/s]Extractor Estimating: 311it [03:14,  1.54it/s]Extractor Estimating: 312it [03:14,  1.58it/s]Extractor Estimating: 313it [03:15,  1.60it/s]Extractor Estimating: 314it [03:15,  1.60it/s]Extractor Estimating: 315it [03:16,  1.58it/s]Extractor Estimating: 316it [03:17,  1.64it/s]Extractor Estimating: 317it [03:17,  1.68it/s]Extractor Estimating: 318it [03:18,  1.70it/s]Extractor Estimating: 319it [03:18,  1.69it/s]Extractor Estimating: 320it [03:19,  1.64it/s]Extractor Estimating: 321it [03:20,  1.66it/s]Extractor Estimating: 322it [03:20,  1.63it/s]Extractor Estimating: 323it [03:21,  1.59it/s]Extractor Estimating: 324it [03:22,  1.53it/s]Extractor Estimating: 325it [03:22,  1.49it/s]Extractor Estimating: 326it [03:23,  1.51it/s]Extractor Estimating: 327it [03:24,  1.48it/s]Extractor Estimating: 328it [03:24,  1.54it/s]Extractor Estimating: 329it [03:25,  1.54it/s]Extractor Estimating: 330it [03:26,  1.56it/s]Extractor Estimating: 331it [03:26,  1.59it/s]Extractor Estimating: 332it [03:27,  1.54it/s]Extractor Estimating: 333it [03:28,  1.55it/s]Extractor Estimating: 334it [03:28,  1.58it/s]Extractor Estimating: 335it [03:29,  1.34it/s]Extractor Estimating: 336it [03:30,  1.41it/s]Extractor Estimating: 337it [03:30,  1.53it/s]Extractor Estimating: 338it [03:31,  1.51it/s]Extractor Estimating: 339it [03:32,  1.49it/s]Extractor Estimating: 340it [03:33,  1.22it/s]Extractor Estimating: 341it [03:33,  1.31it/s]Extractor Estimating: 342it [03:34,  1.41it/s]Extractor Estimating: 343it [03:35,  1.44it/s]Extractor Estimating: 344it [03:35,  1.46it/s]Extractor Estimating: 345it [03:36,  1.48it/s]Extractor Estimating: 346it [03:37,  1.57it/s]Extractor Estimating: 347it [03:37,  1.58it/s]Extractor Estimating: 348it [03:38,  1.60it/s]Extractor Estimating: 349it [03:38,  1.56it/s]Extractor Estimating: 350it [03:39,  1.55it/s]Extractor Estimating: 351it [03:40,  1.59it/s]Extractor Estimating: 352it [03:40,  1.56it/s]Extractor Estimating: 353it [03:41,  1.55it/s]Extractor Estimating: 354it [03:42,  1.56it/s]Extractor Estimating: 355it [03:42,  1.61it/s]Extractor Estimating: 356it [03:43,  1.64it/s]Extractor Estimating: 357it [03:44,  1.57it/s]Extractor Estimating: 358it [03:44,  1.62it/s]Extractor Estimating: 359it [03:45,  1.67it/s]Extractor Estimating: 360it [03:45,  1.69it/s]Extractor Estimating: 361it [03:46,  1.70it/s]Extractor Estimating: 362it [03:46,  1.67it/s]Extractor Estimating: 363it [03:47,  1.64it/s]Extractor Estimating: 364it [03:48,  1.61it/s]Extractor Estimating: 365it [03:48,  1.66it/s]Extractor Estimating: 366it [03:49,  1.59it/s]Extractor Estimating: 367it [03:50,  1.63it/s]Extractor Estimating: 368it [03:50,  1.67it/s]Extractor Estimating: 369it [03:51,  1.63it/s]Extractor Estimating: 370it [03:51,  1.68it/s]Extractor Estimating: 371it [03:52,  1.68it/s]Extractor Estimating: 372it [03:52,  1.71it/s]Extractor Estimating: 373it [03:53,  1.64it/s]Extractor Estimating: 374it [03:54,  1.66it/s]Extractor Estimating: 375it [03:54,  1.96it/s]Extractor Estimating: 375it [03:54,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:52,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:52,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:52,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:52,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:52,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:00:53,193 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:00:53,194 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:00:53,765 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:00:54,818 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:00:54,818 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:57,694 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:57,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:57,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:57,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:00:57,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:00:58,322 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:00:58,323 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:00:58,908 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:00:59,065 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:00:59,065 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:36:26,606 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:36:26,608 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7471 mean pseudo reward: 0.9587925965343647
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 24692
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24792, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24792, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.073, loss:652.9747
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.100, loss:632.4003
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.086, loss:620.1451
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.084, loss:595.2125
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.075, loss:592.5226
>> valid entity prec:0.5243, rec:0.4676, f1:0.4943
>> valid relation prec:0.0130, rec:0.0046, f1:0.0068
>> valid relation with NER prec:0.0130, rec:0.0046, f1:0.0068
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.222, loss:616.8419
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.080, loss:568.7547
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.092, loss:591.5809
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.066, loss:628.2127
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.078, loss:587.5506
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5078, rec:0.4652, f1:0.4856
>> valid relation prec:0.0041, rec:0.0016, f1:0.0023
>> valid relation with NER prec:0.0041, rec:0.0016, f1:0.0023
g_step 1100, step 164, avg_time 3.232, loss:604.0238
g_step 1200, step 264, avg_time 1.079, loss:616.4967
g_step 1300, step 52, avg_time 1.066, loss:572.8007
g_step 1400, step 152, avg_time 1.075, loss:580.3315
g_step 1500, step 252, avg_time 1.103, loss:589.1334
>> valid entity prec:0.5031, rec:0.4530, f1:0.4768
>> valid relation prec:0.0143, rec:0.0051, f1:0.0075
>> valid relation with NER prec:0.0143, rec:0.0051, f1:0.0075
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 40, avg_time 3.216, loss:583.5004
g_step 1700, step 140, avg_time 1.078, loss:523.3773
g_step 1800, step 240, avg_time 1.092, loss:582.5934
g_step 1900, step 28, avg_time 1.063, loss:529.5244
g_step 2000, step 128, avg_time 1.079, loss:506.4943
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5153, rec:0.4582, f1:0.4851
>> valid relation prec:0.0206, rec:0.0075, f1:0.0110
>> valid relation with NER prec:0.0206, rec:0.0075, f1:0.0110
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 228, avg_time 3.219, loss:532.9358
g_step 2200, step 16, avg_time 1.079, loss:530.3112
g_step 2300, step 116, avg_time 1.077, loss:488.7140
g_step 2400, step 216, avg_time 1.087, loss:512.2794
g_step 2500, step 4, avg_time 1.079, loss:519.1941
>> valid entity prec:0.5042, rec:0.4709, f1:0.4870
>> valid relation prec:0.0133, rec:0.0050, f1:0.0072
>> valid relation with NER prec:0.0133, rec:0.0050, f1:0.0072
g_step 2600, step 104, avg_time 3.243, loss:456.9847
g_step 2700, step 204, avg_time 1.078, loss:486.9006
g_step 2800, step 304, avg_time 1.084, loss:508.6105
g_step 2900, step 92, avg_time 1.086, loss:441.1993
g_step 3000, step 192, avg_time 1.073, loss:468.3145
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5050, rec:0.4716, f1:0.4877
>> valid relation prec:0.0075, rec:0.0029, f1:0.0042
>> valid relation with NER prec:0.0075, rec:0.0029, f1:0.0042
g_step 3100, step 292, avg_time 3.240, loss:479.8205
g_step 3200, step 80, avg_time 1.083, loss:449.0539
g_step 3300, step 180, avg_time 1.075, loss:435.3862
g_step 3400, step 280, avg_time 1.072, loss:466.3548
g_step 3500, step 68, avg_time 1.064, loss:435.7851
>> valid entity prec:0.5057, rec:0.4531, f1:0.4780
>> valid relation prec:0.0037, rec:0.0016, f1:0.0022
>> valid relation with NER prec:0.0037, rec:0.0016, f1:0.0022
g_step 3600, step 168, avg_time 3.241, loss:427.5383
g_step 3700, step 268, avg_time 1.082, loss:432.7445
g_step 3800, step 56, avg_time 1.080, loss:444.1819
g_step 3900, step 156, avg_time 1.073, loss:416.9792
g_step 4000, step 256, avg_time 1.083, loss:438.6150
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4914, rec:0.4271, f1:0.4570
>> valid relation prec:0.0103, rec:0.0040, f1:0.0058
>> valid relation with NER prec:0.0103, rec:0.0040, f1:0.0058
g_step 4100, step 44, avg_time 3.221, loss:409.2882
g_step 4200, step 144, avg_time 1.074, loss:401.1553
g_step 4300, step 244, avg_time 1.081, loss:406.0436
g_step 4400, step 32, avg_time 1.080, loss:408.2846
g_step 4500, step 132, avg_time 1.078, loss:381.0056
>> valid entity prec:0.4816, rec:0.4653, f1:0.4733
>> valid relation prec:0.0060, rec:0.0027, f1:0.0037
>> valid relation with NER prec:0.0060, rec:0.0027, f1:0.0037
g_step 4600, step 232, avg_time 3.238, loss:408.9617
g_step 4700, step 20, avg_time 1.092, loss:384.3028
g_step 4800, step 120, avg_time 1.077, loss:374.8483
g_step 4900, step 220, avg_time 1.090, loss:376.8676
g_step 5000, step 8, avg_time 1.089, loss:398.3837
learning rate was adjusted to 0.0008
>> valid entity prec:0.5236, rec:0.4512, f1:0.4848
>> valid relation prec:0.0066, rec:0.0022, f1:0.0033
>> valid relation with NER prec:0.0066, rec:0.0022, f1:0.0033
g_step 5100, step 108, avg_time 3.229, loss:349.7672
g_step 5200, step 208, avg_time 1.103, loss:367.5695
g_step 5300, step 308, avg_time 1.081, loss:370.1058
g_step 5400, step 96, avg_time 1.079, loss:336.1428
g_step 5500, step 196, avg_time 1.085, loss:349.6671
>> valid entity prec:0.5428, rec:0.4349, f1:0.4829
>> valid relation prec:0.0112, rec:0.0048, f1:0.0067
>> valid relation with NER prec:0.0112, rec:0.0048, f1:0.0067
g_step 5600, step 296, avg_time 3.236, loss:359.7633
g_step 5700, step 84, avg_time 1.075, loss:335.6187
g_step 5800, step 184, avg_time 1.083, loss:341.1572
g_step 5900, step 284, avg_time 1.079, loss:354.6784
g_step 6000, step 72, avg_time 1.088, loss:318.4745
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5200, rec:0.4144, f1:0.4613
>> valid relation prec:0.0091, rec:0.0037, f1:0.0052
>> valid relation with NER prec:0.0091, rec:0.0037, f1:0.0052
g_step 6100, step 172, avg_time 3.230, loss:323.8950
g_step 6200, step 272, avg_time 1.088, loss:350.3013
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:36:26 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:36:26 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-36-26_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:36:27 - WARNING - datasets.builder -   Using custom data configuration default-bcecb280339757fa
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bcecb280339757fa/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:36:27,820 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:36:27,821 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:36:27,821 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:36:27,822 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:36:27,828 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:36:27,831 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:36:27,958 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:36:31,065 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:36:31,069 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bcecb280339757fa/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.18ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.97ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.32ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.47ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.54ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.59ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.62ba/s]100%|██████████| 8/8 [00:01<00:00,  5.43ba/s]100%|██████████| 8/8 [00:01<00:00,  4.71ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.60ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.43ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.82ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  4.04ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.13ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.20ba/s]100%|██████████| 7/7 [00:01<00:00,  4.40ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.28ba/s] 25%|██▌       | 2/8 [00:00<00:00,  9.18ba/s] 50%|█████     | 4/8 [00:00<00:00,  9.74ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.91ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.89ba/s]100%|██████████| 8/8 [00:00<00:00, 10.34ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.52ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.61ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.86ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.89ba/s]100%|██████████| 7/7 [00:00<00:00, 10.92ba/s]
[INFO|trainer.py:414] 2023-08-29 08:36:36,180 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:36:36,185 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:36:36,185 >>   Num examples = 7513
[INFO|trainer.py:1149] 2023-08-29 08:36:36,185 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:36:36,185 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:36:36,185 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:36:36,185 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:36:36,185 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:54,  3.34it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:47,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.47it/s]  1%|          | 7/585 [00:02<02:46,  3.47it/s]  1%|▏         | 8/585 [00:02<02:46,  3.48it/s]  2%|▏         | 9/585 [00:02<02:45,  3.48it/s]  2%|▏         | 10/585 [00:02<02:45,  3.48it/s]  2%|▏         | 11/585 [00:03<02:44,  3.48it/s]  2%|▏         | 12/585 [00:03<02:44,  3.48it/s]  2%|▏         | 13/585 [00:03<02:44,  3.48it/s]  2%|▏         | 14/585 [00:04<02:43,  3.48it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.47it/s]  3%|▎         | 18/585 [00:05<02:43,  3.48it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:41,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:41,  3.48it/s]  4%|▍         | 24/585 [00:06<02:41,  3.48it/s]  4%|▍         | 25/585 [00:07<02:40,  3.48it/s]  4%|▍         | 26/585 [00:07<02:40,  3.48it/s]  5%|▍         | 27/585 [00:07<02:40,  3.48it/s]  5%|▍         | 28/585 [00:08<02:40,  3.48it/s]  5%|▍         | 29/585 [00:08<02:39,  3.48it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:38,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:37,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:36,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.47it/s]  7%|▋         | 42/585 [00:12<02:36,  3.48it/s]  7%|▋         | 43/585 [00:12<02:35,  3.48it/s]  8%|▊         | 44/585 [00:12<02:35,  3.48it/s]  8%|▊         | 45/585 [00:12<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:35,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:14<02:34,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.47it/s]  9%|▉         | 52/585 [00:14<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:33,  3.47it/s]  9%|▉         | 54/585 [00:15<02:32,  3.48it/s]  9%|▉         | 55/585 [00:15<02:32,  3.48it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:31,  3.47it/s] 10%|█         | 59/585 [00:16<02:31,  3.47it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:30,  3.47it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 76/585 [00:21<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:22<02:25,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:23<02:31,  3.31it/s] 14%|█▍        | 83/585 [00:23<02:29,  3.36it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.39it/s] 15%|█▍        | 85/585 [00:24<02:26,  3.42it/s] 15%|█▍        | 86/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.45it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:25<02:22,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:21,  3.47it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.47it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.47it/s] 16%|█▋        | 96/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 97/585 [00:27<02:20,  3.47it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.47it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 107/585 [00:30<02:17,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.47it/s] 19%|█▉        | 114/585 [00:32<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.47it/s] 20%|██        | 117/585 [00:33<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 08:37:10,051 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:37:10,051 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:37:10,051 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.36it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.90it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.01it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.06it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.85it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.58it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.27it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.12it/s][A
  6%|▌         | 48/782 [00:01<00:15, 47.02it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.90it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.99it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.99it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.98it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.03it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.02it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.85it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.82it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.72it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.79it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.90it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.84it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.88it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.93it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.92it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.96it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.92it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.83it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.78it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.90it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.90it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.86it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.96it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.87it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 46.86it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.97it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.85it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.84it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.86it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.87it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.79it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.89it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.92it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.90it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.85it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.88it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.89it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.86it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.85it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.83it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.81it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.84it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.90it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.89it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.86it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.85it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.86it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.85it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.79it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.86it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.86it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.80it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.84it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.90it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.82it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.90it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.81it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.77it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.83it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.85it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.87it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.86it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.84it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.86it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.84it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.84it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.82it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.75it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.73it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.84it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.83it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.78it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.87it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.85it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.78it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.83it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.91it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.78it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.86it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.84it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.87it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.77it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.84it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.78it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.83it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.81it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.78it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.85it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.86it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.84it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.85it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.81it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.81it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.79it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.87it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.83it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.85it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.82it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.76it/s][A
 70%|███████   | 548/782 [00:11<00:04, 46.82it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.79it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.83it/s][A
 72%|███████▏  | 563/782 [00:11<00:04, 46.83it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.71it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.79it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.83it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.72it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.81it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.81it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.69it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.76it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.86it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.76it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.78it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.79it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.77it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.71it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.71it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.74it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.68it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.62it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.65it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.75it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.78it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.75it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.81it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.78it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.81it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.74it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.63it/s][A
 90%|████████▉ | 703/782 [00:14<00:01, 46.73it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.80it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.73it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.81it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.76it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.76it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.83it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.74it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.66it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.66it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.73it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.75it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.76it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.79it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.77it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.83it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.83it/s][A 20%|██        | 117/585 [00:50<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:37:26,776 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 08:37:26,795 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:37:29,066 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:37:29,081 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:37:29,089 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<57:21,  7.37s/it] 20%|██        | 119/585 [00:57<40:44,  5.25s/it] 21%|██        | 120/585 [00:58<29:07,  3.76s/it] 21%|██        | 121/585 [00:58<21:00,  2.72s/it] 21%|██        | 122/585 [00:58<15:21,  1.99s/it] 21%|██        | 123/585 [00:59<11:23,  1.48s/it] 21%|██        | 124/585 [00:59<08:37,  1.12s/it] 21%|██▏       | 125/585 [00:59<06:40,  1.15it/s] 22%|██▏       | 126/585 [00:59<05:19,  1.44it/s] 22%|██▏       | 127/585 [01:00<04:22,  1.74it/s] 22%|██▏       | 128/585 [01:00<03:43,  2.05it/s] 22%|██▏       | 129/585 [01:00<03:15,  2.33it/s] 22%|██▏       | 130/585 [01:01<02:55,  2.59it/s] 22%|██▏       | 131/585 [01:01<02:42,  2.80it/s] 23%|██▎       | 132/585 [01:01<02:32,  2.97it/s] 23%|██▎       | 133/585 [01:01<02:25,  3.10it/s] 23%|██▎       | 134/585 [01:02<02:20,  3.20it/s] 23%|██▎       | 135/585 [01:02<02:17,  3.28it/s] 23%|██▎       | 136/585 [01:02<02:14,  3.33it/s] 23%|██▎       | 137/585 [01:03<02:12,  3.37it/s] 24%|██▎       | 138/585 [01:03<02:11,  3.40it/s] 24%|██▍       | 139/585 [01:03<02:10,  3.42it/s] 24%|██▍       | 140/585 [01:03<02:09,  3.43it/s] 24%|██▍       | 141/585 [01:04<02:08,  3.44it/s] 24%|██▍       | 142/585 [01:04<02:08,  3.45it/s] 24%|██▍       | 143/585 [01:04<02:07,  3.45it/s] 25%|██▍       | 144/585 [01:05<02:08,  3.44it/s] 25%|██▍       | 145/585 [01:05<02:07,  3.45it/s] 25%|██▍       | 146/585 [01:05<02:07,  3.45it/s] 25%|██▌       | 147/585 [01:06<02:06,  3.46it/s] 25%|██▌       | 148/585 [01:06<02:06,  3.46it/s] 25%|██▌       | 149/585 [01:06<02:05,  3.46it/s] 26%|██▌       | 150/585 [01:06<02:05,  3.46it/s] 26%|██▌       | 151/585 [01:07<02:05,  3.46it/s] 26%|██▌       | 152/585 [01:07<02:04,  3.46it/s] 26%|██▌       | 153/585 [01:07<02:04,  3.46it/s] 26%|██▋       | 154/585 [01:08<02:04,  3.47it/s] 26%|██▋       | 155/585 [01:08<02:04,  3.46it/s] 27%|██▋       | 156/585 [01:08<02:07,  3.36it/s] 27%|██▋       | 157/585 [01:08<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:09<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:09<02:04,  3.43it/s] 27%|██▋       | 160/585 [01:09<02:03,  3.44it/s] 28%|██▊       | 161/585 [01:10<02:02,  3.45it/s] 28%|██▊       | 162/585 [01:10<02:02,  3.46it/s] 28%|██▊       | 163/585 [01:10<02:01,  3.46it/s] 28%|██▊       | 164/585 [01:10<02:01,  3.46it/s] 28%|██▊       | 165/585 [01:11<02:01,  3.46it/s] 28%|██▊       | 166/585 [01:11<02:01,  3.45it/s] 29%|██▊       | 167/585 [01:11<02:00,  3.46it/s] 29%|██▊       | 168/585 [01:12<02:00,  3.46it/s] 29%|██▉       | 169/585 [01:12<02:00,  3.46it/s] 29%|██▉       | 170/585 [01:12<01:59,  3.46it/s] 29%|██▉       | 171/585 [01:12<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:13<01:59,  3.47it/s] 30%|██▉       | 173/585 [01:13<01:58,  3.47it/s] 30%|██▉       | 174/585 [01:13<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:14<01:58,  3.47it/s] 30%|███       | 176/585 [01:14<01:58,  3.47it/s] 30%|███       | 177/585 [01:14<01:58,  3.46it/s] 30%|███       | 178/585 [01:15<01:57,  3.46it/s] 31%|███       | 179/585 [01:15<01:57,  3.46it/s] 31%|███       | 180/585 [01:15<01:57,  3.46it/s] 31%|███       | 181/585 [01:15<01:56,  3.46it/s] 31%|███       | 182/585 [01:16<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:16<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:16<01:55,  3.47it/s] 32%|███▏      | 185/585 [01:17<01:55,  3.47it/s] 32%|███▏      | 186/585 [01:17<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:17<01:54,  3.46it/s] 32%|███▏      | 188/585 [01:17<01:55,  3.45it/s] 32%|███▏      | 189/585 [01:18<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:18<01:54,  3.46it/s] 33%|███▎      | 191/585 [01:18<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:19<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:19<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:19<01:52,  3.46it/s] 33%|███▎      | 195/585 [01:19<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:20<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:20<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:20<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:21<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:21<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:21<01:51,  3.46it/s] 35%|███▍      | 202/585 [01:21<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:22<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:22<01:50,  3.46it/s] 35%|███▌      | 205/585 [01:22<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:23<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:23<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:23<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:23<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:24<01:54,  3.28it/s] 36%|███▌      | 211/585 [01:24<01:52,  3.33it/s] 36%|███▌      | 212/585 [01:24<01:50,  3.37it/s] 36%|███▋      | 213/585 [01:25<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:25<01:48,  3.41it/s] 37%|███▋      | 215/585 [01:25<01:48,  3.42it/s] 37%|███▋      | 216/585 [01:26<01:47,  3.43it/s] 37%|███▋      | 217/585 [01:26<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:26<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:26<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:27<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:27<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:27<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:28<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:28<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:28<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:28<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:29<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:29<01:48,  3.30it/s] 39%|███▉      | 229/585 [01:29<01:46,  3.34it/s] 39%|███▉      | 230/585 [01:30<01:45,  3.38it/s] 39%|███▉      | 231/585 [01:30<01:44,  3.40it/s] 40%|███▉      | 232/585 [01:30<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:31<01:42,  3.42it/s] 40%|████      | 234/585 [01:31<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:38:07,596 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:38:07,596 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:38:07,596 >>   Batch size = 8
{'eval_loss': 0.9131624698638916, 'eval_runtime': 16.6965, 'eval_samples_per_second': 374.51, 'eval_steps_per_second': 46.836, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 56.58it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.28it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.54it/s][A
  3%|▎         | 23/782 [00:00<00:15, 47.86it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.43it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.03it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.94it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.86it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.68it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.71it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.69it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.64it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.58it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.58it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.60it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.62it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.56it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.54it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.53it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.54it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.60it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.63it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.57it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.53it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.59it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.64it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.62it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.56it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.53it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.53it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.59it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.59it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.54it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.58it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.53it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.59it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.56it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.51it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.59it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.55it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.55it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.50it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.57it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.51it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.50it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.48it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.50it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.50it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.55it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.60it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.54it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.48it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.55it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.55it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.60it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.60it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.57it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.59it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.55it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.51it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.51it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.53it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.52it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.51it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.54it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.53it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.58it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.52it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.59it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.51it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.48it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.57it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.51it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.49it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.53it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.44it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.53it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.47it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.55it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.61it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.55it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.49it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.47it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.46it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.55it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.46it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.52it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.48it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.52it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.57it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.57it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.49it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.48it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.42it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.36it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.50it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.49it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.53it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.56it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.50it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.58it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.54it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.50it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.35it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.42it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.43it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.45it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.55it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.44it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.62it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.58it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.56it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.61it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.51it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.48it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.53it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.54it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.50it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.53it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.61it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.59it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.54it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.52it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.47it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.50it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.49it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.55it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.56it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.56it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.54it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.59it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.46it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.25it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.31it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.22it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.31it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.38it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.47it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.46it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.43it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.35it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.46it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.45it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.43it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.45it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.52it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.42it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.41it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.44it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.37it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.42it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.41it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.28it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.27it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.37it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.37it/s][A 40%|████      | 234/585 [01:48<01:42,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:38:24,420 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 08:38:24,442 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:38:26,773 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:38:26,794 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:38:26,807 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:55<43:37,  7.48s/it] 40%|████      | 236/585 [01:55<30:57,  5.32s/it] 41%|████      | 237/585 [01:56<22:06,  3.81s/it] 41%|████      | 238/585 [01:56<15:55,  2.75s/it] 41%|████      | 239/585 [01:56<11:37,  2.02s/it] 41%|████      | 240/585 [01:56<08:36,  1.50s/it] 41%|████      | 241/585 [01:57<06:30,  1.13s/it] 41%|████▏     | 242/585 [01:57<05:02,  1.14it/s] 42%|████▏     | 243/585 [01:57<04:00,  1.42it/s] 42%|████▏     | 244/585 [01:58<03:17,  1.73it/s] 42%|████▏     | 245/585 [01:58<02:47,  2.03it/s] 42%|████▏     | 246/585 [01:58<02:25,  2.32it/s] 42%|████▏     | 247/585 [01:59<02:11,  2.58it/s] 42%|████▏     | 248/585 [01:59<02:00,  2.79it/s] 43%|████▎     | 249/585 [01:59<01:53,  2.97it/s] 43%|████▎     | 250/585 [01:59<01:48,  3.10it/s] 43%|████▎     | 251/585 [02:00<01:44,  3.20it/s] 43%|████▎     | 252/585 [02:00<01:42,  3.25it/s] 43%|████▎     | 253/585 [02:00<01:40,  3.31it/s] 43%|████▎     | 254/585 [02:01<01:38,  3.36it/s] 44%|████▎     | 255/585 [02:01<01:37,  3.39it/s] 44%|████▍     | 256/585 [02:01<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:01<01:35,  3.43it/s] 44%|████▍     | 258/585 [02:02<01:35,  3.44it/s] 44%|████▍     | 259/585 [02:02<01:34,  3.45it/s] 44%|████▍     | 260/585 [02:02<01:34,  3.45it/s] 45%|████▍     | 261/585 [02:03<01:33,  3.46it/s] 45%|████▍     | 262/585 [02:03<01:33,  3.46it/s] 45%|████▍     | 263/585 [02:03<01:33,  3.46it/s] 45%|████▌     | 264/585 [02:03<01:32,  3.46it/s] 45%|████▌     | 265/585 [02:04<01:32,  3.46it/s] 45%|████▌     | 266/585 [02:04<01:32,  3.46it/s] 46%|████▌     | 267/585 [02:04<01:31,  3.47it/s] 46%|████▌     | 268/585 [02:05<01:31,  3.46it/s] 46%|████▌     | 269/585 [02:05<01:31,  3.46it/s] 46%|████▌     | 270/585 [02:05<01:30,  3.46it/s] 46%|████▋     | 271/585 [02:05<01:30,  3.46it/s] 46%|████▋     | 272/585 [02:06<01:30,  3.47it/s] 47%|████▋     | 273/585 [02:06<01:30,  3.47it/s] 47%|████▋     | 274/585 [02:06<01:30,  3.45it/s] 47%|████▋     | 275/585 [02:07<01:29,  3.45it/s] 47%|████▋     | 276/585 [02:07<01:29,  3.46it/s] 47%|████▋     | 277/585 [02:07<01:29,  3.46it/s] 48%|████▊     | 278/585 [02:07<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:08<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:08<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:08<01:27,  3.46it/s] 48%|████▊     | 282/585 [02:09<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:09<01:27,  3.46it/s] 49%|████▊     | 284/585 [02:09<01:26,  3.46it/s] 49%|████▊     | 285/585 [02:09<01:26,  3.45it/s] 49%|████▉     | 286/585 [02:10<01:26,  3.46it/s] 49%|████▉     | 287/585 [02:10<01:26,  3.46it/s] 49%|████▉     | 288/585 [02:10<01:25,  3.46it/s] 49%|████▉     | 289/585 [02:11<01:25,  3.46it/s] 50%|████▉     | 290/585 [02:11<01:25,  3.46it/s] 50%|████▉     | 291/585 [02:11<01:24,  3.46it/s] 50%|████▉     | 292/585 [02:12<01:24,  3.47it/s] 50%|█████     | 293/585 [02:12<01:24,  3.46it/s] 50%|█████     | 294/585 [02:12<01:24,  3.46it/s] 50%|█████     | 295/585 [02:12<01:23,  3.46it/s] 51%|█████     | 296/585 [02:13<01:23,  3.45it/s] 51%|█████     | 297/585 [02:13<01:23,  3.45it/s] 51%|█████     | 298/585 [02:13<01:23,  3.45it/s] 51%|█████     | 299/585 [02:14<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:14<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:14<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:14<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:15<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:15<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:15<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:16<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:16<01:20,  3.44it/s] 53%|█████▎    | 308/585 [02:16<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:16<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:17<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:17<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:17<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:18<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:18<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:18<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:18<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:19<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:19<01:18,  3.40it/s] 55%|█████▍    | 319/585 [02:19<01:17,  3.41it/s] 55%|█████▍    | 320/585 [02:20<01:17,  3.42it/s] 55%|█████▍    | 321/585 [02:20<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:20<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:21<01:16,  3.45it/s] 55%|█████▌    | 324/585 [02:21<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:21<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:21<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:22<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:22<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:22<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:23<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:23<01:13,  3.44it/s] 57%|█████▋    | 332/585 [02:23<01:13,  3.44it/s] 57%|█████▋    | 333/585 [02:23<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:24<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:24<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:24<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:25<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:25<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:25<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:26<01:10,  3.44it/s] 58%|█████▊    | 342/585 [02:26<01:10,  3.44it/s] 59%|█████▊    | 343/585 [02:26<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:27<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:27<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:27<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:27<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:28<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:28<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:28<01:08,  3.45it/s] 60%|██████    | 351/585 [02:29<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:39:05,421 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:39:05,422 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:39:05,422 >>   Batch size = 8
{'eval_loss': 0.9253644347190857, 'eval_runtime': 16.8083, 'eval_samples_per_second': 372.018, 'eval_steps_per_second': 46.525, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 56.94it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.52it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.79it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.15it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.74it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.32it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.95it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.65it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.61it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.59it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.60it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.73it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.79it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.70it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.72it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.70it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.58it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.47it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.37it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.42it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.42it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.44it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.48it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.46it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.50it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.44it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.48it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.48it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.48it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.54it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.64it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.59it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.70it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.75it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.73it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.69it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.68it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.53it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.59it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.58it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.68it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 44.66it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 45.35it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 45.71it/s][A
 29%|██▉       | 228/782 [00:04<00:12, 45.99it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.31it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.42it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.55it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.59it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.41it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.44it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.47it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.63it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.68it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.70it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.74it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.66it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.61it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.61it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.54it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.53it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.53it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.52it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.57it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.69it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.73it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.74it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.72it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.55it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.46it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.57it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.61it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.60it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.63it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.56it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.57it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.62it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.68it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.65it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.58it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.63it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.63it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.58it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.63it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.61it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.62it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.56it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.59it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.71it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.65it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.56it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.63it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.54it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.58it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.65it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.60it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.64it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.72it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.69it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.65it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.67it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.60it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.56it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.64it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.65it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.50it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.59it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.60it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.65it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.66it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.68it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.59it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.63it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.62it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.61it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.59it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.61it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.55it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.63it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.61it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.59it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.59it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.65it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.55it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.55it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.54it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.58it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.58it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.62it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.54it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.57it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.46it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.59it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.57it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.64it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.57it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.55it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.60it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.64it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.58it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.57it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.63it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.54it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.50it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.55it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.55it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.59it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.61it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.56it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.60it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.57it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.57it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.63it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.65it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.51it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.51it/s][A 60%|██████    | 351/585 [02:46<01:07,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:39:22,225 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 08:39:22,245 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:39:24,313 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:39:24,327 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:39:24,336 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:52<28:24,  7.31s/it] 60%|██████    | 353/585 [02:53<20:08,  5.21s/it] 61%|██████    | 354/585 [02:53<14:22,  3.73s/it] 61%|██████    | 355/585 [02:53<10:20,  2.70s/it] 61%|██████    | 356/585 [02:53<07:32,  1.98s/it] 61%|██████    | 357/585 [02:54<05:35,  1.47s/it] 61%|██████    | 358/585 [02:54<04:13,  1.12s/it] 61%|██████▏   | 359/585 [02:54<03:15,  1.15it/s] 62%|██████▏   | 360/585 [02:55<02:36,  1.44it/s] 62%|██████▏   | 361/585 [02:55<02:08,  1.75it/s] 62%|██████▏   | 362/585 [02:55<01:48,  2.05it/s] 62%|██████▏   | 363/585 [02:56<01:34,  2.34it/s] 62%|██████▏   | 364/585 [02:56<01:25,  2.59it/s] 62%|██████▏   | 365/585 [02:56<01:18,  2.80it/s] 63%|██████▎   | 366/585 [02:56<01:13,  2.97it/s] 63%|██████▎   | 367/585 [02:57<01:10,  3.10it/s] 63%|██████▎   | 368/585 [02:57<01:07,  3.20it/s] 63%|██████▎   | 369/585 [02:57<01:05,  3.28it/s] 63%|██████▎   | 370/585 [02:58<01:04,  3.33it/s] 63%|██████▎   | 371/585 [02:58<01:03,  3.37it/s] 64%|██████▎   | 372/585 [02:58<01:02,  3.40it/s] 64%|██████▍   | 373/585 [02:58<01:02,  3.42it/s] 64%|██████▍   | 374/585 [02:59<01:01,  3.43it/s] 64%|██████▍   | 375/585 [02:59<01:00,  3.44it/s] 64%|██████▍   | 376/585 [02:59<01:00,  3.45it/s] 64%|██████▍   | 377/585 [03:00<01:00,  3.45it/s] 65%|██████▍   | 378/585 [03:00<00:59,  3.46it/s] 65%|██████▍   | 379/585 [03:00<00:59,  3.46it/s] 65%|██████▍   | 380/585 [03:00<00:59,  3.46it/s] 65%|██████▌   | 381/585 [03:01<00:58,  3.46it/s] 65%|██████▌   | 382/585 [03:01<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:01<00:58,  3.45it/s] 66%|██████▌   | 384/585 [03:02<00:58,  3.45it/s] 66%|██████▌   | 385/585 [03:02<00:57,  3.46it/s] 66%|██████▌   | 386/585 [03:02<00:57,  3.46it/s] 66%|██████▌   | 387/585 [03:02<00:57,  3.46it/s] 66%|██████▋   | 388/585 [03:03<00:56,  3.46it/s] 66%|██████▋   | 389/585 [03:03<00:56,  3.46it/s] 67%|██████▋   | 390/585 [03:03<00:56,  3.46it/s] 67%|██████▋   | 391/585 [03:04<00:56,  3.46it/s] 67%|██████▋   | 392/585 [03:04<00:55,  3.46it/s] 67%|██████▋   | 393/585 [03:04<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:04<00:55,  3.45it/s] 68%|██████▊   | 395/585 [03:05<00:54,  3.46it/s] 68%|██████▊   | 396/585 [03:05<00:54,  3.46it/s] 68%|██████▊   | 397/585 [03:05<00:54,  3.46it/s] 68%|██████▊   | 398/585 [03:06<00:54,  3.46it/s] 68%|██████▊   | 399/585 [03:06<00:53,  3.46it/s] 68%|██████▊   | 400/585 [03:06<00:53,  3.46it/s] 69%|██████▊   | 401/585 [03:06<00:53,  3.46it/s] 69%|██████▊   | 402/585 [03:07<00:53,  3.45it/s] 69%|██████▉   | 403/585 [03:07<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:07<00:52,  3.45it/s] 69%|██████▉   | 405/585 [03:08<00:52,  3.45it/s] 69%|██████▉   | 406/585 [03:08<00:51,  3.45it/s] 70%|██████▉   | 407/585 [03:08<00:53,  3.34it/s] 70%|██████▉   | 408/585 [03:09<00:52,  3.37it/s] 70%|██████▉   | 409/585 [03:09<00:51,  3.40it/s] 70%|███████   | 410/585 [03:09<00:51,  3.41it/s] 70%|███████   | 411/585 [03:09<00:50,  3.43it/s] 70%|███████   | 412/585 [03:10<00:50,  3.43it/s] 71%|███████   | 413/585 [03:10<00:49,  3.44it/s] 71%|███████   | 414/585 [03:10<00:49,  3.45it/s] 71%|███████   | 415/585 [03:11<00:49,  3.45it/s] 71%|███████   | 416/585 [03:11<00:49,  3.44it/s] 71%|███████▏  | 417/585 [03:11<00:48,  3.44it/s] 71%|███████▏  | 418/585 [03:11<00:48,  3.45it/s] 72%|███████▏  | 419/585 [03:12<00:48,  3.45it/s] 72%|███████▏  | 420/585 [03:12<00:47,  3.45it/s] 72%|███████▏  | 421/585 [03:12<00:47,  3.45it/s] 72%|███████▏  | 422/585 [03:13<00:47,  3.45it/s] 72%|███████▏  | 423/585 [03:13<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:13<00:46,  3.45it/s] 73%|███████▎  | 425/585 [03:13<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:14<00:46,  3.45it/s] 73%|███████▎  | 427/585 [03:14<00:45,  3.45it/s] 73%|███████▎  | 428/585 [03:14<00:45,  3.45it/s] 73%|███████▎  | 429/585 [03:15<00:45,  3.45it/s] 74%|███████▎  | 430/585 [03:15<00:44,  3.45it/s] 74%|███████▎  | 431/585 [03:15<00:44,  3.45it/s] 74%|███████▍  | 432/585 [03:16<00:44,  3.45it/s] 74%|███████▍  | 433/585 [03:16<00:43,  3.45it/s] 74%|███████▍  | 434/585 [03:16<00:43,  3.45it/s] 74%|███████▍  | 435/585 [03:16<00:43,  3.45it/s] 75%|███████▍  | 436/585 [03:17<00:43,  3.45it/s] 75%|███████▍  | 437/585 [03:17<00:42,  3.45it/s] 75%|███████▍  | 438/585 [03:17<00:42,  3.45it/s] 75%|███████▌  | 439/585 [03:18<00:42,  3.45it/s] 75%|███████▌  | 440/585 [03:18<00:42,  3.45it/s] 75%|███████▌  | 441/585 [03:18<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:18<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:19<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:19<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:19<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:20<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:20<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:20<00:39,  3.45it/s] 77%|███████▋  | 449/585 [03:20<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:21<00:39,  3.44it/s] 77%|███████▋  | 451/585 [03:21<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:21<00:38,  3.45it/s] 77%|███████▋  | 453/585 [03:22<00:38,  3.45it/s] 78%|███████▊  | 454/585 [03:22<00:37,  3.45it/s] 78%|███████▊  | 455/585 [03:22<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:22<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:23<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:23<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:23<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:24<00:36,  3.44it/s] 79%|███████▉  | 461/585 [03:24<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:24<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:24<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:25<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:25<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:25<00:34,  3.45it/s] 80%|███████▉  | 467/585 [03:26<00:34,  3.45it/s] 80%|████████  | 468/585 [03:26<00:33,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:40:02,733 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:40:02,733 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:40:02,733 >>   Batch size = 8
{'eval_loss': 0.9413520693778992, 'eval_runtime': 16.7902, 'eval_samples_per_second': 372.42, 'eval_steps_per_second': 46.575, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.03it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.46it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.74it/s][A
  3%|▎         | 23/782 [00:00<00:15, 47.90it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.60it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.36it/s][A
  5%|▍         | 38/782 [00:00<00:15, 46.97it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.81it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.68it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.65it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.74it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.83it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.76it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.78it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.69it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.64it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.50it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.46it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.57it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.60it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.62it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.57it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.60it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.61it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.69it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.67it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.62it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.59it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.61it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.62it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.65it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.57it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.64it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.58it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.61it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.65it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.69it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.39it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.56it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.57it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.48it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.56it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.54it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.54it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.61it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.65it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.65it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.59it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.64it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.60it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.65it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.67it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.66it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.63it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.61it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.53it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.62it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.62it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.66it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.69it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.58it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.31it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.44it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.46it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.54it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.48it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.41it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.42it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.38it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.39it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.39it/s][A
 46%|████▋     | 363/782 [00:08<00:09, 46.49it/s][A
 47%|████▋     | 368/782 [00:08<00:27, 14.99it/s][A
 48%|████▊     | 373/782 [00:08<00:21, 18.82it/s][A
 48%|████▊     | 378/782 [00:08<00:17, 22.90it/s][A
 49%|████▉     | 383/782 [00:08<00:14, 27.01it/s][A
 50%|████▉     | 388/782 [00:09<00:12, 30.90it/s][A
 50%|█████     | 393/782 [00:09<00:11, 34.33it/s][A
 51%|█████     | 398/782 [00:09<00:10, 37.22it/s][A
 52%|█████▏    | 403/782 [00:09<00:09, 39.63it/s][A
 52%|█████▏    | 408/782 [00:09<00:09, 41.42it/s][A
 53%|█████▎    | 413/782 [00:09<00:08, 42.82it/s][A
 53%|█████▎    | 418/782 [00:09<00:08, 43.82it/s][A
 54%|█████▍    | 423/782 [00:09<00:08, 44.49it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 44.97it/s][A
 55%|█████▌    | 433/782 [00:10<00:07, 45.40it/s][A
 56%|█████▌    | 438/782 [00:10<00:07, 45.72it/s][A
 57%|█████▋    | 443/782 [00:10<00:07, 45.86it/s][A
 57%|█████▋    | 448/782 [00:10<00:07, 45.91it/s][A
 58%|█████▊    | 453/782 [00:10<00:07, 45.94it/s][A
 59%|█████▊    | 458/782 [00:10<00:07, 46.00it/s][A
 59%|█████▉    | 463/782 [00:10<00:06, 46.10it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 45.79it/s][A
 60%|██████    | 473/782 [00:10<00:06, 45.93it/s][A
 61%|██████    | 478/782 [00:11<00:06, 46.02it/s][A
 62%|██████▏   | 483/782 [00:11<00:06, 46.21it/s][A
 62%|██████▏   | 488/782 [00:11<00:06, 46.35it/s][A
 63%|██████▎   | 493/782 [00:11<00:06, 46.37it/s][A
 64%|██████▎   | 498/782 [00:11<00:06, 46.31it/s][A
 64%|██████▍   | 503/782 [00:11<00:06, 46.29it/s][A
 65%|██████▍   | 508/782 [00:11<00:05, 46.33it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.37it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.37it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.33it/s][A
 68%|██████▊   | 528/782 [00:12<00:05, 46.35it/s][A
 68%|██████▊   | 533/782 [00:12<00:05, 46.37it/s][A
 69%|██████▉   | 538/782 [00:12<00:05, 46.32it/s][A
 69%|██████▉   | 543/782 [00:12<00:05, 46.04it/s][A
 70%|███████   | 548/782 [00:12<00:05, 46.25it/s][A
 71%|███████   | 553/782 [00:12<00:04, 46.34it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 46.27it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.30it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.30it/s][A
 73%|███████▎  | 573/782 [00:13<00:04, 46.31it/s][A
 74%|███████▍  | 578/782 [00:13<00:04, 46.37it/s][A
 75%|███████▍  | 583/782 [00:13<00:04, 46.32it/s][A
 75%|███████▌  | 588/782 [00:13<00:04, 46.37it/s][A
 76%|███████▌  | 593/782 [00:13<00:04, 46.37it/s][A
 76%|███████▋  | 598/782 [00:13<00:03, 46.33it/s][A
 77%|███████▋  | 603/782 [00:13<00:03, 46.34it/s][A
 78%|███████▊  | 608/782 [00:13<00:04, 43.37it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 44.23it/s][A
 79%|███████▉  | 618/782 [00:14<00:03, 44.90it/s][A
 80%|███████▉  | 623/782 [00:14<00:03, 45.28it/s][A
 80%|████████  | 628/782 [00:14<00:03, 45.57it/s][A
 81%|████████  | 633/782 [00:14<00:03, 45.81it/s][A
 82%|████████▏ | 638/782 [00:14<00:03, 46.00it/s][A
 82%|████████▏ | 643/782 [00:14<00:03, 46.13it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 46.19it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.20it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.19it/s][A
 85%|████████▍ | 663/782 [00:15<00:02, 46.15it/s][A
 85%|████████▌ | 668/782 [00:15<00:02, 46.25it/s][A
 86%|████████▌ | 673/782 [00:15<00:02, 46.32it/s][A
 87%|████████▋ | 678/782 [00:15<00:02, 46.31it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 46.24it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 46.23it/s][A
 89%|████████▊ | 693/782 [00:15<00:01, 46.33it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.30it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.33it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.36it/s][A
 91%|█████████ | 713/782 [00:16<00:01, 46.33it/s][A
 92%|█████████▏| 718/782 [00:16<00:01, 46.42it/s][A
 92%|█████████▏| 723/782 [00:16<00:01, 46.42it/s][A
 93%|█████████▎| 728/782 [00:16<00:01, 46.34it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 46.39it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 46.42it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.37it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.40it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.35it/s][A
 97%|█████████▋| 758/782 [00:17<00:00, 46.32it/s][A
 98%|█████████▊| 763/782 [00:17<00:00, 46.37it/s][A
 98%|█████████▊| 768/782 [00:17<00:00, 46.41it/s][A
 99%|█████████▉| 773/782 [00:17<00:00, 46.41it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 46.41it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:17<00:00, 46.41it/s][A 80%|████████  | 468/585 [03:44<00:33,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:40:20,354 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 08:40:20,377 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:40:22,789 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:40:22,813 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:40:22,829 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:51<14:49,  7.67s/it] 80%|████████  | 470/585 [03:51<10:27,  5.45s/it] 81%|████████  | 471/585 [03:51<07:25,  3.90s/it] 81%|████████  | 472/585 [03:52<05:18,  2.82s/it] 81%|████████  | 473/585 [03:52<03:50,  2.06s/it] 81%|████████  | 474/585 [03:52<02:49,  1.53s/it] 81%|████████  | 475/585 [03:53<02:07,  1.16s/it] 81%|████████▏ | 476/585 [03:53<01:37,  1.12it/s] 82%|████████▏ | 477/585 [03:53<01:17,  1.40it/s] 82%|████████▏ | 478/585 [03:53<01:02,  1.70it/s] 82%|████████▏ | 479/585 [03:54<00:52,  2.01it/s] 82%|████████▏ | 480/585 [03:54<00:45,  2.30it/s] 82%|████████▏ | 481/585 [03:54<00:40,  2.55it/s] 82%|████████▏ | 482/585 [03:55<00:37,  2.77it/s] 83%|████████▎ | 483/585 [03:55<00:34,  2.94it/s] 83%|████████▎ | 484/585 [03:55<00:32,  3.08it/s] 83%|████████▎ | 485/585 [03:55<00:31,  3.19it/s] 83%|████████▎ | 486/585 [03:56<00:30,  3.27it/s] 83%|████████▎ | 487/585 [03:56<00:29,  3.32it/s] 83%|████████▎ | 488/585 [03:56<00:28,  3.36it/s] 84%|████████▎ | 489/585 [03:57<00:28,  3.39it/s] 84%|████████▍ | 490/585 [03:57<00:27,  3.41it/s] 84%|████████▍ | 491/585 [03:57<00:27,  3.42it/s] 84%|████████▍ | 492/585 [03:57<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:58<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:58<00:26,  3.44it/s] 85%|████████▍ | 495/585 [03:58<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:59<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:59<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:59<00:25,  3.46it/s] 85%|████████▌ | 499/585 [03:59<00:24,  3.46it/s] 85%|████████▌ | 500/585 [04:00<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:00<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:00<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:00<00:23,  3.46it/s] 86%|████████▌ | 503/585 [04:01<00:23,  3.45it/s] 86%|████████▌ | 504/585 [04:01<00:23,  3.46it/s] 86%|████████▋ | 505/585 [04:01<00:23,  3.46it/s] 86%|████████▋ | 506/585 [04:02<00:22,  3.46it/s] 87%|████████▋ | 507/585 [04:02<00:22,  3.46it/s] 87%|████████▋ | 508/585 [04:02<00:22,  3.46it/s] 87%|████████▋ | 509/585 [04:02<00:21,  3.46it/s] 87%|████████▋ | 510/585 [04:03<00:21,  3.46it/s] 87%|████████▋ | 511/585 [04:03<00:21,  3.45it/s] 88%|████████▊ | 512/585 [04:03<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:04<00:20,  3.46it/s] 88%|████████▊ | 514/585 [04:04<00:20,  3.44it/s] 88%|████████▊ | 515/585 [04:04<00:20,  3.45it/s] 88%|████████▊ | 516/585 [04:04<00:19,  3.45it/s] 88%|████████▊ | 517/585 [04:05<00:19,  3.46it/s] 89%|████████▊ | 518/585 [04:05<00:19,  3.46it/s] 89%|████████▊ | 519/585 [04:05<00:19,  3.46it/s] 89%|████████▉ | 520/585 [04:06<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:06<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:06<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:06<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:07<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:07<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:07<00:17,  3.46it/s] 90%|█████████ | 527/585 [04:08<00:16,  3.46it/s] 90%|█████████ | 528/585 [04:08<00:16,  3.45it/s] 90%|█████████ | 529/585 [04:08<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:08<00:15,  3.46it/s] 91%|█████████ | 531/585 [04:09<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:09<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:09<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:10<00:14,  3.46it/s] 91%|█████████▏| 535/585 [04:10<00:14,  3.46it/s] 92%|█████████▏| 536/585 [04:10<00:14,  3.45it/s] 92%|█████████▏| 537/585 [04:10<00:13,  3.45it/s] 92%|█████████▏| 538/585 [04:11<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:11<00:13,  3.44it/s] 92%|█████████▏| 540/585 [04:11<00:13,  3.45it/s] 92%|█████████▏| 541/585 [04:12<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:12<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:12<00:12,  3.45it/s] 93%|█████████▎| 544/585 [04:13<00:11,  3.45it/s] 93%|█████████▎| 545/585 [04:13<00:11,  3.45it/s] 93%|█████████▎| 546/585 [04:13<00:11,  3.45it/s] 94%|█████████▎| 547/585 [04:13<00:11,  3.45it/s] 94%|█████████▎| 548/585 [04:14<00:10,  3.45it/s] 94%|█████████▍| 549/585 [04:14<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:14<00:10,  3.44it/s] 94%|█████████▍| 551/585 [04:15<00:09,  3.44it/s] 94%|█████████▍| 552/585 [04:15<00:09,  3.45it/s] 95%|█████████▍| 553/585 [04:15<00:09,  3.45it/s] 95%|█████████▍| 554/585 [04:15<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:16<00:08,  3.45it/s] 95%|█████████▌| 556/585 [04:16<00:08,  3.45it/s] 95%|█████████▌| 557/585 [04:16<00:08,  3.45it/s] 95%|█████████▌| 558/585 [04:17<00:07,  3.45it/s] 96%|█████████▌| 559/585 [04:17<00:07,  3.45it/s] 96%|█████████▌| 560/585 [04:17<00:07,  3.45it/s] 96%|█████████▌| 561/585 [04:17<00:06,  3.45it/s] 96%|█████████▌| 562/585 [04:18<00:06,  3.45it/s] 96%|█████████▌| 563/585 [04:18<00:06,  3.45it/s] 96%|█████████▋| 564/585 [04:18<00:06,  3.45it/s] 97%|█████████▋| 565/585 [04:19<00:05,  3.45it/s] 97%|█████████▋| 566/585 [04:19<00:05,  3.45it/s] 97%|█████████▋| 567/585 [04:19<00:05,  3.45it/s] 97%|█████████▋| 568/585 [04:19<00:04,  3.45it/s] 97%|█████████▋| 569/585 [04:20<00:04,  3.45it/s] 97%|█████████▋| 570/585 [04:20<00:04,  3.45it/s] 98%|█████████▊| 571/585 [04:20<00:04,  3.45it/s] 98%|█████████▊| 572/585 [04:21<00:03,  3.44it/s] 98%|█████████▊| 573/585 [04:21<00:03,  3.45it/s] 98%|█████████▊| 574/585 [04:21<00:03,  3.45it/s] 98%|█████████▊| 575/585 [04:22<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:22<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:22<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:22<00:02,  3.45it/s] 99%|█████████▉| 579/585 [04:23<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:23<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:23<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:24<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:24<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:24<00:00,  3.45it/s]100%|██████████| 585/585 [04:24<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 08:41:01,092 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:41:01,092 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:41:01,092 >>   Batch size = 8
{'eval_loss': 0.9459353089332581, 'eval_runtime': 17.6096, 'eval_samples_per_second': 355.09, 'eval_steps_per_second': 44.408, 'epoch': 4.0}
{'loss': 0.5859, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.31it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.56it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.72it/s][A
  3%|▎         | 23/782 [00:00<00:15, 47.92it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.49it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.22it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.12it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.90it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.69it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.68it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.74it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.72it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.70it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.65it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.62it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.66it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.72it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.62it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.58it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.62it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.57it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.67it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.62it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.65it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.66it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.70it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.58it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.61it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.56it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.56it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.64it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.63it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.59it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.65it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.61it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.59it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.63it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.62it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.60it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.60it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.62it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.60it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.52it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.50it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.56it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.59it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.63it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.63it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.54it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.57it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.64it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.63it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.56it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.62it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.62it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.55it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.57it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.60it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.57it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.68it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.64it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.53it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.58it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.62it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.62it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.65it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.60it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.59it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.60it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.53it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.60it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.66it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.62it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.64it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.60it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.64it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.58it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.63it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.56it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.60it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.62it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.59it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.62it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.59it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.62it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.65it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.60it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.49it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.53it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.60it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.57it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.60it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.64it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.60it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.60it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.63it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.58it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.55it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.59it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.59it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.62it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.56it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.58it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.64it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.63it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.63it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.63it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.50it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.59it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.58it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.57it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.60it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.65it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.61it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.56it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.64it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.54it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.61it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.61it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.56it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.59it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.63it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.58it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.63it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.58it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.61it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.51it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.58it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.53it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.59it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.55it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.55it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.52it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.57it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.50it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.52it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.48it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.42it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.41it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.39it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.45it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.50it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.55it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.55it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.62it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.54it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.57it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.59it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.63it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.57it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.53it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.60it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.54it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.51it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.46it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.46it/s][A100%|██████████| 585/585 [04:41<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:41:17,882 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 08:41:17,908 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:41:21,272 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:41:21,461 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:41:21,542 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:41:27,178 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:41:27,181 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117 (score: 0.9131624698638916).
                                                 100%|██████████| 585/585 [04:53<00:00,  3.45it/s]100%|██████████| 585/585 [04:53<00:00,  2.00it/s]
[INFO|trainer.py:1894] 2023-08-29 08:41:29,228 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 08:41:29,390 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:41:31,717 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:41:31,732 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:41:31,743 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:41:31,910 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   train_loss               =     0.5822
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   train_runtime            = 0:04:53.01
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   train_samples            =       7513
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   train_samples_per_second =      128.2
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:31,911 >>   train_steps_per_second   =      1.996
{'eval_loss': 0.9512843489646912, 'eval_runtime': 16.7802, 'eval_samples_per_second': 372.641, 'eval_steps_per_second': 46.602, 'epoch': 5.0}
{'train_runtime': 293.0187, 'train_samples_per_second': 128.2, 'train_steps_per_second': 1.996, 'train_loss': 0.582213879446698, 'epoch': 5.0}
08/29/2023 08:41:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:41:31,942 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:41:31,942 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 08:41:31,943 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 58.47it/s]  2%|▏         | 12/782 [00:00<00:15, 51.25it/s]  2%|▏         | 18/782 [00:00<00:15, 49.26it/s]  3%|▎         | 23/782 [00:00<00:15, 48.50it/s]  4%|▎         | 28/782 [00:00<00:15, 48.04it/s]  4%|▍         | 33/782 [00:00<00:15, 47.78it/s]  5%|▍         | 38/782 [00:00<00:15, 47.54it/s]  5%|▌         | 43/782 [00:00<00:15, 47.36it/s]  6%|▌         | 48/782 [00:00<00:15, 47.05it/s]  7%|▋         | 53/782 [00:01<00:15, 47.23it/s]  7%|▋         | 58/782 [00:01<00:15, 47.15it/s]  8%|▊         | 63/782 [00:01<00:15, 47.12it/s]  9%|▊         | 68/782 [00:01<00:15, 47.10it/s]  9%|▉         | 73/782 [00:01<00:15, 47.10it/s] 10%|▉         | 78/782 [00:01<00:14, 47.11it/s] 11%|█         | 83/782 [00:01<00:14, 47.08it/s] 11%|█▏        | 88/782 [00:01<00:14, 46.95it/s] 12%|█▏        | 93/782 [00:01<00:14, 47.02it/s] 13%|█▎        | 98/782 [00:02<00:14, 47.03it/s] 13%|█▎        | 103/782 [00:02<00:14, 46.92it/s] 14%|█▍        | 108/782 [00:02<00:14, 46.98it/s] 14%|█▍        | 113/782 [00:02<00:14, 47.03it/s] 15%|█▌        | 118/782 [00:02<00:14, 46.94it/s] 16%|█▌        | 123/782 [00:02<00:14, 47.05it/s] 16%|█▋        | 128/782 [00:02<00:13, 47.10it/s] 17%|█▋        | 133/782 [00:02<00:13, 46.98it/s] 18%|█▊        | 138/782 [00:02<00:13, 46.97it/s] 18%|█▊        | 143/782 [00:03<00:13, 46.99it/s] 19%|█▉        | 148/782 [00:03<00:13, 46.98it/s] 20%|█▉        | 153/782 [00:03<00:13, 47.03it/s] 20%|██        | 158/782 [00:03<00:13, 46.95it/s] 21%|██        | 163/782 [00:03<00:13, 46.91it/s] 21%|██▏       | 168/782 [00:03<00:13, 46.98it/s] 22%|██▏       | 173/782 [00:03<00:12, 47.04it/s] 23%|██▎       | 178/782 [00:03<00:12, 47.04it/s] 23%|██▎       | 183/782 [00:03<00:12, 46.98it/s] 24%|██▍       | 188/782 [00:03<00:12, 47.00it/s] 25%|██▍       | 193/782 [00:04<00:12, 46.98it/s] 25%|██▌       | 198/782 [00:04<00:12, 46.96it/s] 26%|██▌       | 203/782 [00:04<00:12, 46.97it/s] 27%|██▋       | 208/782 [00:04<00:12, 46.94it/s] 27%|██▋       | 213/782 [00:04<00:12, 46.95it/s] 28%|██▊       | 218/782 [00:04<00:11, 47.01it/s] 29%|██▊       | 223/782 [00:04<00:11, 47.03it/s] 29%|██▉       | 228/782 [00:04<00:11, 46.94it/s] 30%|██▉       | 233/782 [00:04<00:11, 47.00it/s] 30%|███       | 238/782 [00:05<00:11, 46.96it/s] 31%|███       | 243/782 [00:05<00:11, 46.90it/s] 32%|███▏      | 248/782 [00:05<00:11, 46.96it/s] 32%|███▏      | 253/782 [00:05<00:11, 46.93it/s] 33%|███▎      | 258/782 [00:05<00:11, 46.90it/s] 34%|███▎      | 263/782 [00:05<00:11, 46.99it/s] 34%|███▍      | 268/782 [00:05<00:10, 46.98it/s] 35%|███▍      | 273/782 [00:05<00:10, 46.92it/s] 36%|███▌      | 278/782 [00:05<00:10, 47.00it/s] 36%|███▌      | 283/782 [00:05<00:10, 46.96it/s] 37%|███▋      | 288/782 [00:06<00:10, 46.89it/s] 37%|███▋      | 293/782 [00:06<00:10, 46.91it/s] 38%|███▊      | 298/782 [00:06<00:10, 46.91it/s] 39%|███▊      | 303/782 [00:06<00:10, 46.91it/s] 39%|███▉      | 308/782 [00:06<00:10, 46.97it/s] 40%|████      | 313/782 [00:06<00:09, 46.95it/s] 41%|████      | 318/782 [00:06<00:09, 46.95it/s] 41%|████▏     | 323/782 [00:06<00:09, 46.97it/s] 42%|████▏     | 328/782 [00:06<00:09, 46.94it/s] 43%|████▎     | 333/782 [00:07<00:09, 46.92it/s] 43%|████▎     | 338/782 [00:07<00:09, 46.95it/s] 44%|████▍     | 343/782 [00:07<00:09, 46.89it/s] 45%|████▍     | 348/782 [00:07<00:09, 46.92it/s] 45%|████▌     | 353/782 [00:07<00:09, 46.90it/s] 46%|████▌     | 358/782 [00:07<00:09, 46.91it/s] 46%|████▋     | 363/782 [00:07<00:08, 46.88it/s] 47%|████▋     | 368/782 [00:07<00:08, 46.90it/s] 48%|████▊     | 373/782 [00:07<00:08, 46.92it/s] 48%|████▊     | 378/782 [00:08<00:08, 46.94it/s] 49%|████▉     | 383/782 [00:08<00:08, 46.89it/s] 50%|████▉     | 388/782 [00:08<00:08, 46.88it/s] 50%|█████     | 393/782 [00:08<00:08, 46.87it/s] 51%|█████     | 398/782 [00:08<00:08, 46.91it/s] 52%|█████▏    | 403/782 [00:08<00:08, 46.90it/s] 52%|█████▏    | 408/782 [00:08<00:07, 46.84it/s] 53%|█████▎    | 413/782 [00:08<00:07, 46.90it/s] 53%|█████▎    | 418/782 [00:08<00:07, 46.93it/s] 54%|█████▍    | 423/782 [00:08<00:07, 46.80it/s] 55%|█████▍    | 428/782 [00:09<00:07, 46.85it/s] 55%|█████▌    | 433/782 [00:09<00:07, 46.84it/s] 56%|█████▌    | 438/782 [00:09<00:07, 46.79it/s] 57%|█████▋    | 443/782 [00:09<00:07, 46.83it/s] 57%|█████▋    | 448/782 [00:09<00:07, 46.86it/s] 58%|█████▊    | 453/782 [00:09<00:07, 46.82it/s] 59%|█████▊    | 458/782 [00:09<00:06, 46.83it/s] 59%|█████▉    | 463/782 [00:09<00:06, 46.85it/s] 60%|█████▉    | 468/782 [00:09<00:06, 46.79it/s] 60%|██████    | 473/782 [00:10<00:06, 46.83it/s] 61%|██████    | 478/782 [00:10<00:06, 46.89it/s] 62%|██████▏   | 483/782 [00:10<00:06, 46.85it/s] 62%|██████▏   | 488/782 [00:10<00:06, 46.93it/s] 63%|██████▎   | 493/782 [00:10<00:06, 46.86it/s] 64%|██████▎   | 498/782 [00:10<00:06, 46.83it/s] 64%|██████▍   | 503/782 [00:10<00:05, 46.87it/s] 65%|██████▍   | 508/782 [00:10<00:05, 46.90it/s] 66%|██████▌   | 513/782 [00:10<00:05, 46.92it/s] 66%|██████▌   | 518/782 [00:11<00:05, 46.92it/s] 67%|██████▋   | 523/782 [00:11<00:05, 46.94it/s] 68%|██████▊   | 528/782 [00:11<00:05, 46.86it/s] 68%|██████▊   | 533/782 [00:11<00:05, 46.93it/s] 69%|██████▉   | 538/782 [00:11<00:05, 46.94it/s] 69%|██████▉   | 543/782 [00:11<00:05, 46.86it/s] 70%|███████   | 548/782 [00:11<00:04, 46.85it/s] 71%|███████   | 553/782 [00:11<00:04, 46.91it/s] 71%|███████▏  | 558/782 [00:11<00:04, 46.83it/s] 72%|███████▏  | 563/782 [00:11<00:04, 46.81it/s] 73%|███████▎  | 568/782 [00:12<00:04, 46.85it/s] 73%|███████▎  | 573/782 [00:12<00:04, 46.83it/s] 74%|███████▍  | 578/782 [00:12<00:04, 46.87it/s] 75%|███████▍  | 583/782 [00:12<00:04, 46.89it/s] 75%|███████▌  | 588/782 [00:12<00:04, 46.79it/s] 76%|███████▌  | 593/782 [00:12<00:04, 46.80it/s] 76%|███████▋  | 598/782 [00:12<00:03, 46.84it/s] 77%|███████▋  | 603/782 [00:12<00:03, 46.77it/s] 78%|███████▊  | 608/782 [00:12<00:03, 46.84it/s] 78%|███████▊  | 613/782 [00:13<00:03, 45.35it/s] 79%|███████▉  | 618/782 [00:13<00:03, 45.77it/s] 80%|███████▉  | 623/782 [00:13<00:03, 46.06it/s] 80%|████████  | 628/782 [00:13<00:03, 46.34it/s] 81%|████████  | 633/782 [00:13<00:03, 46.45it/s] 82%|████████▏ | 638/782 [00:13<00:03, 46.60it/s] 82%|████████▏ | 643/782 [00:13<00:02, 46.67it/s] 83%|████████▎ | 648/782 [00:13<00:02, 46.71it/s] 84%|████████▎ | 653/782 [00:13<00:02, 46.74it/s] 84%|████████▍ | 658/782 [00:14<00:02, 46.74it/s] 85%|████████▍ | 663/782 [00:14<00:02, 46.68it/s] 85%|████████▌ | 668/782 [00:14<00:02, 46.73it/s] 86%|████████▌ | 673/782 [00:14<00:02, 46.77it/s] 87%|████████▋ | 678/782 [00:14<00:02, 46.75it/s] 87%|████████▋ | 683/782 [00:14<00:02, 46.83it/s] 88%|████████▊ | 688/782 [00:14<00:02, 46.84it/s] 89%|████████▊ | 693/782 [00:14<00:01, 46.81it/s] 89%|████████▉ | 698/782 [00:14<00:01, 46.75it/s] 90%|████████▉ | 703/782 [00:14<00:01, 46.70it/s] 91%|█████████ | 708/782 [00:15<00:01, 46.67it/s] 91%|█████████ | 713/782 [00:15<00:01, 46.74it/s] 92%|█████████▏| 718/782 [00:15<00:01, 46.72it/s] 92%|█████████▏| 723/782 [00:15<00:01, 46.72it/s] 93%|█████████▎| 728/782 [00:15<00:01, 46.83it/s] 94%|█████████▎| 733/782 [00:15<00:01, 46.82it/s] 94%|█████████▍| 738/782 [00:15<00:00, 46.77it/s] 95%|█████████▌| 743/782 [00:15<00:00, 46.77it/s] 96%|█████████▌| 748/782 [00:15<00:00, 46.79it/s] 96%|█████████▋| 753/782 [00:16<00:00, 46.79it/s] 97%|█████████▋| 758/782 [00:16<00:00, 46.71it/s] 98%|█████████▊| 763/782 [00:16<00:00, 46.70it/s] 98%|█████████▊| 768/782 [00:16<00:00, 46.76it/s] 99%|█████████▉| 773/782 [00:16<00:00, 46.78it/s] 99%|█████████▉| 778/782 [00:16<00:00, 46.84it/s]100%|██████████| 782/782 [00:16<00:00, 46.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:41:48,630 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,630 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,630 >>   eval_loss               =     0.9132
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,630 >>   eval_runtime            = 0:00:16.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,631 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,631 >>   eval_samples_per_second =    374.708
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,631 >>   eval_steps_per_second   =     46.861
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:41:48,631 >>   perplexity              =     2.4922
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:55,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:55,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:55,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:55,673 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:55,674 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:41:56,266 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:41:56,267 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:41:56,823 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:41:57,856 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:41:57,856 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:42:00,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:42:00,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:42:00,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:42:00,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:42:00,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:42:01,348 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:42:01,350 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:42:02,116 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:42:02,267 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:42:02,267 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.43it/s]Extractor Predicting: 4it [00:02,  1.44it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.44it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.57it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.53it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.44it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:49,  1.48it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:51,  1.49it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:54,  1.52it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.49it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [00:59,  1.51it/s]Extractor Predicting: 91it [01:00,  1.37it/s]Extractor Predicting: 92it [01:01,  1.39it/s]Extractor Predicting: 93it [01:02,  1.42it/s]Extractor Predicting: 94it [01:02,  1.45it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:04,  1.45it/s]Extractor Predicting: 97it [01:04,  1.44it/s]Extractor Predicting: 98it [01:05,  1.45it/s]Extractor Predicting: 99it [01:06,  1.44it/s]Extractor Predicting: 100it [01:06,  1.46it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.45it/s]Extractor Predicting: 108it [01:12,  1.45it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:13,  1.48it/s]Extractor Predicting: 111it [01:14,  1.49it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:15,  1.52it/s]Extractor Predicting: 114it [01:16,  1.51it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.47it/s]Extractor Predicting: 118it [01:19,  1.47it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:23,  1.49it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.49it/s]Extractor Predicting: 127it [01:25,  1.48it/s]Extractor Predicting: 128it [01:25,  1.47it/s]Extractor Predicting: 129it [01:26,  1.48it/s]Extractor Predicting: 130it [01:27,  1.46it/s]Extractor Predicting: 131it [01:27,  1.46it/s]Extractor Predicting: 132it [01:28,  1.45it/s]Extractor Predicting: 133it [01:29,  1.47it/s]Extractor Predicting: 134it [01:29,  1.46it/s]Extractor Predicting: 135it [01:30,  1.45it/s]Extractor Predicting: 136it [01:31,  1.46it/s]Extractor Predicting: 137it [01:31,  1.46it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.48it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.50it/s]Extractor Predicting: 142it [01:35,  1.50it/s]Extractor Predicting: 143it [01:35,  1.47it/s]Extractor Predicting: 144it [01:36,  1.49it/s]Extractor Predicting: 145it [01:37,  1.49it/s]Extractor Predicting: 146it [01:37,  1.50it/s]Extractor Predicting: 147it [01:38,  1.49it/s]Extractor Predicting: 148it [01:39,  1.54it/s]Extractor Predicting: 149it [01:39,  1.53it/s]Extractor Predicting: 150it [01:40,  1.52it/s]Extractor Predicting: 151it [01:41,  1.49it/s]Extractor Predicting: 152it [01:41,  1.54it/s]Extractor Predicting: 153it [01:42,  1.51it/s]Extractor Predicting: 154it [01:43,  1.51it/s]Extractor Predicting: 155it [01:43,  1.52it/s]Extractor Predicting: 156it [01:44,  1.51it/s]Extractor Predicting: 157it [01:45,  1.55it/s]Extractor Predicting: 158it [01:45,  1.55it/s]Extractor Predicting: 159it [01:46,  1.56it/s]Extractor Predicting: 160it [01:47,  1.51it/s]Extractor Predicting: 161it [01:47,  1.50it/s]Extractor Predicting: 162it [01:48,  1.52it/s]Extractor Predicting: 163it [01:49,  1.54it/s]Extractor Predicting: 164it [01:49,  1.56it/s]Extractor Predicting: 165it [01:50,  1.59it/s]Extractor Predicting: 166it [01:50,  1.60it/s]Extractor Predicting: 167it [01:51,  1.58it/s]Extractor Predicting: 168it [01:52,  1.41it/s]Extractor Predicting: 169it [01:53,  1.46it/s]Extractor Predicting: 170it [01:53,  1.49it/s]Extractor Predicting: 171it [01:54,  1.50it/s]Extractor Predicting: 172it [01:55,  1.49it/s]Extractor Predicting: 173it [01:55,  1.52it/s]Extractor Predicting: 174it [01:56,  1.49it/s]Extractor Predicting: 175it [01:57,  1.50it/s]Extractor Predicting: 176it [01:57,  1.51it/s]Extractor Predicting: 177it [01:58,  1.52it/s]Extractor Predicting: 178it [01:58,  1.52it/s]Extractor Predicting: 179it [01:59,  1.51it/s]Extractor Predicting: 180it [02:00,  1.51it/s]Extractor Predicting: 181it [02:00,  1.53it/s]Extractor Predicting: 182it [02:01,  1.51it/s]Extractor Predicting: 183it [02:02,  1.53it/s]Extractor Predicting: 184it [02:02,  1.55it/s]Extractor Predicting: 185it [02:03,  1.53it/s]Extractor Predicting: 186it [02:04,  1.50it/s]Extractor Predicting: 187it [02:04,  1.55it/s]Extractor Predicting: 188it [02:05,  1.52it/s]Extractor Predicting: 189it [02:06,  1.53it/s]Extractor Predicting: 190it [02:06,  1.55it/s]Extractor Predicting: 191it [02:07,  1.58it/s]Extractor Predicting: 192it [02:08,  1.58it/s]Extractor Predicting: 193it [02:08,  1.56it/s]Extractor Predicting: 194it [02:09,  1.57it/s]Extractor Predicting: 195it [02:09,  1.56it/s]Extractor Predicting: 196it [02:10,  1.53it/s]Extractor Predicting: 197it [02:11,  1.57it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:12,  1.52it/s]Extractor Predicting: 200it [02:13,  1.57it/s]Extractor Predicting: 201it [02:13,  1.58it/s]Extractor Predicting: 202it [02:14,  1.59it/s]Extractor Predicting: 203it [02:15,  1.59it/s]Extractor Predicting: 204it [02:15,  1.58it/s]Extractor Predicting: 205it [02:16,  1.57it/s]Extractor Predicting: 206it [02:16,  1.60it/s]Extractor Predicting: 207it [02:17,  1.61it/s]Extractor Predicting: 208it [02:18,  1.60it/s]Extractor Predicting: 209it [02:18,  1.57it/s]Extractor Predicting: 210it [02:19,  1.58it/s]Extractor Predicting: 211it [02:20,  1.59it/s]Extractor Predicting: 212it [02:20,  1.59it/s]Extractor Predicting: 213it [02:21,  1.57it/s]Extractor Predicting: 214it [02:21,  1.60it/s]Extractor Predicting: 215it [02:22,  1.57it/s]Extractor Predicting: 216it [02:23,  1.56it/s]Extractor Predicting: 217it [02:23,  1.56it/s]Extractor Predicting: 218it [02:24,  1.58it/s]Extractor Predicting: 219it [02:25,  1.56it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:26,  1.57it/s]Extractor Predicting: 222it [02:27,  1.56it/s]Extractor Predicting: 223it [02:27,  1.56it/s]Extractor Predicting: 224it [02:28,  1.58it/s]Extractor Predicting: 225it [02:29,  1.56it/s]Extractor Predicting: 226it [02:29,  1.56it/s]Extractor Predicting: 227it [02:30,  1.58it/s]Extractor Predicting: 228it [02:30,  1.73it/s]Extractor Predicting: 228it [02:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:41,449 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:41,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:41,454 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:41,454 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:41,454 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:44:42,045 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:44:42,046 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:44:42,626 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:44:43,643 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:44:43,643 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:46,579 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:46,594 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:46,594 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:46,594 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:44:46,594 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:44:47,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:44:47,244 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:44:47,819 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:44:47,966 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:44:47,966 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.06392045454545454,
  "recall": 0.01439309131616824,
  "score": 0.023495627202715048,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:09,  1.47it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.47it/s]Extractor Predicting: 28it [00:18,  1.43it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.45it/s]Extractor Predicting: 37it [00:24,  1.44it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.39it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.45it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:29,  1.45it/s]Extractor Predicting: 44it [00:29,  1.45it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.60it/s]Extractor Predicting: 48it [00:32,  1.61it/s]Extractor Predicting: 49it [00:32,  1.64it/s]Extractor Predicting: 50it [00:33,  1.59it/s]Extractor Predicting: 51it [00:34,  1.59it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.53it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:40,  1.58it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.56it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.58it/s]Extractor Predicting: 71it [00:46,  1.60it/s]Extractor Predicting: 72it [00:47,  1.60it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:49,  1.46it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:51,  1.52it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:52,  1.53it/s]Extractor Predicting: 80it [00:53,  1.52it/s]Extractor Predicting: 81it [00:53,  1.51it/s]Extractor Predicting: 82it [00:54,  1.53it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.54it/s]Extractor Predicting: 85it [00:56,  1.54it/s]Extractor Predicting: 86it [00:56,  1.54it/s]Extractor Predicting: 87it [00:57,  1.55it/s]Extractor Predicting: 88it [00:58,  1.55it/s]Extractor Predicting: 89it [00:58,  1.56it/s]Extractor Predicting: 90it [00:59,  1.57it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:02,  1.55it/s]Extractor Predicting: 96it [01:03,  1.55it/s]Extractor Predicting: 97it [01:04,  1.53it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:05,  1.51it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:07,  1.54it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:09,  1.51it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.55it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:14,  1.59it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.59it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:18,  1.63it/s]Extractor Predicting: 120it [01:18,  1.65it/s]Extractor Predicting: 121it [01:19,  1.64it/s]Extractor Predicting: 122it [01:19,  1.65it/s]Extractor Predicting: 123it [01:20,  1.65it/s]Extractor Predicting: 124it [01:21,  1.65it/s]Extractor Predicting: 125it [01:21,  1.57it/s]Extractor Predicting: 126it [01:22,  1.61it/s]Extractor Predicting: 127it [01:23,  1.58it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.55it/s]Extractor Predicting: 130it [01:25,  1.59it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.60it/s]Extractor Predicting: 133it [01:26,  1.60it/s]Extractor Predicting: 134it [01:27,  1.57it/s]Extractor Predicting: 135it [01:28,  1.60it/s]Extractor Predicting: 136it [01:28,  1.62it/s]Extractor Predicting: 137it [01:29,  1.60it/s]Extractor Predicting: 138it [01:30,  1.57it/s]Extractor Predicting: 139it [01:30,  1.57it/s]Extractor Predicting: 140it [01:31,  1.60it/s]Extractor Predicting: 141it [01:31,  1.62it/s]Extractor Predicting: 142it [01:32,  1.66it/s]Extractor Predicting: 143it [01:33,  1.65it/s]Extractor Predicting: 144it [01:33,  1.66it/s]Extractor Predicting: 145it [01:34,  1.63it/s]Extractor Predicting: 146it [01:34,  1.62it/s]Extractor Predicting: 147it [01:35,  1.63it/s]Extractor Predicting: 148it [01:36,  1.63it/s]Extractor Predicting: 149it [01:36,  1.57it/s]Extractor Predicting: 150it [01:37,  1.53it/s]Extractor Predicting: 151it [01:38,  1.55it/s]Extractor Predicting: 152it [01:38,  1.56it/s]Extractor Predicting: 153it [01:39,  1.59it/s]Extractor Predicting: 154it [01:40,  1.56it/s]Extractor Predicting: 155it [01:40,  1.51it/s]Extractor Predicting: 156it [01:41,  1.52it/s]Extractor Predicting: 157it [01:42,  1.50it/s]Extractor Predicting: 158it [01:42,  1.55it/s]Extractor Predicting: 159it [01:43,  1.55it/s]Extractor Predicting: 160it [01:44,  1.57it/s]Extractor Predicting: 161it [01:44,  1.59it/s]Extractor Predicting: 162it [01:45,  1.60it/s]Extractor Predicting: 163it [01:45,  1.58it/s]Extractor Predicting: 164it [01:46,  1.60it/s]Extractor Predicting: 165it [01:47,  1.58it/s]Extractor Predicting: 166it [01:47,  1.60it/s]Extractor Predicting: 167it [01:48,  1.58it/s]Extractor Predicting: 168it [01:49,  1.58it/s]Extractor Predicting: 169it [01:49,  1.60it/s]Extractor Predicting: 170it [01:50,  1.58it/s]Extractor Predicting: 171it [01:50,  1.56it/s]Extractor Predicting: 172it [01:51,  1.56it/s]Extractor Predicting: 173it [01:52,  1.54it/s]Extractor Predicting: 174it [01:52,  1.55it/s]Extractor Predicting: 175it [01:53,  1.55it/s]Extractor Predicting: 176it [01:54,  1.56it/s]Extractor Predicting: 177it [01:54,  1.54it/s]Extractor Predicting: 178it [01:55,  1.51it/s]Extractor Predicting: 179it [01:56,  1.54it/s]Extractor Predicting: 180it [01:57,  1.41it/s]Extractor Predicting: 181it [01:57,  1.47it/s]Extractor Predicting: 182it [01:58,  1.53it/s]Extractor Predicting: 183it [01:58,  1.53it/s]Extractor Predicting: 184it [01:59,  1.57it/s]Extractor Predicting: 185it [02:00,  1.58it/s]Extractor Predicting: 186it [02:00,  1.56it/s]Extractor Predicting: 187it [02:01,  1.59it/s]Extractor Predicting: 188it [02:01,  1.60it/s]Extractor Predicting: 189it [02:02,  1.55it/s]Extractor Predicting: 190it [02:03,  1.49it/s]Extractor Predicting: 191it [02:04,  1.51it/s]Extractor Predicting: 192it [02:04,  1.49it/s]Extractor Predicting: 193it [02:05,  1.50it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:06,  1.48it/s]Extractor Predicting: 196it [02:07,  1.47it/s]Extractor Predicting: 197it [02:08,  1.48it/s]Extractor Predicting: 198it [02:08,  1.50it/s]Extractor Predicting: 199it [02:09,  1.49it/s]Extractor Predicting: 200it [02:10,  1.48it/s]Extractor Predicting: 201it [02:10,  1.48it/s]Extractor Predicting: 202it [02:11,  1.48it/s]Extractor Predicting: 203it [02:12,  1.49it/s]Extractor Predicting: 204it [02:12,  1.48it/s]Extractor Predicting: 205it [02:13,  1.47it/s]Extractor Predicting: 206it [02:14,  1.47it/s]Extractor Predicting: 207it [02:14,  1.43it/s]Extractor Predicting: 208it [02:15,  1.44it/s]Extractor Predicting: 209it [02:16,  1.50it/s]Extractor Predicting: 210it [02:16,  1.52it/s]Extractor Predicting: 211it [02:17,  1.52it/s]Extractor Predicting: 212it [02:18,  1.54it/s]Extractor Predicting: 213it [02:18,  1.52it/s]Extractor Predicting: 214it [02:19,  1.49it/s]Extractor Predicting: 215it [02:20,  1.48it/s]Extractor Predicting: 216it [02:20,  1.47it/s]Extractor Predicting: 217it [02:21,  1.45it/s]Extractor Predicting: 218it [02:22,  1.48it/s]Extractor Predicting: 219it [02:22,  1.50it/s]Extractor Predicting: 220it [02:23,  1.49it/s]Extractor Predicting: 221it [02:24,  1.51it/s]Extractor Predicting: 222it [02:24,  1.47it/s]Extractor Predicting: 223it [02:25,  1.45it/s]Extractor Predicting: 224it [02:26,  1.42it/s]Extractor Predicting: 225it [02:27,  1.41it/s]Extractor Predicting: 226it [02:27,  1.42it/s]Extractor Predicting: 227it [02:28,  1.41it/s]Extractor Predicting: 228it [02:29,  1.40it/s]Extractor Predicting: 229it [02:29,  1.39it/s]Extractor Predicting: 230it [02:30,  1.39it/s]Extractor Predicting: 231it [02:31,  1.39it/s]Extractor Predicting: 232it [02:32,  1.40it/s]Extractor Predicting: 233it [02:32,  1.40it/s]Extractor Predicting: 234it [02:33,  1.42it/s]Extractor Predicting: 235it [02:34,  1.41it/s]Extractor Predicting: 236it [02:34,  1.42it/s]Extractor Predicting: 237it [02:35,  1.39it/s]Extractor Predicting: 238it [02:36,  1.39it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:37,  1.45it/s]Extractor Predicting: 241it [02:38,  1.46it/s]Extractor Predicting: 242it [02:39,  1.47it/s]Extractor Predicting: 243it [02:39,  1.48it/s]Extractor Predicting: 244it [02:40,  1.45it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:41,  1.53it/s]Extractor Predicting: 246it [02:41,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:36,259 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:36,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:36,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:36,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:36,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:47:36,980 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:47:36,981 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:37,552 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:38,588 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:38,589 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:41,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:41,689 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:41,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:41,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:47:41,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:47:42,348 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:47:42,349 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:42,906 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:43,054 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:43,054 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.36670071501532175,
  "recall": 0.06084745762711864,
  "score": 0.10437563599360372,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.71it/s]Extractor Predicting: 15it [00:09,  1.51it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:47:53,582 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:53,583 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:47:53,586 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:53,587 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:47:53,589 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:47:56,644 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:47:56,646 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:47:56,657 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:56,657 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:47:56,661 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:56,666 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4107142857142857,
  "recall": 0.032303370786516857,
  "score": 0.059895833333333336,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:47:56,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:57,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:58,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:59,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:59,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:00,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:01,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:01,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:02,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:03,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:03,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:04,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:05,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:05,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:06,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:07,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:07,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:08,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:09,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:09,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:10,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:19, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:11,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:11,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:12,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:13,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:14,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:14,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:15,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:15,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:16,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:17,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:17,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:18,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:19,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:19,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:20,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:21,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:21,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:22,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:23,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:24,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:59, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:24,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:26,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:26,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:27,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:28,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:28,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:29,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:30,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:31,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:32,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:33,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:34,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:34,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:36,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:37,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:37,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:38,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:39,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:39,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:40,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:45<03:04, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:41,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:42,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:43,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:44,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:44,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:45,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:46,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:46,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:47,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:48,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:48,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:49,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:50,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:50,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:51,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:52,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:53,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:53,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:54,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:55,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:55,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:59<02:45, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:56,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:57,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:57,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:58,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:59,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:00,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:00,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:01,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:01,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:02,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:03,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:03,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:04,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:05,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:05,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:06,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:06,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:07,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:08,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:08,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:22, 14.26s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:09,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:09,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:10,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:11,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:12,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:12,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:13,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:14,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:14,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:15,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:16,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:16,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:17,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:18,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:19,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:19,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:20,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:21,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:22,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:22,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:08, 14.28s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:23,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:24,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:24,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:25,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:26,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:26,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:27,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:29,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:30,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:31,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:31,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:32,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:33,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:33,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:34,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:35,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:35,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:39<01:49, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:36,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:37,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:37,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:38,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:39,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:39,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:40,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:41,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:42,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:42,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:43,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:43,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:44,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:45,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:46,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:46,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:47,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:48,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:48,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:49,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:50,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:54<01:38, 14.10s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:51,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:51,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:52,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:53,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:54,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:55,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:56,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:56,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:57,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:58,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:59,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:59,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:00,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:01,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:01,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:02,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:03,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:04,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:05,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:05,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:06,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:07,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:08,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:12<01:31, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:09,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:09,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:10,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:11,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:11,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:12,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:14,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:14,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:15,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:16,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:16,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:17,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:18,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:19,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:20,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:20,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:21,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:22,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:23,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:24,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:28<01:17, 15.57s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:25,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:25,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:26,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:27,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:27,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:28,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:29,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:30,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:30,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:31,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:32,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:33,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:34,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:35,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:35,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:36,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:37,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:37,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:38,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:39,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:39,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:40,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:44<01:03, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:41,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:42,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:42,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:43,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:44,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:44,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:45,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:46,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:46,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:47,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:48,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:48,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:49,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:50,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:50,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:51,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:52,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:52,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:53,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:54,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:54,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:58<00:45, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:55,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:56,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:56,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:57,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:58,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:58,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:59,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:00,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:00,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:02,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:03,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:03,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:04,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:05,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:05,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:06,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:07,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:08,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:08,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:12<00:29, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:09,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:10,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:10,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:11,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:12,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:12,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:13,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:14,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:14,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:15,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:16,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:17,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:18,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:19,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:19,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:20,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:21,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:22,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:22,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:23,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:24,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:24,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:25,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:29<00:15, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:26,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:27,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:27,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:28,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:29,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:30,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:30,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:31,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:32,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:33,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:33,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:34,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:35,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:35,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:36,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:36,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:37,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:38,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:38,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:39,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:40,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:44<00:00, 15.23s/it]Generating: 100%|██████████| 15/15 [03:44<00:00, 14.94s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:47,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:47,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:47,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:47,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:47,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:51:47,400 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:51:47,401 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:51:47,661 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:51:48,723 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:51:48,723 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:50,019 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:50,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:50,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:50,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:51:50,021 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:51:50,335 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:51:50,338 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:51:50,598 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:51:50,750 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:51:50,750 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.80859375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8707386363636364, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.9241071428571429, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : screenwriter . Context : Later in 2008 , he appeared in The New Yorker magazine s All My Children . Head Entity : All My Children , Tail Entity : All Your Children .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : use .', 'success_rate': 0.8369565217391305, 'errors': {'', "('application', 'use', '', 'The main application of LZO is in .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : voice type .', 'success_rate': 0.890625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 9694
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9794, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.67it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.59it/s]Extractor Estimating: 8it [00:05,  1.62it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.69it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:08,  1.66it/s]Extractor Estimating: 14it [00:08,  1.65it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:10,  1.70it/s]Extractor Estimating: 18it [00:10,  1.73it/s]Extractor Estimating: 19it [00:11,  1.71it/s]Extractor Estimating: 20it [00:12,  1.75it/s]Extractor Estimating: 21it [00:12,  1.76it/s]Extractor Estimating: 22it [00:13,  1.61it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.64it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:15,  1.62it/s]Extractor Estimating: 27it [00:16,  1.57it/s]Extractor Estimating: 28it [00:17,  1.55it/s]Extractor Estimating: 29it [00:17,  1.59it/s]Extractor Estimating: 30it [00:18,  1.63it/s]Extractor Estimating: 31it [00:18,  1.64it/s]Extractor Estimating: 32it [00:19,  1.64it/s]Extractor Estimating: 33it [00:20,  1.65it/s]Extractor Estimating: 34it [00:20,  1.63it/s]Extractor Estimating: 35it [00:21,  1.62it/s]Extractor Estimating: 36it [00:22,  1.60it/s]Extractor Estimating: 37it [00:22,  1.58it/s]Extractor Estimating: 38it [00:23,  1.59it/s]Extractor Estimating: 39it [00:23,  1.60it/s]Extractor Estimating: 40it [00:24,  1.60it/s]Extractor Estimating: 41it [00:25,  1.58it/s]Extractor Estimating: 42it [00:25,  1.59it/s]Extractor Estimating: 43it [00:26,  1.60it/s]Extractor Estimating: 44it [00:27,  1.63it/s]Extractor Estimating: 45it [00:27,  1.63it/s]Extractor Estimating: 46it [00:28,  1.63it/s]Extractor Estimating: 47it [00:28,  1.62it/s]Extractor Estimating: 48it [00:29,  1.62it/s]Extractor Estimating: 49it [00:30,  1.49it/s]Extractor Estimating: 50it [00:31,  1.50it/s]Extractor Estimating: 51it [00:31,  1.54it/s]Extractor Estimating: 52it [00:32,  1.57it/s]Extractor Estimating: 53it [00:32,  1.62it/s]Extractor Estimating: 54it [00:33,  1.64it/s]Extractor Estimating: 55it [00:34,  1.59it/s]Extractor Estimating: 56it [00:34,  1.64it/s]Extractor Estimating: 57it [00:35,  1.64it/s]Extractor Estimating: 58it [00:35,  1.69it/s]Extractor Estimating: 59it [00:36,  1.65it/s]Extractor Estimating: 60it [00:37,  1.64it/s]Extractor Estimating: 61it [00:37,  1.65it/s]Extractor Estimating: 62it [00:38,  1.71it/s]Extractor Estimating: 63it [00:38,  1.71it/s]Extractor Estimating: 64it [00:39,  1.67it/s]Extractor Estimating: 65it [00:40,  1.65it/s]Extractor Estimating: 66it [00:40,  1.65it/s]Extractor Estimating: 67it [00:41,  1.68it/s]Extractor Estimating: 68it [00:41,  1.76it/s]Extractor Estimating: 69it [00:42,  1.71it/s]Extractor Estimating: 70it [00:42,  1.70it/s]Extractor Estimating: 71it [00:43,  1.69it/s]Extractor Estimating: 72it [00:44,  1.69it/s]Extractor Estimating: 73it [00:44,  1.71it/s]Extractor Estimating: 74it [00:45,  1.70it/s]Extractor Estimating: 75it [00:45,  1.68it/s]Extractor Estimating: 76it [00:46,  1.65it/s]Extractor Estimating: 77it [00:47,  1.67it/s]Extractor Estimating: 78it [00:47,  1.70it/s]Extractor Estimating: 79it [00:48,  1.69it/s]Extractor Estimating: 80it [00:48,  1.73it/s]Extractor Estimating: 81it [00:49,  1.76it/s]Extractor Estimating: 82it [00:49,  1.75it/s]Extractor Estimating: 83it [00:50,  1.75it/s]Extractor Estimating: 84it [00:51,  1.73it/s]Extractor Estimating: 85it [00:51,  1.76it/s]Extractor Estimating: 86it [00:52,  1.74it/s]Extractor Estimating: 87it [00:52,  1.69it/s]Extractor Estimating: 88it [00:53,  1.77it/s]Extractor Estimating: 89it [00:53,  1.77it/s]Extractor Estimating: 90it [00:54,  1.76it/s]Extractor Estimating: 91it [00:55,  1.75it/s]Extractor Estimating: 92it [00:55,  1.73it/s]Extractor Estimating: 93it [00:56,  1.70it/s]Extractor Estimating: 94it [00:56,  1.70it/s]Extractor Estimating: 95it [00:57,  1.69it/s]Extractor Estimating: 96it [00:58,  1.67it/s]Extractor Estimating: 97it [00:58,  1.69it/s]Extractor Estimating: 98it [00:59,  1.65it/s]Extractor Estimating: 99it [00:59,  1.64it/s]Extractor Estimating: 100it [01:00,  1.66it/s]Extractor Estimating: 101it [01:01,  1.75it/s]Extractor Estimating: 102it [01:01,  1.80it/s]Extractor Estimating: 103it [01:02,  1.80it/s]Extractor Estimating: 104it [01:02,  1.77it/s]Extractor Estimating: 105it [01:03,  1.83it/s]Extractor Estimating: 106it [01:03,  1.81it/s]Extractor Estimating: 107it [01:04,  1.85it/s]Extractor Estimating: 108it [01:04,  1.86it/s]Extractor Estimating: 109it [01:05,  1.76it/s]Extractor Estimating: 110it [01:05,  1.78it/s]Extractor Estimating: 111it [01:06,  1.81it/s]Extractor Estimating: 112it [01:07,  1.84it/s]Extractor Estimating: 113it [01:07,  1.86it/s]Extractor Estimating: 114it [01:08,  1.80it/s]Extractor Estimating: 115it [01:08,  1.79it/s]Extractor Estimating: 116it [01:09,  1.86it/s]Extractor Estimating: 117it [01:09,  1.84it/s]Extractor Estimating: 118it [01:10,  1.87it/s]Extractor Estimating: 119it [01:10,  1.81it/s]Extractor Estimating: 120it [01:11,  1.80it/s]Extractor Estimating: 121it [01:11,  1.87it/s]Extractor Estimating: 122it [01:12,  1.87it/s]Extractor Estimating: 123it [01:12,  1.87it/s]Extractor Estimating: 124it [01:13,  1.89it/s]Extractor Estimating: 125it [01:14,  1.88it/s]Extractor Estimating: 126it [01:14,  1.82it/s]Extractor Estimating: 127it [01:15,  1.70it/s]Extractor Estimating: 128it [01:15,  1.62it/s]Extractor Estimating: 129it [01:16,  1.62it/s]Extractor Estimating: 130it [01:17,  1.63it/s]Extractor Estimating: 131it [01:17,  1.63it/s]Extractor Estimating: 132it [01:18,  1.64it/s]Extractor Estimating: 133it [01:18,  1.68it/s]Extractor Estimating: 134it [01:19,  1.67it/s]Extractor Estimating: 135it [01:20,  1.72it/s]Extractor Estimating: 136it [01:20,  1.72it/s]Extractor Estimating: 137it [01:21,  1.64it/s]Extractor Estimating: 138it [01:21,  1.65it/s]Extractor Estimating: 139it [01:22,  1.54it/s]Extractor Estimating: 140it [01:23,  1.60it/s]Extractor Estimating: 141it [01:23,  1.56it/s]Extractor Estimating: 142it [01:24,  1.58it/s]Extractor Estimating: 143it [01:25,  1.59it/s]Extractor Estimating: 144it [01:25,  1.56it/s]Extractor Estimating: 145it [01:26,  1.57it/s]Extractor Estimating: 146it [01:27,  1.62it/s]Extractor Estimating: 147it [01:27,  1.55it/s]Extractor Estimating: 148it [01:28,  1.55it/s]Extractor Estimating: 149it [01:29,  1.40it/s]Extractor Estimating: 150it [01:29,  1.49it/s]Extractor Estimating: 151it [01:30,  1.56it/s]Extractor Estimating: 152it [01:31,  1.56it/s]Extractor Estimating: 153it [01:31,  1.59it/s]Extractor Estimating: 154it [01:32,  1.56it/s]Extractor Estimating: 155it [01:32,  1.59it/s]Extractor Estimating: 156it [01:33,  1.61it/s]Extractor Estimating: 157it [01:34,  1.60it/s]Extractor Estimating: 158it [01:34,  1.63it/s]Extractor Estimating: 159it [01:35,  1.68it/s]Extractor Estimating: 160it [01:35,  1.72it/s]Extractor Estimating: 161it [01:36,  1.71it/s]Extractor Estimating: 162it [01:37,  1.69it/s]Extractor Estimating: 163it [01:37,  1.68it/s]Extractor Estimating: 164it [01:38,  1.66it/s]Extractor Estimating: 165it [01:38,  1.67it/s]Extractor Estimating: 166it [01:39,  1.64it/s]Extractor Estimating: 167it [01:40,  1.60it/s]Extractor Estimating: 168it [01:40,  1.60it/s]Extractor Estimating: 169it [01:41,  1.59it/s]Extractor Estimating: 170it [01:42,  1.60it/s]Extractor Estimating: 171it [01:42,  1.61it/s]Extractor Estimating: 172it [01:43,  1.62it/s]Extractor Estimating: 173it [01:43,  1.66it/s]Extractor Estimating: 174it [01:44,  1.67it/s]Extractor Estimating: 175it [01:45,  1.66it/s]Extractor Estimating: 176it [01:45,  1.60it/s]Extractor Estimating: 177it [01:46,  1.53it/s]Extractor Estimating: 178it [01:47,  1.54it/s]Extractor Estimating: 179it [01:47,  1.56it/s]Extractor Estimating: 180it [01:48,  1.56it/s]Extractor Estimating: 181it [01:49,  1.54it/s]Extractor Estimating: 182it [01:49,  1.52it/s]Extractor Estimating: 183it [01:50,  1.53it/s]Extractor Estimating: 184it [01:51,  1.52it/s]Extractor Estimating: 185it [01:51,  1.54it/s]Extractor Estimating: 186it [01:52,  1.59it/s]Extractor Estimating: 187it [01:52,  1.61it/s]Extractor Estimating: 188it [01:53,  1.67it/s]Extractor Estimating: 189it [01:54,  1.65it/s]Extractor Estimating: 190it [01:54,  1.61it/s]Extractor Estimating: 191it [01:55,  1.60it/s]Extractor Estimating: 192it [01:55,  1.60it/s]Extractor Estimating: 193it [01:56,  1.58it/s]Extractor Estimating: 194it [01:57,  1.60it/s]Extractor Estimating: 195it [01:57,  1.54it/s]Extractor Estimating: 196it [01:58,  1.53it/s]Extractor Estimating: 197it [01:59,  1.50it/s]Extractor Estimating: 198it [01:59,  1.49it/s]Extractor Estimating: 199it [02:00,  1.52it/s]Extractor Estimating: 200it [02:01,  1.53it/s]Extractor Estimating: 201it [02:01,  1.57it/s]Extractor Estimating: 202it [02:02,  1.61it/s]Extractor Estimating: 203it [02:02,  1.63it/s]Extractor Estimating: 204it [02:03,  1.63it/s]Extractor Estimating: 205it [02:04,  1.50it/s]Extractor Estimating: 206it [02:04,  1.57it/s]Extractor Estimating: 207it [02:05,  1.62it/s]Extractor Estimating: 208it [02:06,  1.60it/s]Extractor Estimating: 209it [02:06,  1.56it/s]Extractor Estimating: 210it [02:07,  1.64it/s]Extractor Estimating: 211it [02:08,  1.57it/s]Extractor Estimating: 212it [02:08,  1.54it/s]Extractor Estimating: 213it [02:09,  1.63it/s]Extractor Estimating: 214it [02:10,  1.50it/s]Extractor Estimating: 215it [02:10,  1.60it/s]Extractor Estimating: 216it [02:11,  1.59it/s]Extractor Estimating: 217it [02:11,  1.61it/s]Extractor Estimating: 218it [02:12,  1.61it/s]Extractor Estimating: 219it [02:13,  1.57it/s]Extractor Estimating: 220it [02:13,  1.61it/s]Extractor Estimating: 221it [02:14,  1.64it/s]Extractor Estimating: 222it [02:15,  1.55it/s]Extractor Estimating: 223it [02:15,  1.58it/s]Extractor Estimating: 224it [02:16,  1.59it/s]Extractor Estimating: 225it [02:16,  1.58it/s]Extractor Estimating: 226it [02:17,  1.61it/s]Extractor Estimating: 227it [02:18,  1.60it/s]Extractor Estimating: 228it [02:18,  1.62it/s]Extractor Estimating: 229it [02:19,  1.59it/s]Extractor Estimating: 230it [02:19,  1.65it/s]Extractor Estimating: 231it [02:20,  1.68it/s]Extractor Estimating: 232it [02:21,  1.68it/s]Extractor Estimating: 233it [02:21,  1.63it/s]Extractor Estimating: 234it [02:22,  1.60it/s]Extractor Estimating: 235it [02:23,  1.59it/s]Extractor Estimating: 236it [02:23,  1.58it/s]Extractor Estimating: 237it [02:24,  1.62it/s]Extractor Estimating: 238it [02:24,  1.63it/s]Extractor Estimating: 239it [02:25,  1.63it/s]Extractor Estimating: 240it [02:26,  1.58it/s]Extractor Estimating: 241it [02:26,  1.54it/s]Extractor Estimating: 242it [02:27,  1.62it/s]Extractor Estimating: 243it [02:28,  1.62it/s]Extractor Estimating: 244it [02:28,  1.57it/s]Extractor Estimating: 245it [02:29,  1.61it/s]Extractor Estimating: 246it [02:29,  1.66it/s]Extractor Estimating: 247it [02:30,  1.61it/s]Extractor Estimating: 248it [02:31,  1.67it/s]Extractor Estimating: 249it [02:31,  1.66it/s]Extractor Estimating: 250it [02:32,  1.61it/s]Extractor Estimating: 251it [02:32,  1.60it/s]Extractor Estimating: 252it [02:33,  1.58it/s]Extractor Estimating: 253it [02:34,  1.61it/s]Extractor Estimating: 254it [02:34,  1.56it/s]Extractor Estimating: 255it [02:35,  1.58it/s]Extractor Estimating: 256it [02:36,  1.56it/s]Extractor Estimating: 257it [02:36,  1.57it/s]Extractor Estimating: 258it [02:37,  1.54it/s]Extractor Estimating: 259it [02:38,  1.56it/s]Extractor Estimating: 260it [02:38,  1.54it/s]Extractor Estimating: 261it [02:39,  1.56it/s]Extractor Estimating: 262it [02:40,  1.57it/s]Extractor Estimating: 263it [02:40,  1.60it/s]Extractor Estimating: 264it [02:41,  1.61it/s]Extractor Estimating: 265it [02:41,  1.63it/s]Extractor Estimating: 266it [02:42,  1.62it/s]Extractor Estimating: 267it [02:43,  1.67it/s]Extractor Estimating: 268it [02:43,  1.64it/s]Extractor Estimating: 269it [02:44,  1.64it/s]Extractor Estimating: 270it [02:44,  1.63it/s]Extractor Estimating: 271it [02:45,  1.65it/s]Extractor Estimating: 272it [02:46,  1.66it/s]Extractor Estimating: 273it [02:46,  1.66it/s]Extractor Estimating: 274it [02:47,  1.67it/s]Extractor Estimating: 275it [02:47,  1.72it/s]Extractor Estimating: 276it [02:48,  1.74it/s]Extractor Estimating: 277it [02:48,  1.77it/s]Extractor Estimating: 278it [02:49,  1.72it/s]Extractor Estimating: 279it [02:50,  1.72it/s]Extractor Estimating: 280it [02:50,  1.75it/s]Extractor Estimating: 281it [02:51,  1.79it/s]Extractor Estimating: 282it [02:51,  1.78it/s]Extractor Estimating: 283it [02:52,  1.79it/s]Extractor Estimating: 284it [02:52,  1.79it/s]Extractor Estimating: 285it [02:53,  1.79it/s]Extractor Estimating: 286it [02:54,  1.76it/s]Extractor Estimating: 287it [02:54,  1.82it/s]Extractor Estimating: 288it [02:55,  1.82it/s]Extractor Estimating: 289it [02:55,  1.79it/s]Extractor Estimating: 290it [02:56,  1.79it/s]Extractor Estimating: 291it [02:56,  1.76it/s]Extractor Estimating: 292it [02:57,  1.76it/s]Extractor Estimating: 293it [02:57,  1.69it/s]Extractor Estimating: 294it [02:58,  1.70it/s]Extractor Estimating: 295it [02:59,  1.72it/s]Extractor Estimating: 296it [02:59,  1.76it/s]Extractor Estimating: 297it [03:00,  1.79it/s]Extractor Estimating: 298it [03:00,  1.81it/s]Extractor Estimating: 299it [03:01,  1.85it/s]Extractor Estimating: 300it [03:01,  1.83it/s]Extractor Estimating: 301it [03:02,  1.79it/s]Extractor Estimating: 302it [03:03,  1.72it/s]Extractor Estimating: 303it [03:03,  1.69it/s]Extractor Estimating: 304it [03:04,  1.68it/s]Extractor Estimating: 305it [03:04,  1.63it/s]Extractor Estimating: 306it [03:05,  1.65it/s]Extractor Estimating: 307it [03:06,  1.70it/s]Extractor Estimating: 308it [03:06,  1.71it/s]Extractor Estimating: 309it [03:07,  1.72it/s]Extractor Estimating: 310it [03:07,  1.70it/s]Extractor Estimating: 311it [03:08,  1.66it/s]Extractor Estimating: 312it [03:09,  1.69it/s]Extractor Estimating: 313it [03:09,  1.67it/s]Extractor Estimating: 314it [03:10,  1.66it/s]Extractor Estimating: 315it [03:10,  1.66it/s]Extractor Estimating: 316it [03:11,  1.52it/s]Extractor Estimating: 317it [03:12,  1.62it/s]Extractor Estimating: 318it [03:12,  1.63it/s]Extractor Estimating: 319it [03:13,  1.63it/s]Extractor Estimating: 320it [03:13,  1.64it/s]Extractor Estimating: 321it [03:14,  1.64it/s]Extractor Estimating: 322it [03:15,  1.53it/s]Extractor Estimating: 323it [03:15,  1.59it/s]Extractor Estimating: 324it [03:16,  1.60it/s]Extractor Estimating: 325it [03:17,  1.58it/s]Extractor Estimating: 326it [03:17,  1.59it/s]Extractor Estimating: 327it [03:18,  1.58it/s]Extractor Estimating: 328it [03:19,  1.54it/s]Extractor Estimating: 329it [03:19,  1.52it/s]Extractor Estimating: 330it [03:20,  1.53it/s]Extractor Estimating: 331it [03:21,  1.49it/s]Extractor Estimating: 332it [03:21,  1.51it/s]Extractor Estimating: 333it [03:22,  1.53it/s]Extractor Estimating: 334it [03:23,  1.56it/s]Extractor Estimating: 335it [03:23,  1.56it/s]Extractor Estimating: 336it [03:24,  1.51it/s]Extractor Estimating: 337it [03:25,  1.52it/s]Extractor Estimating: 338it [03:25,  1.57it/s]Extractor Estimating: 339it [03:26,  1.56it/s]Extractor Estimating: 340it [03:26,  1.55it/s]Extractor Estimating: 341it [03:27,  1.63it/s]Extractor Estimating: 342it [03:28,  1.66it/s]Extractor Estimating: 343it [03:28,  1.61it/s]Extractor Estimating: 344it [03:29,  1.62it/s]Extractor Estimating: 345it [03:29,  1.64it/s]Extractor Estimating: 346it [03:30,  1.66it/s]Extractor Estimating: 347it [03:31,  1.65it/s]Extractor Estimating: 348it [03:31,  1.66it/s]Extractor Estimating: 349it [03:32,  1.65it/s]Extractor Estimating: 350it [03:32,  1.62it/s]Extractor Estimating: 351it [03:33,  1.64it/s]Extractor Estimating: 352it [03:34,  1.65it/s]Extractor Estimating: 353it [03:34,  1.69it/s]Extractor Estimating: 354it [03:35,  1.70it/s]Extractor Estimating: 355it [03:35,  1.72it/s]Extractor Estimating: 356it [03:36,  1.70it/s]Extractor Estimating: 357it [03:37,  1.70it/s]Extractor Estimating: 358it [03:37,  1.68it/s]Extractor Estimating: 359it [03:38,  1.74it/s]Extractor Estimating: 360it [03:38,  1.74it/s]Extractor Estimating: 361it [03:39,  1.71it/s]Extractor Estimating: 362it [03:39,  1.73it/s]Extractor Estimating: 363it [03:40,  1.77it/s]Extractor Estimating: 364it [03:41,  1.74it/s]Extractor Estimating: 365it [03:41,  1.76it/s]Extractor Estimating: 366it [03:42,  1.76it/s]Extractor Estimating: 367it [03:42,  1.76it/s]Extractor Estimating: 368it [03:43,  1.81it/s]Extractor Estimating: 369it [03:43,  1.74it/s]Extractor Estimating: 370it [03:44,  1.77it/s]Extractor Estimating: 371it [03:45,  1.72it/s]Extractor Estimating: 372it [03:45,  1.72it/s]Extractor Estimating: 373it [03:46,  1.67it/s]Extractor Estimating: 374it [03:46,  1.67it/s]Extractor Estimating: 375it [03:47,  1.96it/s]Extractor Estimating: 375it [03:47,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:59,711 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:59,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:59,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:59,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:55:59,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:56:00,348 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:56:00,349 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:56:00,913 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:56:01,960 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:56:01,961 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:04,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:04,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:04,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:04,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:56:04,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:56:05,446 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:56:05,448 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:56:05,998 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:56:06,152 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:56:06,152 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:29:55,686 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:29:55,811 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7492 mean pseudo reward: 0.950005482559203
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 21878
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21978, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21978, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.096, loss:643.8841
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.066, loss:636.7234
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.063, loss:617.2371
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.058, loss:580.2708
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.073, loss:599.9663
>> valid entity prec:0.4818, rec:0.4274, f1:0.4530
>> valid relation prec:0.0056, rec:0.0019, f1:0.0029
>> valid relation with NER prec:0.0056, rec:0.0019, f1:0.0029
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 3.216, loss:627.0376
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.074, loss:555.2877
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.059, loss:583.8885
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.058, loss:592.6939
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.057, loss:605.2136
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5326, rec:0.4845, f1:0.5074
>> valid relation prec:0.0191, rec:0.0058, f1:0.0089
>> valid relation with NER prec:0.0191, rec:0.0058, f1:0.0089
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 3.202, loss:575.6910
g_step 1200, step 261, avg_time 1.069, loss:592.2581
g_step 1300, step 48, avg_time 1.069, loss:542.3032
g_step 1400, step 148, avg_time 1.053, loss:562.6435
g_step 1500, step 248, avg_time 1.064, loss:559.9963
>> valid entity prec:0.5233, rec:0.4473, f1:0.4823
>> valid relation prec:0.0060, rec:0.0021, f1:0.0031
>> valid relation with NER prec:0.0060, rec:0.0021, f1:0.0031
g_step 1600, step 35, avg_time 3.193, loss:536.3844
g_step 1700, step 135, avg_time 1.052, loss:510.8266
g_step 1800, step 235, avg_time 1.066, loss:544.0757
g_step 1900, step 22, avg_time 1.067, loss:518.9123
g_step 2000, step 122, avg_time 1.052, loss:484.8979
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4825, rec:0.4604, f1:0.4712
>> valid relation prec:0.0059, rec:0.0022, f1:0.0033
>> valid relation with NER prec:0.0059, rec:0.0022, f1:0.0033
g_step 2100, step 222, avg_time 3.228, loss:516.4282
g_step 2200, step 9, avg_time 1.054, loss:504.6377
g_step 2300, step 109, avg_time 1.056, loss:462.9696
g_step 2400, step 209, avg_time 1.068, loss:488.7305
g_step 2500, step 309, avg_time 1.070, loss:480.3705
>> valid entity prec:0.5324, rec:0.4705, f1:0.4995
>> valid relation prec:0.0065, rec:0.0022, f1:0.0033
>> valid relation with NER prec:0.0065, rec:0.0022, f1:0.0033
g_step 2600, step 96, avg_time 3.209, loss:450.9845
g_step 2700, step 196, avg_time 1.055, loss:445.7515
g_step 2800, step 296, avg_time 1.066, loss:476.1799
g_step 2900, step 83, avg_time 1.047, loss:429.3335
g_step 3000, step 183, avg_time 1.051, loss:450.3465
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4921, rec:0.4347, f1:0.4616
>> valid relation prec:0.0099, rec:0.0035, f1:0.0052
>> valid relation with NER prec:0.0099, rec:0.0035, f1:0.0052
g_step 3100, step 283, avg_time 3.214, loss:449.6481
g_step 3200, step 70, avg_time 1.052, loss:440.2886
g_step 3300, step 170, avg_time 1.060, loss:424.3835
g_step 3400, step 270, avg_time 1.054, loss:423.9320
g_step 3500, step 57, avg_time 1.070, loss:433.0952
>> valid entity prec:0.4940, rec:0.4666, f1:0.4799
>> valid relation prec:0.0161, rec:0.0064, f1:0.0092
>> valid relation with NER prec:0.0161, rec:0.0064, f1:0.0092
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 157, avg_time 3.205, loss:413.9699
g_step 3700, step 257, avg_time 1.057, loss:404.3027
g_step 3800, step 44, avg_time 1.064, loss:408.4919
g_step 3900, step 144, avg_time 1.066, loss:397.9493
g_step 4000, step 244, avg_time 1.069, loss:407.2243
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5152, rec:0.4342, f1:0.4712
>> valid relation prec:0.0092, rec:0.0034, f1:0.0049
>> valid relation with NER prec:0.0092, rec:0.0034, f1:0.0049
g_step 4100, step 31, avg_time 3.192, loss:415.9771
g_step 4200, step 131, avg_time 1.076, loss:380.0239
g_step 4300, step 231, avg_time 1.056, loss:393.7543
g_step 4400, step 18, avg_time 1.044, loss:395.8330
g_step 4500, step 118, avg_time 1.062, loss:360.6283
>> valid entity prec:0.4986, rec:0.5124, f1:0.5054
>> valid relation prec:0.0088, rec:0.0042, f1:0.0057
>> valid relation with NER prec:0.0088, rec:0.0042, f1:0.0057
g_step 4600, step 218, avg_time 3.206, loss:361.4045
g_step 4700, step 5, avg_time 1.076, loss:379.5101
g_step 4800, step 105, avg_time 1.112, loss:338.4113
g_step 4900, step 205, avg_time 1.051, loss:373.4970
g_step 5000, step 305, avg_time 1.055, loss:385.6929
learning rate was adjusted to 0.0008
>> valid entity prec:0.4918, rec:0.4433, f1:0.4663
>> valid relation prec:0.0063, rec:0.0027, f1:0.0038
>> valid relation with NER prec:0.0063, rec:0.0027, f1:0.0038
g_step 5100, step 92, avg_time 3.204, loss:346.0131
g_step 5200, step 192, avg_time 1.055, loss:337.3301
g_step 5300, step 292, avg_time 1.062, loss:351.0544
g_step 5400, step 79, avg_time 1.093, loss:318.6467
g_step 5500, step 179, avg_time 1.042, loss:329.1529
>> valid entity prec:0.5016, rec:0.4799, f1:0.4905
>> valid relation prec:0.0086, rec:0.0030, f1:0.0045
>> valid relation with NER prec:0.0086, rec:0.0030, f1:0.0045
g_step 5600, step 279, avg_time 3.232, loss:353.2050
g_step 5700, step 66, avg_time 1.052, loss:302.6227
g_step 5800, step 166, avg_time 1.070, loss:330.4195
g_step 5900, step 266, avg_time 1.052, loss:341.8694
g_step 6000, step 53, avg_time 1.050, loss:310.2491
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5144, rec:0.4531, f1:0.4818
>> valid relation prec:0.0159, rec:0.0070, f1:0.0098
>> valid relation with NER prec:0.0159, rec:0.0070, f1:0.0098
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 153, avg_time 3.193, loss:310.4162
g_step 6200, step 253, avg_time 1.053, loss:319.7234
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:29:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:29:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-29-55_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:29:56 - WARNING - datasets.builder -   Using custom data configuration default-9cb8e5d0d634e65b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9cb8e5d0d634e65b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:29:59,473 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:29:59,493 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:29:59,493 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:29:59,494 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:29:59,563 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:29:59,595 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:29:59,978 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:30:03,150 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:30:03,186 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9cb8e5d0d634e65b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.24ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.63ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.34ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.83ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.15ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.36ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.52ba/s]100%|██████████| 8/8 [00:01<00:00,  5.41ba/s]100%|██████████| 8/8 [00:01<00:00,  4.19ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.53ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.39ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.81ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.70ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  3.92ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.08ba/s]100%|██████████| 7/7 [00:01<00:00,  4.26ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.33ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.64ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.09ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  8.82ba/s]100%|██████████| 8/8 [00:00<00:00,  8.45ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  5.09ba/s] 43%|████▎     | 3/7 [00:00<00:00,  8.19ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.11ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  7.06ba/s]100%|██████████| 7/7 [00:00<00:00,  8.33ba/s]
[INFO|trainer.py:414] 2023-08-29 11:30:10,127 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:30:10,226 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:30:10,226 >>   Num examples = 7509
[INFO|trainer.py:1149] 2023-08-29 11:30:10,226 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:30:10,226 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:30:10,226 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:30:10,226 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:30:10,226 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:56,  3.31it/s]  0%|          | 2/585 [00:00<02:50,  3.42it/s]  1%|          | 3/585 [00:00<02:48,  3.45it/s]  1%|          | 4/585 [00:01<02:47,  3.47it/s]  1%|          | 5/585 [00:01<02:46,  3.48it/s]  1%|          | 6/585 [00:01<02:55,  3.29it/s]  1%|          | 7/585 [00:02<02:52,  3.35it/s]  1%|▏         | 8/585 [00:02<02:49,  3.40it/s]  2%|▏         | 9/585 [00:02<02:48,  3.42it/s]  2%|▏         | 10/585 [00:02<02:46,  3.44it/s]  2%|▏         | 11/585 [00:03<02:46,  3.45it/s]  2%|▏         | 12/585 [00:03<02:45,  3.47it/s]  2%|▏         | 13/585 [00:03<02:44,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:43,  3.48it/s]  3%|▎         | 16/585 [00:04<02:43,  3.48it/s]  3%|▎         | 17/585 [00:04<02:43,  3.48it/s]  3%|▎         | 18/585 [00:05<02:42,  3.48it/s]  3%|▎         | 19/585 [00:05<02:42,  3.48it/s]  3%|▎         | 20/585 [00:05<02:42,  3.48it/s]  4%|▎         | 21/585 [00:06<02:41,  3.48it/s]  4%|▍         | 22/585 [00:06<02:41,  3.48it/s]  4%|▍         | 23/585 [00:06<02:46,  3.38it/s]  4%|▍         | 24/585 [00:06<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:43,  3.43it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:41,  3.46it/s]  5%|▍         | 28/585 [00:08<02:40,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:39,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.48it/s]  6%|▌         | 34/585 [00:09<02:38,  3.48it/s]  6%|▌         | 35/585 [00:10<02:38,  3.48it/s]  6%|▌         | 36/585 [00:10<02:37,  3.48it/s]  6%|▋         | 37/585 [00:10<02:37,  3.48it/s]  6%|▋         | 38/585 [00:10<02:37,  3.48it/s]  7%|▋         | 39/585 [00:11<02:36,  3.48it/s]  7%|▋         | 40/585 [00:11<02:36,  3.48it/s]  7%|▋         | 41/585 [00:11<02:46,  3.26it/s]  7%|▋         | 42/585 [00:12<02:43,  3.32it/s]  7%|▋         | 43/585 [00:12<02:40,  3.37it/s]  8%|▊         | 44/585 [00:12<02:39,  3.40it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:35,  3.45it/s]  8%|▊         | 48/585 [00:13<02:42,  3.31it/s]  8%|▊         | 49/585 [00:14<02:39,  3.36it/s]  9%|▊         | 50/585 [00:14<02:37,  3.39it/s]  9%|▊         | 51/585 [00:14<02:36,  3.42it/s]  9%|▉         | 52/585 [00:15<02:35,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:16<02:32,  3.47it/s] 10%|▉         | 58/585 [00:16<02:48,  3.12it/s] 10%|█         | 59/585 [00:17<02:43,  3.22it/s] 10%|█         | 60/585 [00:17<02:39,  3.29it/s] 10%|█         | 61/585 [00:17<02:36,  3.34it/s] 11%|█         | 62/585 [00:18<02:34,  3.38it/s] 11%|█         | 63/585 [00:18<02:32,  3.41it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:19<02:52,  3.01it/s] 11%|█▏        | 66/585 [00:19<02:45,  3.14it/s] 11%|█▏        | 67/585 [00:19<02:40,  3.23it/s] 12%|█▏        | 68/585 [00:19<02:36,  3.30it/s] 12%|█▏        | 69/585 [00:20<02:33,  3.35it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.39it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 75/585 [00:22<02:41,  3.17it/s] 13%|█▎        | 76/585 [00:22<02:36,  3.25it/s] 13%|█▎        | 77/585 [00:22<02:33,  3.31it/s] 13%|█▎        | 78/585 [00:22<02:30,  3.36it/s] 14%|█▎        | 79/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 80/585 [00:23<02:41,  3.13it/s] 14%|█▍        | 81/585 [00:23<02:36,  3.21it/s] 14%|█▍        | 82/585 [00:24<02:44,  3.06it/s] 14%|█▍        | 83/585 [00:24<02:38,  3.17it/s] 14%|█▍        | 84/585 [00:24<02:33,  3.26it/s] 15%|█▍        | 85/585 [00:25<02:30,  3.32it/s] 15%|█▍        | 86/585 [00:25<02:28,  3.35it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.39it/s] 15%|█▌        | 88/585 [00:26<02:25,  3.41it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.45it/s] 16%|█▌        | 92/585 [00:27<02:28,  3.31it/s] 16%|█▌        | 93/585 [00:27<02:26,  3.36it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 95/585 [00:28<02:23,  3.41it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:30<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:30<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:31<02:17,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.47it/s] 19%|█▊        | 109/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 110/585 [00:32<02:24,  3.30it/s] 19%|█▉        | 111/585 [00:32<02:21,  3.35it/s] 19%|█▉        | 112/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.40it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.42it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 116/585 [00:34<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 11:30:44,729 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:30:44,729 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:30:44,729 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.50it/s][A
  2%|▏         | 12/782 [00:00<00:15, 51.08it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.23it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.41it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.90it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.48it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.34it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.97it/s][A
  6%|▌         | 48/782 [00:00<00:15, 46.94it/s][A
  7%|▋         | 53/782 [00:01<00:15, 47.04it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.06it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.10it/s][A
  9%|▊         | 68/782 [00:01<00:15, 47.18it/s][A
  9%|▉         | 73/782 [00:01<00:15, 47.05it/s][A
 10%|▉         | 78/782 [00:01<00:14, 47.03it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.97it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.78it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.88it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.82it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.94it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.98it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 47.04it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 47.02it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 44.19it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 45.01it/s][A
 17%|█▋        | 133/782 [00:02<00:14, 45.67it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.08it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.22it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.51it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.70it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.81it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.83it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.71it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.81it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.85it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.98it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.97it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.92it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.95it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.93it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 47.03it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.95it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.89it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 45.75it/s][A
 29%|██▉       | 228/782 [00:04<00:12, 46.16it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.47it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.66it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.55it/s][A
 32%|███▏      | 248/782 [00:05<00:18, 28.21it/s][A
 32%|███▏      | 253/782 [00:05<00:16, 32.27it/s][A
 33%|███▎      | 258/782 [00:05<00:14, 35.63it/s][A
 34%|███▎      | 263/782 [00:05<00:13, 38.41it/s][A
 34%|███▍      | 268/782 [00:05<00:12, 40.52it/s][A
 35%|███▍      | 273/782 [00:06<00:12, 42.33it/s][A
 36%|███▌      | 278/782 [00:06<00:11, 43.63it/s][A
 36%|███▌      | 283/782 [00:06<00:11, 44.64it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 45.33it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 45.65it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 45.98it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.18it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.52it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.65it/s][A
 41%|████      | 318/782 [00:07<00:09, 46.75it/s][A
 41%|████▏     | 323/782 [00:07<00:09, 46.78it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.91it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.90it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.82it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.82it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.82it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 44.36it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 45.17it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 45.75it/s][A
 47%|████▋     | 368/782 [00:08<00:09, 45.91it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.31it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.54it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.59it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.73it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.72it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.70it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.85it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.92it/s][A
 53%|█████▎    | 413/782 [00:09<00:07, 46.86it/s][A
 53%|█████▎    | 418/782 [00:09<00:07, 46.92it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 47.01it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.86it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.78it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.90it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.87it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.92it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.96it/s][A
 59%|█████▊    | 458/782 [00:10<00:06, 46.89it/s][A
 59%|█████▉    | 463/782 [00:10<00:06, 46.92it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.88it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.92it/s][A
 61%|██████    | 478/782 [00:10<00:06, 47.00it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.82it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.71it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.83it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.93it/s][A
 64%|██████▍   | 503/782 [00:11<00:07, 36.08it/s][A
 65%|██████▍   | 508/782 [00:11<00:07, 38.88it/s][A
 66%|██████▌   | 513/782 [00:11<00:06, 40.87it/s][A
 66%|██████▌   | 518/782 [00:11<00:06, 42.52it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 43.79it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 44.63it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 45.27it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 45.80it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.14it/s][A
 70%|███████   | 548/782 [00:12<00:05, 46.16it/s][A
 71%|███████   | 553/782 [00:12<00:04, 46.40it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 46.63it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.68it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.73it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.82it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.78it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.73it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 43.33it/s][A
 76%|███████▌  | 593/782 [00:13<00:04, 44.28it/s][A
 76%|███████▋  | 598/782 [00:13<00:04, 44.99it/s][A
 77%|███████▋  | 603/782 [00:13<00:03, 45.61it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.05it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.32it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.46it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.53it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.53it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.73it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.80it/s][A
 82%|████████▏ | 643/782 [00:14<00:02, 46.76it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 46.80it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.81it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.91it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.91it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.90it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.81it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.88it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 40.04it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 41.87it/s][A
 89%|████████▊ | 693/782 [00:15<00:02, 43.34it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 44.30it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 45.10it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 45.64it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 45.96it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.18it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.31it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.44it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 46.60it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 46.72it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.70it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.78it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.82it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.81it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.88it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.83it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.68it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:17<00:00, 46.77it/s][A 20%|██        | 117/585 [00:51<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:31:02,368 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 11:31:02,652 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:31:07,047 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:31:07,251 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:31:07,338 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:05<1:14:42,  9.60s/it] 20%|██        | 119/585 [01:06<52:55,  6.81s/it]   21%|██        | 120/585 [01:06<37:38,  4.86s/it] 21%|██        | 121/585 [01:06<26:57,  3.49s/it] 21%|██        | 122/585 [01:06<19:29,  2.53s/it] 21%|██        | 123/585 [01:07<14:16,  1.85s/it] 21%|██        | 124/585 [01:07<10:38,  1.38s/it] 21%|██▏       | 125/585 [01:07<08:05,  1.06s/it] 22%|██▏       | 126/585 [01:08<06:18,  1.21it/s] 22%|██▏       | 127/585 [01:08<05:04,  1.51it/s] 22%|██▏       | 128/585 [01:08<04:11,  1.81it/s] 22%|██▏       | 129/585 [01:08<03:35,  2.12it/s] 22%|██▏       | 130/585 [01:09<03:19,  2.28it/s] 22%|██▏       | 131/585 [01:09<02:58,  2.54it/s] 23%|██▎       | 132/585 [01:09<02:43,  2.77it/s] 23%|██▎       | 133/585 [01:10<02:33,  2.95it/s] 23%|██▎       | 134/585 [01:10<02:26,  3.09it/s] 23%|██▎       | 135/585 [01:10<02:20,  3.19it/s] 23%|██▎       | 136/585 [01:11<02:17,  3.27it/s] 23%|██▎       | 137/585 [01:11<02:14,  3.33it/s] 24%|██▎       | 138/585 [01:11<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:11<02:11,  3.40it/s] 24%|██▍       | 140/585 [01:12<02:10,  3.42it/s] 24%|██▍       | 141/585 [01:12<02:31,  2.93it/s] 24%|██▍       | 142/585 [01:12<02:24,  3.07it/s] 24%|██▍       | 143/585 [01:13<02:18,  3.18it/s] 25%|██▍       | 144/585 [01:13<02:15,  3.26it/s] 25%|██▍       | 145/585 [01:13<02:12,  3.32it/s] 25%|██▍       | 146/585 [01:14<02:10,  3.37it/s] 25%|██▌       | 147/585 [01:14<02:08,  3.40it/s] 25%|██▌       | 148/585 [01:14<02:07,  3.42it/s] 25%|██▌       | 149/585 [01:14<02:06,  3.44it/s] 26%|██▌       | 150/585 [01:15<02:06,  3.45it/s] 26%|██▌       | 151/585 [01:15<02:20,  3.08it/s] 26%|██▌       | 152/585 [01:15<02:15,  3.19it/s] 26%|██▌       | 153/585 [01:16<02:11,  3.27it/s] 26%|██▋       | 154/585 [01:16<02:09,  3.33it/s] 26%|██▋       | 155/585 [01:16<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:17<02:06,  3.40it/s] 27%|██▋       | 157/585 [01:17<02:04,  3.43it/s] 27%|██▋       | 158/585 [01:17<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:17<02:03,  3.45it/s] 27%|██▋       | 160/585 [01:18<02:02,  3.46it/s] 28%|██▊       | 161/585 [01:18<02:02,  3.46it/s] 28%|██▊       | 162/585 [01:18<02:14,  3.16it/s] 28%|██▊       | 163/585 [01:19<02:10,  3.24it/s] 28%|██▊       | 164/585 [01:19<02:07,  3.31it/s] 28%|██▊       | 165/585 [01:19<02:05,  3.35it/s] 28%|██▊       | 166/585 [01:20<02:03,  3.39it/s] 29%|██▊       | 167/585 [01:20<02:02,  3.41it/s] 29%|██▊       | 168/585 [01:20<02:01,  3.43it/s] 29%|██▉       | 169/585 [01:20<02:00,  3.44it/s] 29%|██▉       | 170/585 [01:21<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:21<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:21<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:22<01:58,  3.46it/s] 30%|██▉       | 174/585 [01:22<01:58,  3.47it/s] 30%|██▉       | 175/585 [01:22<01:58,  3.47it/s] 30%|███       | 176/585 [01:22<01:57,  3.47it/s] 30%|███       | 177/585 [01:23<01:57,  3.47it/s] 30%|███       | 178/585 [01:23<02:18,  2.94it/s] 31%|███       | 179/585 [01:23<02:12,  3.07it/s] 31%|███       | 180/585 [01:24<02:07,  3.18it/s] 31%|███       | 181/585 [01:24<02:03,  3.26it/s] 31%|███       | 182/585 [01:24<02:01,  3.32it/s] 31%|███▏      | 183/585 [01:25<01:59,  3.36it/s] 31%|███▏      | 184/585 [01:25<01:58,  3.40it/s] 32%|███▏      | 185/585 [01:25<01:57,  3.42it/s] 32%|███▏      | 186/585 [01:26<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:26<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:26<01:58,  3.36it/s] 32%|███▏      | 189/585 [01:26<01:56,  3.39it/s] 32%|███▏      | 190/585 [01:27<01:55,  3.41it/s] 33%|███▎      | 191/585 [01:27<01:54,  3.43it/s] 33%|███▎      | 192/585 [01:27<01:54,  3.44it/s] 33%|███▎      | 193/585 [01:28<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:28<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:28<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:28<01:52,  3.47it/s] 34%|███▎      | 197/585 [01:29<01:51,  3.47it/s] 34%|███▍      | 198/585 [01:29<01:51,  3.47it/s] 34%|███▍      | 199/585 [01:29<01:55,  3.35it/s] 34%|███▍      | 200/585 [01:30<01:53,  3.38it/s] 34%|███▍      | 201/585 [01:30<01:52,  3.41it/s] 35%|███▍      | 202/585 [01:30<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:30<01:51,  3.44it/s] 35%|███▍      | 204/585 [01:31<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:31<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:31<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:32<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:32<01:48,  3.46it/s] 36%|███▌      | 209/585 [01:32<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:33<01:54,  3.27it/s] 36%|███▌      | 211/585 [01:33<01:52,  3.33it/s] 36%|███▌      | 212/585 [01:33<01:50,  3.37it/s] 36%|███▋      | 213/585 [01:33<01:49,  3.40it/s] 37%|███▋      | 214/585 [01:34<01:48,  3.42it/s] 37%|███▋      | 215/585 [01:34<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:34<01:47,  3.45it/s] 37%|███▋      | 217/585 [01:35<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:35<01:46,  3.46it/s] 37%|███▋      | 219/585 [01:35<01:45,  3.46it/s] 38%|███▊      | 220/585 [01:35<01:45,  3.46it/s] 38%|███▊      | 221/585 [01:36<01:50,  3.29it/s] 38%|███▊      | 222/585 [01:36<01:48,  3.34it/s] 38%|███▊      | 223/585 [01:36<01:47,  3.37it/s] 38%|███▊      | 224/585 [01:37<01:46,  3.40it/s] 38%|███▊      | 225/585 [01:37<01:45,  3.42it/s] 39%|███▊      | 226/585 [01:37<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:37<01:43,  3.44it/s] 39%|███▉      | 228/585 [01:38<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:38<01:43,  3.46it/s] 39%|███▉      | 230/585 [01:38<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:39<01:42,  3.46it/s] 40%|███▉      | 232/585 [01:39<01:49,  3.23it/s] 40%|███▉      | 233/585 [01:39<01:46,  3.30it/s] 40%|████      | 234/585 [01:40<02:06,  2.78it/s][INFO|trainer.py:2140] 2023-08-29 11:31:50,569 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:31:50,570 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:31:50,570 >>   Batch size = 8
{'eval_loss': 0.9393563866615295, 'eval_runtime': 17.1858, 'eval_samples_per_second': 363.846, 'eval_steps_per_second': 45.503, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.68it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.77it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.97it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.22it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.92it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.64it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.45it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.88it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.83it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.92it/s][A
  7%|▋         | 58/782 [00:01<00:15, 47.00it/s][A
  8%|▊         | 63/782 [00:01<00:15, 47.01it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.90it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.87it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.84it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.95it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.93it/s][A
 12%|█▏        | 93/782 [00:01<00:15, 44.97it/s][A
 13%|█▎        | 98/782 [00:02<00:15, 45.57it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 45.91it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.27it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.37it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.63it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.71it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.77it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.75it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.67it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.73it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.83it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.89it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.91it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.80it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.86it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 46.92it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.85it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.85it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.88it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.94it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.89it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.91it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.87it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.93it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.89it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.95it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.85it/s][A
 30%|██▉       | 233/782 [00:04<00:12, 42.34it/s][A
 30%|███       | 238/782 [00:05<00:25, 21.04it/s][A
 31%|███       | 243/782 [00:05<00:21, 25.35it/s][A
 32%|███▏      | 248/782 [00:05<00:18, 29.44it/s][A
 32%|███▏      | 253/782 [00:05<00:15, 33.17it/s][A
 33%|███▎      | 258/782 [00:05<00:14, 36.40it/s][A
 34%|███▎      | 263/782 [00:06<00:13, 39.01it/s][A
 34%|███▍      | 268/782 [00:06<00:12, 41.13it/s][A
 35%|███▍      | 273/782 [00:06<00:11, 42.67it/s][A
 36%|███▌      | 278/782 [00:06<00:11, 43.84it/s][A
 36%|███▌      | 283/782 [00:06<00:11, 44.61it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 45.20it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 45.73it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.10it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.36it/s][A
 39%|███▉      | 308/782 [00:07<00:10, 46.51it/s][A
 40%|████      | 313/782 [00:07<00:10, 46.70it/s][A
 41%|████      | 318/782 [00:07<00:09, 46.75it/s][A
 41%|████▏     | 323/782 [00:07<00:09, 46.73it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.79it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.76it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.81it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.77it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.81it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 45.03it/s][A
 46%|████▌     | 358/782 [00:08<00:09, 45.65it/s][A
 46%|████▋     | 363/782 [00:08<00:09, 46.07it/s][A
 47%|████▋     | 368/782 [00:08<00:08, 46.33it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.41it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.57it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.72it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.82it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.72it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.67it/s][A
 52%|█████▏    | 403/782 [00:09<00:08, 46.80it/s][A
 52%|█████▏    | 408/782 [00:09<00:07, 46.79it/s][A
 53%|█████▎    | 413/782 [00:09<00:07, 46.85it/s][A
 53%|█████▎    | 418/782 [00:09<00:07, 46.89it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.90it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.91it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.92it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.81it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.75it/s][A
 57%|█████▋    | 448/782 [00:10<00:07, 46.80it/s][A
 58%|█████▊    | 453/782 [00:10<00:07, 46.77it/s][A
 59%|█████▊    | 458/782 [00:10<00:06, 46.80it/s][A
 59%|█████▉    | 463/782 [00:10<00:06, 46.85it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.94it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.88it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.87it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.83it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.81it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.84it/s][A
 64%|██████▎   | 498/782 [00:11<00:06, 46.83it/s][A
 64%|██████▍   | 503/782 [00:11<00:05, 46.79it/s][A
 65%|██████▍   | 508/782 [00:11<00:05, 46.79it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.85it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.93it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.98it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.63it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.63it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.72it/s][A
 69%|██████▉   | 543/782 [00:12<00:05, 46.73it/s][A
 70%|███████   | 548/782 [00:12<00:04, 46.82it/s][A
 71%|███████   | 553/782 [00:12<00:04, 46.80it/s][A
 71%|███████▏  | 558/782 [00:12<00:04, 46.76it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.81it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.88it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.85it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.86it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.83it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.77it/s][A
 76%|███████▌  | 593/782 [00:13<00:04, 46.84it/s][A
 76%|███████▋  | 598/782 [00:13<00:03, 46.84it/s][A
 77%|███████▋  | 603/782 [00:13<00:03, 46.84it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.80it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.84it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 44.68it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 45.39it/s][A
 80%|████████  | 628/782 [00:13<00:03, 45.80it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.11it/s][A
 82%|████████▏ | 638/782 [00:14<00:03, 46.31it/s][A
 82%|████████▏ | 643/782 [00:14<00:02, 46.55it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 46.70it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.78it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.63it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.62it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.75it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.74it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.62it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 46.72it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 46.84it/s][A
 89%|████████▊ | 693/782 [00:15<00:01, 46.83it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.92it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.74it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.66it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.73it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.84it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.83it/s][A
 93%|█████████▎| 728/782 [00:16<00:01, 46.78it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 46.75it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 46.86it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.83it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.84it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.70it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 42.56it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 43.85it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 44.74it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 45.43it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 45.77it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:17<00:00, 45.77it/s][A 40%|████      | 234/585 [01:57<02:06,  2.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:32:08,178 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 11:32:08,488 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:32:12,377 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:32:12,529 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:32:12,584 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:11<56:00,  9.60s/it] 40%|████      | 236/585 [02:11<39:38,  6.81s/it] 41%|████      | 237/585 [02:12<28:10,  4.86s/it] 41%|████      | 238/585 [02:12<20:09,  3.49s/it] 41%|████      | 239/585 [02:12<14:34,  2.53s/it] 41%|████      | 240/585 [02:12<10:39,  1.85s/it] 41%|████      | 241/585 [02:13<07:56,  1.38s/it] 41%|████▏     | 242/585 [02:13<06:02,  1.06s/it] 42%|████▏     | 243/585 [02:13<04:42,  1.21it/s] 42%|████▏     | 244/585 [02:14<03:46,  1.51it/s] 42%|████▏     | 245/585 [02:14<03:07,  1.82it/s] 42%|████▏     | 246/585 [02:14<02:39,  2.12it/s] 42%|████▏     | 247/585 [02:14<02:27,  2.29it/s] 42%|████▏     | 248/585 [02:15<02:12,  2.55it/s] 43%|████▎     | 249/585 [02:15<02:01,  2.77it/s] 43%|████▎     | 250/585 [02:15<01:53,  2.95it/s] 43%|████▎     | 251/585 [02:16<01:48,  3.09it/s] 43%|████▎     | 252/585 [02:16<01:44,  3.20it/s] 43%|████▎     | 253/585 [02:16<01:41,  3.28it/s] 43%|████▎     | 254/585 [02:17<01:39,  3.33it/s] 44%|████▎     | 255/585 [02:17<01:37,  3.38it/s] 44%|████▍     | 256/585 [02:17<01:36,  3.41it/s] 44%|████▍     | 257/585 [02:17<01:35,  3.42it/s] 44%|████▍     | 258/585 [02:18<01:42,  3.20it/s] 44%|████▍     | 259/585 [02:18<01:39,  3.28it/s] 44%|████▍     | 260/585 [02:18<01:37,  3.33it/s] 45%|████▍     | 261/585 [02:19<01:36,  3.37it/s] 45%|████▍     | 262/585 [02:19<01:34,  3.40it/s] 45%|████▍     | 263/585 [02:19<01:33,  3.43it/s] 45%|████▌     | 264/585 [02:19<01:33,  3.44it/s] 45%|████▌     | 265/585 [02:20<01:32,  3.45it/s] 45%|████▌     | 266/585 [02:20<01:32,  3.46it/s] 46%|████▌     | 267/585 [02:20<01:31,  3.46it/s] 46%|████▌     | 268/585 [02:21<01:31,  3.46it/s] 46%|████▌     | 269/585 [02:21<01:37,  3.25it/s] 46%|████▌     | 270/585 [02:21<01:35,  3.31it/s] 46%|████▋     | 271/585 [02:22<01:33,  3.36it/s] 46%|████▋     | 272/585 [02:22<01:32,  3.39it/s] 47%|████▋     | 273/585 [02:22<01:31,  3.41it/s] 47%|████▋     | 274/585 [02:22<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:23<01:30,  3.44it/s] 47%|████▋     | 276/585 [02:23<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:23<01:29,  3.46it/s] 48%|████▊     | 278/585 [02:24<01:28,  3.46it/s] 48%|████▊     | 279/585 [02:24<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:24<01:28,  3.46it/s] 48%|████▊     | 281/585 [02:24<01:31,  3.31it/s] 48%|████▊     | 282/585 [02:25<01:30,  3.35it/s] 48%|████▊     | 283/585 [02:25<01:29,  3.39it/s] 49%|████▊     | 284/585 [02:25<01:28,  3.41it/s] 49%|████▊     | 285/585 [02:26<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:26<01:26,  3.44it/s] 49%|████▉     | 287/585 [02:26<01:26,  3.45it/s] 49%|████▉     | 288/585 [02:26<01:25,  3.46it/s] 49%|████▉     | 289/585 [02:27<01:25,  3.46it/s] 50%|████▉     | 290/585 [02:27<01:25,  3.46it/s] 50%|████▉     | 291/585 [02:27<01:24,  3.47it/s] 50%|████▉     | 292/585 [02:28<01:27,  3.33it/s] 50%|█████     | 293/585 [02:28<01:26,  3.37it/s] 50%|█████     | 294/585 [02:28<01:25,  3.40it/s] 50%|█████     | 295/585 [02:29<01:24,  3.42it/s] 51%|█████     | 296/585 [02:29<01:24,  3.44it/s] 51%|█████     | 297/585 [02:29<01:23,  3.45it/s] 51%|█████     | 298/585 [02:29<01:23,  3.45it/s] 51%|█████     | 299/585 [02:30<01:22,  3.46it/s] 51%|█████▏    | 300/585 [02:30<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:30<01:21,  3.46it/s] 52%|█████▏    | 302/585 [02:31<01:21,  3.46it/s] 52%|█████▏    | 303/585 [02:31<01:23,  3.36it/s] 52%|█████▏    | 304/585 [02:31<01:22,  3.39it/s] 52%|█████▏    | 305/585 [02:31<01:21,  3.42it/s] 52%|█████▏    | 306/585 [02:32<01:21,  3.43it/s] 52%|█████▏    | 307/585 [02:32<01:20,  3.44it/s] 53%|█████▎    | 308/585 [02:32<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:33<01:19,  3.46it/s] 53%|█████▎    | 310/585 [02:33<01:19,  3.46it/s] 53%|█████▎    | 311/585 [02:33<01:19,  3.46it/s] 53%|█████▎    | 312/585 [02:33<01:18,  3.46it/s] 54%|█████▎    | 313/585 [02:34<01:18,  3.46it/s] 54%|█████▎    | 314/585 [02:34<01:22,  3.30it/s] 54%|█████▍    | 315/585 [02:34<01:20,  3.35it/s] 54%|█████▍    | 316/585 [02:35<01:19,  3.39it/s] 54%|█████▍    | 317/585 [02:35<01:18,  3.41it/s] 54%|█████▍    | 318/585 [02:35<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:36<01:17,  3.44it/s] 55%|█████▍    | 320/585 [02:36<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:36<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:36<01:16,  3.46it/s] 55%|█████▌    | 323/585 [02:37<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:37<01:15,  3.46it/s] 56%|█████▌    | 325/585 [02:37<01:16,  3.39it/s] 56%|█████▌    | 326/585 [02:38<01:15,  3.41it/s] 56%|█████▌    | 327/585 [02:38<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:38<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:38<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:39<01:13,  3.46it/s] 57%|█████▋    | 331/585 [02:39<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:39<01:13,  3.46it/s] 57%|█████▋    | 333/585 [02:40<01:29,  2.81it/s] 57%|█████▋    | 334/585 [02:40<01:24,  2.97it/s] 57%|█████▋    | 335/585 [02:40<01:23,  2.99it/s] 57%|█████▋    | 336/585 [02:41<01:19,  3.12it/s] 58%|█████▊    | 337/585 [02:41<01:17,  3.22it/s] 58%|█████▊    | 338/585 [02:41<01:15,  3.29it/s] 58%|█████▊    | 339/585 [02:42<01:13,  3.34it/s] 58%|█████▊    | 340/585 [02:42<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:42<01:11,  3.40it/s] 58%|█████▊    | 342/585 [02:42<01:11,  3.42it/s] 59%|█████▊    | 343/585 [02:43<01:10,  3.44it/s] 59%|█████▉    | 344/585 [02:43<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:43<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:44<01:11,  3.35it/s] 59%|█████▉    | 347/585 [02:44<01:10,  3.38it/s] 59%|█████▉    | 348/585 [02:44<01:09,  3.41it/s] 60%|█████▉    | 349/585 [02:45<01:08,  3.42it/s] 60%|█████▉    | 350/585 [02:45<01:08,  3.44it/s] 60%|██████    | 351/585 [02:45<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 11:32:55,885 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:32:55,886 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:32:55,886 >>   Batch size = 8
{'eval_loss': 0.9569759368896484, 'eval_runtime': 17.2513, 'eval_samples_per_second': 362.465, 'eval_steps_per_second': 45.33, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:15, 50.35it/s][A
  2%|▏         | 12/782 [00:00<00:16, 46.42it/s][A
  2%|▏         | 17/782 [00:00<00:16, 46.59it/s][A
  3%|▎         | 22/782 [00:00<00:16, 46.74it/s][A
  3%|▎         | 27/782 [00:00<00:16, 46.90it/s][A
  4%|▍         | 32/782 [00:00<00:15, 46.88it/s][A
  5%|▍         | 37/782 [00:00<00:15, 46.98it/s][A
  5%|▌         | 42/782 [00:00<00:15, 46.80it/s][A
  6%|▌         | 47/782 [00:01<00:15, 46.88it/s][A
  7%|▋         | 52/782 [00:01<00:15, 46.72it/s][A
  7%|▋         | 57/782 [00:01<00:15, 46.75it/s][A
  8%|▊         | 62/782 [00:01<00:15, 45.57it/s][A
  9%|▊         | 67/782 [00:01<00:15, 46.05it/s][A
  9%|▉         | 72/782 [00:01<00:15, 46.30it/s][A
 10%|▉         | 77/782 [00:01<00:15, 46.50it/s][A
 10%|█         | 82/782 [00:01<00:14, 46.70it/s][A
 11%|█         | 87/782 [00:01<00:14, 46.75it/s][A
 12%|█▏        | 92/782 [00:01<00:14, 46.80it/s][A
 12%|█▏        | 97/782 [00:02<00:14, 46.67it/s][A
 13%|█▎        | 102/782 [00:02<00:14, 46.74it/s][A
 14%|█▎        | 107/782 [00:02<00:14, 46.57it/s][A
 14%|█▍        | 112/782 [00:02<00:14, 46.73it/s][A
 15%|█▍        | 117/782 [00:02<00:14, 46.73it/s][A
 16%|█▌        | 122/782 [00:02<00:14, 46.87it/s][A
 16%|█▌        | 127/782 [00:02<00:13, 46.84it/s][A
 17%|█▋        | 132/782 [00:02<00:13, 46.92it/s][A
 18%|█▊        | 137/782 [00:02<00:13, 46.97it/s][A
 18%|█▊        | 142/782 [00:03<00:13, 46.98it/s][A
 19%|█▉        | 147/782 [00:03<00:13, 46.74it/s][A
 19%|█▉        | 152/782 [00:03<00:13, 46.73it/s][A
 20%|██        | 157/782 [00:03<00:13, 46.71it/s][A
 21%|██        | 162/782 [00:03<00:13, 46.76it/s][A
 21%|██▏       | 167/782 [00:03<00:13, 46.87it/s][A
 22%|██▏       | 172/782 [00:03<00:13, 46.80it/s][A
 23%|██▎       | 177/782 [00:03<00:12, 46.88it/s][A
 23%|██▎       | 182/782 [00:03<00:12, 46.93it/s][A
 24%|██▍       | 187/782 [00:03<00:12, 46.93it/s][A
 25%|██▍       | 192/782 [00:04<00:12, 46.83it/s][A
 25%|██▌       | 197/782 [00:04<00:12, 46.83it/s][A
 26%|██▌       | 202/782 [00:04<00:12, 46.72it/s][A
 26%|██▋       | 207/782 [00:04<00:13, 43.19it/s][A
 27%|██▋       | 212/782 [00:04<00:12, 44.27it/s][A
 28%|██▊       | 217/782 [00:04<00:12, 45.07it/s][A
 28%|██▊       | 222/782 [00:04<00:12, 45.59it/s][A
 29%|██▉       | 227/782 [00:04<00:12, 46.01it/s][A
 30%|██▉       | 232/782 [00:04<00:11, 46.32it/s][A
 30%|███       | 237/782 [00:05<00:11, 46.53it/s][A
 31%|███       | 242/782 [00:05<00:11, 46.53it/s][A
 32%|███▏      | 247/782 [00:05<00:11, 46.52it/s][A
 32%|███▏      | 252/782 [00:05<00:11, 46.46it/s][A
 33%|███▎      | 257/782 [00:05<00:11, 46.61it/s][A
 34%|███▎      | 262/782 [00:05<00:11, 46.71it/s][A
 34%|███▍      | 267/782 [00:05<00:11, 46.80it/s][A
 35%|███▍      | 272/782 [00:05<00:10, 46.79it/s][A
 35%|███▌      | 277/782 [00:05<00:10, 46.88it/s][A
 36%|███▌      | 282/782 [00:06<00:10, 46.94it/s][A
 37%|███▋      | 287/782 [00:06<00:10, 46.90it/s][A
 37%|███▋      | 292/782 [00:06<00:10, 46.73it/s][A
 38%|███▊      | 297/782 [00:06<00:10, 46.69it/s][A
 39%|███▊      | 302/782 [00:06<00:10, 46.75it/s][A
 39%|███▉      | 307/782 [00:06<00:10, 46.68it/s][A
 40%|███▉      | 312/782 [00:06<00:10, 43.94it/s][A
 41%|████      | 317/782 [00:06<00:10, 44.84it/s][A
 41%|████      | 322/782 [00:06<00:10, 45.50it/s][A
 42%|████▏     | 327/782 [00:07<00:09, 45.94it/s][A
 42%|████▏     | 332/782 [00:07<00:09, 46.26it/s][A
 43%|████▎     | 337/782 [00:07<00:09, 46.36it/s][A
 44%|████▎     | 342/782 [00:07<00:09, 46.57it/s][A
 44%|████▍     | 347/782 [00:07<00:09, 46.72it/s][A
 45%|████▌     | 352/782 [00:07<00:09, 46.60it/s][A
 46%|████▌     | 357/782 [00:07<00:09, 46.51it/s][A
 46%|████▋     | 362/782 [00:07<00:09, 46.65it/s][A
 47%|████▋     | 367/782 [00:07<00:08, 46.75it/s][A
 48%|████▊     | 372/782 [00:08<00:08, 46.80it/s][A
 48%|████▊     | 377/782 [00:08<00:08, 46.89it/s][A
 49%|████▉     | 382/782 [00:08<00:08, 46.94it/s][A
 49%|████▉     | 387/782 [00:08<00:08, 46.78it/s][A
 50%|█████     | 392/782 [00:08<00:08, 46.81it/s][A
 51%|█████     | 397/782 [00:08<00:08, 46.80it/s][A
 51%|█████▏    | 402/782 [00:08<00:08, 46.74it/s][A
 52%|█████▏    | 407/782 [00:08<00:08, 46.73it/s][A
 53%|█████▎    | 412/782 [00:08<00:07, 46.69it/s][A
 53%|█████▎    | 417/782 [00:08<00:07, 46.78it/s][A
 54%|█████▍    | 422/782 [00:09<00:07, 46.84it/s][A
 55%|█████▍    | 427/782 [00:09<00:07, 46.91it/s][A
 55%|█████▌    | 432/782 [00:09<00:07, 46.92it/s][A
 56%|█████▌    | 437/782 [00:09<00:07, 46.84it/s][A
 57%|█████▋    | 442/782 [00:09<00:07, 46.76it/s][A
 57%|█████▋    | 447/782 [00:09<00:07, 46.78it/s][A
 58%|█████▊    | 452/782 [00:09<00:07, 43.33it/s][A
 58%|█████▊    | 457/782 [00:09<00:07, 44.32it/s][A
 59%|█████▉    | 462/782 [00:09<00:07, 45.10it/s][A
 60%|█████▉    | 467/782 [00:10<00:06, 45.64it/s][A
 60%|██████    | 472/782 [00:10<00:06, 46.06it/s][A
 61%|██████    | 477/782 [00:10<00:06, 46.34it/s][A
 62%|██████▏   | 482/782 [00:10<00:06, 46.47it/s][A
 62%|██████▏   | 487/782 [00:10<00:06, 46.63it/s][A
 63%|██████▎   | 492/782 [00:10<00:06, 46.53it/s][A
 64%|██████▎   | 497/782 [00:10<00:06, 46.45it/s][A
 64%|██████▍   | 502/782 [00:10<00:06, 46.63it/s][A
 65%|██████▍   | 507/782 [00:10<00:05, 46.62it/s][A
 65%|██████▌   | 512/782 [00:11<00:05, 46.71it/s][A
 66%|██████▌   | 517/782 [00:11<00:05, 46.81it/s][A
 67%|██████▋   | 522/782 [00:11<00:05, 46.88it/s][A
 67%|██████▋   | 527/782 [00:11<00:05, 46.79it/s][A
 68%|██████▊   | 532/782 [00:11<00:05, 46.90it/s][A
 69%|██████▊   | 537/782 [00:11<00:05, 46.80it/s][A
 69%|██████▉   | 542/782 [00:11<00:05, 46.78it/s][A
 70%|██████▉   | 547/782 [00:11<00:05, 46.73it/s][A
 71%|███████   | 552/782 [00:11<00:04, 46.73it/s][A
 71%|███████   | 557/782 [00:11<00:04, 46.73it/s][A
 72%|███████▏  | 562/782 [00:12<00:04, 46.79it/s][A
 73%|███████▎  | 567/782 [00:12<00:04, 46.87it/s][A
 73%|███████▎  | 572/782 [00:12<00:04, 46.82it/s][A
 74%|███████▍  | 577/782 [00:12<00:04, 46.85it/s][A
 74%|███████▍  | 582/782 [00:12<00:04, 46.80it/s][A
 75%|███████▌  | 587/782 [00:12<00:04, 46.78it/s][A
 76%|███████▌  | 592/782 [00:12<00:04, 41.39it/s][A
 76%|███████▋  | 597/782 [00:12<00:04, 42.91it/s][A
 77%|███████▋  | 602/782 [00:12<00:04, 44.04it/s][A
 78%|███████▊  | 607/782 [00:13<00:03, 44.90it/s][A
 78%|███████▊  | 612/782 [00:13<00:03, 45.52it/s][A
 79%|███████▉  | 617/782 [00:13<00:03, 45.89it/s][A
 80%|███████▉  | 622/782 [00:13<00:03, 46.25it/s][A
 80%|████████  | 627/782 [00:13<00:03, 46.38it/s][A
 81%|████████  | 632/782 [00:13<00:03, 46.31it/s][A
 81%|████████▏ | 637/782 [00:13<00:03, 46.34it/s][A
 82%|████████▏ | 642/782 [00:13<00:03, 46.43it/s][A
 83%|████████▎ | 647/782 [00:13<00:02, 46.51it/s][A
 83%|████████▎ | 652/782 [00:14<00:02, 46.61it/s][A
 84%|████████▍ | 657/782 [00:14<00:02, 46.74it/s][A
 85%|████████▍ | 662/782 [00:14<00:02, 46.84it/s][A
 85%|████████▌ | 667/782 [00:14<00:02, 46.88it/s][A
 86%|████████▌ | 672/782 [00:14<00:02, 46.81it/s][A
 87%|████████▋ | 677/782 [00:14<00:02, 46.67it/s][A
 87%|████████▋ | 682/782 [00:14<00:02, 46.61it/s][A
 88%|████████▊ | 687/782 [00:14<00:02, 46.69it/s][A
 88%|████████▊ | 692/782 [00:14<00:01, 46.73it/s][A
 89%|████████▉ | 697/782 [00:15<00:01, 46.71it/s][A
 90%|████████▉ | 702/782 [00:15<00:01, 46.73it/s][A
 90%|█████████ | 707/782 [00:15<00:01, 46.83it/s][A
 91%|█████████ | 712/782 [00:15<00:01, 46.88it/s][A
 92%|█████████▏| 717/782 [00:15<00:01, 46.88it/s][A
 92%|█████████▏| 722/782 [00:15<00:01, 46.66it/s][A
 93%|█████████▎| 727/782 [00:15<00:01, 46.69it/s][A
 94%|█████████▎| 732/782 [00:15<00:01, 41.86it/s][A
 94%|█████████▍| 737/782 [00:15<00:01, 43.26it/s][A
 95%|█████████▍| 742/782 [00:16<00:00, 44.28it/s][A
 96%|█████████▌| 747/782 [00:16<00:00, 45.06it/s][A
 96%|█████████▌| 752/782 [00:16<00:00, 45.58it/s][A
 97%|█████████▋| 757/782 [00:16<00:00, 46.01it/s][A
 97%|█████████▋| 762/782 [00:16<00:00, 46.30it/s][A
 98%|█████████▊| 767/782 [00:16<00:00, 46.39it/s][A
 99%|█████████▊| 772/782 [00:16<00:00, 46.29it/s][A
 99%|█████████▉| 777/782 [00:16<00:00, 46.35it/s][A
100%|██████████| 782/782 [00:16<00:00, 46.49it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.49it/s][A 60%|██████    | 351/585 [03:02<01:09,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:33:13,249 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 11:33:13,511 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:33:18,043 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:33:18,267 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:33:18,370 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:17<37:30,  9.66s/it] 60%|██████    | 353/585 [03:17<26:34,  6.87s/it] 61%|██████    | 354/585 [03:17<18:50,  4.90s/it] 61%|██████    | 355/585 [03:18<13:28,  3.51s/it] 61%|██████    | 356/585 [03:18<09:42,  2.55s/it] 61%|██████    | 357/585 [03:18<07:06,  1.87s/it] 61%|██████    | 358/585 [03:18<05:16,  1.39s/it] 61%|██████▏   | 359/585 [03:19<04:00,  1.06s/it] 62%|██████▏   | 360/585 [03:19<03:06,  1.21it/s] 62%|██████▏   | 361/585 [03:19<02:29,  1.50it/s] 62%|██████▏   | 362/585 [03:20<02:03,  1.81it/s] 62%|██████▏   | 363/585 [03:20<01:45,  2.11it/s] 62%|██████▏   | 364/585 [03:20<01:36,  2.28it/s] 62%|██████▏   | 365/585 [03:20<01:26,  2.55it/s] 63%|██████▎   | 366/585 [03:21<01:19,  2.77it/s] 63%|██████▎   | 367/585 [03:21<01:13,  2.95it/s] 63%|██████▎   | 368/585 [03:21<01:10,  3.09it/s] 63%|██████▎   | 369/585 [03:22<01:07,  3.20it/s] 63%|██████▎   | 370/585 [03:22<01:05,  3.28it/s] 63%|██████▎   | 371/585 [03:22<01:04,  3.34it/s] 64%|██████▎   | 372/585 [03:23<01:03,  3.38it/s] 64%|██████▍   | 373/585 [03:23<01:10,  3.01it/s] 64%|██████▍   | 374/585 [03:23<01:10,  2.98it/s] 64%|██████▍   | 375/585 [03:24<01:07,  3.10it/s] 64%|██████▍   | 376/585 [03:24<01:05,  3.20it/s] 64%|██████▍   | 377/585 [03:24<01:03,  3.28it/s] 65%|██████▍   | 378/585 [03:24<01:02,  3.33it/s] 65%|██████▍   | 379/585 [03:25<01:01,  3.38it/s] 65%|██████▍   | 380/585 [03:25<01:00,  3.40it/s] 65%|██████▌   | 381/585 [03:25<00:59,  3.42it/s] 65%|██████▌   | 382/585 [03:26<00:59,  3.44it/s] 65%|██████▌   | 383/585 [03:26<01:01,  3.29it/s] 66%|██████▌   | 384/585 [03:26<01:00,  3.34it/s] 66%|██████▌   | 385/585 [03:26<00:59,  3.38it/s] 66%|██████▌   | 386/585 [03:27<00:58,  3.41it/s] 66%|██████▌   | 387/585 [03:27<00:57,  3.42it/s] 66%|██████▋   | 388/585 [03:27<00:57,  3.43it/s] 66%|██████▋   | 389/585 [03:28<00:56,  3.44it/s] 67%|██████▋   | 390/585 [03:28<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:28<00:56,  3.46it/s] 67%|██████▋   | 392/585 [03:29<00:55,  3.46it/s] 67%|██████▋   | 393/585 [03:29<00:55,  3.46it/s] 67%|██████▋   | 394/585 [03:29<00:58,  3.28it/s] 68%|██████▊   | 395/585 [03:29<00:56,  3.34it/s] 68%|██████▊   | 396/585 [03:30<00:56,  3.37it/s] 68%|██████▊   | 397/585 [03:30<00:55,  3.40it/s] 68%|██████▊   | 398/585 [03:30<00:54,  3.42it/s] 68%|██████▊   | 399/585 [03:31<00:54,  3.44it/s] 68%|██████▊   | 400/585 [03:31<00:53,  3.44it/s] 69%|██████▊   | 401/585 [03:31<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:31<00:53,  3.45it/s] 69%|██████▉   | 403/585 [03:32<00:52,  3.46it/s] 69%|██████▉   | 404/585 [03:32<00:52,  3.46it/s] 69%|██████▉   | 405/585 [03:32<00:54,  3.33it/s] 69%|██████▉   | 406/585 [03:33<00:53,  3.37it/s] 70%|██████▉   | 407/585 [03:33<00:52,  3.40it/s] 70%|██████▉   | 408/585 [03:33<00:51,  3.42it/s] 70%|██████▉   | 409/585 [03:34<00:51,  3.43it/s] 70%|███████   | 410/585 [03:34<00:50,  3.44it/s] 70%|███████   | 411/585 [03:34<00:50,  3.45it/s] 70%|███████   | 412/585 [03:34<00:50,  3.45it/s] 71%|███████   | 413/585 [03:35<00:49,  3.46it/s] 71%|███████   | 414/585 [03:35<00:49,  3.46it/s] 71%|███████   | 415/585 [03:35<00:49,  3.46it/s] 71%|███████   | 416/585 [03:36<00:51,  3.30it/s] 71%|███████▏  | 417/585 [03:36<00:50,  3.35it/s] 71%|███████▏  | 418/585 [03:36<00:49,  3.39it/s] 72%|███████▏  | 419/585 [03:36<00:48,  3.41it/s] 72%|███████▏  | 420/585 [03:37<00:48,  3.43it/s] 72%|███████▏  | 421/585 [03:37<00:47,  3.44it/s] 72%|███████▏  | 422/585 [03:37<00:47,  3.45it/s] 72%|███████▏  | 423/585 [03:38<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:38<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:38<00:46,  3.46it/s] 73%|███████▎  | 426/585 [03:38<00:45,  3.46it/s] 73%|███████▎  | 427/585 [03:39<00:47,  3.31it/s] 73%|███████▎  | 428/585 [03:39<00:46,  3.36it/s] 73%|███████▎  | 429/585 [03:39<00:46,  3.39it/s] 74%|███████▎  | 430/585 [03:40<00:55,  2.80it/s] 74%|███████▎  | 431/585 [03:40<00:51,  2.96it/s] 74%|███████▍  | 432/585 [03:40<00:49,  3.10it/s] 74%|███████▍  | 433/585 [03:41<00:47,  3.20it/s] 74%|███████▍  | 434/585 [03:41<00:46,  3.28it/s] 74%|███████▍  | 435/585 [03:41<00:45,  3.33it/s] 75%|███████▍  | 436/585 [03:42<00:44,  3.37it/s] 75%|███████▍  | 437/585 [03:42<00:45,  3.23it/s] 75%|███████▍  | 438/585 [03:42<00:44,  3.30it/s] 75%|███████▌  | 439/585 [03:43<00:43,  3.35it/s] 75%|███████▌  | 440/585 [03:43<00:42,  3.38it/s] 75%|███████▌  | 441/585 [03:43<00:42,  3.41it/s] 76%|███████▌  | 442/585 [03:43<00:41,  3.42it/s] 76%|███████▌  | 443/585 [03:44<00:41,  3.44it/s] 76%|███████▌  | 444/585 [03:44<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:44<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:45<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:45<00:39,  3.46it/s] 77%|███████▋  | 448/585 [03:45<00:42,  3.24it/s] 77%|███████▋  | 449/585 [03:46<00:43,  3.14it/s] 77%|███████▋  | 450/585 [03:46<00:41,  3.23it/s] 77%|███████▋  | 451/585 [03:46<00:40,  3.29it/s] 77%|███████▋  | 452/585 [03:46<00:39,  3.34it/s] 77%|███████▋  | 453/585 [03:47<00:39,  3.38it/s] 78%|███████▊  | 454/585 [03:47<00:38,  3.40it/s] 78%|███████▊  | 455/585 [03:47<00:37,  3.42it/s] 78%|███████▊  | 456/585 [03:48<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:48<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:48<00:38,  3.30it/s] 78%|███████▊  | 459/585 [03:48<00:37,  3.35it/s] 79%|███████▊  | 460/585 [03:49<00:36,  3.38it/s] 79%|███████▉  | 461/585 [03:49<00:36,  3.41it/s] 79%|███████▉  | 462/585 [03:49<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:50<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:50<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:50<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:50<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:51<00:34,  3.46it/s] 80%|████████  | 468/585 [03:51<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 11:34:01,859 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:34:01,859 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:34:01,859 >>   Batch size = 8
{'eval_loss': 0.9707807302474976, 'eval_runtime': 16.9416, 'eval_samples_per_second': 369.091, 'eval_steps_per_second': 46.159, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.07it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.66it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.93it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.08it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.82it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.54it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.29it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.88it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.86it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.86it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.82it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.92it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.92it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.93it/s][A
 10%|▉         | 78/782 [00:01<00:14, 46.98it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.92it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.77it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.78it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.77it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.26it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.40it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.60it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.61it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.76it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.85it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.84it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.74it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.68it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.82it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.82it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.82it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.86it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.80it/s][A
 22%|██▏       | 173/782 [00:03<00:12, 46.89it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.90it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.84it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.78it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.75it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.79it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.85it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.79it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.73it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.79it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.87it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.86it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.83it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.76it/s][A
 31%|███       | 243/782 [00:05<00:12, 41.49it/s][A
 32%|███▏      | 248/782 [00:05<00:12, 42.97it/s][A
 32%|███▏      | 253/782 [00:05<00:12, 44.04it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 44.82it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 45.49it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 45.83it/s][A
 35%|███▍      | 273/782 [00:05<00:11, 46.21it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.41it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.37it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.41it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.56it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.69it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.81it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.67it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.75it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.83it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.86it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.84it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.69it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.66it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.82it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.88it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.75it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.82it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.89it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.84it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.89it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.81it/s][A
 49%|████▉     | 383/782 [00:08<00:09, 44.27it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 44.99it/s][A
 50%|█████     | 393/782 [00:08<00:08, 45.59it/s][A
 51%|█████     | 398/782 [00:08<00:08, 45.93it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.29it/s][A
 52%|█████▏    | 408/782 [00:08<00:08, 46.52it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.60it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.69it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.58it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.60it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.73it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.68it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.71it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.79it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.86it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.83it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.88it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.76it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.67it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.74it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.77it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.78it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.79it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.80it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.85it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.89it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.70it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.76it/s][A
 67%|██████▋   | 523/782 [00:11<00:06, 42.72it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 43.96it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 44.76it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 45.46it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 45.85it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.16it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.39it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.56it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.39it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.45it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.55it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.67it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.70it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.83it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.71it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.80it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.75it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.64it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.65it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.66it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 43.41it/s][A
 80%|████████  | 628/782 [00:13<00:03, 44.38it/s][A
 81%|████████  | 633/782 [00:13<00:03, 45.17it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 45.72it/s][A
 82%|████████▏ | 643/782 [00:13<00:03, 46.09it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.30it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.45it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.56it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 45.24it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 45.66it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 45.98it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.17it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.45it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.59it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.57it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.75it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.66it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.60it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.67it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.58it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.54it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.74it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.81it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.76it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.69it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.72it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.70it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.64it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.64it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.72it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.69it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.76it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.76it/s][A 80%|████████  | 468/585 [04:08<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:34:19,073 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 11:34:19,353 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:34:23,979 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:34:24,262 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:34:24,348 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:27<21:04, 10.90s/it] 80%|████████  | 470/585 [04:27<14:47,  7.72s/it] 81%|████████  | 471/585 [04:27<10:28,  5.51s/it] 81%|████████  | 472/585 [04:28<07:25,  3.94s/it] 81%|████████  | 473/585 [04:28<05:18,  2.85s/it] 81%|████████  | 474/585 [04:28<03:50,  2.08s/it] 81%|████████  | 475/585 [04:28<02:49,  1.54s/it] 81%|████████▏ | 476/585 [04:29<02:06,  1.16s/it] 82%|████████▏ | 477/585 [04:29<01:37,  1.11it/s] 82%|████████▏ | 478/585 [04:29<01:16,  1.39it/s] 82%|████████▏ | 479/585 [04:30<01:02,  1.70it/s] 82%|████████▏ | 480/585 [04:30<00:52,  2.01it/s] 82%|████████▏ | 481/585 [04:30<00:45,  2.30it/s] 82%|████████▏ | 482/585 [04:31<00:42,  2.41it/s] 83%|████████▎ | 483/585 [04:31<00:38,  2.66it/s] 83%|████████▎ | 484/585 [04:31<00:35,  2.86it/s] 83%|████████▎ | 485/585 [04:31<00:33,  3.02it/s] 83%|████████▎ | 486/585 [04:32<00:31,  3.15it/s] 83%|████████▎ | 487/585 [04:32<00:30,  3.24it/s] 83%|████████▎ | 488/585 [04:32<00:29,  3.31it/s] 84%|████████▎ | 489/585 [04:33<00:28,  3.36it/s] 84%|████████▍ | 490/585 [04:33<00:27,  3.39it/s] 84%|████████▍ | 491/585 [04:33<00:27,  3.42it/s] 84%|████████▍ | 492/585 [04:33<00:27,  3.43it/s] 84%|████████▍ | 493/585 [04:34<00:28,  3.19it/s] 84%|████████▍ | 494/585 [04:34<00:27,  3.27it/s] 85%|████████▍ | 495/585 [04:34<00:27,  3.33it/s] 85%|████████▍ | 496/585 [04:35<00:26,  3.37it/s] 85%|████████▍ | 497/585 [04:35<00:25,  3.40it/s] 85%|████████▌ | 498/585 [04:35<00:25,  3.43it/s] 85%|████████▌ | 499/585 [04:36<00:24,  3.44it/s] 85%|████████▌ | 500/585 [04:36<00:24,  3.45it/s]                                                  85%|████████▌ | 500/585 [04:36<00:24,  3.45it/s] 86%|████████▌ | 501/585 [04:36<00:24,  3.46it/s] 86%|████████▌ | 502/585 [04:36<00:23,  3.46it/s] 86%|████████▌ | 503/585 [04:37<00:23,  3.46it/s] 86%|████████▌ | 504/585 [04:37<00:24,  3.30it/s] 86%|████████▋ | 505/585 [04:37<00:23,  3.35it/s] 86%|████████▋ | 506/585 [04:38<00:23,  3.39it/s] 87%|████████▋ | 507/585 [04:38<00:22,  3.42it/s] 87%|████████▋ | 508/585 [04:38<00:22,  3.43it/s] 87%|████████▋ | 509/585 [04:38<00:22,  3.45it/s] 87%|████████▋ | 510/585 [04:39<00:21,  3.45it/s] 87%|████████▋ | 511/585 [04:39<00:21,  3.46it/s] 88%|████████▊ | 512/585 [04:39<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:40<00:20,  3.47it/s] 88%|████████▊ | 514/585 [04:40<00:21,  3.23it/s] 88%|████████▊ | 515/585 [04:40<00:22,  3.12it/s] 88%|████████▊ | 516/585 [04:41<00:21,  3.22it/s] 88%|████████▊ | 517/585 [04:41<00:20,  3.29it/s] 89%|████████▊ | 518/585 [04:41<00:20,  3.34it/s] 89%|████████▊ | 519/585 [04:41<00:19,  3.38it/s] 89%|████████▉ | 520/585 [04:42<00:19,  3.41it/s] 89%|████████▉ | 521/585 [04:42<00:18,  3.42it/s] 89%|████████▉ | 522/585 [04:42<00:18,  3.44it/s] 89%|████████▉ | 523/585 [04:43<00:17,  3.45it/s] 90%|████████▉ | 524/585 [04:43<00:17,  3.46it/s] 90%|████████▉ | 525/585 [04:43<00:17,  3.46it/s] 90%|████████▉ | 526/585 [04:44<00:18,  3.27it/s] 90%|█████████ | 527/585 [04:44<00:17,  3.33it/s] 90%|█████████ | 528/585 [04:44<00:16,  3.37it/s] 90%|█████████ | 529/585 [04:44<00:16,  3.40it/s] 91%|█████████ | 530/585 [04:45<00:16,  3.42it/s] 91%|█████████ | 531/585 [04:45<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:45<00:15,  3.45it/s] 91%|█████████ | 533/585 [04:46<00:15,  3.40it/s] 91%|█████████▏| 534/585 [04:46<00:14,  3.42it/s] 91%|█████████▏| 535/585 [04:46<00:14,  3.43it/s] 92%|█████████▏| 536/585 [04:46<00:14,  3.44it/s] 92%|█████████▏| 537/585 [04:47<00:14,  3.33it/s] 92%|█████████▏| 538/585 [04:47<00:13,  3.37it/s] 92%|█████████▏| 539/585 [04:47<00:13,  3.40it/s] 92%|█████████▏| 540/585 [04:48<00:13,  3.42it/s] 92%|█████████▏| 541/585 [04:48<00:12,  3.43it/s] 93%|█████████▎| 542/585 [04:48<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:49<00:12,  3.45it/s] 93%|█████████▎| 544/585 [04:49<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:49<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:49<00:11,  3.46it/s] 94%|█████████▎| 547/585 [04:50<00:10,  3.46it/s] 94%|█████████▎| 548/585 [04:50<00:11,  3.35it/s] 94%|█████████▍| 549/585 [04:50<00:10,  3.38it/s] 94%|█████████▍| 550/585 [04:51<00:10,  3.41it/s] 94%|█████████▍| 551/585 [04:51<00:09,  3.42it/s] 94%|█████████▍| 552/585 [04:51<00:09,  3.44it/s] 95%|█████████▍| 553/585 [04:51<00:09,  3.45it/s] 95%|█████████▍| 554/585 [04:52<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:52<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:52<00:08,  3.46it/s] 95%|█████████▌| 557/585 [04:53<00:08,  3.46it/s] 95%|█████████▌| 558/585 [04:53<00:07,  3.47it/s] 96%|█████████▌| 559/585 [04:53<00:07,  3.29it/s] 96%|█████████▌| 560/585 [04:54<00:07,  3.34it/s] 96%|█████████▌| 561/585 [04:54<00:07,  3.38it/s] 96%|█████████▌| 562/585 [04:54<00:06,  3.40it/s] 96%|█████████▌| 563/585 [04:54<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:55<00:06,  3.44it/s] 97%|█████████▋| 565/585 [04:55<00:05,  3.44it/s] 97%|█████████▋| 566/585 [04:55<00:05,  3.45it/s] 97%|█████████▋| 567/585 [04:56<00:05,  3.46it/s] 97%|█████████▋| 568/585 [04:56<00:04,  3.46it/s] 97%|█████████▋| 569/585 [04:56<00:04,  3.46it/s] 97%|█████████▋| 570/585 [04:56<00:04,  3.46it/s] 98%|█████████▊| 571/585 [04:57<00:04,  3.46it/s] 98%|█████████▊| 572/585 [04:57<00:03,  3.46it/s] 98%|█████████▊| 573/585 [04:57<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:58<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:58<00:02,  3.47it/s] 98%|█████████▊| 576/585 [04:58<00:02,  3.30it/s] 99%|█████████▊| 577/585 [04:58<00:02,  3.35it/s] 99%|█████████▉| 578/585 [04:59<00:02,  3.38it/s] 99%|█████████▉| 579/585 [04:59<00:01,  3.41it/s] 99%|█████████▉| 580/585 [04:59<00:01,  3.42it/s] 99%|█████████▉| 581/585 [05:00<00:01,  3.44it/s] 99%|█████████▉| 582/585 [05:00<00:00,  3.45it/s]100%|█████████▉| 583/585 [05:00<00:00,  3.45it/s]100%|█████████▉| 584/585 [05:00<00:00,  3.46it/s]100%|██████████| 585/585 [05:01<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 11:35:11,493 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:35:11,493 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:35:11,493 >>   Batch size = 8
{'eval_loss': 0.9794083833694458, 'eval_runtime': 16.8879, 'eval_samples_per_second': 370.265, 'eval_steps_per_second': 46.305, 'epoch': 4.0}
{'loss': 0.4894, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.43it/s][A
  2%|▏         | 12/782 [00:00<00:17, 44.86it/s][A
  2%|▏         | 17/782 [00:00<00:16, 45.65it/s][A
  3%|▎         | 22/782 [00:00<00:16, 46.14it/s][A
  3%|▎         | 27/782 [00:00<00:16, 46.37it/s][A
  4%|▍         | 32/782 [00:00<00:16, 46.60it/s][A
  5%|▍         | 37/782 [00:00<00:15, 46.78it/s][A
  5%|▌         | 42/782 [00:00<00:15, 46.86it/s][A
  6%|▌         | 47/782 [00:01<00:15, 46.89it/s][A
  7%|▋         | 52/782 [00:01<00:15, 46.61it/s][A
  7%|▋         | 57/782 [00:01<00:15, 46.66it/s][A
  8%|▊         | 62/782 [00:01<00:15, 46.71it/s][A
  9%|▊         | 67/782 [00:01<00:15, 46.77it/s][A
  9%|▉         | 72/782 [00:01<00:15, 46.73it/s][A
 10%|▉         | 77/782 [00:01<00:15, 46.89it/s][A
 10%|█         | 82/782 [00:01<00:14, 46.91it/s][A
 11%|█         | 87/782 [00:01<00:14, 46.98it/s][A
 12%|█▏        | 92/782 [00:01<00:14, 46.97it/s][A
 12%|█▏        | 97/782 [00:02<00:14, 46.88it/s][A
 13%|█▎        | 102/782 [00:02<00:14, 46.75it/s][A
 14%|█▎        | 107/782 [00:02<00:14, 46.79it/s][A
 14%|█▍        | 112/782 [00:02<00:14, 46.82it/s][A
 15%|█▍        | 117/782 [00:02<00:14, 46.79it/s][A
 16%|█▌        | 122/782 [00:02<00:14, 46.90it/s][A
 16%|█▌        | 127/782 [00:02<00:13, 46.86it/s][A
 17%|█▋        | 132/782 [00:02<00:13, 46.85it/s][A
 18%|█▊        | 137/782 [00:02<00:13, 46.93it/s][A
 18%|█▊        | 142/782 [00:03<00:13, 46.90it/s][A
 19%|█▉        | 147/782 [00:03<00:13, 46.89it/s][A
 19%|█▉        | 152/782 [00:03<00:15, 41.55it/s][A
 20%|██        | 157/782 [00:03<00:14, 43.07it/s][A
 21%|██        | 162/782 [00:03<00:14, 44.18it/s][A
 21%|██▏       | 167/782 [00:03<00:13, 45.03it/s][A
 22%|██▏       | 172/782 [00:03<00:13, 45.57it/s][A
 23%|██▎       | 177/782 [00:03<00:14, 40.84it/s][A
 23%|██▎       | 182/782 [00:03<00:14, 42.58it/s][A
 24%|██▍       | 187/782 [00:04<00:13, 43.83it/s][A
 25%|██▍       | 192/782 [00:04<00:13, 44.73it/s][A
 25%|██▌       | 197/782 [00:04<00:12, 45.31it/s][A
 26%|██▌       | 202/782 [00:04<00:12, 45.77it/s][A
 26%|██▋       | 207/782 [00:04<00:12, 46.10it/s][A
 27%|██▋       | 212/782 [00:04<00:12, 46.38it/s][A
 28%|██▊       | 217/782 [00:04<00:12, 46.53it/s][A
 28%|██▊       | 222/782 [00:04<00:12, 46.42it/s][A
 29%|██▉       | 227/782 [00:04<00:11, 46.48it/s][A
 30%|██▉       | 232/782 [00:05<00:11, 46.67it/s][A
 30%|███       | 237/782 [00:05<00:11, 46.76it/s][A
 31%|███       | 242/782 [00:05<00:11, 46.75it/s][A
 32%|███▏      | 247/782 [00:05<00:11, 46.83it/s][A
 32%|███▏      | 252/782 [00:05<00:11, 46.84it/s][A
 33%|███▎      | 257/782 [00:05<00:11, 46.88it/s][A
 34%|███▎      | 262/782 [00:05<00:11, 46.86it/s][A
 34%|███▍      | 267/782 [00:05<00:11, 46.71it/s][A
 35%|███▍      | 272/782 [00:05<00:10, 46.65it/s][A
 35%|███▌      | 277/782 [00:06<00:10, 46.73it/s][A
 36%|███▌      | 282/782 [00:06<00:10, 46.72it/s][A
 37%|███▋      | 287/782 [00:06<00:10, 46.79it/s][A
 37%|███▋      | 292/782 [00:06<00:12, 40.81it/s][A
 38%|███▊      | 297/782 [00:06<00:11, 42.49it/s][A
 39%|███▊      | 302/782 [00:06<00:10, 43.72it/s][A
 39%|███▉      | 307/782 [00:06<00:10, 44.64it/s][A
 40%|███▉      | 312/782 [00:06<00:10, 45.34it/s][A
 41%|████      | 317/782 [00:06<00:10, 45.79it/s][A
 41%|████      | 322/782 [00:07<00:09, 46.13it/s][A
 42%|████▏     | 327/782 [00:07<00:09, 46.39it/s][A
 42%|████▏     | 332/782 [00:07<00:09, 46.31it/s][A
 43%|████▎     | 337/782 [00:07<00:09, 45.96it/s][A
 44%|████▎     | 342/782 [00:07<00:09, 46.31it/s][A
 44%|████▍     | 347/782 [00:07<00:09, 46.43it/s][A
 45%|████▌     | 352/782 [00:07<00:09, 46.54it/s][A
 46%|████▌     | 357/782 [00:07<00:09, 46.75it/s][A
 46%|████▋     | 362/782 [00:07<00:08, 46.78it/s][A
 47%|████▋     | 367/782 [00:07<00:08, 46.88it/s][A
 48%|████▊     | 372/782 [00:08<00:08, 46.87it/s][A
 48%|████▊     | 377/782 [00:08<00:08, 46.70it/s][A
 49%|████▉     | 382/782 [00:08<00:08, 46.64it/s][A
 49%|████▉     | 387/782 [00:08<00:08, 46.62it/s][A
 50%|█████     | 392/782 [00:08<00:08, 46.69it/s][A
 51%|█████     | 397/782 [00:08<00:08, 46.79it/s][A
 51%|█████▏    | 402/782 [00:08<00:08, 46.76it/s][A
 52%|█████▏    | 407/782 [00:08<00:08, 46.83it/s][A
 53%|█████▎    | 412/782 [00:08<00:07, 46.92it/s][A
 53%|█████▎    | 417/782 [00:09<00:07, 46.90it/s][A
 54%|█████▍    | 422/782 [00:09<00:07, 46.83it/s][A
 55%|█████▍    | 427/782 [00:09<00:07, 46.79it/s][A
 55%|█████▌    | 432/782 [00:09<00:07, 43.94it/s][A
 56%|█████▌    | 437/782 [00:09<00:07, 44.80it/s][A
 57%|█████▋    | 442/782 [00:09<00:07, 45.45it/s][A
 57%|█████▋    | 447/782 [00:09<00:07, 45.89it/s][A
 58%|█████▊    | 452/782 [00:09<00:07, 46.25it/s][A
 58%|█████▊    | 457/782 [00:09<00:07, 46.39it/s][A
 59%|█████▉    | 462/782 [00:10<00:06, 46.50it/s][A
 60%|█████▉    | 467/782 [00:10<00:06, 46.43it/s][A
 60%|██████    | 472/782 [00:10<00:06, 46.60it/s][A
 61%|██████    | 477/782 [00:10<00:06, 46.64it/s][A
 62%|██████▏   | 482/782 [00:10<00:06, 46.65it/s][A
 62%|██████▏   | 487/782 [00:10<00:06, 46.73it/s][A
 63%|██████▎   | 492/782 [00:10<00:06, 46.78it/s][A
 64%|██████▎   | 497/782 [00:10<00:06, 46.91it/s][A
 64%|██████▍   | 502/782 [00:10<00:05, 46.91it/s][A
 65%|██████▍   | 507/782 [00:10<00:05, 46.80it/s][A
 65%|██████▌   | 512/782 [00:11<00:05, 46.84it/s][A
 66%|██████▌   | 517/782 [00:11<00:05, 46.73it/s][A
 67%|██████▋   | 522/782 [00:11<00:05, 46.66it/s][A
 67%|██████▋   | 527/782 [00:11<00:05, 46.72it/s][A
 68%|██████▊   | 532/782 [00:11<00:05, 46.71it/s][A
 69%|██████▊   | 537/782 [00:11<00:05, 46.69it/s][A
 69%|██████▉   | 542/782 [00:11<00:05, 46.78it/s][A
 70%|██████▉   | 547/782 [00:11<00:05, 46.86it/s][A
 71%|███████   | 552/782 [00:11<00:04, 46.85it/s][A
 71%|███████   | 557/782 [00:12<00:04, 46.84it/s][A
 72%|███████▏  | 562/782 [00:12<00:04, 46.68it/s][A
 73%|███████▎  | 567/782 [00:12<00:04, 46.73it/s][A
 73%|███████▎  | 572/782 [00:12<00:04, 44.44it/s][A
 74%|███████▍  | 577/782 [00:12<00:04, 45.21it/s][A
 74%|███████▍  | 582/782 [00:12<00:04, 45.64it/s][A
 75%|███████▌  | 587/782 [00:12<00:04, 46.04it/s][A
 76%|███████▌  | 592/782 [00:12<00:04, 46.35it/s][A
 76%|███████▋  | 597/782 [00:12<00:03, 46.52it/s][A
 77%|███████▋  | 602/782 [00:13<00:03, 46.68it/s][A
 78%|███████▊  | 607/782 [00:13<00:03, 46.66it/s][A
 78%|███████▊  | 612/782 [00:13<00:03, 46.59it/s][A
 79%|███████▉  | 617/782 [00:13<00:03, 46.53it/s][A
 80%|███████▉  | 622/782 [00:13<00:03, 46.59it/s][A
 80%|████████  | 627/782 [00:13<00:03, 46.61it/s][A
 81%|████████  | 632/782 [00:13<00:03, 46.79it/s][A
 81%|████████▏ | 637/782 [00:13<00:03, 46.82it/s][A
 82%|████████▏ | 642/782 [00:13<00:02, 46.90it/s][A
 83%|████████▎ | 647/782 [00:14<00:02, 46.88it/s][A
 83%|████████▎ | 652/782 [00:14<00:02, 46.88it/s][A
 84%|████████▍ | 657/782 [00:14<00:02, 46.77it/s][A
 85%|████████▍ | 662/782 [00:14<00:02, 46.74it/s][A
 85%|████████▌ | 667/782 [00:14<00:02, 46.64it/s][A
 86%|████████▌ | 672/782 [00:14<00:02, 46.62it/s][A
 87%|████████▋ | 677/782 [00:14<00:02, 46.69it/s][A
 87%|████████▋ | 682/782 [00:14<00:02, 46.75it/s][A
 88%|████████▊ | 687/782 [00:14<00:02, 46.80it/s][A
 88%|████████▊ | 692/782 [00:14<00:01, 46.87it/s][A
 89%|████████▉ | 697/782 [00:15<00:01, 46.80it/s][A
 90%|████████▉ | 702/782 [00:15<00:01, 46.84it/s][A
 90%|█████████ | 707/782 [00:15<00:01, 46.80it/s][A
 91%|█████████ | 712/782 [00:15<00:01, 43.38it/s][A
 92%|█████████▏| 717/782 [00:15<00:01, 44.22it/s][A
 92%|█████████▏| 722/782 [00:15<00:01, 45.06it/s][A
 93%|█████████▎| 727/782 [00:15<00:01, 45.61it/s][A
 94%|█████████▎| 732/782 [00:15<00:01, 46.01it/s][A
 94%|█████████▍| 737/782 [00:15<00:00, 46.27it/s][A
 95%|█████████▍| 742/782 [00:16<00:00, 46.44it/s][A
 96%|█████████▌| 747/782 [00:16<00:00, 46.64it/s][A
 96%|█████████▌| 752/782 [00:16<00:00, 46.52it/s][A
 97%|█████████▋| 757/782 [00:16<00:00, 46.52it/s][A
 97%|█████████▋| 762/782 [00:16<00:00, 46.43it/s][A
 98%|█████████▊| 767/782 [00:16<00:00, 46.57it/s][A
 99%|█████████▊| 772/782 [00:16<00:00, 46.72it/s][A
 99%|█████████▉| 777/782 [00:16<00:00, 46.79it/s][A
100%|██████████| 782/782 [00:16<00:00, 46.83it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.83it/s][A100%|██████████| 585/585 [05:18<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:35:28,714 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 11:35:29,093 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:35:34,078 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:35:34,371 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:35:34,505 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:35:45,307 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:35:45,340 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117 (score: 0.9393563866615295).
                                                 100%|██████████| 585/585 [05:43<00:00,  3.46it/s]100%|██████████| 585/585 [05:43<00:00,  1.70it/s]
[INFO|trainer.py:1894] 2023-08-29 11:35:54,101 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 11:35:54,258 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:35:58,044 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:35:58,221 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:35:58,311 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:35:58,845 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   train_loss               =     0.4861
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   train_runtime            = 0:05:43.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   train_samples            =       7509
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   train_samples_per_second =    109.207
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:35:58,846 >>   train_steps_per_second   =      1.702
{'eval_loss': 0.9829003810882568, 'eval_runtime': 16.9357, 'eval_samples_per_second': 369.219, 'eval_steps_per_second': 46.175, 'epoch': 5.0}
{'train_runtime': 343.7963, 'train_samples_per_second': 109.207, 'train_steps_per_second': 1.702, 'train_loss': 0.48607449001736114, 'epoch': 5.0}
08/29/2023 11:35:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:35:59,039 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:35:59,039 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 11:35:59,039 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 4/782 [00:00<00:46, 16.56it/s]  1%|          | 9/782 [00:00<00:27, 28.60it/s]  2%|▏         | 14/782 [00:00<00:21, 35.31it/s]  2%|▏         | 19/782 [00:00<00:19, 39.44it/s]  3%|▎         | 24/782 [00:00<00:18, 41.99it/s]  4%|▎         | 29/782 [00:00<00:17, 43.68it/s]  4%|▍         | 34/782 [00:00<00:16, 44.90it/s]  5%|▍         | 39/782 [00:00<00:16, 45.74it/s]  6%|▌         | 44/782 [00:01<00:16, 46.09it/s]  6%|▋         | 49/782 [00:01<00:15, 46.46it/s]  7%|▋         | 54/782 [00:01<00:15, 46.67it/s]  8%|▊         | 59/782 [00:01<00:15, 46.78it/s]  8%|▊         | 64/782 [00:01<00:15, 46.92it/s]  9%|▉         | 69/782 [00:01<00:15, 47.06it/s]  9%|▉         | 74/782 [00:01<00:15, 47.02it/s] 10%|█         | 79/782 [00:01<00:14, 47.07it/s] 11%|█         | 84/782 [00:01<00:14, 47.18it/s] 11%|█▏        | 89/782 [00:02<00:14, 47.22it/s] 12%|█▏        | 94/782 [00:02<00:14, 46.11it/s] 13%|█▎        | 99/782 [00:02<00:14, 46.52it/s] 13%|█▎        | 104/782 [00:02<00:14, 46.76it/s] 14%|█▍        | 109/782 [00:02<00:14, 46.98it/s] 15%|█▍        | 114/782 [00:02<00:14, 47.16it/s] 15%|█▌        | 119/782 [00:02<00:14, 47.20it/s] 16%|█▌        | 124/782 [00:02<00:13, 47.20it/s] 16%|█▋        | 129/782 [00:02<00:13, 47.28it/s] 17%|█▋        | 134/782 [00:02<00:13, 47.22it/s] 18%|█▊        | 139/782 [00:03<00:13, 47.21it/s] 18%|█▊        | 144/782 [00:03<00:13, 47.22it/s] 19%|█▉        | 149/782 [00:03<00:13, 47.21it/s] 20%|█▉        | 154/782 [00:03<00:13, 47.27it/s] 20%|██        | 159/782 [00:03<00:13, 47.38it/s] 21%|██        | 164/782 [00:03<00:13, 47.36it/s] 22%|██▏       | 169/782 [00:03<00:12, 47.41it/s] 22%|██▏       | 174/782 [00:03<00:12, 47.38it/s] 23%|██▎       | 179/782 [00:03<00:12, 47.25it/s] 24%|██▎       | 184/782 [00:04<00:12, 47.28it/s] 24%|██▍       | 189/782 [00:04<00:12, 47.26it/s] 25%|██▍       | 194/782 [00:04<00:12, 47.22it/s] 25%|██▌       | 199/782 [00:04<00:12, 47.23it/s] 26%|██▌       | 204/782 [00:04<00:12, 47.32it/s] 27%|██▋       | 209/782 [00:04<00:12, 47.40it/s] 27%|██▋       | 214/782 [00:04<00:11, 47.36it/s] 28%|██▊       | 219/782 [00:04<00:11, 47.34it/s] 29%|██▊       | 224/782 [00:04<00:11, 47.25it/s] 29%|██▉       | 229/782 [00:05<00:11, 47.13it/s] 30%|██▉       | 234/782 [00:05<00:11, 47.13it/s] 31%|███       | 239/782 [00:05<00:11, 46.02it/s] 31%|███       | 244/782 [00:05<00:11, 46.45it/s] 32%|███▏      | 249/782 [00:05<00:11, 46.65it/s] 32%|███▏      | 254/782 [00:05<00:11, 46.98it/s] 33%|███▎      | 259/782 [00:05<00:11, 47.04it/s] 34%|███▍      | 264/782 [00:05<00:10, 47.22it/s] 34%|███▍      | 269/782 [00:05<00:10, 47.27it/s] 35%|███▌      | 274/782 [00:05<00:10, 47.18it/s] 36%|███▌      | 279/782 [00:06<00:10, 47.20it/s] 36%|███▋      | 284/782 [00:06<00:10, 47.15it/s] 37%|███▋      | 289/782 [00:06<00:10, 47.11it/s] 38%|███▊      | 294/782 [00:06<00:10, 47.22it/s] 38%|███▊      | 299/782 [00:06<00:10, 47.16it/s] 39%|███▉      | 304/782 [00:06<00:10, 47.19it/s] 40%|███▉      | 309/782 [00:06<00:10, 47.27it/s] 40%|████      | 314/782 [00:06<00:09, 46.87it/s] 41%|████      | 319/782 [00:06<00:09, 47.34it/s] 41%|████▏     | 324/782 [00:07<00:09, 47.25it/s] 42%|████▏     | 329/782 [00:07<00:09, 47.25it/s] 43%|████▎     | 334/782 [00:07<00:09, 47.24it/s] 43%|████▎     | 339/782 [00:07<00:09, 47.20it/s] 44%|████▍     | 344/782 [00:07<00:09, 47.15it/s] 45%|████▍     | 349/782 [00:07<00:09, 47.30it/s] 45%|████▌     | 354/782 [00:07<00:09, 47.25it/s] 46%|████▌     | 359/782 [00:07<00:08, 47.27it/s] 47%|████▋     | 364/782 [00:07<00:08, 47.34it/s] 47%|████▋     | 369/782 [00:07<00:08, 47.24it/s] 48%|████▊     | 374/782 [00:08<00:08, 47.26it/s] 48%|████▊     | 379/782 [00:08<00:08, 47.25it/s] 49%|████▉     | 384/782 [00:08<00:08, 47.20it/s] 50%|████▉     | 389/782 [00:08<00:08, 47.19it/s] 50%|█████     | 394/782 [00:08<00:08, 47.12it/s] 51%|█████     | 399/782 [00:08<00:08, 47.19it/s] 52%|█████▏    | 404/782 [00:08<00:08, 47.20it/s] 52%|█████▏    | 409/782 [00:08<00:07, 47.24it/s] 53%|█████▎    | 414/782 [00:08<00:07, 47.28it/s] 54%|█████▎    | 419/782 [00:09<00:07, 47.24it/s] 54%|█████▍    | 424/782 [00:09<00:07, 47.26it/s] 55%|█████▍    | 429/782 [00:09<00:07, 47.20it/s] 55%|█████▌    | 434/782 [00:09<00:07, 47.24it/s] 56%|█████▌    | 439/782 [00:09<00:07, 47.23it/s] 57%|█████▋    | 444/782 [00:09<00:07, 47.12it/s] 57%|█████▋    | 449/782 [00:09<00:07, 46.98it/s] 58%|█████▊    | 454/782 [00:09<00:06, 47.28it/s] 59%|█████▊    | 459/782 [00:09<00:06, 47.26it/s] 59%|█████▉    | 464/782 [00:09<00:06, 47.35it/s] 60%|█████▉    | 469/782 [00:10<00:06, 47.35it/s] 61%|██████    | 474/782 [00:10<00:06, 47.17it/s] 61%|██████▏   | 479/782 [00:10<00:06, 47.23it/s] 62%|██████▏   | 484/782 [00:10<00:06, 47.20it/s] 63%|██████▎   | 489/782 [00:10<00:06, 47.14it/s] 63%|██████▎   | 494/782 [00:10<00:06, 47.09it/s] 64%|██████▍   | 499/782 [00:10<00:05, 47.23it/s] 64%|██████▍   | 504/782 [00:10<00:05, 47.24it/s] 65%|██████▌   | 509/782 [00:10<00:06, 44.51it/s] 66%|██████▌   | 514/782 [00:11<00:05, 45.38it/s] 66%|██████▋   | 519/782 [00:11<00:05, 45.92it/s] 67%|██████▋   | 524/782 [00:11<00:05, 46.27it/s] 68%|██████▊   | 529/782 [00:11<00:05, 46.64it/s] 68%|██████▊   | 534/782 [00:11<00:05, 46.82it/s] 69%|██████▉   | 539/782 [00:11<00:05, 46.94it/s] 70%|██████▉   | 544/782 [00:11<00:05, 47.08it/s] 70%|███████   | 549/782 [00:11<00:04, 47.01it/s] 71%|███████   | 554/782 [00:11<00:04, 46.97it/s] 71%|███████▏  | 559/782 [00:12<00:04, 47.00it/s] 72%|███████▏  | 564/782 [00:12<00:04, 47.09it/s] 73%|███████▎  | 569/782 [00:12<00:04, 47.16it/s] 73%|███████▎  | 574/782 [00:12<00:04, 47.21it/s] 74%|███████▍  | 579/782 [00:12<00:04, 47.21it/s] 75%|███████▍  | 584/782 [00:12<00:04, 47.28it/s] 75%|███████▌  | 589/782 [00:12<00:04, 47.21it/s] 76%|███████▌  | 594/782 [00:12<00:04, 46.96it/s] 77%|███████▋  | 599/782 [00:12<00:03, 47.08it/s] 77%|███████▋  | 604/782 [00:12<00:03, 47.09it/s] 78%|███████▊  | 609/782 [00:13<00:03, 47.00it/s] 79%|███████▊  | 614/782 [00:13<00:03, 47.16it/s] 79%|███████▉  | 619/782 [00:13<00:03, 47.23it/s] 80%|███████▉  | 624/782 [00:13<00:03, 47.16it/s] 80%|████████  | 629/782 [00:13<00:03, 47.21it/s] 81%|████████  | 634/782 [00:13<00:03, 47.27it/s] 82%|████████▏ | 639/782 [00:13<00:03, 47.12it/s] 82%|████████▏ | 644/782 [00:13<00:02, 47.11it/s] 83%|████████▎ | 649/782 [00:13<00:02, 47.17it/s] 84%|████████▎ | 654/782 [00:14<00:02, 43.82it/s] 84%|████████▍ | 659/782 [00:14<00:02, 44.86it/s] 85%|████████▍ | 664/782 [00:14<00:02, 45.50it/s] 86%|████████▌ | 669/782 [00:14<00:02, 46.03it/s] 86%|████████▌ | 674/782 [00:14<00:02, 46.41it/s] 87%|████████▋ | 679/782 [00:14<00:02, 46.67it/s] 87%|████████▋ | 684/782 [00:14<00:02, 46.75it/s] 88%|████████▊ | 689/782 [00:14<00:01, 46.93it/s] 89%|████████▊ | 694/782 [00:14<00:01, 46.90it/s] 89%|████████▉ | 699/782 [00:15<00:01, 46.87it/s] 90%|█████████ | 704/782 [00:15<00:01, 46.97it/s] 91%|█████████ | 709/782 [00:15<00:01, 47.08it/s] 91%|█████████▏| 714/782 [00:15<00:01, 47.15it/s] 92%|█████████▏| 719/782 [00:15<00:01, 47.22it/s] 93%|█████████▎| 724/782 [00:15<00:01, 47.16it/s] 93%|█████████▎| 729/782 [00:15<00:01, 47.20it/s] 94%|█████████▍| 734/782 [00:15<00:01, 47.14it/s] 95%|█████████▍| 739/782 [00:15<00:00, 47.15it/s] 95%|█████████▌| 744/782 [00:15<00:00, 47.08it/s] 96%|█████████▌| 749/782 [00:16<00:00, 47.01it/s] 96%|█████████▋| 754/782 [00:16<00:00, 47.11it/s] 97%|█████████▋| 759/782 [00:16<00:00, 47.20it/s] 98%|█████████▊| 764/782 [00:16<00:00, 47.21it/s] 98%|█████████▊| 769/782 [00:16<00:00, 47.16it/s] 99%|█████████▉| 774/782 [00:16<00:00, 47.12it/s]100%|█████████▉| 779/782 [00:16<00:00, 47.15it/s]100%|██████████| 782/782 [00:16<00:00, 46.60it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:36:15,842 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   eval_loss               =     0.9394
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   eval_runtime            = 0:00:16.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   eval_samples_per_second =    372.139
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   eval_steps_per_second   =      46.54
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:36:15,842 >>   perplexity              =     2.5583
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:29,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:29,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:29,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:29,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:29,444 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:36:30,512 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:36:30,513 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:31,181 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:36:32,331 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:36:32,331 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:35,521 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:35,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:35,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:35,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:35,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:36:36,530 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:36:36,532 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:37,227 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:36:37,508 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:36:37,508 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.52it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:21,  1.52it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:25,  1.49it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:31,  1.53it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.49it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.43it/s]Extractor Predicting: 62it [00:41,  1.44it/s]Extractor Predicting: 63it [00:41,  1.43it/s]Extractor Predicting: 64it [00:42,  1.45it/s]Extractor Predicting: 65it [00:43,  1.46it/s]Extractor Predicting: 66it [00:44,  1.45it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:46,  1.45it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:47,  1.45it/s]Extractor Predicting: 72it [00:48,  1.47it/s]Extractor Predicting: 73it [00:48,  1.45it/s]Extractor Predicting: 74it [00:49,  1.45it/s]Extractor Predicting: 75it [00:50,  1.46it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:51,  1.45it/s]Extractor Predicting: 78it [00:52,  1.47it/s]Extractor Predicting: 79it [00:52,  1.45it/s]Extractor Predicting: 80it [00:53,  1.44it/s]Extractor Predicting: 81it [00:54,  1.46it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.47it/s]Extractor Predicting: 84it [00:56,  1.43it/s]Extractor Predicting: 85it [00:57,  1.44it/s]Extractor Predicting: 86it [00:57,  1.45it/s]Extractor Predicting: 87it [00:58,  1.47it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [00:59,  1.45it/s]Extractor Predicting: 90it [01:00,  1.47it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:03,  1.48it/s]Extractor Predicting: 95it [01:03,  1.50it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:05,  1.46it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:06,  1.45it/s]Extractor Predicting: 100it [01:07,  1.48it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.50it/s]Extractor Predicting: 103it [01:09,  1.51it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:11,  1.49it/s]Extractor Predicting: 107it [01:11,  1.46it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:13,  1.48it/s]Extractor Predicting: 111it [01:14,  1.50it/s]Extractor Predicting: 112it [01:15,  1.49it/s]Extractor Predicting: 113it [01:15,  1.52it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:17,  1.49it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.48it/s]Extractor Predicting: 119it [01:20,  1.47it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:21,  1.50it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.46it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:25,  1.47it/s]Extractor Predicting: 128it [01:26,  1.46it/s]Extractor Predicting: 129it [01:26,  1.46it/s]Extractor Predicting: 130it [01:27,  1.44it/s]Extractor Predicting: 131it [01:28,  1.45it/s]Extractor Predicting: 132it [01:28,  1.44it/s]Extractor Predicting: 133it [01:29,  1.46it/s]Extractor Predicting: 134it [01:30,  1.43it/s]Extractor Predicting: 135it [01:30,  1.43it/s]Extractor Predicting: 136it [01:31,  1.45it/s]Extractor Predicting: 137it [01:32,  1.46it/s]Extractor Predicting: 138it [01:32,  1.48it/s]Extractor Predicting: 139it [01:33,  1.46it/s]Extractor Predicting: 140it [01:34,  1.46it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.50it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:37,  1.47it/s]Extractor Predicting: 145it [01:37,  1.48it/s]Extractor Predicting: 146it [01:38,  1.36it/s]Extractor Predicting: 147it [01:39,  1.39it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:40,  1.46it/s]Extractor Predicting: 150it [01:41,  1.46it/s]Extractor Predicting: 151it [01:41,  1.45it/s]Extractor Predicting: 152it [01:42,  1.49it/s]Extractor Predicting: 153it [01:43,  1.48it/s]Extractor Predicting: 154it [01:43,  1.49it/s]Extractor Predicting: 155it [01:44,  1.50it/s]Extractor Predicting: 156it [01:45,  1.49it/s]Extractor Predicting: 157it [01:45,  1.54it/s]Extractor Predicting: 158it [01:46,  1.54it/s]Extractor Predicting: 159it [01:47,  1.53it/s]Extractor Predicting: 160it [01:47,  1.49it/s]Extractor Predicting: 161it [01:48,  1.48it/s]Extractor Predicting: 162it [01:49,  1.49it/s]Extractor Predicting: 163it [01:49,  1.52it/s]Extractor Predicting: 164it [01:50,  1.54it/s]Extractor Predicting: 165it [01:51,  1.57it/s]Extractor Predicting: 166it [01:51,  1.57it/s]Extractor Predicting: 167it [01:52,  1.56it/s]Extractor Predicting: 168it [01:53,  1.53it/s]Extractor Predicting: 169it [01:53,  1.52it/s]Extractor Predicting: 170it [01:54,  1.54it/s]Extractor Predicting: 171it [01:55,  1.53it/s]Extractor Predicting: 172it [01:55,  1.51it/s]Extractor Predicting: 173it [01:56,  1.54it/s]Extractor Predicting: 174it [01:57,  1.47it/s]Extractor Predicting: 175it [01:57,  1.50it/s]Extractor Predicting: 176it [01:58,  1.51it/s]Extractor Predicting: 177it [01:58,  1.52it/s]Extractor Predicting: 178it [01:59,  1.52it/s]Extractor Predicting: 179it [02:00,  1.51it/s]Extractor Predicting: 180it [02:00,  1.51it/s]Extractor Predicting: 181it [02:01,  1.54it/s]Extractor Predicting: 182it [02:02,  1.52it/s]Extractor Predicting: 183it [02:02,  1.55it/s]Extractor Predicting: 184it [02:03,  1.54it/s]Extractor Predicting: 185it [02:04,  1.54it/s]Extractor Predicting: 186it [02:04,  1.51it/s]Extractor Predicting: 187it [02:05,  1.55it/s]Extractor Predicting: 188it [02:06,  1.52it/s]Extractor Predicting: 189it [02:06,  1.53it/s]Extractor Predicting: 190it [02:07,  1.56it/s]Extractor Predicting: 191it [02:08,  1.58it/s]Extractor Predicting: 192it [02:08,  1.58it/s]Extractor Predicting: 193it [02:09,  1.57it/s]Extractor Predicting: 194it [02:10,  1.55it/s]Extractor Predicting: 195it [02:10,  1.56it/s]Extractor Predicting: 196it [02:11,  1.53it/s]Extractor Predicting: 197it [02:11,  1.57it/s]Extractor Predicting: 198it [02:12,  1.54it/s]Extractor Predicting: 199it [02:13,  1.52it/s]Extractor Predicting: 200it [02:13,  1.58it/s]Extractor Predicting: 201it [02:14,  1.60it/s]Extractor Predicting: 202it [02:15,  1.60it/s]Extractor Predicting: 203it [02:15,  1.61it/s]Extractor Predicting: 204it [02:16,  1.60it/s]Extractor Predicting: 205it [02:16,  1.59it/s]Extractor Predicting: 206it [02:17,  1.59it/s]Extractor Predicting: 207it [02:18,  1.61it/s]Extractor Predicting: 208it [02:18,  1.60it/s]Extractor Predicting: 209it [02:19,  1.57it/s]Extractor Predicting: 210it [02:20,  1.58it/s]Extractor Predicting: 211it [02:20,  1.57it/s]Extractor Predicting: 212it [02:21,  1.58it/s]Extractor Predicting: 213it [02:22,  1.56it/s]Extractor Predicting: 214it [02:22,  1.59it/s]Extractor Predicting: 215it [02:23,  1.56it/s]Extractor Predicting: 216it [02:23,  1.54it/s]Extractor Predicting: 217it [02:24,  1.55it/s]Extractor Predicting: 218it [02:25,  1.57it/s]Extractor Predicting: 219it [02:25,  1.55it/s]Extractor Predicting: 220it [02:26,  1.40it/s]Extractor Predicting: 221it [02:27,  1.45it/s]Extractor Predicting: 222it [02:28,  1.47it/s]Extractor Predicting: 223it [02:28,  1.50it/s]Extractor Predicting: 224it [02:29,  1.52it/s]Extractor Predicting: 225it [02:29,  1.52it/s]Extractor Predicting: 226it [02:30,  1.51it/s]Extractor Predicting: 227it [02:31,  1.54it/s]Extractor Predicting: 228it [02:31,  1.70it/s]Extractor Predicting: 228it [02:31,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:23,108 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:23,139 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:23,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:23,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:23,140 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:39:23,975 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:39:23,976 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:39:24,292 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:39:25,371 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:39:25,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:27,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:27,242 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:27,242 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:27,242 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:27,242 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:39:27,734 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:39:27,736 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:39:28,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:39:28,283 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:39:28,283 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.0825910931174089,
  "recall": 0.016312170158324004,
  "score": 0.027243589743589744,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:09,  1.45it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:12,  1.41it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:14,  1.46it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:16,  1.44it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:21,  1.47it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:23,  1.43it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.42it/s]Extractor Predicting: 37it [00:25,  1.42it/s]Extractor Predicting: 38it [00:25,  1.40it/s]Extractor Predicting: 39it [00:26,  1.37it/s]Extractor Predicting: 40it [00:27,  1.41it/s]Extractor Predicting: 41it [00:28,  1.43it/s]Extractor Predicting: 42it [00:28,  1.43it/s]Extractor Predicting: 43it [00:29,  1.43it/s]Extractor Predicting: 44it [00:30,  1.45it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.47it/s]Extractor Predicting: 47it [00:32,  1.57it/s]Extractor Predicting: 48it [00:32,  1.58it/s]Extractor Predicting: 49it [00:33,  1.62it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:34,  1.58it/s]Extractor Predicting: 52it [00:35,  1.51it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.55it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.55it/s]Extractor Predicting: 61it [00:41,  1.58it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.54it/s]Extractor Predicting: 64it [00:43,  1.55it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:44,  1.56it/s]Extractor Predicting: 67it [00:45,  1.54it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:46,  1.56it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:47,  1.59it/s]Extractor Predicting: 72it [00:48,  1.57it/s]Extractor Predicting: 73it [00:48,  1.55it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.56it/s]Extractor Predicting: 77it [00:51,  1.53it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:52,  1.53it/s]Extractor Predicting: 80it [00:53,  1.53it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:54,  1.53it/s]Extractor Predicting: 83it [00:55,  1.52it/s]Extractor Predicting: 84it [00:56,  1.55it/s]Extractor Predicting: 85it [00:56,  1.55it/s]Extractor Predicting: 86it [00:57,  1.56it/s]Extractor Predicting: 87it [00:57,  1.56it/s]Extractor Predicting: 88it [00:58,  1.56it/s]Extractor Predicting: 89it [00:59,  1.57it/s]Extractor Predicting: 90it [00:59,  1.57it/s]Extractor Predicting: 91it [01:00,  1.53it/s]Extractor Predicting: 92it [01:01,  1.55it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:03,  1.55it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:05,  1.55it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:07,  1.52it/s]Extractor Predicting: 102it [01:07,  1.53it/s]Extractor Predicting: 103it [01:08,  1.55it/s]Extractor Predicting: 104it [01:09,  1.51it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.53it/s]Extractor Predicting: 108it [01:11,  1.53it/s]Extractor Predicting: 109it [01:12,  1.54it/s]Extractor Predicting: 110it [01:13,  1.51it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:14,  1.55it/s]Extractor Predicting: 113it [01:14,  1.58it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:16,  1.52it/s]Extractor Predicting: 116it [01:17,  1.40it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:18,  1.49it/s]Extractor Predicting: 119it [01:18,  1.56it/s]Extractor Predicting: 120it [01:19,  1.58it/s]Extractor Predicting: 121it [01:20,  1.59it/s]Extractor Predicting: 122it [01:20,  1.61it/s]Extractor Predicting: 123it [01:21,  1.62it/s]Extractor Predicting: 124it [01:21,  1.63it/s]Extractor Predicting: 125it [01:22,  1.55it/s]Extractor Predicting: 126it [01:23,  1.60it/s]Extractor Predicting: 127it [01:23,  1.57it/s]Extractor Predicting: 128it [01:24,  1.53it/s]Extractor Predicting: 129it [01:25,  1.53it/s]Extractor Predicting: 130it [01:25,  1.57it/s]Extractor Predicting: 131it [01:26,  1.55it/s]Extractor Predicting: 132it [01:27,  1.58it/s]Extractor Predicting: 133it [01:27,  1.58it/s]Extractor Predicting: 134it [01:28,  1.57it/s]Extractor Predicting: 135it [01:29,  1.59it/s]Extractor Predicting: 136it [01:29,  1.61it/s]Extractor Predicting: 137it [01:30,  1.58it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:31,  1.55it/s]Extractor Predicting: 140it [01:32,  1.59it/s]Extractor Predicting: 141it [01:32,  1.61it/s]Extractor Predicting: 142it [01:33,  1.63it/s]Extractor Predicting: 143it [01:34,  1.62it/s]Extractor Predicting: 144it [01:34,  1.64it/s]Extractor Predicting: 145it [01:35,  1.61it/s]Extractor Predicting: 146it [01:35,  1.60it/s]Extractor Predicting: 147it [01:36,  1.60it/s]Extractor Predicting: 148it [01:37,  1.61it/s]Extractor Predicting: 149it [01:37,  1.57it/s]Extractor Predicting: 150it [01:38,  1.52it/s]Extractor Predicting: 151it [01:39,  1.55it/s]Extractor Predicting: 152it [01:39,  1.53it/s]Extractor Predicting: 153it [01:40,  1.57it/s]Extractor Predicting: 154it [01:41,  1.54it/s]Extractor Predicting: 155it [01:41,  1.50it/s]Extractor Predicting: 156it [01:42,  1.51it/s]Extractor Predicting: 157it [01:43,  1.47it/s]Extractor Predicting: 158it [01:43,  1.53it/s]Extractor Predicting: 159it [01:44,  1.54it/s]Extractor Predicting: 160it [01:45,  1.57it/s]Extractor Predicting: 161it [01:45,  1.59it/s]Extractor Predicting: 162it [01:46,  1.58it/s]Extractor Predicting: 163it [01:46,  1.56it/s]Extractor Predicting: 164it [01:47,  1.59it/s]Extractor Predicting: 165it [01:48,  1.57it/s]Extractor Predicting: 166it [01:48,  1.60it/s]Extractor Predicting: 167it [01:49,  1.57it/s]Extractor Predicting: 168it [01:50,  1.58it/s]Extractor Predicting: 169it [01:50,  1.60it/s]Extractor Predicting: 170it [01:51,  1.56it/s]Extractor Predicting: 171it [01:52,  1.55it/s]Extractor Predicting: 172it [01:52,  1.52it/s]Extractor Predicting: 173it [01:53,  1.51it/s]Extractor Predicting: 174it [01:53,  1.52it/s]Extractor Predicting: 175it [01:54,  1.53it/s]Extractor Predicting: 176it [01:55,  1.54it/s]Extractor Predicting: 177it [01:55,  1.52it/s]Extractor Predicting: 178it [01:56,  1.49it/s]Extractor Predicting: 179it [01:57,  1.53it/s]Extractor Predicting: 180it [01:57,  1.54it/s]Extractor Predicting: 181it [01:58,  1.58it/s]Extractor Predicting: 182it [01:59,  1.60it/s]Extractor Predicting: 183it [01:59,  1.59it/s]Extractor Predicting: 184it [02:00,  1.61it/s]Extractor Predicting: 185it [02:00,  1.60it/s]Extractor Predicting: 186it [02:01,  1.58it/s]Extractor Predicting: 187it [02:02,  1.61it/s]Extractor Predicting: 188it [02:02,  1.63it/s]Extractor Predicting: 189it [02:03,  1.57it/s]Extractor Predicting: 190it [02:04,  1.51it/s]Extractor Predicting: 191it [02:04,  1.53it/s]Extractor Predicting: 192it [02:05,  1.51it/s]Extractor Predicting: 193it [02:06,  1.49it/s]Extractor Predicting: 194it [02:06,  1.50it/s]Extractor Predicting: 195it [02:07,  1.48it/s]Extractor Predicting: 196it [02:08,  1.47it/s]Extractor Predicting: 197it [02:08,  1.49it/s]Extractor Predicting: 198it [02:09,  1.51it/s]Extractor Predicting: 199it [02:10,  1.50it/s]Extractor Predicting: 200it [02:10,  1.49it/s]Extractor Predicting: 201it [02:11,  1.46it/s]Extractor Predicting: 202it [02:12,  1.47it/s]Extractor Predicting: 203it [02:12,  1.49it/s]Extractor Predicting: 204it [02:13,  1.48it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:15,  1.35it/s]Extractor Predicting: 207it [02:15,  1.35it/s]Extractor Predicting: 208it [02:16,  1.37it/s]Extractor Predicting: 209it [02:17,  1.44it/s]Extractor Predicting: 210it [02:17,  1.48it/s]Extractor Predicting: 211it [02:18,  1.49it/s]Extractor Predicting: 212it [02:19,  1.51it/s]Extractor Predicting: 213it [02:19,  1.50it/s]Extractor Predicting: 214it [02:20,  1.48it/s]Extractor Predicting: 215it [02:21,  1.49it/s]Extractor Predicting: 216it [02:21,  1.46it/s]Extractor Predicting: 217it [02:22,  1.44it/s]Extractor Predicting: 218it [02:23,  1.48it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.49it/s]Extractor Predicting: 221it [02:25,  1.51it/s]Extractor Predicting: 222it [02:26,  1.47it/s]Extractor Predicting: 223it [02:26,  1.42it/s]Extractor Predicting: 224it [02:27,  1.41it/s]Extractor Predicting: 225it [02:28,  1.40it/s]Extractor Predicting: 226it [02:28,  1.41it/s]Extractor Predicting: 227it [02:29,  1.40it/s]Extractor Predicting: 228it [02:30,  1.39it/s]Extractor Predicting: 229it [02:31,  1.39it/s]Extractor Predicting: 230it [02:31,  1.39it/s]Extractor Predicting: 231it [02:32,  1.40it/s]Extractor Predicting: 232it [02:33,  1.40it/s]Extractor Predicting: 233it [02:33,  1.41it/s]Extractor Predicting: 234it [02:34,  1.42it/s]Extractor Predicting: 235it [02:35,  1.43it/s]Extractor Predicting: 236it [02:36,  1.43it/s]Extractor Predicting: 237it [02:36,  1.37it/s]Extractor Predicting: 238it [02:37,  1.38it/s]Extractor Predicting: 239it [02:38,  1.41it/s]Extractor Predicting: 240it [02:38,  1.44it/s]Extractor Predicting: 241it [02:39,  1.46it/s]Extractor Predicting: 242it [02:40,  1.47it/s]Extractor Predicting: 243it [02:40,  1.48it/s]Extractor Predicting: 244it [02:41,  1.45it/s]Extractor Predicting: 245it [02:42,  1.49it/s]Extractor Predicting: 246it [02:42,  1.52it/s]Extractor Predicting: 246it [02:42,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:22,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:22,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:22,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:22,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:22,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:42:23,424 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:42:23,425 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:42:24,017 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:42:25,012 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:42:25,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:28,098 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:28,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:28,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:28,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:42:28,101 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:42:28,764 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:42:28,821 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:42:29,450 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:42:29,600 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:42:29,600 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.37307152875175315,
  "recall": 0.04508474576271186,
  "score": 0.08044760320580673,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:04,  1.39it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.68it/s]Extractor Predicting: 15it [00:10,  1.49it/s]
[INFO|configuration_utils.py:515] 2023-08-29 11:42:42,369 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:42:42,370 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:42:42,487 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:42:42,489 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 11:42:42,544 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:42:57,949 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 11:42:57,992 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 11:42:58,254 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:42:58,255 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:42:58,357 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:42:58,418 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.021067415730337078,
  "score": 0.04043126684636118,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 11:42:58,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:42:59,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:00,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:00,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:01,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:02,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:02,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:03,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:04,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:05,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:06,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:06,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:07,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:08,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:08,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:10,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:10,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:11,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:12,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:12,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:23, 14.54s/it][WARNING|generation_utils.py:914] 2023-08-29 11:43:13,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:14,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:14,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:15,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:16,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:16,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:17,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:18,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:18,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:19,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:20,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:20,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:21,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:22,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:22,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:23,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:24,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:24,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:25,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:26,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:00, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-29 11:43:26,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:27,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:28,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:28,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:29,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:30,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:30,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:31,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:32,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:33,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:33,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:35,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:36,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:36,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:37,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:38,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:38,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:39,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:40,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:46, 13.87s/it][WARNING|generation_utils.py:914] 2023-08-29 11:43:40,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:41,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:41,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:42,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:43,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:44,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:44,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:45,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:46,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:46,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:47,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:48,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:48,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:49,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:50,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:50,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:51,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:52,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:52,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:53,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:53,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:32, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-29 11:43:54,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:55,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:55,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:56,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:56,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:57,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:58,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:58,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:59,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:43:59,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:00,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:01,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:01,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:02,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:02,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:04,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:04,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:05,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:05,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:10, 13.09s/it][WARNING|generation_utils.py:914] 2023-08-29 11:44:06,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:06,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:07,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:08,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:09,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:09,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:10,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:10,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:11,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:12,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:13,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:13,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:14,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:15,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:16,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:16,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:17,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:18,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:18,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:19,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:21<02:00, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-29 11:44:20,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:20,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:21,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:22,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:22,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:23,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:24,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:24,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:25,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:25,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:26,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:27,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:27,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:28,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:29,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:29,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:30,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:31,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:31,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:32,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:34<01:45, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-29 11:44:33,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:33,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:34,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:35,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:35,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:36,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:37,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:38,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:38,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:39,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:40,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:41,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:41,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:42,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:43,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:43,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:44,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:45,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:46,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:46,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:35, 13.64s/it][WARNING|generation_utils.py:914] 2023-08-29 11:44:47,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:48,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:48,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:49,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:50,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:50,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:51,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:52,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:52,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:53,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:54,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:54,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:55,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:56,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:56,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:57,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:58,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:58,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:44:59,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:00,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:00,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:01,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:03<01:23, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-29 11:45:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:02,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:03,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:04,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:05,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:05,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:06,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:07,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:07,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:08,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:09,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:09,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:10,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:11,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:12,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:12,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:13,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:14,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:14,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:15,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:16,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:17,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:18<01:12, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-29 11:45:17,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:18,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:19,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:19,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:20,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:21,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:21,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:22,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:23,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:23,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:24,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:25,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:26,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:27,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:27,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:28,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:29,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:29,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:30,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:31,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:31,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:33<00:58, 14.56s/it][WARNING|generation_utils.py:914] 2023-08-29 11:45:32,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:33,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:33,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:34,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:35,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:35,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:36,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:37,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:37,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:38,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:39,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:39,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:40,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:41,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:41,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:42,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:43,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:43,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:44,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:45,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:46<00:42, 14.12s/it][WARNING|generation_utils.py:914] 2023-08-29 11:45:45,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:47,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:47,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:48,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:49,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:49,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:50,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:51,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:51,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:52,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:52,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:53,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:54,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:54,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:56,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:57,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:58,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:00<00:27, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-29 11:45:59,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:45:59,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:00,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:01,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:02,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:02,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:03,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:04,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:04,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:05,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:06,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:07,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:08,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:09,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:09,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:10,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:11,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:11,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:12,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:13,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:13,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:15<00:14, 14.40s/it][WARNING|generation_utils.py:914] 2023-08-29 11:46:14,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:15,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:15,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:16,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:17,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:17,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:18,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:19,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:19,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:20,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:21,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:21,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:22,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:23,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:23,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:24,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:25,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:25,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:26,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:46:27,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:29<00:00, 14.05s/it]Generating: 100%|██████████| 15/15 [03:29<00:00, 13.94s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:36,526 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:36,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:36,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:36,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:36,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:46:37,597 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:46:37,598 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:46:38,273 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:46:39,427 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:46:39,427 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:42,421 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:42,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:42,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:42,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:46:42,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:46:43,218 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:46:43,220 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:46:43,843 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:46:44,138 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:46:44,138 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 380, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 536, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : notable work .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : successful candidate .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in life , he came to love the beauty and natural beauty of the family at home . Head Entity : sister , Tail Entity : the beauty .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in 1778 he came to London , where he founded the Royal College of Art . Head Entity : Royal College of Art , Tail Entity : King Edward VII .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8579545454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : voice type .', 'success_rate': 0.971875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 8462
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8562, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.63it/s]Extractor Estimating: 8it [00:04,  1.63it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.64it/s]Extractor Estimating: 14it [00:08,  1.58it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:09,  1.64it/s]Extractor Estimating: 17it [00:10,  1.66it/s]Extractor Estimating: 18it [00:11,  1.70it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:12,  1.68it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:14,  1.74it/s]Extractor Estimating: 24it [00:14,  1.74it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:15,  1.67it/s]Extractor Estimating: 27it [00:16,  1.61it/s]Extractor Estimating: 28it [00:17,  1.57it/s]Extractor Estimating: 29it [00:17,  1.57it/s]Extractor Estimating: 30it [00:18,  1.57it/s]Extractor Estimating: 31it [00:19,  1.47it/s]Extractor Estimating: 32it [00:19,  1.49it/s]Extractor Estimating: 33it [00:20,  1.52it/s]Extractor Estimating: 34it [00:21,  1.56it/s]Extractor Estimating: 35it [00:21,  1.55it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:23,  1.54it/s]Extractor Estimating: 38it [00:23,  1.52it/s]Extractor Estimating: 39it [00:24,  1.51it/s]Extractor Estimating: 40it [00:25,  1.44it/s]Extractor Estimating: 41it [00:25,  1.39it/s]Extractor Estimating: 42it [00:26,  1.45it/s]Extractor Estimating: 43it [00:27,  1.47it/s]Extractor Estimating: 44it [00:27,  1.45it/s]Extractor Estimating: 45it [00:28,  1.48it/s]Extractor Estimating: 46it [00:29,  1.44it/s]Extractor Estimating: 47it [00:29,  1.46it/s]Extractor Estimating: 48it [00:30,  1.47it/s]Extractor Estimating: 49it [00:31,  1.48it/s]Extractor Estimating: 50it [00:31,  1.50it/s]Extractor Estimating: 51it [00:32,  1.54it/s]Extractor Estimating: 52it [00:33,  1.55it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.59it/s]Extractor Estimating: 55it [00:35,  1.62it/s]Extractor Estimating: 56it [00:35,  1.60it/s]Extractor Estimating: 57it [00:36,  1.68it/s]Extractor Estimating: 58it [00:36,  1.63it/s]Extractor Estimating: 59it [00:37,  1.68it/s]Extractor Estimating: 60it [00:37,  1.69it/s]Extractor Estimating: 61it [00:38,  1.72it/s]Extractor Estimating: 62it [00:39,  1.66it/s]Extractor Estimating: 63it [00:39,  1.60it/s]Extractor Estimating: 64it [00:40,  1.66it/s]Extractor Estimating: 65it [00:41,  1.67it/s]Extractor Estimating: 66it [00:41,  1.74it/s]Extractor Estimating: 67it [00:42,  1.72it/s]Extractor Estimating: 68it [00:42,  1.73it/s]Extractor Estimating: 69it [00:43,  1.69it/s]Extractor Estimating: 70it [00:43,  1.69it/s]Extractor Estimating: 71it [00:44,  1.63it/s]Extractor Estimating: 72it [00:45,  1.66it/s]Extractor Estimating: 73it [00:45,  1.65it/s]Extractor Estimating: 74it [00:46,  1.66it/s]Extractor Estimating: 75it [00:46,  1.72it/s]Extractor Estimating: 76it [00:47,  1.71it/s]Extractor Estimating: 77it [00:48,  1.73it/s]Extractor Estimating: 78it [00:48,  1.78it/s]Extractor Estimating: 79it [00:49,  1.79it/s]Extractor Estimating: 80it [00:49,  1.69it/s]Extractor Estimating: 81it [00:50,  1.69it/s]Extractor Estimating: 82it [00:50,  1.73it/s]Extractor Estimating: 83it [00:51,  1.71it/s]Extractor Estimating: 84it [00:52,  1.75it/s]Extractor Estimating: 85it [00:52,  1.77it/s]Extractor Estimating: 86it [00:53,  1.71it/s]Extractor Estimating: 87it [00:53,  1.66it/s]Extractor Estimating: 88it [00:54,  1.66it/s]Extractor Estimating: 89it [00:55,  1.71it/s]Extractor Estimating: 90it [00:55,  1.69it/s]Extractor Estimating: 91it [00:56,  1.78it/s]Extractor Estimating: 92it [00:56,  1.80it/s]Extractor Estimating: 93it [00:57,  1.77it/s]Extractor Estimating: 94it [00:57,  1.80it/s]Extractor Estimating: 95it [00:58,  1.80it/s]Extractor Estimating: 96it [00:58,  1.80it/s]Extractor Estimating: 97it [00:59,  1.74it/s]Extractor Estimating: 98it [01:00,  1.78it/s]Extractor Estimating: 99it [01:00,  1.79it/s]Extractor Estimating: 100it [01:01,  1.77it/s]Extractor Estimating: 101it [01:01,  1.85it/s]Extractor Estimating: 102it [01:02,  1.88it/s]Extractor Estimating: 103it [01:02,  1.89it/s]Extractor Estimating: 104it [01:03,  1.85it/s]Extractor Estimating: 105it [01:03,  1.89it/s]Extractor Estimating: 106it [01:04,  1.86it/s]Extractor Estimating: 107it [01:04,  1.81it/s]Extractor Estimating: 108it [01:05,  1.84it/s]Extractor Estimating: 109it [01:05,  1.93it/s]Extractor Estimating: 110it [01:06,  1.93it/s]Extractor Estimating: 111it [01:06,  2.02it/s]Extractor Estimating: 112it [01:07,  2.09it/s]Extractor Estimating: 113it [01:07,  2.00it/s]Extractor Estimating: 114it [01:08,  1.77it/s]Extractor Estimating: 115it [01:09,  1.78it/s]Extractor Estimating: 116it [01:09,  1.80it/s]Extractor Estimating: 117it [01:10,  1.84it/s]Extractor Estimating: 118it [01:10,  1.86it/s]Extractor Estimating: 119it [01:11,  1.92it/s]Extractor Estimating: 120it [01:11,  1.86it/s]Extractor Estimating: 121it [01:12,  1.88it/s]Extractor Estimating: 122it [01:12,  1.92it/s]Extractor Estimating: 123it [01:13,  1.96it/s]Extractor Estimating: 124it [01:13,  2.03it/s]Extractor Estimating: 125it [01:14,  2.00it/s]Extractor Estimating: 126it [01:14,  1.88it/s]Extractor Estimating: 127it [01:15,  1.79it/s]Extractor Estimating: 128it [01:16,  1.78it/s]Extractor Estimating: 129it [01:16,  1.72it/s]Extractor Estimating: 130it [01:17,  1.71it/s]Extractor Estimating: 131it [01:17,  1.77it/s]Extractor Estimating: 132it [01:18,  1.75it/s]Extractor Estimating: 133it [01:18,  1.74it/s]Extractor Estimating: 134it [01:19,  1.76it/s]Extractor Estimating: 135it [01:20,  1.71it/s]Extractor Estimating: 136it [01:20,  1.69it/s]Extractor Estimating: 137it [01:21,  1.68it/s]Extractor Estimating: 138it [01:21,  1.69it/s]Extractor Estimating: 139it [01:22,  1.62it/s]Extractor Estimating: 140it [01:23,  1.60it/s]Extractor Estimating: 141it [01:23,  1.56it/s]Extractor Estimating: 142it [01:24,  1.63it/s]Extractor Estimating: 143it [01:25,  1.64it/s]Extractor Estimating: 144it [01:25,  1.68it/s]Extractor Estimating: 145it [01:26,  1.66it/s]Extractor Estimating: 146it [01:26,  1.68it/s]Extractor Estimating: 147it [01:27,  1.69it/s]Extractor Estimating: 148it [01:27,  1.69it/s]Extractor Estimating: 149it [01:28,  1.65it/s]Extractor Estimating: 150it [01:29,  1.65it/s]Extractor Estimating: 151it [01:29,  1.60it/s]Extractor Estimating: 152it [01:30,  1.54it/s]Extractor Estimating: 153it [01:31,  1.56it/s]Extractor Estimating: 154it [01:31,  1.58it/s]Extractor Estimating: 155it [01:32,  1.59it/s]Extractor Estimating: 156it [01:33,  1.61it/s]Extractor Estimating: 157it [01:33,  1.63it/s]Extractor Estimating: 158it [01:34,  1.62it/s]Extractor Estimating: 159it [01:34,  1.56it/s]Extractor Estimating: 160it [01:35,  1.57it/s]Extractor Estimating: 161it [01:36,  1.59it/s]Extractor Estimating: 162it [01:36,  1.64it/s]Extractor Estimating: 163it [01:37,  1.66it/s]Extractor Estimating: 164it [01:38,  1.64it/s]Extractor Estimating: 165it [01:38,  1.63it/s]Extractor Estimating: 166it [01:39,  1.65it/s]Extractor Estimating: 167it [01:39,  1.62it/s]Extractor Estimating: 168it [01:40,  1.55it/s]Extractor Estimating: 169it [01:41,  1.53it/s]Extractor Estimating: 170it [01:41,  1.55it/s]Extractor Estimating: 171it [01:42,  1.55it/s]Extractor Estimating: 172it [01:43,  1.57it/s]Extractor Estimating: 173it [01:43,  1.59it/s]Extractor Estimating: 174it [01:44,  1.58it/s]Extractor Estimating: 175it [01:44,  1.61it/s]Extractor Estimating: 176it [01:45,  1.60it/s]Extractor Estimating: 177it [01:46,  1.61it/s]Extractor Estimating: 178it [01:46,  1.60it/s]Extractor Estimating: 179it [01:47,  1.58it/s]Extractor Estimating: 180it [01:48,  1.59it/s]Extractor Estimating: 181it [01:48,  1.62it/s]Extractor Estimating: 182it [01:49,  1.60it/s]Extractor Estimating: 183it [01:50,  1.57it/s]Extractor Estimating: 184it [01:50,  1.58it/s]Extractor Estimating: 185it [01:51,  1.58it/s]Extractor Estimating: 186it [01:51,  1.58it/s]Extractor Estimating: 187it [01:52,  1.54it/s]Extractor Estimating: 188it [01:53,  1.54it/s]Extractor Estimating: 189it [01:53,  1.52it/s]Extractor Estimating: 190it [01:54,  1.54it/s]Extractor Estimating: 191it [01:55,  1.53it/s]Extractor Estimating: 192it [01:55,  1.51it/s]Extractor Estimating: 193it [01:56,  1.46it/s]Extractor Estimating: 194it [01:57,  1.48it/s]Extractor Estimating: 195it [01:58,  1.45it/s]Extractor Estimating: 196it [01:58,  1.47it/s]Extractor Estimating: 197it [01:59,  1.51it/s]Extractor Estimating: 198it [01:59,  1.51it/s]Extractor Estimating: 199it [02:00,  1.43it/s]Extractor Estimating: 200it [02:01,  1.42it/s]Extractor Estimating: 201it [02:02,  1.49it/s]Extractor Estimating: 202it [02:02,  1.54it/s]Extractor Estimating: 203it [02:03,  1.55it/s]Extractor Estimating: 204it [02:03,  1.59it/s]Extractor Estimating: 205it [02:04,  1.67it/s]Extractor Estimating: 206it [02:04,  1.70it/s]Extractor Estimating: 207it [02:05,  1.73it/s]Extractor Estimating: 208it [02:06,  1.65it/s]Extractor Estimating: 209it [02:06,  1.69it/s]Extractor Estimating: 210it [02:07,  1.67it/s]Extractor Estimating: 211it [02:08,  1.66it/s]Extractor Estimating: 212it [02:08,  1.69it/s]Extractor Estimating: 213it [02:09,  1.68it/s]Extractor Estimating: 214it [02:09,  1.72it/s]Extractor Estimating: 215it [02:10,  1.70it/s]Extractor Estimating: 216it [02:10,  1.69it/s]Extractor Estimating: 217it [02:11,  1.68it/s]Extractor Estimating: 218it [02:12,  1.70it/s]Extractor Estimating: 219it [02:12,  1.75it/s]Extractor Estimating: 220it [02:13,  1.73it/s]Extractor Estimating: 221it [02:13,  1.73it/s]Extractor Estimating: 222it [02:14,  1.72it/s]Extractor Estimating: 223it [02:15,  1.68it/s]Extractor Estimating: 224it [02:15,  1.72it/s]Extractor Estimating: 225it [02:16,  1.69it/s]Extractor Estimating: 226it [02:16,  1.68it/s]Extractor Estimating: 227it [02:17,  1.66it/s]Extractor Estimating: 228it [02:17,  1.73it/s]Extractor Estimating: 229it [02:18,  1.76it/s]Extractor Estimating: 230it [02:19,  1.78it/s]Extractor Estimating: 231it [02:19,  1.74it/s]Extractor Estimating: 232it [02:20,  1.76it/s]Extractor Estimating: 233it [02:20,  1.80it/s]Extractor Estimating: 234it [02:21,  1.77it/s]Extractor Estimating: 235it [02:21,  1.74it/s]Extractor Estimating: 236it [02:22,  1.79it/s]Extractor Estimating: 237it [02:22,  1.76it/s]Extractor Estimating: 238it [02:23,  1.76it/s]Extractor Estimating: 239it [02:24,  1.72it/s]Extractor Estimating: 240it [02:24,  1.69it/s]Extractor Estimating: 241it [02:25,  1.69it/s]Extractor Estimating: 242it [02:25,  1.74it/s]Extractor Estimating: 243it [02:26,  1.64it/s]Extractor Estimating: 244it [02:27,  1.77it/s]Extractor Estimating: 245it [02:27,  1.75it/s]Extractor Estimating: 246it [02:28,  1.74it/s]Extractor Estimating: 247it [02:28,  1.66it/s]Extractor Estimating: 248it [02:29,  1.67it/s]Extractor Estimating: 249it [02:30,  1.73it/s]Extractor Estimating: 250it [02:30,  1.73it/s]Extractor Estimating: 251it [02:31,  1.72it/s]Extractor Estimating: 252it [02:31,  1.63it/s]Extractor Estimating: 253it [02:32,  1.56it/s]Extractor Estimating: 254it [02:33,  1.59it/s]Extractor Estimating: 255it [02:33,  1.61it/s]Extractor Estimating: 256it [02:34,  1.62it/s]Extractor Estimating: 257it [02:34,  1.65it/s]Extractor Estimating: 258it [02:35,  1.64it/s]Extractor Estimating: 259it [02:36,  1.62it/s]Extractor Estimating: 260it [02:36,  1.58it/s]Extractor Estimating: 261it [02:37,  1.60it/s]Extractor Estimating: 262it [02:38,  1.65it/s]Extractor Estimating: 263it [02:38,  1.67it/s]Extractor Estimating: 264it [02:39,  1.66it/s]Extractor Estimating: 265it [02:40,  1.52it/s]Extractor Estimating: 266it [02:40,  1.56it/s]Extractor Estimating: 267it [02:41,  1.65it/s]Extractor Estimating: 268it [02:41,  1.64it/s]Extractor Estimating: 269it [02:42,  1.63it/s]Extractor Estimating: 270it [02:42,  1.65it/s]Extractor Estimating: 271it [02:43,  1.64it/s]Extractor Estimating: 272it [02:44,  1.63it/s]Extractor Estimating: 273it [02:44,  1.68it/s]Extractor Estimating: 274it [02:45,  1.66it/s]Extractor Estimating: 275it [02:46,  1.66it/s]Extractor Estimating: 276it [02:46,  1.72it/s]Extractor Estimating: 277it [02:47,  1.68it/s]Extractor Estimating: 278it [02:47,  1.73it/s]Extractor Estimating: 279it [02:48,  1.78it/s]Extractor Estimating: 280it [02:48,  1.81it/s]Extractor Estimating: 281it [02:49,  1.87it/s]Extractor Estimating: 282it [02:49,  1.86it/s]Extractor Estimating: 283it [02:50,  1.90it/s]Extractor Estimating: 284it [02:50,  1.93it/s]Extractor Estimating: 285it [02:51,  1.86it/s]Extractor Estimating: 286it [02:51,  1.86it/s]Extractor Estimating: 287it [02:52,  1.79it/s]Extractor Estimating: 288it [02:53,  1.80it/s]Extractor Estimating: 289it [02:53,  1.82it/s]Extractor Estimating: 290it [02:54,  1.73it/s]Extractor Estimating: 291it [02:54,  1.75it/s]Extractor Estimating: 292it [02:55,  1.81it/s]Extractor Estimating: 293it [02:55,  1.82it/s]Extractor Estimating: 294it [02:56,  1.81it/s]Extractor Estimating: 295it [02:56,  1.83it/s]Extractor Estimating: 296it [02:57,  1.80it/s]Extractor Estimating: 297it [02:58,  1.82it/s]Extractor Estimating: 298it [02:58,  1.90it/s]Extractor Estimating: 299it [02:59,  1.83it/s]Extractor Estimating: 300it [02:59,  1.88it/s]Extractor Estimating: 301it [03:00,  1.85it/s]Extractor Estimating: 302it [03:00,  1.84it/s]Extractor Estimating: 303it [03:01,  1.88it/s]Extractor Estimating: 304it [03:01,  1.86it/s]Extractor Estimating: 305it [03:02,  1.82it/s]Extractor Estimating: 306it [03:02,  1.80it/s]Extractor Estimating: 307it [03:03,  1.76it/s]Extractor Estimating: 308it [03:04,  1.76it/s]Extractor Estimating: 309it [03:04,  1.74it/s]Extractor Estimating: 310it [03:05,  1.72it/s]Extractor Estimating: 311it [03:05,  1.74it/s]Extractor Estimating: 312it [03:06,  1.81it/s]Extractor Estimating: 313it [03:06,  1.78it/s]Extractor Estimating: 314it [03:07,  1.73it/s]Extractor Estimating: 315it [03:08,  1.73it/s]Extractor Estimating: 316it [03:08,  1.73it/s]Extractor Estimating: 317it [03:09,  1.72it/s]Extractor Estimating: 318it [03:09,  1.70it/s]Extractor Estimating: 319it [03:10,  1.68it/s]Extractor Estimating: 320it [03:11,  1.72it/s]Extractor Estimating: 321it [03:11,  1.68it/s]Extractor Estimating: 322it [03:12,  1.70it/s]Extractor Estimating: 323it [03:12,  1.64it/s]Extractor Estimating: 324it [03:13,  1.62it/s]Extractor Estimating: 325it [03:14,  1.67it/s]Extractor Estimating: 326it [03:14,  1.61it/s]Extractor Estimating: 327it [03:15,  1.56it/s]Extractor Estimating: 328it [03:16,  1.57it/s]Extractor Estimating: 329it [03:16,  1.54it/s]Extractor Estimating: 330it [03:17,  1.60it/s]Extractor Estimating: 331it [03:17,  1.60it/s]Extractor Estimating: 332it [03:18,  1.55it/s]Extractor Estimating: 333it [03:19,  1.56it/s]Extractor Estimating: 334it [03:19,  1.62it/s]Extractor Estimating: 335it [03:20,  1.65it/s]Extractor Estimating: 336it [03:21,  1.64it/s]Extractor Estimating: 337it [03:21,  1.65it/s]Extractor Estimating: 338it [03:22,  1.67it/s]Extractor Estimating: 339it [03:22,  1.72it/s]Extractor Estimating: 340it [03:23,  1.66it/s]Extractor Estimating: 341it [03:24,  1.63it/s]Extractor Estimating: 342it [03:24,  1.62it/s]Extractor Estimating: 343it [03:25,  1.67it/s]Extractor Estimating: 344it [03:25,  1.63it/s]Extractor Estimating: 345it [03:26,  1.62it/s]Extractor Estimating: 346it [03:27,  1.61it/s]Extractor Estimating: 347it [03:27,  1.64it/s]Extractor Estimating: 348it [03:28,  1.60it/s]Extractor Estimating: 349it [03:28,  1.65it/s]Extractor Estimating: 350it [03:29,  1.65it/s]Extractor Estimating: 351it [03:30,  1.67it/s]Extractor Estimating: 352it [03:30,  1.73it/s]Extractor Estimating: 353it [03:31,  1.76it/s]Extractor Estimating: 354it [03:31,  1.78it/s]Extractor Estimating: 355it [03:32,  1.82it/s]Extractor Estimating: 356it [03:32,  1.81it/s]Extractor Estimating: 357it [03:33,  1.71it/s]Extractor Estimating: 358it [03:34,  1.74it/s]Extractor Estimating: 359it [03:34,  1.72it/s]Extractor Estimating: 360it [03:35,  1.76it/s]Extractor Estimating: 361it [03:35,  1.82it/s]Extractor Estimating: 362it [03:36,  1.86it/s]Extractor Estimating: 363it [03:36,  1.82it/s]Extractor Estimating: 364it [03:37,  1.64it/s]Extractor Estimating: 365it [03:38,  1.70it/s]Extractor Estimating: 366it [03:38,  1.71it/s]Extractor Estimating: 367it [03:39,  1.74it/s]Extractor Estimating: 368it [03:39,  1.76it/s]Extractor Estimating: 369it [03:40,  1.78it/s]Extractor Estimating: 370it [03:40,  1.68it/s]Extractor Estimating: 371it [03:41,  1.71it/s]Extractor Estimating: 372it [03:42,  1.74it/s]Extractor Estimating: 373it [03:42,  1.76it/s]Extractor Estimating: 374it [03:43,  1.74it/s]Extractor Estimating: 375it [03:43,  1.80it/s]Extractor Estimating: 375it [03:43,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:50,655 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:50,700 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:50,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:50,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:50,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:50:51,434 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:50:51,435 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:50:51,826 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:50:53,048 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:50:53,048 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:50:55,297 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:50:55,914 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:50:55,915 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:50:56,686 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:50:56,981 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:50:56,981 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 14:24:17,908 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 14:24:18,210 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7493 mean pseudo reward: 0.9372351564988414
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl'}
train vocab size: 18677
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18777, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=18777, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.051, loss:647.4502
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.062, loss:645.1093
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.049, loss:637.7935
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.062, loss:594.8069
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.056, loss:592.3990
>> valid entity prec:0.5226, rec:0.5023, f1:0.5122
>> valid relation prec:0.0282, rec:0.0109, f1:0.0157
>> valid relation with NER prec:0.0282, rec:0.0109, f1:0.0157
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 3.214, loss:601.8941
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.070, loss:543.8305
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.050, loss:562.7307
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.051, loss:594.7226
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.047, loss:554.3954
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5220, rec:0.5009, f1:0.5112
>> valid relation prec:0.0447, rec:0.0179, f1:0.0256
>> valid relation with NER prec:0.0447, rec:0.0179, f1:0.0256
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 3.218, loss:552.4094
g_step 1200, step 261, avg_time 1.067, loss:555.2578
g_step 1300, step 48, avg_time 1.037, loss:510.9851
g_step 1400, step 148, avg_time 1.056, loss:533.5474
g_step 1500, step 248, avg_time 1.077, loss:544.9508
>> valid entity prec:0.5085, rec:0.4954, f1:0.5019
>> valid relation prec:0.0319, rec:0.0158, f1:0.0212
>> valid relation with NER prec:0.0319, rec:0.0158, f1:0.0212
g_step 1600, step 35, avg_time 3.184, loss:514.8780
g_step 1700, step 135, avg_time 1.059, loss:496.0222
g_step 1800, step 235, avg_time 1.052, loss:502.4228
g_step 1900, step 22, avg_time 1.064, loss:507.8068
g_step 2000, step 122, avg_time 1.062, loss:471.8628
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5093, rec:0.4331, f1:0.4681
>> valid relation prec:0.0446, rec:0.0173, f1:0.0249
>> valid relation with NER prec:0.0446, rec:0.0173, f1:0.0249
g_step 2100, step 222, avg_time 3.192, loss:478.3195
g_step 2200, step 9, avg_time 1.054, loss:489.5318
g_step 2300, step 109, avg_time 1.052, loss:450.7837
g_step 2400, step 209, avg_time 1.056, loss:480.8100
g_step 2500, step 309, avg_time 1.061, loss:463.6882
>> valid entity prec:0.5037, rec:0.4513, f1:0.4761
>> valid relation prec:0.0430, rec:0.0189, f1:0.0262
>> valid relation with NER prec:0.0430, rec:0.0189, f1:0.0262
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 3.189, loss:433.9641
g_step 2700, step 196, avg_time 1.060, loss:438.1517
g_step 2800, step 296, avg_time 1.061, loss:460.3967
g_step 2900, step 83, avg_time 1.051, loss:418.2411
g_step 3000, step 183, avg_time 1.055, loss:413.2487
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5038, rec:0.4781, f1:0.4906
>> valid relation prec:0.0331, rec:0.0160, f1:0.0216
>> valid relation with NER prec:0.0331, rec:0.0160, f1:0.0216
g_step 3100, step 283, avg_time 3.205, loss:447.0157
g_step 3200, step 70, avg_time 1.057, loss:407.1508
g_step 3300, step 170, avg_time 1.068, loss:395.0768
g_step 3400, step 270, avg_time 1.053, loss:433.8015
g_step 3500, step 57, avg_time 1.051, loss:398.2077
>> valid entity prec:0.5097, rec:0.4594, f1:0.4832
>> valid relation prec:0.0270, rec:0.0125, f1:0.0171
>> valid relation with NER prec:0.0270, rec:0.0125, f1:0.0171
g_step 3600, step 157, avg_time 3.204, loss:398.0039
g_step 3700, step 257, avg_time 1.059, loss:430.1058
g_step 3800, step 44, avg_time 1.052, loss:392.5859
g_step 3900, step 144, avg_time 1.056, loss:384.7553
g_step 4000, step 244, avg_time 1.062, loss:394.2549
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4847, rec:0.4721, f1:0.4783
>> valid relation prec:0.0367, rec:0.0166, f1:0.0229
>> valid relation with NER prec:0.0367, rec:0.0166, f1:0.0229
g_step 4100, step 31, avg_time 3.211, loss:372.2301
g_step 4200, step 131, avg_time 1.051, loss:358.1733
g_step 4300, step 231, avg_time 1.066, loss:407.4281
g_step 4400, step 18, avg_time 1.048, loss:353.0134
g_step 4500, step 118, avg_time 1.049, loss:350.5152
>> valid entity prec:0.5149, rec:0.4528, f1:0.4818
>> valid relation prec:0.0255, rec:0.0126, f1:0.0169
>> valid relation with NER prec:0.0255, rec:0.0126, f1:0.0169
g_step 4600, step 218, avg_time 3.213, loss:355.9273
g_step 4700, step 5, avg_time 1.058, loss:383.4632
g_step 4800, step 105, avg_time 1.058, loss:335.8883
g_step 4900, step 205, avg_time 1.062, loss:351.0079
g_step 5000, step 305, avg_time 1.053, loss:378.4183
learning rate was adjusted to 0.0008
>> valid entity prec:0.4972, rec:0.4491, f1:0.4719
>> valid relation prec:0.0292, rec:0.0125, f1:0.0175
>> valid relation with NER prec:0.0292, rec:0.0125, f1:0.0175
g_step 5100, step 92, avg_time 3.189, loss:325.2056
g_step 5200, step 192, avg_time 1.060, loss:337.5015
g_step 5300, step 292, avg_time 1.071, loss:340.3268
g_step 5400, step 79, avg_time 1.061, loss:318.2517
g_step 5500, step 179, avg_time 1.063, loss:330.3937
>> valid entity prec:0.5051, rec:0.4600, f1:0.4815
>> valid relation prec:0.0327, rec:0.0162, f1:0.0216
>> valid relation with NER prec:0.0327, rec:0.0162, f1:0.0216
g_step 5600, step 279, avg_time 3.198, loss:339.9811
g_step 5700, step 66, avg_time 1.041, loss:325.3461
g_step 5800, step 166, avg_time 1.079, loss:322.7950
g_step 5900, step 266, avg_time 1.054, loss:314.0880
g_step 6000, step 53, avg_time 1.042, loss:315.5020
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5132, rec:0.4577, f1:0.4838
>> valid relation prec:0.0329, rec:0.0176, f1:0.0229
>> valid relation with NER prec:0.0329, rec:0.0176, f1:0.0229
g_step 6100, step 153, avg_time 3.205, loss:302.6626
g_step 6200, step 253, avg_time 1.065, loss:306.7785
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 14:24:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 14:24:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_14-24-17_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 14:24:19 - WARNING - datasets.builder -   Using custom data configuration default-4952d3156d96ed48
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4952d3156d96ed48/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 14:24:21,687 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:24:21,688 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 14:24:21,688 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:24:21,689 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 14:24:21,835 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:24:21,898 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 14:24:22,491 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 14:24:46,021 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 14:24:46,042 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4952d3156d96ed48/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.56ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.56ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.09ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.58ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.99ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.28ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.50ba/s]100%|██████████| 8/8 [00:01<00:00,  5.43ba/s]100%|██████████| 8/8 [00:01<00:00,  4.37ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.12ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.79ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.71ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.99ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.14ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.25ba/s]100%|██████████| 7/7 [00:01<00:00,  4.51ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  4.85ba/s] 25%|██▌       | 2/8 [00:00<00:00,  6.95ba/s] 50%|█████     | 4/8 [00:00<00:00,  8.68ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  9.31ba/s]100%|██████████| 8/8 [00:00<00:00, 10.68ba/s]100%|██████████| 8/8 [00:00<00:00,  9.45ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.82ba/s] 43%|████▎     | 3/7 [00:00<00:00,  7.14ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  8.47ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  8.83ba/s]100%|██████████| 7/7 [00:00<00:00,  8.99ba/s]
[INFO|trainer.py:414] 2023-08-29 14:24:53,811 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 14:24:53,910 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 14:24:53,931 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 14:24:53,931 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 14:24:53,931 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 14:24:53,931 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 14:24:53,931 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 14:24:53,931 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:58,  3.27it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:47,  3.46it/s]  1%|          | 5/585 [00:01<02:46,  3.47it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:45,  3.49it/s]  1%|▏         | 8/585 [00:02<02:45,  3.49it/s]  2%|▏         | 9/585 [00:02<02:44,  3.49it/s]  2%|▏         | 10/585 [00:02<02:44,  3.49it/s]  2%|▏         | 11/585 [00:03<02:44,  3.49it/s]  2%|▏         | 12/585 [00:03<02:44,  3.49it/s]  2%|▏         | 13/585 [00:03<02:43,  3.49it/s]  2%|▏         | 14/585 [00:04<02:43,  3.49it/s]  3%|▎         | 15/585 [00:04<02:43,  3.49it/s]  3%|▎         | 16/585 [00:04<02:43,  3.49it/s]  3%|▎         | 17/585 [00:04<02:42,  3.49it/s]  3%|▎         | 18/585 [00:05<02:42,  3.49it/s]  3%|▎         | 19/585 [00:05<02:48,  3.36it/s]  3%|▎         | 20/585 [00:05<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:44,  3.42it/s]  4%|▍         | 22/585 [00:06<02:43,  3.44it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.48it/s]  4%|▍         | 26/585 [00:07<02:41,  3.46it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.48it/s]  5%|▌         | 31/585 [00:08<02:39,  3.48it/s]  5%|▌         | 32/585 [00:09<02:38,  3.48it/s]  6%|▌         | 33/585 [00:09<02:38,  3.49it/s]  6%|▌         | 34/585 [00:09<02:37,  3.49it/s]  6%|▌         | 35/585 [00:10<02:37,  3.49it/s]  6%|▌         | 36/585 [00:10<02:37,  3.49it/s]  6%|▋         | 37/585 [00:10<02:48,  3.25it/s]  6%|▋         | 38/585 [00:11<02:45,  3.31it/s]  7%|▋         | 39/585 [00:11<02:42,  3.36it/s]  7%|▋         | 40/585 [00:11<02:40,  3.40it/s]  7%|▋         | 41/585 [00:11<02:38,  3.43it/s]  7%|▋         | 42/585 [00:12<02:37,  3.44it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:36,  3.47it/s]  8%|▊         | 45/585 [00:13<02:35,  3.47it/s]  8%|▊         | 46/585 [00:13<02:35,  3.48it/s]  8%|▊         | 47/585 [00:13<02:34,  3.48it/s]  8%|▊         | 48/585 [00:13<02:34,  3.48it/s]  8%|▊         | 49/585 [00:14<02:34,  3.48it/s]  9%|▊         | 50/585 [00:14<02:33,  3.47it/s]  9%|▊         | 51/585 [00:14<02:33,  3.48it/s]  9%|▉         | 52/585 [00:15<02:33,  3.48it/s]  9%|▉         | 53/585 [00:15<02:33,  3.48it/s]  9%|▉         | 54/585 [00:15<02:39,  3.33it/s]  9%|▉         | 55/585 [00:15<02:37,  3.37it/s] 10%|▉         | 56/585 [00:16<02:35,  3.40it/s] 10%|▉         | 57/585 [00:16<02:34,  3.43it/s] 10%|▉         | 58/585 [00:16<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:32,  3.45it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.47it/s] 11%|█         | 62/585 [00:17<02:30,  3.47it/s] 11%|█         | 63/585 [00:18<02:30,  3.47it/s] 11%|█         | 64/585 [00:18<02:30,  3.47it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:19<02:28,  3.48it/s] 12%|█▏        | 69/585 [00:19<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 72/585 [00:20<02:34,  3.31it/s] 12%|█▏        | 73/585 [00:21<02:37,  3.26it/s] 13%|█▎        | 74/585 [00:21<02:34,  3.32it/s] 13%|█▎        | 75/585 [00:21<02:31,  3.36it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.39it/s] 13%|█▎        | 77/585 [00:22<02:28,  3.42it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<03:31,  2.37it/s] 14%|█▍        | 84/585 [00:24<03:11,  2.61it/s] 15%|█▍        | 85/585 [00:25<02:57,  2.82it/s] 15%|█▍        | 86/585 [00:25<02:46,  2.99it/s] 15%|█▍        | 87/585 [00:25<02:39,  3.12it/s] 15%|█▌        | 88/585 [00:26<02:40,  3.10it/s] 15%|█▌        | 89/585 [00:26<02:35,  3.20it/s] 15%|█▌        | 90/585 [00:26<02:31,  3.28it/s] 16%|█▌        | 91/585 [00:26<02:28,  3.33it/s] 16%|█▌        | 92/585 [00:27<02:26,  3.37it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.42it/s] 16%|█▌        | 95/585 [00:28<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.47it/s] 17%|█▋        | 102/585 [00:30<02:19,  3.47it/s] 18%|█▊        | 103/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.48it/s] 18%|█▊        | 106/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.41it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:32<02:18,  3.44it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:33<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.47it/s] 20%|█▉        | 116/585 [00:34<02:15,  3.47it/s] 20%|██        | 117/585 [00:34<02:14,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 14:25:28,365 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:25:28,366 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:25:28,366 >>   Batch size = 8

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.20it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.93it/s][A
  2%|▏         | 18/782 [00:00<00:15, 49.20it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.33it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.90it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.57it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.29it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.20it/s][A
  6%|▌         | 48/782 [00:00<00:15, 47.21it/s][A
  7%|▋         | 53/782 [00:01<00:19, 38.21it/s][A
  7%|▋         | 58/782 [00:01<00:17, 40.51it/s][A
  8%|▊         | 63/782 [00:01<00:17, 42.29it/s][A
  9%|▊         | 68/782 [00:01<00:19, 36.12it/s][A
  9%|▉         | 73/782 [00:01<00:18, 38.82it/s][A
 10%|▉         | 78/782 [00:01<00:17, 40.93it/s][A
 11%|█         | 83/782 [00:01<00:16, 42.66it/s][A
 11%|█▏        | 88/782 [00:02<00:15, 43.84it/s][A
 12%|█▏        | 93/782 [00:02<00:15, 44.74it/s][A
 13%|█▎        | 98/782 [00:02<00:15, 45.44it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 45.91it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.06it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.40it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.42it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.62it/s][A
 16%|█▋        | 128/782 [00:02<00:13, 46.76it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.73it/s][A
 18%|█▊        | 138/782 [00:03<00:13, 46.86it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.90it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.89it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 47.01it/s][A
 20%|██        | 158/782 [00:03<00:13, 47.03it/s][A
 21%|██        | 163/782 [00:03<00:13, 45.75it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.16it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.34it/s][A
 23%|██▎       | 178/782 [00:03<00:14, 42.19it/s][A
 23%|██▎       | 183/782 [00:04<00:13, 43.54it/s][A
 24%|██▍       | 188/782 [00:04<00:13, 44.59it/s][A
 25%|██▍       | 193/782 [00:04<00:13, 45.17it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 45.66it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.13it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.32it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.52it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.59it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.66it/s][A
 29%|██▉       | 228/782 [00:05<00:11, 46.76it/s][A
 30%|██▉       | 233/782 [00:05<00:11, 46.85it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.77it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.93it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.88it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.99it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 47.02it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.96it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.76it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.78it/s][A
 36%|███▌      | 278/782 [00:06<00:10, 46.85it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.97it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.93it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 47.00it/s][A
 38%|███▊      | 298/782 [00:06<00:12, 40.33it/s][A
 39%|███▊      | 303/782 [00:06<00:12, 37.64it/s][A
 39%|███▉      | 308/782 [00:06<00:11, 40.02it/s][A
 40%|████      | 313/782 [00:06<00:11, 41.79it/s][A
 41%|████      | 318/782 [00:07<00:10, 43.22it/s][A
 41%|████▏     | 323/782 [00:07<00:10, 44.21it/s][A
 42%|████▏     | 328/782 [00:07<00:10, 44.91it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 45.55it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.03it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.17it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.44it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.59it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.65it/s][A
 46%|████▋     | 363/782 [00:08<00:08, 46.70it/s][A
 47%|████▋     | 368/782 [00:08<00:08, 46.78it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.84it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.94it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.83it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.86it/s][A
 50%|█████     | 393/782 [00:08<00:10, 37.21it/s][A
 51%|█████     | 398/782 [00:08<00:09, 39.59it/s][A
 52%|█████▏    | 403/782 [00:08<00:09, 41.63it/s][A
 52%|█████▏    | 408/782 [00:09<00:08, 43.01it/s][A
 53%|█████▎    | 413/782 [00:09<00:08, 44.24it/s][A
 53%|█████▎    | 418/782 [00:09<00:08, 44.84it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 45.42it/s][A
 55%|█████▍    | 428/782 [00:09<00:08, 43.91it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 44.81it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 45.31it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 45.82it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.00it/s][A
 58%|█████▊    | 453/782 [00:10<00:07, 46.33it/s][A
 59%|█████▊    | 458/782 [00:10<00:06, 46.61it/s][A
 59%|█████▉    | 463/782 [00:10<00:06, 46.69it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.67it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.73it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.73it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.89it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.85it/s][A
 64%|██████▎   | 498/782 [00:11<00:06, 46.87it/s][A
 64%|██████▍   | 503/782 [00:11<00:05, 46.79it/s][A
 65%|██████▍   | 508/782 [00:11<00:05, 46.76it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.83it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.90it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.88it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.86it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 44.97it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 45.57it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 45.95it/s][A
 70%|███████   | 548/782 [00:12<00:05, 46.21it/s][A
 71%|███████   | 553/782 [00:12<00:05, 43.86it/s][A
 71%|███████▏  | 558/782 [00:12<00:05, 44.61it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 45.22it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 45.69it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 45.97it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.22it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.34it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.38it/s][A
 76%|███████▌  | 593/782 [00:13<00:04, 46.52it/s][A
 76%|███████▋  | 598/782 [00:13<00:03, 46.58it/s][A
 77%|███████▋  | 603/782 [00:13<00:03, 46.60it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.65it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.75it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.76it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.73it/s][A
 80%|████████  | 628/782 [00:13<00:03, 44.56it/s][A
 81%|████████  | 633/782 [00:13<00:03, 45.20it/s][A
 82%|████████▏ | 638/782 [00:14<00:03, 45.67it/s][A
 82%|████████▏ | 643/782 [00:14<00:03, 46.00it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 46.25it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.42it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.50it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.59it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.64it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.70it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 45.19it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 45.58it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 45.98it/s][A
 89%|████████▊ | 693/782 [00:15<00:01, 46.22it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.37it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.48it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.57it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.52it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.66it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.77it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.74it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 46.77it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 46.83it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.83it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.76it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.83it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.70it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.83it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 43.59it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 44.52it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 45.22it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:17<00:00, 45.22it/s][A 20%|██        | 117/585 [00:51<02:14,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:25:45,802 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 14:25:46,026 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:25:50,146 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:25:50,368 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:25:50,494 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:04<1:12:10,  9.27s/it] 20%|██        | 119/585 [01:04<51:10,  6.59s/it]   21%|██        | 120/585 [01:05<36:24,  4.70s/it] 21%|██        | 121/585 [01:05<26:06,  3.38s/it] 21%|██        | 122/585 [01:05<18:53,  2.45s/it] 21%|██        | 123/585 [01:06<13:52,  1.80s/it] 21%|██        | 124/585 [01:06<10:21,  1.35s/it] 21%|██▏       | 125/585 [01:06<07:53,  1.03s/it] 22%|██▏       | 126/585 [01:06<06:10,  1.24it/s] 22%|██▏       | 127/585 [01:07<04:58,  1.54it/s] 22%|██▏       | 128/585 [01:07<04:07,  1.84it/s] 22%|██▏       | 129/585 [01:07<03:32,  2.15it/s] 22%|██▏       | 130/585 [01:08<03:11,  2.38it/s] 22%|██▏       | 131/585 [01:08<02:52,  2.63it/s] 23%|██▎       | 132/585 [01:08<02:39,  2.84it/s] 23%|██▎       | 133/585 [01:09<02:30,  3.00it/s] 23%|██▎       | 134/585 [01:09<02:24,  3.13it/s] 23%|██▎       | 135/585 [01:09<02:19,  3.22it/s] 23%|██▎       | 136/585 [01:09<02:16,  3.29it/s] 23%|██▎       | 137/585 [01:10<02:13,  3.35it/s] 24%|██▎       | 138/585 [01:10<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:10<02:10,  3.41it/s] 24%|██▍       | 140/585 [01:11<02:09,  3.43it/s] 24%|██▍       | 141/585 [01:11<02:24,  3.07it/s] 24%|██▍       | 142/585 [01:11<02:19,  3.18it/s] 24%|██▍       | 143/585 [01:12<02:15,  3.26it/s] 25%|██▍       | 144/585 [01:12<02:12,  3.32it/s] 25%|██▍       | 145/585 [01:12<02:10,  3.36it/s] 25%|██▍       | 146/585 [01:12<02:09,  3.40it/s] 25%|██▌       | 147/585 [01:13<02:08,  3.42it/s] 25%|██▌       | 148/585 [01:13<02:07,  3.43it/s] 25%|██▌       | 149/585 [01:13<02:06,  3.45it/s] 26%|██▌       | 150/585 [01:14<02:06,  3.45it/s] 26%|██▌       | 151/585 [01:14<02:23,  3.03it/s] 26%|██▌       | 152/585 [01:14<02:17,  3.15it/s] 26%|██▌       | 153/585 [01:15<02:13,  3.24it/s] 26%|██▋       | 154/585 [01:15<02:10,  3.30it/s] 26%|██▋       | 155/585 [01:15<02:08,  3.35it/s] 27%|██▋       | 156/585 [01:15<02:06,  3.39it/s] 27%|██▋       | 157/585 [01:16<02:05,  3.41it/s] 27%|██▋       | 158/585 [01:16<02:04,  3.43it/s] 27%|██▋       | 159/585 [01:16<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:17<02:03,  3.45it/s] 28%|██▊       | 161/585 [01:17<02:16,  3.10it/s] 28%|██▊       | 162/585 [01:17<02:12,  3.20it/s] 28%|██▊       | 163/585 [01:18<02:08,  3.28it/s] 28%|██▊       | 164/585 [01:18<02:06,  3.33it/s] 28%|██▊       | 165/585 [01:18<02:04,  3.37it/s] 28%|██▊       | 166/585 [01:18<02:03,  3.40it/s] 29%|██▊       | 167/585 [01:19<02:29,  2.79it/s] 29%|██▊       | 168/585 [01:19<02:20,  2.96it/s] 29%|██▉       | 169/585 [01:19<02:14,  3.10it/s] 29%|██▉       | 170/585 [01:20<02:09,  3.20it/s] 29%|██▉       | 171/585 [01:20<02:06,  3.27it/s] 29%|██▉       | 172/585 [01:20<02:03,  3.33it/s] 30%|██▉       | 173/585 [01:21<02:02,  3.37it/s] 30%|██▉       | 174/585 [01:21<02:14,  3.06it/s] 30%|██▉       | 175/585 [01:21<02:09,  3.16it/s] 30%|███       | 176/585 [01:22<02:05,  3.25it/s] 30%|███       | 177/585 [01:22<02:14,  3.04it/s] 30%|███       | 178/585 [01:22<02:08,  3.16it/s] 31%|███       | 179/585 [01:23<02:05,  3.25it/s] 31%|███       | 180/585 [01:23<02:02,  3.31it/s] 31%|███       | 181/585 [01:23<02:00,  3.35it/s] 31%|███       | 182/585 [01:23<01:59,  3.39it/s] 31%|███▏      | 183/585 [01:25<03:55,  1.71it/s] 31%|███▏      | 184/585 [01:25<03:32,  1.89it/s] 32%|███▏      | 185/585 [01:25<03:02,  2.19it/s] 32%|███▏      | 186/585 [01:26<02:42,  2.46it/s] 32%|███▏      | 187/585 [01:26<02:27,  2.69it/s] 32%|███▏      | 188/585 [01:26<02:17,  2.89it/s] 32%|███▏      | 189/585 [01:27<02:10,  3.04it/s] 32%|███▏      | 190/585 [01:27<02:05,  3.16it/s] 33%|███▎      | 191/585 [01:27<02:01,  3.24it/s] 33%|███▎      | 192/585 [01:27<01:58,  3.31it/s] 33%|███▎      | 193/585 [01:28<01:56,  3.35it/s] 33%|███▎      | 194/585 [01:28<01:55,  3.39it/s] 33%|███▎      | 195/585 [01:28<02:06,  3.09it/s] 34%|███▎      | 196/585 [01:29<02:01,  3.19it/s] 34%|███▎      | 197/585 [01:29<01:58,  3.27it/s] 34%|███▍      | 198/585 [01:29<01:56,  3.33it/s] 34%|███▍      | 199/585 [01:30<01:54,  3.37it/s] 34%|███▍      | 200/585 [01:30<01:53,  3.40it/s] 34%|███▍      | 201/585 [01:30<01:52,  3.41it/s] 35%|███▍      | 202/585 [01:30<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:31<01:51,  3.44it/s] 35%|███▍      | 204/585 [01:31<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:31<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:32<02:01,  3.12it/s] 35%|███▌      | 207/585 [01:32<01:57,  3.22it/s] 36%|███▌      | 208/585 [01:32<01:54,  3.29it/s] 36%|███▌      | 209/585 [01:32<01:52,  3.34it/s] 36%|███▌      | 210/585 [01:33<01:51,  3.38it/s] 36%|███▌      | 211/585 [01:33<01:49,  3.40it/s] 36%|███▌      | 212/585 [01:33<01:48,  3.42it/s] 36%|███▋      | 213/585 [01:34<01:48,  3.44it/s] 37%|███▋      | 214/585 [01:34<01:47,  3.44it/s] 37%|███▋      | 215/585 [01:34<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:35<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:35<02:00,  3.04it/s] 37%|███▋      | 218/585 [01:35<01:56,  3.16it/s] 37%|███▋      | 219/585 [01:36<01:52,  3.24it/s] 38%|███▊      | 220/585 [01:36<01:50,  3.30it/s] 38%|███▊      | 221/585 [01:36<01:48,  3.35it/s] 38%|███▊      | 222/585 [01:36<01:47,  3.38it/s] 38%|███▊      | 223/585 [01:37<01:46,  3.41it/s] 38%|███▊      | 224/585 [01:37<01:45,  3.42it/s] 38%|███▊      | 225/585 [01:37<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:38<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:38<01:54,  3.12it/s] 39%|███▉      | 228/585 [01:38<01:51,  3.22it/s] 39%|███▉      | 229/585 [01:38<01:48,  3.29it/s] 39%|███▉      | 230/585 [01:39<01:46,  3.34it/s] 39%|███▉      | 231/585 [01:39<01:45,  3.37it/s] 40%|███▉      | 232/585 [01:39<01:43,  3.40it/s] 40%|███▉      | 233/585 [01:40<01:43,  3.42it/s] 40%|████      | 234/585 [01:40<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 14:26:34,423 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:26:34,423 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:26:34,423 >>   Batch size = 8
{'eval_loss': 0.9810701608657837, 'eval_runtime': 17.2006, 'eval_samples_per_second': 363.534, 'eval_steps_per_second': 45.464, 'epoch': 1.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.31it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.62it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.93it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.23it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.84it/s][A
  4%|▍         | 33/782 [00:00<00:22, 34.03it/s][A
  5%|▍         | 38/782 [00:00<00:19, 37.27it/s][A
  5%|▌         | 43/782 [00:01<00:18, 39.80it/s][A
  6%|▌         | 48/782 [00:01<00:17, 41.89it/s][A
  7%|▋         | 53/782 [00:01<00:16, 43.26it/s][A
  7%|▋         | 58/782 [00:01<00:16, 44.28it/s][A
  8%|▊         | 63/782 [00:01<00:15, 45.07it/s][A
  9%|▊         | 68/782 [00:01<00:15, 45.60it/s][A
  9%|▉         | 73/782 [00:01<00:15, 45.92it/s][A
 10%|▉         | 78/782 [00:01<00:15, 45.97it/s][A
 11%|█         | 83/782 [00:01<00:15, 46.28it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.41it/s][A
 12%|█▏        | 93/782 [00:02<00:14, 46.61it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.65it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.80it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.84it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.83it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.72it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.83it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.69it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.79it/s][A
 18%|█▊        | 138/782 [00:03<00:13, 46.87it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.80it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.81it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.83it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.80it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.89it/s][A
 21%|██▏       | 168/782 [00:03<00:17, 34.57it/s][A
 22%|██▏       | 173/782 [00:03<00:16, 37.40it/s][A
 23%|██▎       | 178/782 [00:04<00:15, 39.87it/s][A
 23%|██▎       | 183/782 [00:04<00:14, 41.73it/s][A
 24%|██▍       | 188/782 [00:04<00:13, 43.21it/s][A
 25%|██▍       | 193/782 [00:04<00:13, 44.28it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 44.96it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 45.53it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 45.81it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.08it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.31it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.44it/s][A
 29%|██▉       | 228/782 [00:05<00:11, 46.58it/s][A
 30%|██▉       | 233/782 [00:05<00:11, 46.77it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.73it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.74it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.80it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.83it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.79it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.86it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.73it/s][A
 35%|███▍      | 273/782 [00:06<00:10, 46.76it/s][A
 36%|███▌      | 278/782 [00:06<00:10, 46.77it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.83it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.83it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.80it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.77it/s][A
 39%|███▊      | 303/782 [00:06<00:13, 35.19it/s][A
 39%|███▉      | 308/782 [00:06<00:12, 38.09it/s][A
 40%|████      | 313/782 [00:07<00:11, 40.32it/s][A
 41%|████      | 318/782 [00:07<00:11, 42.12it/s][A
 41%|████▏     | 323/782 [00:07<00:10, 43.42it/s][A
 42%|████▏     | 328/782 [00:07<00:10, 44.43it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 45.13it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 45.66it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 45.80it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.14it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.39it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.37it/s][A
 46%|████▋     | 363/782 [00:08<00:08, 46.63it/s][A
 47%|████▋     | 368/782 [00:08<00:08, 46.76it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.72it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.78it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.80it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.71it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.82it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.81it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.80it/s][A
 52%|█████▏    | 408/782 [00:09<00:08, 46.68it/s][A
 53%|█████▎    | 413/782 [00:09<00:10, 34.41it/s][A
 53%|█████▎    | 418/782 [00:09<00:09, 37.43it/s][A
 54%|█████▍    | 423/782 [00:09<00:09, 39.85it/s][A
 55%|█████▍    | 428/782 [00:09<00:08, 41.67it/s][A
 55%|█████▌    | 433/782 [00:09<00:08, 43.20it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 44.15it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 44.91it/s][A
 57%|█████▋    | 448/782 [00:10<00:07, 45.51it/s][A
 58%|█████▊    | 453/782 [00:10<00:07, 45.78it/s][A
 59%|█████▊    | 458/782 [00:10<00:07, 46.08it/s][A
 59%|█████▉    | 463/782 [00:10<00:06, 46.35it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.50it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.52it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.65it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.70it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.82it/s][A
 63%|██████▎   | 493/782 [00:11<00:06, 46.77it/s][A
 64%|██████▎   | 498/782 [00:11<00:06, 46.75it/s][A
 64%|██████▍   | 503/782 [00:11<00:05, 46.66it/s][A
 65%|██████▍   | 508/782 [00:11<00:05, 46.72it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.67it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.77it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.84it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.85it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.82it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.78it/s][A
 69%|██████▉   | 543/782 [00:12<00:05, 46.68it/s][A
 70%|███████   | 548/782 [00:12<00:06, 38.57it/s][A
 71%|███████   | 553/782 [00:12<00:05, 40.81it/s][A
 71%|███████▏  | 558/782 [00:12<00:05, 42.47it/s][A
 72%|███████▏  | 563/782 [00:12<00:05, 43.65it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 44.54it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 45.29it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 45.70it/s][A
 75%|███████▍  | 583/782 [00:13<00:04, 46.12it/s][A
 75%|███████▌  | 588/782 [00:13<00:04, 46.06it/s][A
 76%|███████▌  | 593/782 [00:13<00:04, 46.22it/s][A
 76%|███████▋  | 598/782 [00:13<00:03, 46.38it/s][A
 77%|███████▋  | 603/782 [00:13<00:03, 46.54it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.60it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.70it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.80it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.82it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.89it/s][A
 81%|████████  | 633/782 [00:14<00:03, 46.71it/s][A
 82%|████████▏ | 638/782 [00:14<00:03, 46.62it/s][A
 82%|████████▏ | 643/782 [00:14<00:02, 46.67it/s][A
 83%|████████▎ | 648/782 [00:14<00:02, 46.68it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.65it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.73it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.72it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.84it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.86it/s][A
 87%|████████▋ | 678/782 [00:15<00:02, 46.77it/s][A
 87%|████████▋ | 683/782 [00:15<00:02, 46.83it/s][A
 88%|████████▊ | 688/782 [00:15<00:02, 43.13it/s][A
 89%|████████▊ | 693/782 [00:15<00:02, 44.20it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 44.91it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 45.39it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 45.72it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.11it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.28it/s][A
 92%|█████████▏| 723/782 [00:16<00:01, 46.39it/s][A
 93%|█████████▎| 728/782 [00:16<00:01, 46.43it/s][A
 94%|█████████▎| 733/782 [00:16<00:01, 46.45it/s][A
 94%|█████████▍| 738/782 [00:16<00:00, 46.53it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.55it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.69it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.72it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.83it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.86it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.78it/s][A
 99%|█████████▉| 773/782 [00:17<00:00, 46.74it/s][A
 99%|█████████▉| 778/782 [00:17<00:00, 46.69it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:17<00:00, 46.69it/s][A 40%|████      | 234/585 [01:57<01:42,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:26:51,927 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 14:26:52,157 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:26:56,630 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:26:56,796 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:26:56,895 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:13<58:54, 10.10s/it] 40%|████      | 236/585 [02:13<41:44,  7.18s/it] 41%|████      | 237/585 [02:14<29:37,  5.11s/it] 41%|████      | 238/585 [02:14<21:10,  3.66s/it] 41%|████      | 239/585 [02:14<15:16,  2.65s/it] 41%|████      | 240/585 [02:14<11:09,  1.94s/it] 41%|████      | 241/585 [02:15<08:17,  1.45s/it] 41%|████▏     | 242/585 [02:15<06:16,  1.10s/it] 42%|████▏     | 243/585 [02:15<04:52,  1.17it/s] 42%|████▏     | 244/585 [02:16<03:53,  1.46it/s] 42%|████▏     | 245/585 [02:16<03:12,  1.77it/s] 42%|████▏     | 246/585 [02:16<02:43,  2.07it/s] 42%|████▏     | 247/585 [02:16<02:25,  2.32it/s] 42%|████▏     | 248/585 [02:17<02:10,  2.57it/s] 43%|████▎     | 249/585 [02:17<02:00,  2.79it/s] 43%|████▎     | 250/585 [02:17<01:52,  2.97it/s] 43%|████▎     | 251/585 [02:18<01:47,  3.10it/s] 43%|████▎     | 252/585 [02:18<01:48,  3.08it/s] 43%|████▎     | 253/585 [02:18<01:44,  3.18it/s] 43%|████▎     | 254/585 [02:20<03:59,  1.38it/s] 44%|████▎     | 255/585 [02:20<03:15,  1.68it/s] 44%|████▍     | 256/585 [02:20<02:44,  2.00it/s] 44%|████▍     | 257/585 [02:21<02:23,  2.29it/s] 44%|████▍     | 258/585 [02:21<02:08,  2.55it/s] 44%|████▍     | 259/585 [02:21<01:57,  2.78it/s] 44%|████▍     | 260/585 [02:22<01:49,  2.96it/s] 45%|████▍     | 261/585 [02:22<01:44,  3.10it/s] 45%|████▍     | 262/585 [02:22<01:40,  3.20it/s] 45%|████▍     | 263/585 [02:23<01:38,  3.28it/s] 45%|████▌     | 264/585 [02:23<01:36,  3.34it/s] 45%|████▌     | 265/585 [02:23<01:39,  3.20it/s] 45%|████▌     | 266/585 [02:23<01:37,  3.28it/s] 46%|████▌     | 267/585 [02:24<01:35,  3.33it/s] 46%|████▌     | 268/585 [02:24<01:33,  3.37it/s] 46%|████▌     | 269/585 [02:24<01:32,  3.40it/s] 46%|████▌     | 270/585 [02:25<01:41,  3.12it/s] 46%|████▋     | 271/585 [02:25<01:37,  3.20it/s] 46%|████▋     | 272/585 [02:25<01:35,  3.28it/s] 47%|████▋     | 273/585 [02:26<01:33,  3.34it/s] 47%|████▋     | 274/585 [02:26<01:31,  3.38it/s] 47%|████▋     | 275/585 [02:26<01:34,  3.27it/s] 47%|████▋     | 276/585 [02:26<01:32,  3.33it/s] 47%|████▋     | 277/585 [02:27<01:31,  3.37it/s] 48%|████▊     | 278/585 [02:27<01:30,  3.40it/s] 48%|████▊     | 279/585 [02:27<01:29,  3.42it/s] 48%|████▊     | 280/585 [02:28<01:28,  3.44it/s] 48%|████▊     | 281/585 [02:28<01:28,  3.45it/s] 48%|████▊     | 282/585 [02:28<01:27,  3.46it/s] 48%|████▊     | 283/585 [02:28<01:27,  3.46it/s] 49%|████▊     | 284/585 [02:29<01:26,  3.47it/s] 49%|████▊     | 285/585 [02:29<01:26,  3.47it/s] 49%|████▉     | 286/585 [02:29<01:28,  3.38it/s] 49%|████▉     | 287/585 [02:30<01:27,  3.41it/s] 49%|████▉     | 288/585 [02:30<01:26,  3.42it/s] 49%|████▉     | 289/585 [02:30<01:26,  3.44it/s] 50%|████▉     | 290/585 [02:30<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:31<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:31<01:24,  3.46it/s] 50%|█████     | 293/585 [02:31<01:24,  3.46it/s] 50%|█████     | 294/585 [02:32<01:23,  3.47it/s] 50%|█████     | 295/585 [02:32<01:23,  3.47it/s] 51%|█████     | 296/585 [02:32<01:23,  3.47it/s] 51%|█████     | 297/585 [02:33<01:26,  3.34it/s] 51%|█████     | 298/585 [02:33<01:25,  3.38it/s] 51%|█████     | 299/585 [02:33<01:23,  3.41it/s] 51%|█████▏    | 300/585 [02:33<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:34<01:22,  3.44it/s] 52%|█████▏    | 302/585 [02:34<01:22,  3.44it/s] 52%|█████▏    | 303/585 [02:34<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:35<01:21,  3.46it/s] 52%|█████▏    | 305/585 [02:35<01:21,  3.46it/s] 52%|█████▏    | 306/585 [02:35<01:20,  3.46it/s] 52%|█████▏    | 307/585 [02:35<01:20,  3.46it/s] 53%|█████▎    | 308/585 [02:36<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:36<01:20,  3.41it/s] 53%|█████▎    | 310/585 [02:36<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:37<01:19,  3.44it/s] 53%|█████▎    | 312/585 [02:37<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:37<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:37<01:18,  3.46it/s] 54%|█████▍    | 315/585 [02:38<01:18,  3.46it/s] 54%|█████▍    | 316/585 [02:38<01:17,  3.46it/s] 54%|█████▍    | 317/585 [02:38<01:17,  3.46it/s] 54%|█████▍    | 318/585 [02:39<01:17,  3.46it/s] 55%|█████▍    | 319/585 [02:39<01:20,  3.32it/s] 55%|█████▍    | 320/585 [02:39<01:18,  3.36it/s] 55%|█████▍    | 321/585 [02:40<01:17,  3.39it/s] 55%|█████▌    | 322/585 [02:40<01:17,  3.41it/s] 55%|█████▌    | 323/585 [02:40<01:16,  3.42it/s] 55%|█████▌    | 324/585 [02:40<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:41<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:41<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:41<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:42<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:42<01:13,  3.46it/s] 56%|█████▋    | 330/585 [02:42<01:14,  3.41it/s] 57%|█████▋    | 331/585 [02:42<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:43<01:13,  3.44it/s] 57%|█████▋    | 333/585 [02:43<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:43<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:44<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:44<01:11,  3.46it/s] 58%|█████▊    | 337/585 [02:44<01:11,  3.46it/s] 58%|█████▊    | 338/585 [02:44<01:11,  3.46it/s] 58%|█████▊    | 339/585 [02:45<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:45<01:10,  3.47it/s] 58%|█████▊    | 341/585 [02:45<01:12,  3.38it/s] 58%|█████▊    | 342/585 [02:46<01:11,  3.40it/s] 59%|█████▊    | 343/585 [02:46<01:10,  3.42it/s] 59%|█████▉    | 344/585 [02:46<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:47<01:09,  3.44it/s] 59%|█████▉    | 346/585 [02:47<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:47<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:47<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:48<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:48<01:07,  3.46it/s] 60%|██████    | 351/585 [02:48<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 14:27:42,737 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:27:42,737 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:27:42,737 >>   Batch size = 8
{'eval_loss': 1.0027083158493042, 'eval_runtime': 17.3192, 'eval_samples_per_second': 361.045, 'eval_steps_per_second': 45.152, 'epoch': 2.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.20it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.57it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.69it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.05it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.66it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.41it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.16it/s][A
  5%|▌         | 43/782 [00:00<00:15, 47.04it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.98it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.95it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.90it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.81it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.84it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.76it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.77it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.83it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.72it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.79it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.84it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.87it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 45.70it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.10it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.26it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.51it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.50it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.58it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.58it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.73it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.78it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.86it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.76it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.78it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.87it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.77it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.87it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.82it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.69it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.77it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.79it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.82it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.78it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.78it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.87it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.78it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.81it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.74it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.75it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.82it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.72it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.84it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.71it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.87it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.85it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.89it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.60it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.88it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.79it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.85it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.87it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.80it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.78it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.89it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.79it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.69it/s][A
 42%|████▏     | 328/782 [00:06<00:09, 46.76it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.75it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.81it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.84it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.85it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.74it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.80it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.80it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.78it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.66it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.82it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.77it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.76it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.84it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.75it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.82it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.82it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.79it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.81it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.75it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.82it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.82it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.77it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.84it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.72it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.78it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.82it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.86it/s][A
 60%|█████▉    | 468/782 [00:09<00:06, 46.81it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.84it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.73it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.82it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.90it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.75it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.78it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.78it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.80it/s][A
 66%|██████▌   | 513/782 [00:10<00:05, 46.86it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.76it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.70it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.77it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.76it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.85it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.87it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.75it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.70it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.77it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.68it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.78it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.76it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.77it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.80it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.80it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.85it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.75it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.82it/s][A
 78%|███████▊  | 608/782 [00:12<00:03, 46.68it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.75it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.81it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.80it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.81it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.87it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.78it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.77it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.80it/s][A
 84%|████████▎ | 653/782 [00:13<00:02, 46.82it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.74it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.80it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.79it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.75it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.81it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.69it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.76it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.68it/s][A
 89%|████████▉ | 698/782 [00:14<00:01, 46.76it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.79it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.64it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.79it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.79it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.79it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.83it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.80it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.78it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.72it/s][A
 96%|█████████▌| 748/782 [00:15<00:00, 46.73it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.79it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.78it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.77it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.72it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.80it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.84it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.84it/s][A 60%|██████    | 351/585 [03:05<01:07,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:27:59,475 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 14:27:59,495 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:28:01,724 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:28:01,751 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:28:01,763 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:13<29:50,  7.68s/it] 60%|██████    | 353/585 [03:13<21:08,  5.47s/it] 61%|██████    | 354/585 [03:14<15:04,  3.92s/it] 61%|██████    | 355/585 [03:14<10:50,  2.83s/it] 61%|██████    | 356/585 [03:14<07:52,  2.07s/it] 61%|██████    | 357/585 [03:15<05:49,  1.53s/it] 61%|██████    | 358/585 [03:15<04:23,  1.16s/it] 61%|██████▏   | 359/585 [03:15<03:22,  1.11it/s] 62%|██████▏   | 360/585 [03:15<02:40,  1.40it/s] 62%|██████▏   | 361/585 [03:16<02:11,  1.70it/s] 62%|██████▏   | 362/585 [03:16<01:50,  2.01it/s] 62%|██████▏   | 363/585 [03:16<01:36,  2.30it/s] 62%|██████▏   | 364/585 [03:17<01:28,  2.50it/s] 62%|██████▏   | 365/585 [03:17<01:20,  2.73it/s] 63%|██████▎   | 366/585 [03:17<01:15,  2.92it/s] 63%|██████▎   | 367/585 [03:18<01:11,  3.06it/s] 63%|██████▎   | 368/585 [03:18<01:08,  3.17it/s] 63%|██████▎   | 369/585 [03:18<01:06,  3.26it/s] 63%|██████▎   | 370/585 [03:18<01:04,  3.32it/s] 63%|██████▎   | 371/585 [03:19<01:03,  3.36it/s] 64%|██████▎   | 372/585 [03:19<01:02,  3.39it/s] 64%|██████▍   | 373/585 [03:19<01:02,  3.41it/s] 64%|██████▍   | 374/585 [03:20<01:01,  3.43it/s] 64%|██████▍   | 375/585 [03:20<01:02,  3.36it/s] 64%|██████▍   | 376/585 [03:20<01:01,  3.39it/s] 64%|██████▍   | 377/585 [03:20<01:00,  3.41it/s] 65%|██████▍   | 378/585 [03:21<01:00,  3.43it/s] 65%|██████▍   | 379/585 [03:21<00:59,  3.44it/s] 65%|██████▍   | 380/585 [03:21<00:59,  3.44it/s] 65%|██████▌   | 381/585 [03:22<00:59,  3.45it/s] 65%|██████▌   | 382/585 [03:22<00:58,  3.46it/s] 65%|██████▌   | 383/585 [03:22<00:58,  3.46it/s] 66%|██████▌   | 384/585 [03:22<00:58,  3.46it/s] 66%|██████▌   | 385/585 [03:23<00:57,  3.46it/s] 66%|██████▌   | 386/585 [03:23<00:57,  3.46it/s] 66%|██████▌   | 387/585 [03:23<00:57,  3.46it/s] 66%|██████▋   | 388/585 [03:24<00:56,  3.46it/s] 66%|██████▋   | 389/585 [03:24<00:56,  3.46it/s] 67%|██████▋   | 390/585 [03:24<00:56,  3.46it/s] 67%|██████▋   | 391/585 [03:24<00:55,  3.46it/s] 67%|██████▋   | 392/585 [03:25<01:01,  3.12it/s] 67%|██████▋   | 393/585 [03:25<00:59,  3.21it/s] 67%|██████▋   | 394/585 [03:25<00:58,  3.28it/s] 68%|██████▊   | 395/585 [03:26<00:56,  3.33it/s] 68%|██████▊   | 396/585 [03:26<00:56,  3.37it/s] 68%|██████▊   | 397/585 [03:26<00:55,  3.40it/s] 68%|██████▊   | 398/585 [03:27<00:54,  3.42it/s] 68%|██████▊   | 399/585 [03:27<00:54,  3.43it/s] 68%|██████▊   | 400/585 [03:27<00:53,  3.44it/s] 69%|██████▊   | 401/585 [03:27<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:28<00:53,  3.45it/s] 69%|██████▉   | 403/585 [03:28<00:55,  3.30it/s] 69%|██████▉   | 404/585 [03:28<00:54,  3.34it/s] 69%|██████▉   | 405/585 [03:29<00:53,  3.38it/s] 69%|██████▉   | 406/585 [03:29<00:52,  3.40it/s] 70%|██████▉   | 407/585 [03:29<00:51,  3.42it/s] 70%|██████▉   | 408/585 [03:30<00:51,  3.44it/s] 70%|██████▉   | 409/585 [03:30<00:51,  3.45it/s] 70%|███████   | 410/585 [03:30<00:50,  3.45it/s] 70%|███████   | 411/585 [03:30<00:50,  3.45it/s] 70%|███████   | 412/585 [03:31<00:49,  3.46it/s] 71%|███████   | 413/585 [03:31<00:49,  3.46it/s] 71%|███████   | 414/585 [03:31<00:51,  3.34it/s] 71%|███████   | 415/585 [03:32<00:50,  3.37it/s] 71%|███████   | 416/585 [03:32<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:32<00:49,  3.42it/s] 71%|███████▏  | 418/585 [03:32<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:33<00:48,  3.44it/s] 72%|███████▏  | 420/585 [03:33<00:47,  3.45it/s] 72%|███████▏  | 421/585 [03:33<00:47,  3.45it/s] 72%|███████▏  | 422/585 [03:34<00:47,  3.46it/s] 72%|███████▏  | 423/585 [03:34<00:46,  3.46it/s] 72%|███████▏  | 424/585 [03:34<00:46,  3.46it/s] 73%|███████▎  | 425/585 [03:35<00:48,  3.31it/s] 73%|███████▎  | 426/585 [03:35<00:47,  3.35it/s] 73%|███████▎  | 427/585 [03:35<00:46,  3.39it/s] 73%|███████▎  | 428/585 [03:35<00:46,  3.41it/s] 73%|███████▎  | 429/585 [03:36<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:36<00:45,  3.44it/s] 74%|███████▎  | 431/585 [03:36<00:44,  3.45it/s] 74%|███████▍  | 432/585 [03:37<00:44,  3.45it/s] 74%|███████▍  | 433/585 [03:37<00:43,  3.46it/s] 74%|███████▍  | 434/585 [03:37<00:43,  3.46it/s] 74%|███████▍  | 435/585 [03:37<00:43,  3.46it/s] 75%|███████▍  | 436/585 [03:38<00:46,  3.18it/s] 75%|███████▍  | 437/585 [03:38<00:45,  3.26it/s] 75%|███████▍  | 438/585 [03:38<00:44,  3.32it/s] 75%|███████▌  | 439/585 [03:39<00:43,  3.36it/s] 75%|███████▌  | 440/585 [03:39<00:42,  3.39it/s] 75%|███████▌  | 441/585 [03:39<00:42,  3.41it/s] 76%|███████▌  | 442/585 [03:40<00:41,  3.43it/s] 76%|███████▌  | 443/585 [03:40<00:41,  3.44it/s] 76%|███████▌  | 444/585 [03:40<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:40<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:41<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:42<01:06,  2.08it/s] 77%|███████▋  | 448/585 [03:42<00:57,  2.37it/s] 77%|███████▋  | 449/585 [03:42<00:52,  2.61it/s] 77%|███████▋  | 450/585 [03:42<00:47,  2.82it/s] 77%|███████▋  | 451/585 [03:43<00:44,  2.99it/s] 77%|███████▋  | 452/585 [03:43<00:42,  3.11it/s] 77%|███████▋  | 453/585 [03:43<00:41,  3.21it/s] 78%|███████▊  | 454/585 [03:44<00:39,  3.29it/s] 78%|███████▊  | 455/585 [03:44<00:38,  3.34it/s] 78%|███████▊  | 456/585 [03:44<00:40,  3.16it/s] 78%|███████▊  | 457/585 [03:45<00:39,  3.25it/s] 78%|███████▊  | 458/585 [03:45<00:38,  3.31it/s] 78%|███████▊  | 459/585 [03:45<00:37,  3.36it/s] 79%|███████▊  | 460/585 [03:45<00:36,  3.39it/s] 79%|███████▉  | 461/585 [03:46<00:36,  3.41it/s] 79%|███████▉  | 462/585 [03:46<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:46<00:35,  3.44it/s] 79%|███████▉  | 464/585 [03:47<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:47<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:47<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:48<00:36,  3.25it/s] 80%|████████  | 468/585 [03:48<00:35,  3.31it/s][INFO|trainer.py:2140] 2023-08-29 14:28:42,279 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:28:42,279 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:28:42,279 >>   Batch size = 8
{'eval_loss': 1.0159302949905396, 'eval_runtime': 16.7275, 'eval_samples_per_second': 373.815, 'eval_steps_per_second': 46.749, 'epoch': 3.0}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.46it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.79it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.94it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.19it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.79it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.39it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.25it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.89it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.87it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.89it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.83it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.88it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.87it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.92it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.81it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.85it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.75it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.76it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.80it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.76it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.82it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.92it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 45.83it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.01it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.33it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.37it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.58it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.63it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.73it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.82it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.77it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.78it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.75it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.65it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.83it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.78it/s][A
 24%|██▍       | 188/782 [00:04<00:12, 46.78it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.70it/s][A
 25%|██▌       | 198/782 [00:04<00:13, 42.87it/s][A
 26%|██▌       | 203/782 [00:04<00:13, 44.07it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 44.93it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 45.51it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 45.84it/s][A
 29%|██▊       | 223/782 [00:04<00:12, 46.13it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.39it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.62it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.42it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.55it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.56it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.63it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.74it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.84it/s][A
 34%|███▍      | 268/782 [00:05<00:11, 46.72it/s][A
 35%|███▍      | 273/782 [00:05<00:10, 46.84it/s][A
 36%|███▌      | 278/782 [00:05<00:10, 46.83it/s][A
 36%|███▌      | 283/782 [00:06<00:10, 46.87it/s][A
 37%|███▋      | 288/782 [00:06<00:10, 46.65it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 46.55it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 46.70it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.79it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.87it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.82it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.78it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.70it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.82it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.79it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 44.96it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 45.52it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 45.98it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.27it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.46it/s][A
 46%|████▋     | 363/782 [00:07<00:09, 46.50it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.59it/s][A
 48%|████▊     | 373/782 [00:07<00:08, 46.74it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.68it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.61it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.59it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.74it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.76it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.89it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.83it/s][A
 53%|█████▎    | 413/782 [00:08<00:07, 46.92it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 46.79it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 46.71it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.69it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.77it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.78it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.69it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.72it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.73it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.86it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.82it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.88it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.57it/s][A
 61%|██████    | 478/782 [00:10<00:06, 43.49it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 44.41it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 45.22it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 45.69it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.11it/s][A
 64%|██████▍   | 503/782 [00:10<00:06, 46.15it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.44it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.47it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.44it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.43it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.55it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.74it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.77it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.78it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.78it/s][A
 71%|███████   | 553/782 [00:11<00:04, 46.88it/s][A
 71%|███████▏  | 558/782 [00:11<00:04, 46.86it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 46.78it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 46.56it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 46.60it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.63it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.72it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.83it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.82it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.81it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.84it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.41it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.50it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 43.15it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 44.05it/s][A
 80%|████████  | 628/782 [00:13<00:03, 44.93it/s][A
 81%|████████  | 633/782 [00:13<00:03, 45.41it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 45.98it/s][A
 82%|████████▏ | 643/782 [00:13<00:03, 46.12it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.39it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.57it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.45it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.47it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.55it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.59it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.73it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.83it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.64it/s][A
 89%|████████▊ | 693/782 [00:14<00:01, 46.83it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 46.80it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 46.69it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 46.66it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 46.73it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 46.62it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.76it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.82it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.85it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.85it/s][A
 95%|█████████▌| 743/782 [00:15<00:00, 46.45it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.42it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.46it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 43.28it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 44.28it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 44.92it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 45.51it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 45.96it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 45.96it/s][A 80%|████████  | 468/585 [04:05<00:35,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:28:59,233 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 14:28:59,407 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:29:03,718 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:29:03,946 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:29:04,032 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:20<18:59,  9.82s/it] 80%|████████  | 470/585 [04:20<13:22,  6.98s/it] 81%|████████  | 471/585 [04:20<09:26,  4.97s/it] 81%|████████  | 472/585 [04:21<06:42,  3.57s/it] 81%|████████  | 473/585 [04:21<04:49,  2.58s/it] 81%|████████  | 474/585 [04:21<03:30,  1.89s/it] 81%|████████  | 475/585 [04:22<02:35,  1.41s/it] 81%|████████▏ | 476/585 [04:22<01:57,  1.07s/it] 82%|████████▏ | 477/585 [04:22<01:30,  1.19it/s] 82%|████████▏ | 478/585 [04:22<01:12,  1.49it/s] 82%|████████▏ | 479/585 [04:23<00:59,  1.79it/s] 82%|████████▏ | 480/585 [04:23<00:50,  2.10it/s] 82%|████████▏ | 481/585 [04:23<00:43,  2.38it/s] 82%|████████▏ | 482/585 [04:24<00:39,  2.63it/s] 83%|████████▎ | 483/585 [04:24<00:35,  2.84it/s] 83%|████████▎ | 484/585 [04:24<00:33,  3.00it/s] 83%|████████▎ | 485/585 [04:24<00:31,  3.13it/s] 83%|████████▎ | 486/585 [04:25<00:30,  3.23it/s] 83%|████████▎ | 487/585 [04:25<00:32,  2.99it/s] 83%|████████▎ | 488/585 [04:25<00:31,  3.11it/s] 84%|████████▎ | 489/585 [04:26<00:29,  3.21it/s] 84%|████████▍ | 490/585 [04:26<00:30,  3.13it/s] 84%|████████▍ | 491/585 [04:26<00:29,  3.23it/s] 84%|████████▍ | 492/585 [04:27<00:28,  3.30it/s] 84%|████████▍ | 493/585 [04:27<00:27,  3.35it/s] 84%|████████▍ | 494/585 [04:27<00:26,  3.38it/s] 85%|████████▍ | 495/585 [04:28<00:26,  3.41it/s] 85%|████████▍ | 496/585 [04:28<00:25,  3.43it/s] 85%|████████▍ | 497/585 [04:28<00:25,  3.44it/s] 85%|████████▌ | 498/585 [04:28<00:25,  3.45it/s] 85%|████████▌ | 499/585 [04:29<00:24,  3.46it/s] 85%|████████▌ | 500/585 [04:29<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [04:29<00:24,  3.46it/s] 86%|████████▌ | 501/585 [04:29<00:25,  3.33it/s] 86%|████████▌ | 502/585 [04:30<00:24,  3.37it/s] 86%|████████▌ | 503/585 [04:30<00:24,  3.40it/s] 86%|████████▌ | 504/585 [04:30<00:23,  3.42it/s] 86%|████████▋ | 505/585 [04:30<00:23,  3.43it/s] 86%|████████▋ | 506/585 [04:31<00:22,  3.44it/s] 87%|████████▋ | 507/585 [04:31<00:22,  3.45it/s] 87%|████████▋ | 508/585 [04:31<00:22,  3.45it/s] 87%|████████▋ | 509/585 [04:32<00:21,  3.46it/s] 87%|████████▋ | 510/585 [04:32<00:21,  3.46it/s] 87%|████████▋ | 511/585 [04:32<00:21,  3.47it/s] 88%|████████▊ | 512/585 [04:32<00:21,  3.37it/s] 88%|████████▊ | 513/585 [04:33<00:21,  3.40it/s] 88%|████████▊ | 514/585 [04:33<00:20,  3.42it/s] 88%|████████▊ | 515/585 [04:33<00:20,  3.44it/s] 88%|████████▊ | 516/585 [04:34<00:20,  3.45it/s] 88%|████████▊ | 517/585 [04:34<00:19,  3.45it/s] 89%|████████▊ | 518/585 [04:34<00:19,  3.45it/s] 89%|████████▊ | 519/585 [04:35<00:19,  3.46it/s] 89%|████████▉ | 520/585 [04:35<00:18,  3.46it/s] 89%|████████▉ | 521/585 [04:35<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:35<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:36<00:18,  3.32it/s] 90%|████████▉ | 524/585 [04:36<00:18,  3.36it/s] 90%|████████▉ | 525/585 [04:36<00:17,  3.39it/s] 90%|████████▉ | 526/585 [04:37<00:17,  3.42it/s] 90%|█████████ | 527/585 [04:37<00:16,  3.43it/s] 90%|█████████ | 528/585 [04:37<00:16,  3.44it/s] 90%|█████████ | 529/585 [04:37<00:16,  3.45it/s] 91%|█████████ | 530/585 [04:38<00:15,  3.45it/s] 91%|█████████ | 531/585 [04:38<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:38<00:15,  3.46it/s] 91%|█████████ | 533/585 [04:39<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:39<00:15,  3.21it/s] 91%|█████████▏| 535/585 [04:39<00:15,  3.28it/s] 92%|█████████▏| 536/585 [04:40<00:14,  3.33it/s] 92%|█████████▏| 537/585 [04:40<00:14,  3.37it/s] 92%|█████████▏| 538/585 [04:40<00:13,  3.40it/s] 92%|█████████▏| 539/585 [04:40<00:13,  3.42it/s] 92%|█████████▏| 540/585 [04:41<00:13,  3.43it/s] 92%|█████████▏| 541/585 [04:41<00:12,  3.44it/s] 93%|█████████▎| 542/585 [04:41<00:12,  3.45it/s] 93%|█████████▎| 543/585 [04:42<00:12,  3.45it/s] 93%|█████████▎| 544/585 [04:42<00:11,  3.45it/s] 93%|█████████▎| 545/585 [04:42<00:12,  3.22it/s] 93%|█████████▎| 546/585 [04:42<00:11,  3.29it/s] 94%|█████████▎| 547/585 [04:43<00:11,  3.34it/s] 94%|█████████▎| 548/585 [04:43<00:10,  3.38it/s] 94%|█████████▍| 549/585 [04:43<00:10,  3.40it/s] 94%|█████████▍| 550/585 [04:44<00:10,  3.42it/s] 94%|█████████▍| 551/585 [04:44<00:09,  3.44it/s] 94%|█████████▍| 552/585 [04:44<00:09,  3.44it/s] 95%|█████████▍| 553/585 [04:45<00:09,  3.45it/s] 95%|█████████▍| 554/585 [04:45<00:08,  3.46it/s] 95%|█████████▍| 555/585 [04:45<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:45<00:09,  3.15it/s] 95%|█████████▌| 557/585 [04:46<00:08,  3.23it/s] 95%|█████████▌| 558/585 [04:46<00:08,  3.30it/s] 96%|█████████▌| 559/585 [04:46<00:07,  3.35it/s] 96%|█████████▌| 560/585 [04:47<00:07,  3.38it/s] 96%|█████████▌| 561/585 [04:47<00:07,  3.41it/s] 96%|█████████▌| 562/585 [04:47<00:06,  3.42it/s] 96%|█████████▌| 563/585 [04:47<00:06,  3.44it/s] 96%|█████████▋| 564/585 [04:48<00:06,  3.44it/s] 97%|█████████▋| 565/585 [04:48<00:05,  3.45it/s] 97%|█████████▋| 566/585 [04:48<00:05,  3.46it/s] 97%|█████████▋| 567/585 [04:49<00:05,  3.41it/s] 97%|█████████▋| 568/585 [04:49<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:49<00:04,  3.44it/s] 97%|█████████▋| 570/585 [04:50<00:04,  3.45it/s] 98%|█████████▊| 571/585 [04:50<00:04,  3.45it/s] 98%|█████████▊| 572/585 [04:50<00:03,  3.46it/s] 98%|█████████▊| 573/585 [04:50<00:03,  3.46it/s] 98%|█████████▊| 574/585 [04:51<00:03,  3.46it/s] 98%|█████████▊| 575/585 [04:51<00:02,  3.46it/s] 98%|█████████▊| 576/585 [04:51<00:02,  3.46it/s] 99%|█████████▊| 577/585 [04:52<00:02,  3.46it/s] 99%|█████████▉| 578/585 [04:52<00:02,  3.42it/s] 99%|█████████▉| 579/585 [04:52<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:52<00:01,  3.44it/s] 99%|█████████▉| 581/585 [04:53<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:53<00:00,  3.46it/s]100%|█████████▉| 583/585 [04:53<00:00,  3.46it/s]100%|█████████▉| 584/585 [04:54<00:00,  3.46it/s]100%|██████████| 585/585 [04:54<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 14:29:48,298 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:29:48,299 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:29:48,299 >>   Batch size = 8
{'eval_loss': 1.0234252214431763, 'eval_runtime': 16.8631, 'eval_samples_per_second': 370.81, 'eval_steps_per_second': 46.373, 'epoch': 4.0}
{'loss': 0.3867, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/782 [00:00<?, ?it/s][A
  1%|          | 6/782 [00:00<00:13, 57.44it/s][A
  2%|▏         | 12/782 [00:00<00:15, 50.71it/s][A
  2%|▏         | 18/782 [00:00<00:15, 48.83it/s][A
  3%|▎         | 23/782 [00:00<00:15, 48.07it/s][A
  4%|▎         | 28/782 [00:00<00:15, 47.66it/s][A
  4%|▍         | 33/782 [00:00<00:15, 47.34it/s][A
  5%|▍         | 38/782 [00:00<00:15, 47.20it/s][A
  5%|▌         | 43/782 [00:00<00:15, 46.91it/s][A
  6%|▌         | 48/782 [00:01<00:15, 46.90it/s][A
  7%|▋         | 53/782 [00:01<00:15, 46.89it/s][A
  7%|▋         | 58/782 [00:01<00:15, 46.91it/s][A
  8%|▊         | 63/782 [00:01<00:15, 46.88it/s][A
  9%|▊         | 68/782 [00:01<00:15, 46.87it/s][A
  9%|▉         | 73/782 [00:01<00:15, 46.83it/s][A
 10%|▉         | 78/782 [00:01<00:15, 46.75it/s][A
 11%|█         | 83/782 [00:01<00:14, 46.85it/s][A
 11%|█▏        | 88/782 [00:01<00:14, 46.77it/s][A
 12%|█▏        | 93/782 [00:01<00:14, 46.77it/s][A
 13%|█▎        | 98/782 [00:02<00:14, 46.78it/s][A
 13%|█▎        | 103/782 [00:02<00:14, 46.72it/s][A
 14%|█▍        | 108/782 [00:02<00:14, 46.79it/s][A
 14%|█▍        | 113/782 [00:02<00:14, 46.84it/s][A
 15%|█▌        | 118/782 [00:02<00:14, 46.80it/s][A
 16%|█▌        | 123/782 [00:02<00:14, 46.76it/s][A
 16%|█▋        | 128/782 [00:02<00:14, 46.65it/s][A
 17%|█▋        | 133/782 [00:02<00:13, 46.76it/s][A
 18%|█▊        | 138/782 [00:02<00:13, 46.86it/s][A
 18%|█▊        | 143/782 [00:03<00:13, 46.74it/s][A
 19%|█▉        | 148/782 [00:03<00:13, 46.80it/s][A
 20%|█▉        | 153/782 [00:03<00:13, 46.79it/s][A
 20%|██        | 158/782 [00:03<00:13, 46.64it/s][A
 21%|██        | 163/782 [00:03<00:13, 46.72it/s][A
 21%|██▏       | 168/782 [00:03<00:13, 46.81it/s][A
 22%|██▏       | 173/782 [00:03<00:13, 46.81it/s][A
 23%|██▎       | 178/782 [00:03<00:12, 46.77it/s][A
 23%|██▎       | 183/782 [00:03<00:12, 46.70it/s][A
 24%|██▍       | 188/782 [00:03<00:12, 46.82it/s][A
 25%|██▍       | 193/782 [00:04<00:12, 46.77it/s][A
 25%|██▌       | 198/782 [00:04<00:12, 46.76it/s][A
 26%|██▌       | 203/782 [00:04<00:12, 46.72it/s][A
 27%|██▋       | 208/782 [00:04<00:12, 46.78it/s][A
 27%|██▋       | 213/782 [00:04<00:12, 46.84it/s][A
 28%|██▊       | 218/782 [00:04<00:12, 46.86it/s][A
 29%|██▊       | 223/782 [00:04<00:11, 46.78it/s][A
 29%|██▉       | 228/782 [00:04<00:11, 46.62it/s][A
 30%|██▉       | 233/782 [00:04<00:11, 46.75it/s][A
 30%|███       | 238/782 [00:05<00:11, 46.77it/s][A
 31%|███       | 243/782 [00:05<00:11, 46.82it/s][A
 32%|███▏      | 248/782 [00:05<00:11, 46.70it/s][A
 32%|███▏      | 253/782 [00:05<00:11, 46.77it/s][A
 33%|███▎      | 258/782 [00:05<00:11, 46.74it/s][A
 34%|███▎      | 263/782 [00:05<00:11, 46.66it/s][A
 34%|███▍      | 268/782 [00:05<00:10, 46.75it/s][A
 35%|███▍      | 273/782 [00:05<00:12, 40.97it/s][A
 36%|███▌      | 278/782 [00:05<00:11, 42.54it/s][A
 36%|███▌      | 283/782 [00:06<00:11, 43.81it/s][A
 37%|███▋      | 288/782 [00:06<00:11, 44.66it/s][A
 37%|███▋      | 293/782 [00:06<00:10, 45.26it/s][A
 38%|███▊      | 298/782 [00:06<00:10, 45.60it/s][A
 39%|███▊      | 303/782 [00:06<00:10, 46.00it/s][A
 39%|███▉      | 308/782 [00:06<00:10, 46.27it/s][A
 40%|████      | 313/782 [00:06<00:10, 46.43it/s][A
 41%|████      | 318/782 [00:06<00:09, 46.41it/s][A
 41%|████▏     | 323/782 [00:06<00:09, 46.48it/s][A
 42%|████▏     | 328/782 [00:07<00:09, 46.65it/s][A
 43%|████▎     | 333/782 [00:07<00:09, 46.63it/s][A
 43%|████▎     | 338/782 [00:07<00:09, 46.79it/s][A
 44%|████▍     | 343/782 [00:07<00:09, 46.72it/s][A
 45%|████▍     | 348/782 [00:07<00:09, 46.83it/s][A
 45%|████▌     | 353/782 [00:07<00:09, 46.77it/s][A
 46%|████▌     | 358/782 [00:07<00:09, 46.70it/s][A
 46%|████▋     | 363/782 [00:07<00:08, 46.79it/s][A
 47%|████▋     | 368/782 [00:07<00:08, 46.74it/s][A
 48%|████▊     | 373/782 [00:08<00:08, 46.72it/s][A
 48%|████▊     | 378/782 [00:08<00:08, 46.79it/s][A
 49%|████▉     | 383/782 [00:08<00:08, 46.77it/s][A
 50%|████▉     | 388/782 [00:08<00:08, 46.82it/s][A
 50%|█████     | 393/782 [00:08<00:08, 46.86it/s][A
 51%|█████     | 398/782 [00:08<00:08, 46.74it/s][A
 52%|█████▏    | 403/782 [00:08<00:08, 46.85it/s][A
 52%|█████▏    | 408/782 [00:08<00:07, 46.84it/s][A
 53%|█████▎    | 413/782 [00:08<00:08, 45.19it/s][A
 53%|█████▎    | 418/782 [00:08<00:07, 45.61it/s][A
 54%|█████▍    | 423/782 [00:09<00:07, 45.96it/s][A
 55%|█████▍    | 428/782 [00:09<00:07, 46.20it/s][A
 55%|█████▌    | 433/782 [00:09<00:07, 46.46it/s][A
 56%|█████▌    | 438/782 [00:09<00:07, 46.56it/s][A
 57%|█████▋    | 443/782 [00:09<00:07, 46.58it/s][A
 57%|█████▋    | 448/782 [00:09<00:07, 46.58it/s][A
 58%|█████▊    | 453/782 [00:09<00:07, 46.66it/s][A
 59%|█████▊    | 458/782 [00:09<00:06, 46.68it/s][A
 59%|█████▉    | 463/782 [00:09<00:06, 46.76it/s][A
 60%|█████▉    | 468/782 [00:10<00:06, 46.78it/s][A
 60%|██████    | 473/782 [00:10<00:06, 46.61it/s][A
 61%|██████    | 478/782 [00:10<00:06, 46.68it/s][A
 62%|██████▏   | 483/782 [00:10<00:06, 46.75it/s][A
 62%|██████▏   | 488/782 [00:10<00:06, 46.79it/s][A
 63%|██████▎   | 493/782 [00:10<00:06, 46.71it/s][A
 64%|██████▎   | 498/782 [00:10<00:06, 46.75it/s][A
 64%|██████▍   | 503/782 [00:10<00:05, 46.74it/s][A
 65%|██████▍   | 508/782 [00:10<00:05, 46.84it/s][A
 66%|██████▌   | 513/782 [00:11<00:05, 46.68it/s][A
 66%|██████▌   | 518/782 [00:11<00:05, 46.67it/s][A
 67%|██████▋   | 523/782 [00:11<00:05, 46.81it/s][A
 68%|██████▊   | 528/782 [00:11<00:05, 46.73it/s][A
 68%|██████▊   | 533/782 [00:11<00:05, 46.78it/s][A
 69%|██████▉   | 538/782 [00:11<00:05, 46.82it/s][A
 69%|██████▉   | 543/782 [00:11<00:05, 46.67it/s][A
 70%|███████   | 548/782 [00:11<00:05, 46.75it/s][A
 71%|███████   | 553/782 [00:11<00:05, 42.89it/s][A
 71%|███████▏  | 558/782 [00:12<00:05, 43.94it/s][A
 72%|███████▏  | 563/782 [00:12<00:04, 44.69it/s][A
 73%|███████▎  | 568/782 [00:12<00:04, 45.33it/s][A
 73%|███████▎  | 573/782 [00:12<00:04, 45.75it/s][A
 74%|███████▍  | 578/782 [00:12<00:04, 46.09it/s][A
 75%|███████▍  | 583/782 [00:12<00:04, 46.29it/s][A
 75%|███████▌  | 588/782 [00:12<00:04, 46.37it/s][A
 76%|███████▌  | 593/782 [00:12<00:04, 46.59it/s][A
 76%|███████▋  | 598/782 [00:12<00:03, 46.53it/s][A
 77%|███████▋  | 603/782 [00:12<00:03, 46.68it/s][A
 78%|███████▊  | 608/782 [00:13<00:03, 46.62it/s][A
 78%|███████▊  | 613/782 [00:13<00:03, 46.71it/s][A
 79%|███████▉  | 618/782 [00:13<00:03, 46.69it/s][A
 80%|███████▉  | 623/782 [00:13<00:03, 46.79it/s][A
 80%|████████  | 628/782 [00:13<00:03, 46.83it/s][A
 81%|████████  | 633/782 [00:13<00:03, 46.72it/s][A
 82%|████████▏ | 638/782 [00:13<00:03, 46.71it/s][A
 82%|████████▏ | 643/782 [00:13<00:02, 46.80it/s][A
 83%|████████▎ | 648/782 [00:13<00:02, 46.82it/s][A
 84%|████████▎ | 653/782 [00:14<00:02, 46.84it/s][A
 84%|████████▍ | 658/782 [00:14<00:02, 46.75it/s][A
 85%|████████▍ | 663/782 [00:14<00:02, 46.62it/s][A
 85%|████████▌ | 668/782 [00:14<00:02, 46.73it/s][A
 86%|████████▌ | 673/782 [00:14<00:02, 46.72it/s][A
 87%|████████▋ | 678/782 [00:14<00:02, 46.73it/s][A
 87%|████████▋ | 683/782 [00:14<00:02, 46.73it/s][A
 88%|████████▊ | 688/782 [00:14<00:02, 46.75it/s][A
 89%|████████▊ | 693/782 [00:14<00:02, 42.04it/s][A
 89%|████████▉ | 698/782 [00:15<00:01, 43.42it/s][A
 90%|████████▉ | 703/782 [00:15<00:01, 44.35it/s][A
 91%|█████████ | 708/782 [00:15<00:01, 44.99it/s][A
 91%|█████████ | 713/782 [00:15<00:01, 45.63it/s][A
 92%|█████████▏| 718/782 [00:15<00:01, 45.91it/s][A
 92%|█████████▏| 723/782 [00:15<00:01, 46.14it/s][A
 93%|█████████▎| 728/782 [00:15<00:01, 46.31it/s][A
 94%|█████████▎| 733/782 [00:15<00:01, 46.34it/s][A
 94%|█████████▍| 738/782 [00:15<00:00, 46.42it/s][A
 95%|█████████▌| 743/782 [00:16<00:00, 46.40it/s][A
 96%|█████████▌| 748/782 [00:16<00:00, 46.57it/s][A
 96%|█████████▋| 753/782 [00:16<00:00, 46.71it/s][A
 97%|█████████▋| 758/782 [00:16<00:00, 46.67it/s][A
 98%|█████████▊| 763/782 [00:16<00:00, 46.70it/s][A
 98%|█████████▊| 768/782 [00:16<00:00, 46.72it/s][A
 99%|█████████▉| 773/782 [00:16<00:00, 46.74it/s][A
 99%|█████████▉| 778/782 [00:16<00:00, 46.78it/s][A
                                                 [A                                                 
100%|██████████| 782/782 [00:16<00:00, 46.78it/s][A100%|██████████| 585/585 [05:11<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:30:05,279 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 14:30:05,540 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:30:09,679 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:30:09,919 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:30:10,028 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 14:30:19,315 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 14:30:19,316 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117 (score: 0.9810701608657837).
                                                 100%|██████████| 585/585 [05:38<00:00,  3.46it/s]100%|██████████| 585/585 [05:38<00:00,  1.73it/s]
[INFO|trainer.py:1894] 2023-08-29 14:30:32,239 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 14:30:32,428 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:30:36,711 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:30:37,028 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:30:37,211 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:30:38,159 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   train_loss               =      0.384
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   train_runtime            = 0:05:38.15
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   train_samples_per_second =    110.895
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:38,159 >>   train_steps_per_second   =       1.73
{'eval_loss': 1.031203031539917, 'eval_runtime': 16.8533, 'eval_samples_per_second': 371.024, 'eval_steps_per_second': 46.4, 'epoch': 5.0}
{'train_runtime': 338.1582, 'train_samples_per_second': 110.895, 'train_steps_per_second': 1.73, 'train_loss': 0.38403573647523537, 'epoch': 5.0}
08/29/2023 14:30:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 14:30:38,685 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:30:38,685 >>   Num examples = 6253
[INFO|trainer.py:2145] 2023-08-29 14:30:38,685 >>   Batch size = 8
  0%|          | 0/782 [00:00<?, ?it/s]  1%|          | 6/782 [00:00<00:13, 59.18it/s]  2%|▏         | 12/782 [00:00<00:14, 51.92it/s]  2%|▏         | 18/782 [00:00<00:15, 49.65it/s]  3%|▎         | 24/782 [00:00<00:15, 48.64it/s]  4%|▎         | 29/782 [00:00<00:15, 48.31it/s]  4%|▍         | 34/782 [00:00<00:15, 48.15it/s]  5%|▍         | 39/782 [00:00<00:15, 47.88it/s]  6%|▌         | 44/782 [00:00<00:15, 47.65it/s]  6%|▋         | 49/782 [00:01<00:15, 47.42it/s]  7%|▋         | 54/782 [00:01<00:17, 41.00it/s]  8%|▊         | 59/782 [00:01<00:16, 42.78it/s]  8%|▊         | 64/782 [00:01<00:16, 44.05it/s]  9%|▉         | 69/782 [00:01<00:15, 44.94it/s]  9%|▉         | 74/782 [00:01<00:15, 45.57it/s] 10%|█         | 79/782 [00:01<00:15, 46.05it/s] 11%|█         | 84/782 [00:01<00:15, 46.40it/s] 11%|█▏        | 89/782 [00:01<00:14, 46.81it/s] 12%|█▏        | 94/782 [00:02<00:14, 46.91it/s] 13%|█▎        | 99/782 [00:02<00:14, 46.97it/s] 13%|█▎        | 104/782 [00:02<00:14, 47.07it/s] 14%|█▍        | 109/782 [00:02<00:14, 47.22it/s] 15%|█▍        | 114/782 [00:02<00:14, 47.18it/s] 15%|█▌        | 119/782 [00:02<00:14, 47.03it/s] 16%|█▌        | 124/782 [00:02<00:13, 47.10it/s] 16%|█▋        | 129/782 [00:02<00:13, 47.06it/s] 17%|█▋        | 134/782 [00:02<00:13, 47.18it/s] 18%|█▊        | 139/782 [00:02<00:13, 47.28it/s] 18%|█▊        | 144/782 [00:03<00:13, 47.20it/s] 19%|█▉        | 149/782 [00:03<00:13, 47.25it/s] 20%|█▉        | 154/782 [00:03<00:13, 47.34it/s] 20%|██        | 159/782 [00:03<00:13, 47.32it/s] 21%|██        | 164/782 [00:03<00:13, 47.37it/s] 22%|██▏       | 169/782 [00:03<00:12, 47.31it/s] 22%|██▏       | 174/782 [00:03<00:12, 47.27it/s] 23%|██▎       | 179/782 [00:03<00:12, 47.10it/s] 24%|██▎       | 184/782 [00:03<00:12, 47.20it/s] 24%|██▍       | 189/782 [00:04<00:12, 47.32it/s] 25%|██▍       | 194/782 [00:04<00:14, 40.62it/s] 25%|██▌       | 199/782 [00:04<00:13, 42.40it/s] 26%|██▌       | 204/782 [00:04<00:13, 43.74it/s] 27%|██▋       | 209/782 [00:04<00:12, 44.79it/s] 27%|██▋       | 214/782 [00:04<00:12, 45.63it/s] 28%|██▊       | 219/782 [00:04<00:12, 46.12it/s] 29%|██▊       | 224/782 [00:04<00:12, 46.43it/s] 29%|██▉       | 229/782 [00:04<00:11, 46.74it/s] 30%|██▉       | 234/782 [00:05<00:11, 46.74it/s] 31%|███       | 239/782 [00:05<00:11, 46.84it/s] 31%|███       | 244/782 [00:05<00:11, 46.91it/s] 32%|███▏      | 249/782 [00:05<00:11, 47.03it/s] 32%|███▏      | 254/782 [00:05<00:11, 47.01it/s] 33%|███▎      | 259/782 [00:05<00:11, 47.23it/s] 34%|███▍      | 264/782 [00:05<00:10, 47.24it/s] 34%|███▍      | 269/782 [00:05<00:10, 47.34it/s] 35%|███▌      | 274/782 [00:05<00:10, 47.10it/s] 36%|███▌      | 279/782 [00:05<00:10, 47.13it/s] 36%|███▋      | 284/782 [00:06<00:10, 47.20it/s] 37%|███▋      | 289/782 [00:06<00:10, 47.23it/s] 38%|███▊      | 294/782 [00:06<00:10, 47.32it/s] 38%|███▊      | 299/782 [00:06<00:10, 47.27it/s] 39%|███▉      | 304/782 [00:06<00:10, 47.23it/s] 40%|███▉      | 309/782 [00:06<00:09, 47.35it/s] 40%|████      | 314/782 [00:06<00:09, 47.41it/s] 41%|████      | 319/782 [00:06<00:09, 47.40it/s] 41%|████▏     | 324/782 [00:06<00:09, 47.26it/s] 42%|████▏     | 329/782 [00:07<00:09, 47.19it/s] 43%|████▎     | 334/782 [00:07<00:10, 43.86it/s] 43%|████▎     | 339/782 [00:07<00:09, 44.92it/s] 44%|████▍     | 344/782 [00:07<00:09, 45.62it/s] 45%|████▍     | 349/782 [00:07<00:09, 46.19it/s] 45%|████▌     | 354/782 [00:07<00:09, 46.57it/s] 46%|████▌     | 359/782 [00:07<00:09, 46.82it/s] 47%|████▋     | 364/782 [00:07<00:08, 46.91it/s] 47%|████▋     | 369/782 [00:07<00:08, 47.06it/s] 48%|████▊     | 374/782 [00:08<00:08, 46.93it/s] 48%|████▊     | 379/782 [00:08<00:08, 46.98it/s] 49%|████▉     | 384/782 [00:08<00:08, 46.93it/s] 50%|████▉     | 389/782 [00:08<00:08, 46.86it/s] 50%|█████     | 394/782 [00:08<00:08, 46.93it/s] 51%|█████     | 399/782 [00:08<00:08, 47.12it/s] 52%|█████▏    | 404/782 [00:08<00:08, 47.21it/s] 52%|█████▏    | 409/782 [00:08<00:07, 47.31it/s] 53%|█████▎    | 414/782 [00:08<00:07, 47.29it/s] 54%|█████▎    | 419/782 [00:08<00:07, 47.18it/s] 54%|█████▍    | 424/782 [00:09<00:07, 47.20it/s] 55%|█████▍    | 429/782 [00:09<00:07, 47.12it/s] 55%|█████▌    | 434/782 [00:09<00:07, 47.08it/s] 56%|█████▌    | 439/782 [00:09<00:07, 47.18it/s] 57%|█████▋    | 444/782 [00:09<00:07, 47.27it/s] 57%|█████▋    | 449/782 [00:09<00:07, 47.29it/s] 58%|█████▊    | 454/782 [00:09<00:06, 47.23it/s] 59%|█████▊    | 459/782 [00:09<00:06, 47.23it/s] 59%|█████▉    | 464/782 [00:09<00:06, 47.28it/s] 60%|█████▉    | 469/782 [00:10<00:06, 47.15it/s] 61%|██████    | 474/782 [00:10<00:06, 47.20it/s] 61%|██████▏   | 479/782 [00:10<00:07, 41.61it/s] 62%|██████▏   | 484/782 [00:10<00:06, 43.20it/s] 63%|██████▎   | 489/782 [00:10<00:06, 44.39it/s] 63%|██████▎   | 494/782 [00:10<00:06, 45.26it/s] 64%|██████▍   | 499/782 [00:10<00:06, 44.89it/s] 64%|██████▍   | 504/782 [00:10<00:06, 45.61it/s] 65%|██████▌   | 509/782 [00:10<00:05, 46.00it/s] 66%|██████▌   | 514/782 [00:11<00:05, 46.38it/s] 66%|██████▋   | 519/782 [00:11<00:05, 46.59it/s] 67%|██████▋   | 524/782 [00:11<00:05, 46.63it/s] 68%|██████▊   | 529/782 [00:11<00:05, 46.85it/s] 68%|██████▊   | 534/782 [00:11<00:05, 47.01it/s] 69%|██████▉   | 539/782 [00:11<00:05, 47.03it/s] 70%|██████▉   | 544/782 [00:11<00:05, 47.18it/s] 70%|███████   | 549/782 [00:11<00:04, 47.24it/s] 71%|███████   | 554/782 [00:11<00:04, 47.11it/s] 71%|███████▏  | 559/782 [00:11<00:04, 47.06it/s] 72%|███████▏  | 564/782 [00:12<00:04, 47.19it/s] 73%|███████▎  | 569/782 [00:12<00:04, 47.04it/s] 73%|███████▎  | 574/782 [00:12<00:04, 47.15it/s] 74%|███████▍  | 579/782 [00:12<00:04, 47.19it/s] 75%|███████▍  | 584/782 [00:12<00:04, 47.23it/s] 75%|███████▌  | 589/782 [00:12<00:04, 47.26it/s] 76%|███████▌  | 594/782 [00:12<00:03, 47.21it/s] 77%|███████▋  | 599/782 [00:12<00:03, 47.15it/s] 77%|███████▋  | 604/782 [00:12<00:03, 47.17it/s] 78%|███████▊  | 609/782 [00:13<00:03, 47.08it/s] 79%|███████▊  | 614/782 [00:13<00:03, 47.22it/s] 79%|███████▉  | 619/782 [00:13<00:03, 47.16it/s] 80%|███████▉  | 624/782 [00:13<00:03, 47.18it/s] 80%|████████  | 629/782 [00:13<00:03, 47.25it/s] 81%|████████  | 634/782 [00:13<00:03, 47.15it/s] 82%|████████▏ | 639/782 [00:13<00:03, 41.82it/s] 82%|████████▏ | 644/782 [00:13<00:03, 43.30it/s] 83%|████████▎ | 649/782 [00:13<00:02, 44.41it/s] 84%|████████▎ | 654/782 [00:14<00:02, 45.19it/s] 84%|████████▍ | 659/782 [00:14<00:02, 45.78it/s] 85%|████████▍ | 664/782 [00:14<00:02, 46.22it/s] 86%|████████▌ | 669/782 [00:14<00:02, 46.57it/s] 86%|████████▌ | 674/782 [00:14<00:02, 46.77it/s] 87%|████████▋ | 679/782 [00:14<00:02, 46.80it/s] 87%|████████▋ | 684/782 [00:14<00:02, 46.86it/s] 88%|████████▊ | 689/782 [00:14<00:01, 46.89it/s] 89%|████████▊ | 694/782 [00:14<00:01, 47.02it/s] 89%|████████▉ | 699/782 [00:15<00:01, 46.94it/s] 90%|█████████ | 704/782 [00:15<00:01, 47.10it/s] 91%|█████████ | 709/782 [00:15<00:01, 47.10it/s] 91%|█████████▏| 714/782 [00:15<00:01, 47.18it/s] 92%|█████████▏| 719/782 [00:15<00:01, 47.18it/s] 93%|█████████▎| 724/782 [00:15<00:01, 47.12it/s] 93%|█████████▎| 729/782 [00:15<00:01, 47.11it/s] 94%|█████████▍| 734/782 [00:15<00:01, 47.08it/s] 95%|█████████▍| 739/782 [00:15<00:00, 47.13it/s] 95%|█████████▌| 744/782 [00:15<00:00, 47.08it/s] 96%|█████████▌| 749/782 [00:16<00:00, 47.05it/s] 96%|█████████▋| 754/782 [00:16<00:00, 47.15it/s] 97%|█████████▋| 759/782 [00:16<00:00, 47.11it/s] 98%|█████████▊| 764/782 [00:16<00:00, 47.19it/s] 98%|█████████▊| 769/782 [00:16<00:00, 47.17it/s] 99%|█████████▉| 774/782 [00:16<00:00, 47.18it/s]100%|█████████▉| 779/782 [00:16<00:00, 42.78it/s]100%|██████████| 782/782 [00:16<00:00, 46.52it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:30:55,520 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   eval_loss               =     0.9811
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   eval_runtime            = 0:00:16.83
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   eval_samples            =       6253
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   eval_samples_per_second =    371.433
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   eval_steps_per_second   =     46.451
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:30:55,520 >>   perplexity              =     2.6673
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:08,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:08,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:08,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:08,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:08,053 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:31:09,152 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:31:09,153 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:31:09,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:31:10,987 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:31:10,987 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:14,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:14,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:14,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:14,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:31:14,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:31:14,922 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:31:14,923 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:31:15,548 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:31:15,781 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:31:15,781 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/dev.jsonl', 'labels': ['member of', 'member of sports team', 'notable work', 'owned by', 'successful candidate'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15698
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15798, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.25it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:05,  1.45it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:08,  1.53it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.54it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.56it/s]Extractor Predicting: 21it [00:14,  1.56it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:22,  1.49it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.47it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.52it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.51it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.55it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:38,  1.52it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:45,  1.52it/s]Extractor Predicting: 69it [00:46,  1.48it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.45it/s]Extractor Predicting: 72it [00:48,  1.47it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.48it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:51,  1.46it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:56,  1.48it/s]Extractor Predicting: 86it [00:57,  1.45it/s]Extractor Predicting: 87it [00:58,  1.47it/s]Extractor Predicting: 88it [00:58,  1.48it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:02,  1.47it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:04,  1.45it/s]Extractor Predicting: 97it [01:05,  1.34it/s]Extractor Predicting: 98it [01:05,  1.37it/s]Extractor Predicting: 99it [01:06,  1.39it/s]Extractor Predicting: 100it [01:07,  1.39it/s]Extractor Predicting: 101it [01:08,  1.42it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:09,  1.46it/s]Extractor Predicting: 104it [01:10,  1.48it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:11,  1.47it/s]Extractor Predicting: 107it [01:12,  1.44it/s]Extractor Predicting: 108it [01:12,  1.44it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.45it/s]Extractor Predicting: 111it [01:14,  1.47it/s]Extractor Predicting: 112it [01:15,  1.46it/s]Extractor Predicting: 113it [01:16,  1.50it/s]Extractor Predicting: 114it [01:16,  1.49it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:18,  1.45it/s]Extractor Predicting: 117it [01:19,  1.44it/s]Extractor Predicting: 118it [01:19,  1.46it/s]Extractor Predicting: 119it [01:20,  1.45it/s]Extractor Predicting: 120it [01:21,  1.42it/s]Extractor Predicting: 121it [01:21,  1.45it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.51it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.43it/s]Extractor Predicting: 126it [01:25,  1.46it/s]Extractor Predicting: 127it [01:25,  1.46it/s]Extractor Predicting: 128it [01:26,  1.46it/s]Extractor Predicting: 129it [01:27,  1.46it/s]Extractor Predicting: 130it [01:27,  1.42it/s]Extractor Predicting: 131it [01:28,  1.43it/s]Extractor Predicting: 132it [01:29,  1.43it/s]Extractor Predicting: 133it [01:30,  1.45it/s]Extractor Predicting: 134it [01:30,  1.45it/s]Extractor Predicting: 135it [01:31,  1.43it/s]Extractor Predicting: 136it [01:32,  1.45it/s]Extractor Predicting: 137it [01:32,  1.46it/s]Extractor Predicting: 138it [01:33,  1.48it/s]Extractor Predicting: 139it [01:34,  1.48it/s]Extractor Predicting: 140it [01:34,  1.48it/s]Extractor Predicting: 141it [01:35,  1.48it/s]Extractor Predicting: 142it [01:36,  1.49it/s]Extractor Predicting: 143it [01:36,  1.47it/s]Extractor Predicting: 144it [01:37,  1.49it/s]Extractor Predicting: 145it [01:38,  1.49it/s]Extractor Predicting: 146it [01:38,  1.49it/s]Extractor Predicting: 147it [01:39,  1.48it/s]Extractor Predicting: 148it [01:40,  1.53it/s]Extractor Predicting: 149it [01:40,  1.53it/s]Extractor Predicting: 150it [01:41,  1.51it/s]Extractor Predicting: 151it [01:42,  1.48it/s]Extractor Predicting: 152it [01:42,  1.53it/s]Extractor Predicting: 153it [01:43,  1.51it/s]Extractor Predicting: 154it [01:44,  1.51it/s]Extractor Predicting: 155it [01:44,  1.52it/s]Extractor Predicting: 156it [01:45,  1.49it/s]Extractor Predicting: 157it [01:46,  1.54it/s]Extractor Predicting: 158it [01:46,  1.54it/s]Extractor Predicting: 159it [01:47,  1.56it/s]Extractor Predicting: 160it [01:48,  1.51it/s]Extractor Predicting: 161it [01:48,  1.48it/s]Extractor Predicting: 162it [01:49,  1.50it/s]Extractor Predicting: 163it [01:49,  1.53it/s]Extractor Predicting: 164it [01:50,  1.55it/s]Extractor Predicting: 165it [01:51,  1.59it/s]Extractor Predicting: 166it [01:51,  1.58it/s]Extractor Predicting: 167it [01:52,  1.57it/s]Extractor Predicting: 168it [01:53,  1.55it/s]Extractor Predicting: 169it [01:53,  1.57it/s]Extractor Predicting: 170it [01:54,  1.58it/s]Extractor Predicting: 171it [01:55,  1.55it/s]Extractor Predicting: 172it [01:55,  1.53it/s]Extractor Predicting: 173it [01:56,  1.56it/s]Extractor Predicting: 174it [01:57,  1.52it/s]Extractor Predicting: 175it [01:57,  1.54it/s]Extractor Predicting: 176it [01:58,  1.51it/s]Extractor Predicting: 177it [01:59,  1.39it/s]Extractor Predicting: 178it [01:59,  1.43it/s]Extractor Predicting: 179it [02:00,  1.46it/s]Extractor Predicting: 180it [02:01,  1.47it/s]Extractor Predicting: 181it [02:01,  1.49it/s]Extractor Predicting: 182it [02:02,  1.48it/s]Extractor Predicting: 183it [02:03,  1.51it/s]Extractor Predicting: 184it [02:03,  1.53it/s]Extractor Predicting: 185it [02:04,  1.52it/s]Extractor Predicting: 186it [02:05,  1.49it/s]Extractor Predicting: 187it [02:05,  1.51it/s]Extractor Predicting: 188it [02:06,  1.48it/s]Extractor Predicting: 189it [02:07,  1.51it/s]Extractor Predicting: 190it [02:07,  1.53it/s]Extractor Predicting: 191it [02:08,  1.56it/s]Extractor Predicting: 192it [02:09,  1.52it/s]Extractor Predicting: 193it [02:09,  1.52it/s]Extractor Predicting: 194it [02:10,  1.53it/s]Extractor Predicting: 195it [02:11,  1.52it/s]Extractor Predicting: 196it [02:11,  1.50it/s]Extractor Predicting: 197it [02:12,  1.50it/s]Extractor Predicting: 198it [02:13,  1.50it/s]Extractor Predicting: 199it [02:13,  1.49it/s]Extractor Predicting: 200it [02:14,  1.56it/s]Extractor Predicting: 201it [02:14,  1.58it/s]Extractor Predicting: 202it [02:15,  1.55it/s]Extractor Predicting: 203it [02:16,  1.57it/s]Extractor Predicting: 204it [02:16,  1.57it/s]Extractor Predicting: 205it [02:17,  1.56it/s]Extractor Predicting: 206it [02:18,  1.59it/s]Extractor Predicting: 207it [02:18,  1.58it/s]Extractor Predicting: 208it [02:19,  1.58it/s]Extractor Predicting: 209it [02:20,  1.56it/s]Extractor Predicting: 210it [02:20,  1.57it/s]Extractor Predicting: 211it [02:21,  1.59it/s]Extractor Predicting: 212it [02:21,  1.55it/s]Extractor Predicting: 213it [02:22,  1.55it/s]Extractor Predicting: 214it [02:23,  1.57it/s]Extractor Predicting: 215it [02:23,  1.56it/s]Extractor Predicting: 216it [02:24,  1.56it/s]Extractor Predicting: 217it [02:25,  1.53it/s]Extractor Predicting: 218it [02:25,  1.56it/s]Extractor Predicting: 219it [02:26,  1.54it/s]Extractor Predicting: 220it [02:27,  1.54it/s]Extractor Predicting: 221it [02:27,  1.56it/s]Extractor Predicting: 222it [02:28,  1.54it/s]Extractor Predicting: 223it [02:29,  1.56it/s]Extractor Predicting: 224it [02:29,  1.57it/s]Extractor Predicting: 225it [02:30,  1.56it/s]Extractor Predicting: 226it [02:30,  1.57it/s]Extractor Predicting: 227it [02:31,  1.56it/s]Extractor Predicting: 228it [02:32,  1.71it/s]Extractor Predicting: 228it [02:32,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:02,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:03,002 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:03,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:03,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:03,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:34:03,912 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:34:03,913 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:34:04,581 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:34:05,712 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:34:05,712 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:08,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:08,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:08,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:08,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:34:08,878 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:34:09,749 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:34:09,750 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:34:10,405 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:34:10,660 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:34:10,660 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.10373019928461931,
  "recall": 0.032464417079801695,
  "score": 0.04945188794153471,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 18402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.47it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:09,  1.46it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:22,  1.47it/s]Extractor Predicting: 34it [00:22,  1.41it/s]Extractor Predicting: 35it [00:23,  1.43it/s]Extractor Predicting: 36it [00:24,  1.42it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.41it/s]Extractor Predicting: 39it [00:26,  1.35it/s]Extractor Predicting: 40it [00:27,  1.39it/s]Extractor Predicting: 41it [00:27,  1.42it/s]Extractor Predicting: 42it [00:28,  1.44it/s]Extractor Predicting: 43it [00:29,  1.44it/s]Extractor Predicting: 44it [00:29,  1.42it/s]Extractor Predicting: 45it [00:30,  1.41it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:31,  1.59it/s]Extractor Predicting: 48it [00:32,  1.60it/s]Extractor Predicting: 49it [00:32,  1.60it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:34,  1.58it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.54it/s]Extractor Predicting: 57it [00:38,  1.54it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.55it/s]Extractor Predicting: 61it [00:40,  1.58it/s]Extractor Predicting: 62it [00:41,  1.55it/s]Extractor Predicting: 63it [00:42,  1.55it/s]Extractor Predicting: 64it [00:42,  1.56it/s]Extractor Predicting: 65it [00:43,  1.58it/s]Extractor Predicting: 66it [00:44,  1.57it/s]Extractor Predicting: 67it [00:44,  1.57it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:47,  1.60it/s]Extractor Predicting: 72it [00:47,  1.60it/s]Extractor Predicting: 73it [00:48,  1.57it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:50,  1.57it/s]Extractor Predicting: 77it [00:51,  1.56it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:53,  1.42it/s]Extractor Predicting: 81it [00:53,  1.42it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:57,  1.52it/s]Extractor Predicting: 87it [00:57,  1.53it/s]Extractor Predicting: 88it [00:58,  1.55it/s]Extractor Predicting: 89it [00:59,  1.56it/s]Extractor Predicting: 90it [00:59,  1.58it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:01,  1.54it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:02,  1.56it/s]Extractor Predicting: 96it [01:03,  1.55it/s]Extractor Predicting: 97it [01:04,  1.54it/s]Extractor Predicting: 98it [01:04,  1.54it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:06,  1.54it/s]Extractor Predicting: 101it [01:06,  1.55it/s]Extractor Predicting: 102it [01:07,  1.56it/s]Extractor Predicting: 103it [01:08,  1.56it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:10,  1.52it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:12,  1.55it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:13,  1.56it/s]Extractor Predicting: 112it [01:13,  1.55it/s]Extractor Predicting: 113it [01:14,  1.57it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:17,  1.59it/s]Extractor Predicting: 118it [01:17,  1.59it/s]Extractor Predicting: 119it [01:18,  1.64it/s]Extractor Predicting: 120it [01:18,  1.63it/s]Extractor Predicting: 121it [01:19,  1.63it/s]Extractor Predicting: 122it [01:20,  1.64it/s]Extractor Predicting: 123it [01:20,  1.65it/s]Extractor Predicting: 124it [01:21,  1.65it/s]Extractor Predicting: 125it [01:22,  1.59it/s]Extractor Predicting: 126it [01:22,  1.62it/s]Extractor Predicting: 127it [01:23,  1.59it/s]Extractor Predicting: 128it [01:23,  1.55it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:25,  1.56it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:26,  1.59it/s]Extractor Predicting: 133it [01:27,  1.59it/s]Extractor Predicting: 134it [01:27,  1.58it/s]Extractor Predicting: 135it [01:28,  1.60it/s]Extractor Predicting: 136it [01:28,  1.63it/s]Extractor Predicting: 137it [01:29,  1.56it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:30,  1.55it/s]Extractor Predicting: 140it [01:31,  1.59it/s]Extractor Predicting: 141it [01:32,  1.62it/s]Extractor Predicting: 142it [01:32,  1.65it/s]Extractor Predicting: 143it [01:33,  1.65it/s]Extractor Predicting: 144it [01:33,  1.66it/s]Extractor Predicting: 145it [01:34,  1.60it/s]Extractor Predicting: 146it [01:35,  1.59it/s]Extractor Predicting: 147it [01:35,  1.61it/s]Extractor Predicting: 148it [01:36,  1.62it/s]Extractor Predicting: 149it [01:37,  1.57it/s]Extractor Predicting: 150it [01:37,  1.48it/s]Extractor Predicting: 151it [01:38,  1.52it/s]Extractor Predicting: 152it [01:39,  1.54it/s]Extractor Predicting: 153it [01:39,  1.58it/s]Extractor Predicting: 154it [01:40,  1.55it/s]Extractor Predicting: 155it [01:41,  1.49it/s]Extractor Predicting: 156it [01:41,  1.50it/s]Extractor Predicting: 157it [01:42,  1.49it/s]Extractor Predicting: 158it [01:43,  1.54it/s]Extractor Predicting: 159it [01:43,  1.55it/s]Extractor Predicting: 160it [01:44,  1.53it/s]Extractor Predicting: 161it [01:45,  1.57it/s]Extractor Predicting: 162it [01:45,  1.58it/s]Extractor Predicting: 163it [01:46,  1.57it/s]Extractor Predicting: 164it [01:46,  1.59it/s]Extractor Predicting: 165it [01:47,  1.54it/s]Extractor Predicting: 166it [01:48,  1.58it/s]Extractor Predicting: 167it [01:49,  1.43it/s]Extractor Predicting: 168it [01:49,  1.47it/s]Extractor Predicting: 169it [01:50,  1.51it/s]Extractor Predicting: 170it [01:50,  1.48it/s]Extractor Predicting: 171it [01:51,  1.49it/s]Extractor Predicting: 172it [01:52,  1.51it/s]Extractor Predicting: 173it [01:52,  1.50it/s]Extractor Predicting: 174it [01:53,  1.52it/s]Extractor Predicting: 175it [01:54,  1.49it/s]Extractor Predicting: 176it [01:54,  1.51it/s]Extractor Predicting: 177it [01:55,  1.50it/s]Extractor Predicting: 178it [01:56,  1.48it/s]Extractor Predicting: 179it [01:56,  1.51it/s]Extractor Predicting: 180it [01:57,  1.51it/s]Extractor Predicting: 181it [01:58,  1.55it/s]Extractor Predicting: 182it [01:58,  1.59it/s]Extractor Predicting: 183it [01:59,  1.57it/s]Extractor Predicting: 184it [02:00,  1.60it/s]Extractor Predicting: 185it [02:00,  1.57it/s]Extractor Predicting: 186it [02:01,  1.56it/s]Extractor Predicting: 187it [02:01,  1.59it/s]Extractor Predicting: 188it [02:02,  1.60it/s]Extractor Predicting: 189it [02:03,  1.55it/s]Extractor Predicting: 190it [02:04,  1.50it/s]Extractor Predicting: 191it [02:04,  1.51it/s]Extractor Predicting: 192it [02:05,  1.48it/s]Extractor Predicting: 193it [02:06,  1.49it/s]Extractor Predicting: 194it [02:06,  1.49it/s]Extractor Predicting: 195it [02:07,  1.47it/s]Extractor Predicting: 196it [02:08,  1.46it/s]Extractor Predicting: 197it [02:08,  1.46it/s]Extractor Predicting: 198it [02:09,  1.49it/s]Extractor Predicting: 199it [02:10,  1.48it/s]Extractor Predicting: 200it [02:10,  1.47it/s]Extractor Predicting: 201it [02:11,  1.48it/s]Extractor Predicting: 202it [02:12,  1.45it/s]Extractor Predicting: 203it [02:12,  1.47it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:14,  1.45it/s]Extractor Predicting: 206it [02:14,  1.46it/s]Extractor Predicting: 207it [02:15,  1.41it/s]Extractor Predicting: 208it [02:16,  1.42it/s]Extractor Predicting: 209it [02:16,  1.48it/s]Extractor Predicting: 210it [02:17,  1.50it/s]Extractor Predicting: 211it [02:18,  1.51it/s]Extractor Predicting: 212it [02:18,  1.51it/s]Extractor Predicting: 213it [02:19,  1.49it/s]Extractor Predicting: 214it [02:20,  1.47it/s]Extractor Predicting: 215it [02:21,  1.48it/s]Extractor Predicting: 216it [02:21,  1.47it/s]Extractor Predicting: 217it [02:22,  1.43it/s]Extractor Predicting: 218it [02:23,  1.47it/s]Extractor Predicting: 219it [02:23,  1.49it/s]Extractor Predicting: 220it [02:24,  1.49it/s]Extractor Predicting: 221it [02:25,  1.52it/s]Extractor Predicting: 222it [02:25,  1.46it/s]Extractor Predicting: 223it [02:26,  1.44it/s]Extractor Predicting: 224it [02:27,  1.43it/s]Extractor Predicting: 225it [02:27,  1.42it/s]Extractor Predicting: 226it [02:28,  1.42it/s]Extractor Predicting: 227it [02:29,  1.39it/s]Extractor Predicting: 228it [02:30,  1.39it/s]Extractor Predicting: 229it [02:30,  1.38it/s]Extractor Predicting: 230it [02:31,  1.41it/s]Extractor Predicting: 231it [02:32,  1.41it/s]Extractor Predicting: 232it [02:32,  1.41it/s]Extractor Predicting: 233it [02:33,  1.42it/s]Extractor Predicting: 234it [02:34,  1.43it/s]Extractor Predicting: 235it [02:34,  1.44it/s]Extractor Predicting: 236it [02:35,  1.40it/s]Extractor Predicting: 237it [02:36,  1.39it/s]Extractor Predicting: 238it [02:37,  1.39it/s]Extractor Predicting: 239it [02:37,  1.42it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:39,  1.44it/s]Extractor Predicting: 242it [02:39,  1.45it/s]Extractor Predicting: 243it [02:40,  1.47it/s]Extractor Predicting: 244it [02:41,  1.45it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.51it/s]Extractor Predicting: 246it [02:42,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:08,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:08,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:08,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:08,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:08,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:37:09,254 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:37:09,255 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:37:09,957 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:37:11,164 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:37:11,235 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:14,693 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:14,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:14,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:14,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:37:14,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:37:15,644 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:37:15,645 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:37:16,263 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:37:16,467 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:37:16,468 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38423028785982477,
  "recall": 0.10406779661016949,
  "score": 0.16377700720192054,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.41it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.70it/s]Extractor Predicting: 15it [00:10,  1.49it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.40707964601769914,
  "recall": 0.06460674157303371,
  "score": 0.11151515151515153,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'mother', 'occupant', 'organization directed by the office or position', 'screenwriter', 'use', 'voice type'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
