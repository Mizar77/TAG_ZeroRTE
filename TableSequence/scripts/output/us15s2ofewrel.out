Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:26, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:34, 15.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:50<04:53, 17.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:05<04:22, 16.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:23<04:11, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:39<03:51, 16.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:57<03:40, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:14<03:25, 17.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:32<03:12, 17.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:49<02:52, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:06<02:33, 17.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:23<02:17, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:40<01:58, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:57<01:42, 17.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:13<01:24, 16.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:32<01:09, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:48<00:50, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:04<00:33, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:21<00:16, 16.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:37<00:00, 16.48s/it]Generating: 100%|██████████| 20/20 [05:37<00:00, 16.86s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 378, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 578, 'raw': 800}
{'target': 600, 'success': 604, 'raw': 832}
{'prompt': 'Relation : head of government .', 'success_rate': 0.7259615384615384, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : mother . Context : Later in Life , the children of López 's sisters , Isabelle , Juan Andres , Emilie and Isabelle , became the members of the family of López 's sons . Head Entity : Isabelle , Tail Entity : Lupez .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 492, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7319711538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : performer .', 'success_rate': 0.84375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
["Relation : spouse . Context : Later in Life , he married his third wife , a young princess of the family at the end of the third century BC , Margriet , whom he described as her ' sister , queen of Bismarck . Head Entity : Margriet , Tail Entity : Agnes .\n"]
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( October 1887 ) , a young French painter , Louis Boulogne , painted many of the " La Grande Démontagne " , including Boulogne \'s " Montessemble des deux de Château des Gains " . Head Entity : Montessemble des deux de Château des Gains , Tail Entity : Charles Boulogne .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 431, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 496, 'raw': 704}
{'target': 600, 'success': 514, 'raw': 736}
{'target': 600, 'success': 538, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 586, 'raw': 832}
{'target': 600, 'success': 610, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7060185185185185, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n']
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that Piotr Kavlicek would be awarded compensation of $ 5,000 USD in damages against his former Ukrainian colleagues for causing " systematic persecution " following claims by Ukrainian authorities against Oleksandr Kuchma and Kolomoisky . Head Entity : Oleksandr Kuchma , Tail Entity : Ukraine .\n', 'Relation : country of citizenship . Context : After he was elected to serve as a judge on the Supreme Court of the Netherlands , he was appointed to the Court of Appeal for the District of Rotterdam between 1990 and 2001 . Head Entity : court of Appeal , Tail Entity : Netherlands .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 127, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 173, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 222, 'raw': 320}
{'target': 600, 'success': 245, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 562, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 487, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 540, 'raw': 736}
{'target': 600, 'success': 562, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7307692307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : has part .', 'success_rate': 0.7916666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 406, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 514, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7675, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 473, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1141–1143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 1141–1143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n']
['Relation : mouth of the watercourse . Context : Later in the year ( 1141–1143 ) , at Dervy , he was commissioned into the service of King James III , the King of England . Head Entity : Dervy , Tail Entity : watercourse .\n', 'Relation : mouth of the watercourse . Context : Eta and the Cephalopodalleinae are the principal species of crustaceans in the family Cephalopodalleae . Head Entity : Cephalopodalleidae , Tail Entity : Cephalopodalleinae .\n', 'Relation : mouth of the watercourse . Context : This was the main site from which the first British invasion came ( see " The Battle of the Dauphin Sea " , page 18 ) . Head Entity : Dauphin Sea , Tail Entity : Dauphins .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : occupant .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( October 1887 ) , a young French colonialist named Pierre de Coupe had married the Marquis de Rouvoir , a physician of the French nobility . Head Entity : Pierre de Coupe , Tail Entity : Jean - de Coupe .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 396, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 534, 'raw': 736}
{'target': 600, 'success': 560, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7295673076923077, 'errors': {'', "('United States Naval Academy', 'occupation', '', 'The United States Naval Academy built and maintained a permanent Navy SEAL garrison in Elgin , Louisiana , based for 16 - 18 April 1941 .')", '(\'Marguerite Guilen\', \'occupation\', \'\', \'" La Ronde - les Ronde " is a satirical piece written by French writer Marguerite Guilen with her portrait of François Renoul .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8342391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8059895833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 174, 'raw': 256}
{'target': 600, 'success': 196, 'raw': 288}
{'target': 600, 'success': 220, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 270, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 341, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 456, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 613, 'raw': 864}
{'prompt': 'Relation : winner .', 'success_rate': 0.7094907407407407, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'Jules Verneck - de - Sade\', \'winner\', \'\', \'" It Comes Back to Me " is the album of four albums by Swedish producer Jules Verneck - de - Sade .\')'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 16691
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16791, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.95s/it]Extractor Estimating: 2it [00:18,  7.73s/it]Extractor Estimating: 3it [00:18,  4.50s/it]Extractor Estimating: 4it [00:19,  2.96s/it]Extractor Estimating: 5it [00:20,  2.14s/it]Extractor Estimating: 6it [00:20,  1.61s/it]Extractor Estimating: 7it [00:23,  1.83s/it]Extractor Estimating: 8it [00:23,  1.43s/it]Extractor Estimating: 9it [00:24,  1.24s/it]Extractor Estimating: 10it [00:25,  1.06s/it]Extractor Estimating: 11it [00:25,  1.07it/s]Extractor Estimating: 12it [00:26,  1.21it/s]Extractor Estimating: 13it [00:26,  1.33it/s]Extractor Estimating: 14it [00:27,  1.35it/s]Extractor Estimating: 15it [00:28,  1.45it/s]Extractor Estimating: 16it [00:28,  1.54it/s]Extractor Estimating: 17it [00:29,  1.61it/s]Extractor Estimating: 18it [00:29,  1.61it/s]Extractor Estimating: 19it [00:30,  1.60it/s]Extractor Estimating: 20it [00:31,  1.60it/s]Extractor Estimating: 21it [00:31,  1.59it/s]Extractor Estimating: 22it [00:32,  1.63it/s]Extractor Estimating: 23it [00:32,  1.63it/s]Extractor Estimating: 24it [00:33,  1.57it/s]Extractor Estimating: 25it [00:34,  1.58it/s]Extractor Estimating: 26it [00:34,  1.65it/s]Extractor Estimating: 27it [00:35,  1.67it/s]Extractor Estimating: 28it [00:35,  1.70it/s]Extractor Estimating: 29it [00:36,  1.59it/s]Extractor Estimating: 30it [00:37,  1.60it/s]Extractor Estimating: 31it [00:37,  1.63it/s]Extractor Estimating: 32it [00:38,  1.62it/s]Extractor Estimating: 33it [00:39,  1.59it/s]Extractor Estimating: 34it [00:39,  1.58it/s]Extractor Estimating: 35it [00:40,  1.57it/s]Extractor Estimating: 36it [00:41,  1.54it/s]Extractor Estimating: 37it [00:41,  1.52it/s]Extractor Estimating: 38it [00:42,  1.61it/s]Extractor Estimating: 39it [00:43,  1.56it/s]Extractor Estimating: 40it [00:43,  1.49it/s]Extractor Estimating: 41it [00:44,  1.56it/s]Extractor Estimating: 42it [00:45,  1.55it/s]Extractor Estimating: 43it [00:45,  1.55it/s]Extractor Estimating: 44it [00:46,  1.59it/s]Extractor Estimating: 45it [00:46,  1.56it/s]Extractor Estimating: 46it [00:47,  1.55it/s]Extractor Estimating: 47it [00:48,  1.61it/s]Extractor Estimating: 48it [00:48,  1.56it/s]Extractor Estimating: 49it [00:49,  1.60it/s]Extractor Estimating: 50it [00:50,  1.59it/s]Extractor Estimating: 51it [00:50,  1.57it/s]Extractor Estimating: 52it [00:51,  1.55it/s]Extractor Estimating: 53it [00:52,  1.50it/s]Extractor Estimating: 54it [00:52,  1.56it/s]Extractor Estimating: 55it [00:53,  1.54it/s]Extractor Estimating: 56it [00:53,  1.56it/s]Extractor Estimating: 57it [00:54,  1.52it/s]Extractor Estimating: 58it [00:55,  1.58it/s]Extractor Estimating: 59it [00:55,  1.58it/s]Extractor Estimating: 60it [00:56,  1.53it/s]Extractor Estimating: 61it [00:57,  1.54it/s]Extractor Estimating: 62it [00:57,  1.52it/s]Extractor Estimating: 63it [00:58,  1.58it/s]Extractor Estimating: 64it [00:59,  1.57it/s]Extractor Estimating: 65it [00:59,  1.48it/s]Extractor Estimating: 66it [01:00,  1.46it/s]Extractor Estimating: 67it [01:01,  1.48it/s]Extractor Estimating: 68it [01:01,  1.51it/s]Extractor Estimating: 69it [01:02,  1.49it/s]Extractor Estimating: 70it [01:03,  1.47it/s]Extractor Estimating: 71it [01:03,  1.53it/s]Extractor Estimating: 72it [01:04,  1.54it/s]Extractor Estimating: 73it [01:05,  1.54it/s]Extractor Estimating: 74it [01:05,  1.55it/s]Extractor Estimating: 75it [01:06,  1.53it/s]Extractor Estimating: 76it [01:07,  1.54it/s]Extractor Estimating: 77it [01:08,  1.09it/s]Extractor Estimating: 78it [01:09,  1.19it/s]Extractor Estimating: 79it [01:10,  1.25it/s]Extractor Estimating: 80it [01:10,  1.32it/s]Extractor Estimating: 81it [01:11,  1.38it/s]Extractor Estimating: 82it [01:11,  1.42it/s]Extractor Estimating: 83it [01:12,  1.48it/s]Extractor Estimating: 84it [01:13,  1.47it/s]Extractor Estimating: 85it [01:14,  1.44it/s]Extractor Estimating: 86it [01:14,  1.51it/s]Extractor Estimating: 87it [01:15,  1.53it/s]Extractor Estimating: 88it [01:15,  1.47it/s]Extractor Estimating: 89it [01:16,  1.38it/s]Extractor Estimating: 90it [01:17,  1.40it/s]Extractor Estimating: 91it [01:18,  1.51it/s]Extractor Estimating: 92it [01:18,  1.50it/s]Extractor Estimating: 93it [01:19,  1.47it/s]Extractor Estimating: 94it [01:20,  1.51it/s]Extractor Estimating: 95it [01:20,  1.47it/s]Extractor Estimating: 96it [01:21,  1.51it/s]Extractor Estimating: 97it [01:22,  1.46it/s]Extractor Estimating: 98it [01:22,  1.50it/s]Extractor Estimating: 99it [01:23,  1.53it/s]Extractor Estimating: 100it [01:24,  1.49it/s]Extractor Estimating: 101it [01:24,  1.51it/s]Extractor Estimating: 102it [01:25,  1.54it/s]Extractor Estimating: 103it [01:25,  1.62it/s]Extractor Estimating: 104it [01:26,  1.59it/s]Extractor Estimating: 105it [01:27,  1.59it/s]Extractor Estimating: 106it [01:27,  1.60it/s]Extractor Estimating: 107it [01:28,  1.62it/s]Extractor Estimating: 108it [01:28,  1.63it/s]Extractor Estimating: 109it [01:29,  1.66it/s]Extractor Estimating: 110it [01:30,  1.68it/s]Extractor Estimating: 111it [01:30,  1.58it/s]Extractor Estimating: 112it [01:31,  1.57it/s]Extractor Estimating: 113it [01:32,  1.63it/s]Extractor Estimating: 114it [01:32,  1.65it/s]Extractor Estimating: 115it [01:33,  1.51it/s]Extractor Estimating: 116it [01:34,  1.52it/s]Extractor Estimating: 117it [01:34,  1.50it/s]Extractor Estimating: 118it [01:35,  1.53it/s]Extractor Estimating: 119it [01:36,  1.56it/s]Extractor Estimating: 120it [01:36,  1.54it/s]Extractor Estimating: 121it [01:37,  1.60it/s]Extractor Estimating: 122it [01:37,  1.64it/s]Extractor Estimating: 123it [01:38,  1.64it/s]Extractor Estimating: 124it [01:39,  1.68it/s]Extractor Estimating: 125it [01:39,  1.66it/s]Extractor Estimating: 126it [01:40,  1.66it/s]Extractor Estimating: 127it [01:40,  1.56it/s]Extractor Estimating: 128it [01:41,  1.51it/s]Extractor Estimating: 129it [01:42,  1.51it/s]Extractor Estimating: 130it [01:43,  1.49it/s]Extractor Estimating: 131it [01:43,  1.53it/s]Extractor Estimating: 132it [01:44,  1.49it/s]Extractor Estimating: 133it [01:45,  1.48it/s]Extractor Estimating: 134it [01:45,  1.50it/s]Extractor Estimating: 135it [01:46,  1.49it/s]Extractor Estimating: 136it [01:46,  1.55it/s]Extractor Estimating: 137it [01:47,  1.57it/s]Extractor Estimating: 138it [01:48,  1.51it/s]Extractor Estimating: 139it [01:48,  1.57it/s]Extractor Estimating: 140it [01:49,  1.60it/s]Extractor Estimating: 141it [01:50,  1.55it/s]Extractor Estimating: 142it [01:50,  1.54it/s]Extractor Estimating: 143it [01:51,  1.51it/s]Extractor Estimating: 144it [01:52,  1.47it/s]Extractor Estimating: 145it [01:52,  1.51it/s]Extractor Estimating: 146it [01:53,  1.49it/s]Extractor Estimating: 147it [01:54,  1.51it/s]Extractor Estimating: 148it [01:54,  1.46it/s]Extractor Estimating: 149it [01:55,  1.51it/s]Extractor Estimating: 150it [01:56,  1.52it/s]Extractor Estimating: 151it [01:56,  1.55it/s]Extractor Estimating: 152it [01:57,  1.62it/s]Extractor Estimating: 153it [01:57,  1.62it/s]Extractor Estimating: 154it [01:58,  1.72it/s]Extractor Estimating: 155it [01:59,  1.73it/s]Extractor Estimating: 156it [01:59,  1.69it/s]Extractor Estimating: 157it [02:00,  1.70it/s]Extractor Estimating: 158it [02:00,  1.69it/s]Extractor Estimating: 159it [02:01,  1.63it/s]Extractor Estimating: 160it [02:02,  1.69it/s]Extractor Estimating: 161it [02:02,  1.73it/s]Extractor Estimating: 162it [02:03,  1.71it/s]Extractor Estimating: 163it [02:03,  1.75it/s]Extractor Estimating: 164it [02:04,  1.76it/s]Extractor Estimating: 165it [02:04,  1.71it/s]Extractor Estimating: 166it [02:05,  1.71it/s]Extractor Estimating: 167it [02:06,  1.68it/s]Extractor Estimating: 168it [02:06,  1.73it/s]Extractor Estimating: 169it [02:07,  1.77it/s]Extractor Estimating: 170it [02:07,  1.73it/s]Extractor Estimating: 171it [02:11,  1.42s/it]Extractor Estimating: 172it [02:12,  1.27s/it]Extractor Estimating: 173it [02:12,  1.06s/it]Extractor Estimating: 174it [02:13,  1.09it/s]Extractor Estimating: 175it [02:13,  1.22it/s]Extractor Estimating: 176it [02:14,  1.32it/s]Extractor Estimating: 177it [02:15,  1.44it/s]Extractor Estimating: 178it [02:15,  1.46it/s]Extractor Estimating: 179it [02:16,  1.49it/s]Extractor Estimating: 180it [02:16,  1.53it/s]Extractor Estimating: 181it [02:17,  1.57it/s]Extractor Estimating: 182it [02:18,  1.65it/s]Extractor Estimating: 183it [02:18,  1.57it/s]Extractor Estimating: 184it [02:19,  1.52it/s]Extractor Estimating: 185it [02:20,  1.60it/s]Extractor Estimating: 186it [02:20,  1.59it/s]Extractor Estimating: 187it [02:21,  1.56it/s]Extractor Estimating: 188it [02:21,  1.55it/s]Extractor Estimating: 189it [02:22,  1.55it/s]Extractor Estimating: 190it [02:23,  1.59it/s]Extractor Estimating: 191it [02:23,  1.62it/s]Extractor Estimating: 192it [02:24,  1.66it/s]Extractor Estimating: 193it [02:24,  1.66it/s]Extractor Estimating: 194it [02:25,  1.68it/s]Extractor Estimating: 195it [02:26,  1.64it/s]Extractor Estimating: 196it [02:26,  1.66it/s]Extractor Estimating: 197it [02:27,  1.71it/s]Extractor Estimating: 198it [02:27,  1.67it/s]Extractor Estimating: 199it [02:28,  1.66it/s]Extractor Estimating: 200it [02:29,  1.64it/s]Extractor Estimating: 201it [02:29,  1.62it/s]Extractor Estimating: 202it [02:30,  1.49it/s]Extractor Estimating: 203it [02:31,  1.55it/s]Extractor Estimating: 204it [02:31,  1.53it/s]Extractor Estimating: 205it [02:32,  1.54it/s]Extractor Estimating: 206it [02:33,  1.55it/s]Extractor Estimating: 207it [02:33,  1.43it/s]Extractor Estimating: 208it [02:34,  1.45it/s]Extractor Estimating: 209it [02:35,  1.48it/s]Extractor Estimating: 210it [02:35,  1.50it/s]Extractor Estimating: 211it [02:36,  1.49it/s]Extractor Estimating: 212it [02:37,  1.43it/s]Extractor Estimating: 213it [02:38,  1.48it/s]Extractor Estimating: 214it [02:38,  1.51it/s]Extractor Estimating: 215it [02:39,  1.48it/s]Extractor Estimating: 216it [02:40,  1.46it/s]Extractor Estimating: 217it [02:40,  1.47it/s]Extractor Estimating: 218it [02:41,  1.47it/s]Extractor Estimating: 219it [02:42,  1.44it/s]Extractor Estimating: 220it [02:42,  1.49it/s]Extractor Estimating: 221it [02:43,  1.55it/s]Extractor Estimating: 222it [02:44,  1.48it/s]Extractor Estimating: 223it [02:44,  1.55it/s]Extractor Estimating: 224it [02:45,  1.54it/s]Extractor Estimating: 225it [02:45,  1.52it/s]Extractor Estimating: 226it [02:46,  1.46it/s]Extractor Estimating: 227it [02:47,  1.47it/s]Extractor Estimating: 228it [02:48,  1.52it/s]Extractor Estimating: 229it [02:48,  1.56it/s]Extractor Estimating: 230it [02:49,  1.54it/s]Extractor Estimating: 231it [02:50,  1.49it/s]Extractor Estimating: 232it [02:50,  1.51it/s]Extractor Estimating: 233it [02:51,  1.51it/s]Extractor Estimating: 234it [02:51,  1.58it/s]Extractor Estimating: 235it [02:52,  1.58it/s]Extractor Estimating: 236it [02:53,  1.57it/s]Extractor Estimating: 237it [02:53,  1.57it/s]Extractor Estimating: 238it [02:54,  1.57it/s]Extractor Estimating: 239it [02:55,  1.57it/s]Extractor Estimating: 240it [02:55,  1.59it/s]Extractor Estimating: 241it [02:56,  1.52it/s]Extractor Estimating: 242it [02:57,  1.55it/s]Extractor Estimating: 243it [02:57,  1.58it/s]Extractor Estimating: 244it [02:58,  1.54it/s]Extractor Estimating: 245it [02:59,  1.42it/s]Extractor Estimating: 246it [02:59,  1.42it/s]Extractor Estimating: 247it [03:00,  1.50it/s]Extractor Estimating: 248it [03:01,  1.47it/s]Extractor Estimating: 249it [03:01,  1.46it/s]Extractor Estimating: 250it [03:02,  1.52it/s]Extractor Estimating: 251it [03:03,  1.51it/s]Extractor Estimating: 252it [03:03,  1.50it/s]Extractor Estimating: 253it [03:04,  1.52it/s]Extractor Estimating: 254it [03:05,  1.56it/s]Extractor Estimating: 255it [03:05,  1.54it/s]Extractor Estimating: 256it [03:06,  1.49it/s]Extractor Estimating: 257it [03:07,  1.53it/s]Extractor Estimating: 258it [03:07,  1.56it/s]Extractor Estimating: 259it [03:08,  1.59it/s]Extractor Estimating: 260it [03:08,  1.56it/s]Extractor Estimating: 261it [03:09,  1.49it/s]Extractor Estimating: 262it [03:10,  1.50it/s]Extractor Estimating: 263it [03:10,  1.53it/s]Extractor Estimating: 264it [03:11,  1.55it/s]Extractor Estimating: 265it [03:12,  1.54it/s]Extractor Estimating: 266it [03:12,  1.45it/s]Extractor Estimating: 267it [03:13,  1.52it/s]Extractor Estimating: 268it [03:14,  1.58it/s]Extractor Estimating: 269it [03:14,  1.67it/s]Extractor Estimating: 270it [03:15,  1.61it/s]Extractor Estimating: 271it [03:15,  1.62it/s]Extractor Estimating: 272it [03:16,  1.59it/s]Extractor Estimating: 273it [03:17,  1.57it/s]Extractor Estimating: 274it [03:17,  1.56it/s]Extractor Estimating: 275it [03:18,  1.53it/s]Extractor Estimating: 276it [03:19,  1.51it/s]Extractor Estimating: 277it [03:19,  1.55it/s]Extractor Estimating: 278it [03:20,  1.54it/s]Extractor Estimating: 279it [03:21,  1.57it/s]Extractor Estimating: 280it [03:21,  1.54it/s]Extractor Estimating: 281it [03:22,  1.58it/s]Extractor Estimating: 282it [03:23,  1.57it/s]Extractor Estimating: 283it [03:23,  1.57it/s]Extractor Estimating: 284it [03:24,  1.56it/s]Extractor Estimating: 285it [03:25,  1.55it/s]Extractor Estimating: 286it [03:25,  1.56it/s]Extractor Estimating: 287it [03:26,  1.54it/s]Extractor Estimating: 288it [03:27,  1.47it/s]Extractor Estimating: 289it [03:27,  1.49it/s]Extractor Estimating: 290it [03:28,  1.49it/s]Extractor Estimating: 291it [03:29,  1.49it/s]Extractor Estimating: 292it [03:29,  1.47it/s]Extractor Estimating: 293it [03:30,  1.47it/s]Extractor Estimating: 294it [03:31,  1.49it/s]Extractor Estimating: 295it [03:31,  1.51it/s]Extractor Estimating: 296it [03:32,  1.47it/s]Extractor Estimating: 297it [03:33,  1.43it/s]Extractor Estimating: 298it [03:33,  1.41it/s]Extractor Estimating: 299it [03:34,  1.43it/s]Extractor Estimating: 300it [03:35,  1.50it/s]Extractor Estimating: 301it [03:35,  1.56it/s]Extractor Estimating: 302it [03:36,  1.62it/s]Extractor Estimating: 303it [03:37,  1.55it/s]Extractor Estimating: 304it [03:37,  1.60it/s]Extractor Estimating: 305it [03:38,  1.61it/s]Extractor Estimating: 306it [03:38,  1.63it/s]Extractor Estimating: 307it [03:39,  1.63it/s]Extractor Estimating: 308it [03:40,  1.60it/s]Extractor Estimating: 309it [03:40,  1.68it/s]Extractor Estimating: 310it [03:41,  1.70it/s]Extractor Estimating: 311it [03:41,  1.69it/s]Extractor Estimating: 312it [03:42,  1.67it/s]Extractor Estimating: 313it [03:42,  1.68it/s]Extractor Estimating: 314it [03:43,  1.63it/s]Extractor Estimating: 315it [03:44,  1.66it/s]Extractor Estimating: 316it [03:44,  1.72it/s]Extractor Estimating: 317it [03:45,  1.72it/s]Extractor Estimating: 318it [03:45,  1.70it/s]Extractor Estimating: 319it [03:46,  1.66it/s]Extractor Estimating: 320it [03:47,  1.64it/s]Extractor Estimating: 321it [03:47,  1.63it/s]Extractor Estimating: 322it [03:48,  1.66it/s]Extractor Estimating: 323it [03:48,  1.70it/s]Extractor Estimating: 324it [03:49,  1.71it/s]Extractor Estimating: 325it [03:50,  1.70it/s]Extractor Estimating: 326it [03:50,  1.69it/s]Extractor Estimating: 327it [03:51,  1.58it/s]Extractor Estimating: 328it [03:52,  1.54it/s]Extractor Estimating: 329it [03:52,  1.52it/s]Extractor Estimating: 330it [03:53,  1.56it/s]Extractor Estimating: 331it [03:54,  1.50it/s]Extractor Estimating: 332it [03:54,  1.57it/s]Extractor Estimating: 333it [03:55,  1.57it/s]Extractor Estimating: 334it [03:56,  1.56it/s]Extractor Estimating: 335it [03:56,  1.57it/s]Extractor Estimating: 336it [03:57,  1.59it/s]Extractor Estimating: 337it [03:57,  1.52it/s]Extractor Estimating: 338it [03:58,  1.52it/s]Extractor Estimating: 339it [03:59,  1.60it/s]Extractor Estimating: 340it [03:59,  1.59it/s]Extractor Estimating: 341it [04:00,  1.61it/s]Extractor Estimating: 342it [04:00,  1.65it/s]Extractor Estimating: 343it [04:01,  1.65it/s]Extractor Estimating: 344it [04:02,  1.65it/s]Extractor Estimating: 345it [04:02,  1.63it/s]Extractor Estimating: 346it [04:03,  1.64it/s]Extractor Estimating: 347it [04:04,  1.67it/s]Extractor Estimating: 348it [04:04,  1.66it/s]Extractor Estimating: 349it [04:05,  1.61it/s]Extractor Estimating: 350it [04:05,  1.65it/s]Extractor Estimating: 351it [04:06,  1.64it/s]Extractor Estimating: 352it [04:07,  1.53it/s]Extractor Estimating: 353it [04:07,  1.54it/s]Extractor Estimating: 354it [04:08,  1.60it/s]Extractor Estimating: 355it [04:09,  1.63it/s]Extractor Estimating: 356it [04:09,  1.65it/s]Extractor Estimating: 357it [04:10,  1.59it/s]Extractor Estimating: 358it [04:10,  1.61it/s]Extractor Estimating: 359it [04:11,  1.63it/s]Extractor Estimating: 360it [04:12,  1.59it/s]Extractor Estimating: 361it [04:12,  1.56it/s]Extractor Estimating: 362it [04:13,  1.56it/s]Extractor Estimating: 363it [04:14,  1.59it/s]Extractor Estimating: 364it [04:14,  1.59it/s]Extractor Estimating: 365it [04:15,  1.55it/s]Extractor Estimating: 366it [04:15,  1.59it/s]Extractor Estimating: 367it [04:16,  1.53it/s]Extractor Estimating: 368it [04:17,  1.55it/s]Extractor Estimating: 369it [04:18,  1.40it/s]Extractor Estimating: 370it [04:18,  1.41it/s]Extractor Estimating: 371it [04:19,  1.45it/s]Extractor Estimating: 372it [04:20,  1.51it/s]Extractor Estimating: 373it [04:20,  1.55it/s]Extractor Estimating: 374it [04:21,  1.61it/s]Extractor Estimating: 375it [04:22,  1.52it/s]Extractor Estimating: 376it [04:22,  1.54it/s]Extractor Estimating: 377it [04:23,  1.54it/s]Extractor Estimating: 378it [04:23,  1.55it/s]Extractor Estimating: 379it [04:24,  1.60it/s]Extractor Estimating: 380it [04:25,  1.55it/s]Extractor Estimating: 381it [04:25,  1.53it/s]Extractor Estimating: 382it [04:26,  1.57it/s]Extractor Estimating: 383it [04:27,  1.58it/s]Extractor Estimating: 384it [04:27,  1.55it/s]Extractor Estimating: 385it [04:28,  1.41it/s]Extractor Estimating: 386it [04:29,  1.46it/s]Extractor Estimating: 387it [04:29,  1.48it/s]Extractor Estimating: 388it [04:30,  1.52it/s]Extractor Estimating: 389it [04:31,  1.50it/s]Extractor Estimating: 390it [04:31,  1.47it/s]Extractor Estimating: 391it [04:32,  1.49it/s]Extractor Estimating: 392it [04:33,  1.50it/s]Extractor Estimating: 393it [04:33,  1.48it/s]Extractor Estimating: 394it [04:34,  1.53it/s]Extractor Estimating: 395it [04:35,  1.53it/s]Extractor Estimating: 396it [04:35,  1.46it/s]Extractor Estimating: 397it [04:36,  1.49it/s]Extractor Estimating: 398it [04:37,  1.48it/s]Extractor Estimating: 399it [04:37,  1.51it/s]Extractor Estimating: 400it [04:38,  1.52it/s]Extractor Estimating: 401it [04:39,  1.55it/s]Extractor Estimating: 402it [04:39,  1.53it/s]Extractor Estimating: 403it [04:40,  1.60it/s]Extractor Estimating: 404it [04:41,  1.57it/s]Extractor Estimating: 405it [04:41,  1.60it/s]Extractor Estimating: 406it [04:42,  1.63it/s]Extractor Estimating: 407it [04:42,  1.61it/s]Extractor Estimating: 408it [04:43,  1.64it/s]Extractor Estimating: 409it [04:44,  1.64it/s]Extractor Estimating: 410it [04:44,  1.57it/s]Extractor Estimating: 411it [04:45,  1.58it/s]Extractor Estimating: 412it [04:45,  1.62it/s]Extractor Estimating: 413it [04:46,  1.61it/s]Extractor Estimating: 414it [04:47,  1.59it/s]Extractor Estimating: 415it [04:47,  1.59it/s]Extractor Estimating: 416it [04:48,  1.55it/s]Extractor Estimating: 417it [04:49,  1.60it/s]Extractor Estimating: 418it [04:49,  1.56it/s]Extractor Estimating: 419it [04:50,  1.59it/s]Extractor Estimating: 420it [04:51,  1.59it/s]Extractor Estimating: 421it [04:51,  1.57it/s]Extractor Estimating: 422it [04:52,  1.51it/s]Extractor Estimating: 423it [04:53,  1.50it/s]Extractor Estimating: 424it [04:53,  1.56it/s]Extractor Estimating: 425it [04:54,  1.55it/s]Extractor Estimating: 426it [04:54,  1.57it/s]Extractor Estimating: 427it [04:55,  1.54it/s]Extractor Estimating: 428it [04:56,  1.56it/s]Extractor Estimating: 429it [04:57,  1.46it/s]Extractor Estimating: 430it [04:57,  1.47it/s]Extractor Estimating: 431it [04:58,  1.47it/s]Extractor Estimating: 432it [04:59,  1.46it/s]Extractor Estimating: 433it [04:59,  1.44it/s]Extractor Estimating: 434it [05:00,  1.44it/s]Extractor Estimating: 435it [05:01,  1.46it/s]Extractor Estimating: 436it [05:01,  1.51it/s]Extractor Estimating: 437it [05:02,  1.46it/s]Extractor Estimating: 438it [05:03,  1.48it/s]Extractor Estimating: 439it [05:03,  1.47it/s]Extractor Estimating: 440it [05:04,  1.48it/s]Extractor Estimating: 441it [05:05,  1.48it/s]Extractor Estimating: 442it [05:05,  1.45it/s]Extractor Estimating: 443it [05:06,  1.48it/s]Extractor Estimating: 444it [05:07,  1.48it/s]Extractor Estimating: 445it [05:07,  1.46it/s]Extractor Estimating: 446it [05:08,  1.47it/s]Extractor Estimating: 447it [05:09,  1.43it/s]Extractor Estimating: 448it [05:10,  1.42it/s]Extractor Estimating: 449it [05:10,  1.41it/s]Extractor Estimating: 450it [05:11,  1.49it/s]Extractor Estimating: 451it [05:11,  1.58it/s]Extractor Estimating: 452it [05:12,  1.53it/s]Extractor Estimating: 453it [05:13,  1.51it/s]Extractor Estimating: 454it [05:13,  1.56it/s]Extractor Estimating: 455it [05:14,  1.60it/s]Extractor Estimating: 456it [05:15,  1.63it/s]Extractor Estimating: 457it [05:15,  1.51it/s]Extractor Estimating: 458it [05:16,  1.51it/s]Extractor Estimating: 459it [05:17,  1.58it/s]Extractor Estimating: 460it [05:17,  1.63it/s]Extractor Estimating: 461it [05:18,  1.61it/s]Extractor Estimating: 462it [05:18,  1.60it/s]Extractor Estimating: 463it [05:19,  1.62it/s]Extractor Estimating: 464it [05:20,  1.59it/s]Extractor Estimating: 465it [05:20,  1.55it/s]Extractor Estimating: 466it [05:21,  1.63it/s]Extractor Estimating: 467it [05:22,  1.61it/s]Extractor Estimating: 468it [05:22,  1.64it/s]Extractor Estimating: 469it [05:23,  1.57it/s]Extractor Estimating: 470it [05:23,  1.59it/s]Extractor Estimating: 471it [05:24,  1.61it/s]Extractor Estimating: 472it [05:25,  1.54it/s]Extractor Estimating: 473it [05:25,  1.55it/s]Extractor Estimating: 474it [05:26,  1.53it/s]Extractor Estimating: 475it [05:27,  1.58it/s]Extractor Estimating: 476it [05:27,  1.56it/s]Extractor Estimating: 477it [05:28,  1.54it/s]Extractor Estimating: 478it [05:29,  1.58it/s]Extractor Estimating: 479it [05:29,  1.46it/s]Extractor Estimating: 480it [05:30,  1.48it/s]Extractor Estimating: 481it [05:31,  1.49it/s]Extractor Estimating: 482it [05:31,  1.53it/s]Extractor Estimating: 483it [05:32,  1.55it/s]Extractor Estimating: 484it [05:33,  1.57it/s]Extractor Estimating: 485it [05:33,  1.56it/s]Extractor Estimating: 486it [05:34,  1.59it/s]Extractor Estimating: 487it [05:34,  1.55it/s]Extractor Estimating: 488it [05:35,  1.55it/s]Extractor Estimating: 489it [05:36,  1.50it/s]Extractor Estimating: 490it [05:36,  1.56it/s]Extractor Estimating: 491it [05:37,  1.57it/s]Extractor Estimating: 492it [05:38,  1.56it/s]Extractor Estimating: 493it [05:38,  1.58it/s]Extractor Estimating: 494it [05:39,  1.55it/s]Extractor Estimating: 495it [05:40,  1.57it/s]Extractor Estimating: 496it [05:40,  1.57it/s]Extractor Estimating: 497it [05:41,  1.47it/s]Extractor Estimating: 498it [05:42,  1.51it/s]Extractor Estimating: 499it [05:42,  1.49it/s]Extractor Estimating: 500it [05:43,  1.60it/s]Extractor Estimating: 500it [05:43,  1.46it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 2030 mean pseudo reward: 0.9539198376204244
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 15286
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15386, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15386, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 15, avg_time 1.287, loss:352.7563
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 30, avg_time 0.980, loss:289.4299
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 45, avg_time 0.980, loss:237.0315
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 60, avg_time 0.988, loss:212.9501
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 75, avg_time 0.977, loss:190.6417
>> valid entity prec:0.5962, rec:0.5056, f1:0.5472
>> valid relation prec:0.1266, rec:0.0546, f1:0.0763
>> valid relation with NER prec:0.1266, rec:0.0546, f1:0.0763
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 5, avg_time 2.273, loss:184.5487
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 20, avg_time 0.990, loss:171.9589
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 35, avg_time 0.977, loss:185.0661
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 50, avg_time 0.968, loss:170.3979
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 65, avg_time 0.989, loss:182.4411
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5582, rec:0.4789, f1:0.5155
>> valid relation prec:0.1393, rec:0.0638, f1:0.0875
>> valid relation with NER prec:0.1393, rec:0.0638, f1:0.0875
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 80, avg_time 2.253, loss:169.4970
g_step 1200, step 10, avg_time 0.987, loss:155.8606
g_step 1300, step 25, avg_time 0.990, loss:145.6624
g_step 1400, step 40, avg_time 0.985, loss:142.4005
g_step 1500, step 55, avg_time 0.982, loss:142.8591
>> valid entity prec:0.5915, rec:0.5153, f1:0.5508
>> valid relation prec:0.1222, rec:0.0600, f1:0.0805
>> valid relation with NER prec:0.1222, rec:0.0600, f1:0.0805
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 70, avg_time 2.273, loss:130.5882
g_step 1700, step 85, avg_time 0.987, loss:140.3501
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:13:56 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:13:56 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-13-56_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:13:57 - WARNING - datasets.builder -   Using custom data configuration default-ae6a25ad221d8e50
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ae6a25ad221d8e50/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:13:59,298 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:13:59,322 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:13:59,322 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:13:59,323 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:13:59,371 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,403 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,404 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,404 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,404 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,404 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:13:59,404 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:13:59,735 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:14:02,915 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:14:02,969 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ae6a25ad221d8e50/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:14:02 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14bd336405f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:01,  1.08ba/s] 67%|██████▋   | 2/3 [00:01<00:00,  2.01ba/s]100%|██████████| 3/3 [00:01<00:00,  2.64ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.45ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.81ba/s]100%|██████████| 4/4 [00:00<00:00,  4.95ba/s]100%|██████████| 4/4 [00:00<00:00,  4.17ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  4.21ba/s]100%|██████████| 3/3 [00:00<00:00, 10.05ba/s]100%|██████████| 3/3 [00:00<00:00,  8.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.40ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.69ba/s]100%|██████████| 4/4 [00:00<00:00,  8.08ba/s]100%|██████████| 4/4 [00:00<00:00,  6.99ba/s]
[INFO|trainer.py:414] 2023-08-29 00:14:07,896 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:14:08,060 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:14:08,061 >>   Num examples = 2030
[INFO|trainer.py:1149] 2023-08-29 00:14:08,061 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:14:08,061 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:14:08,061 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:14:08,061 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:14:08,061 >>   Total optimization steps = 160
  0%|          | 0/160 [00:00<?, ?it/s]  1%|          | 1/160 [00:00<00:48,  3.29it/s]  1%|▏         | 2/160 [00:00<00:46,  3.39it/s]  2%|▏         | 3/160 [00:00<00:45,  3.42it/s]  2%|▎         | 4/160 [00:01<00:49,  3.15it/s]  3%|▎         | 5/160 [00:01<00:51,  2.99it/s]  4%|▍         | 6/160 [00:01<00:49,  3.12it/s]  4%|▍         | 7/160 [00:02<00:47,  3.20it/s]  5%|▌         | 8/160 [00:02<00:46,  3.26it/s]  6%|▌         | 9/160 [00:02<00:45,  3.30it/s]  6%|▋         | 10/160 [00:03<00:45,  3.33it/s]  7%|▋         | 11/160 [00:03<00:44,  3.35it/s]  8%|▊         | 12/160 [00:03<00:43,  3.37it/s]  8%|▊         | 13/160 [00:03<00:43,  3.38it/s]  9%|▉         | 14/160 [00:04<00:43,  3.39it/s]  9%|▉         | 15/160 [00:04<00:49,  2.94it/s] 10%|█         | 16/160 [00:04<00:46,  3.07it/s] 11%|█         | 17/160 [00:05<00:45,  3.16it/s] 11%|█▏        | 18/160 [00:05<00:43,  3.23it/s] 12%|█▏        | 19/160 [00:05<00:42,  3.28it/s] 12%|█▎        | 20/160 [00:06<00:42,  3.32it/s] 13%|█▎        | 21/160 [00:06<00:41,  3.35it/s] 14%|█▍        | 22/160 [00:07<01:01,  2.23it/s] 14%|█▍        | 23/160 [00:07<00:55,  2.49it/s] 15%|█▌        | 24/160 [00:07<00:52,  2.58it/s] 16%|█▌        | 25/160 [00:08<00:48,  2.78it/s] 16%|█▋        | 26/160 [00:08<00:45,  2.94it/s] 17%|█▋        | 27/160 [00:08<00:43,  3.07it/s] 18%|█▊        | 28/160 [00:09<00:41,  3.16it/s] 18%|█▊        | 29/160 [00:09<00:40,  3.23it/s] 19%|█▉        | 30/160 [00:09<00:39,  3.28it/s] 19%|█▉        | 31/160 [00:09<00:38,  3.31it/s] 20%|██        | 32/160 [00:10<00:35,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 00:14:18,244 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:14:18,244 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:14:18,244 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.46it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.25it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.55it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.22it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.27it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.15it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.13it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.39it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.54it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.84it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.92it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.81it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.62it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.41it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.44it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.51it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.57it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.85it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.90it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.95it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.78it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.58it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.44it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.44it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.52it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.60it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.82it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.92it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 41.66it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 42.68it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.22it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.54it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.79it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.25it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.49it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.28it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.51it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.70it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.59it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.62it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.55it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.63it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.69it/s][A
 54%|█████▍    | 237/436 [00:05<00:05, 39.73it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 42.02it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 40.72it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 41.99it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 42.96it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.64it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.11it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.26it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.28it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.92it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.88it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 42.34it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.18it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.77it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.16it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.52it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.74it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.67it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.41it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.13it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.38it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.53it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.76it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.91it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.99it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.85it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.57it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.34it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.13it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.43it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.63it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.79it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.94it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.91it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.80it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.48it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.31it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 42.68it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.34it/s][A                                                
                                                 [A 20%|██        | 32/160 [00:20<00:35,  3.59it/s]
100%|██████████| 436/436 [00:09<00:00, 43.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:14:28,521 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32
[INFO|configuration_utils.py:351] 2023-08-29 00:14:28,790 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:14:32,575 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:14:32,767 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:14:32,855 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32/special_tokens_map.json
 21%|██        | 33/160 [00:32<14:26,  6.82s/it] 21%|██▏       | 34/160 [00:32<10:13,  4.87s/it] 22%|██▏       | 35/160 [00:32<07:17,  3.50s/it] 22%|██▎       | 36/160 [00:33<05:14,  2.54s/it] 23%|██▎       | 37/160 [00:33<03:49,  1.86s/it] 24%|██▍       | 38/160 [00:33<02:49,  1.39s/it] 24%|██▍       | 39/160 [00:34<02:08,  1.06s/it] 25%|██▌       | 40/160 [00:34<01:39,  1.20it/s] 26%|██▌       | 41/160 [00:34<01:19,  1.49it/s] 26%|██▋       | 42/160 [00:34<01:05,  1.80it/s] 27%|██▋       | 43/160 [00:35<00:55,  2.09it/s] 28%|██▊       | 44/160 [00:35<00:48,  2.37it/s] 28%|██▊       | 45/160 [00:35<00:44,  2.56it/s] 29%|██▉       | 46/160 [00:36<00:41,  2.77it/s] 29%|██▉       | 47/160 [00:36<00:38,  2.93it/s] 30%|███       | 48/160 [00:36<00:36,  3.06it/s] 31%|███       | 49/160 [00:36<00:35,  3.16it/s] 31%|███▏      | 50/160 [00:37<00:34,  3.23it/s] 32%|███▏      | 51/160 [00:37<00:33,  3.28it/s] 32%|███▎      | 52/160 [00:37<00:32,  3.32it/s] 33%|███▎      | 53/160 [00:38<00:31,  3.35it/s] 34%|███▍      | 54/160 [00:38<00:31,  3.37it/s] 34%|███▍      | 55/160 [00:38<00:31,  3.38it/s] 35%|███▌      | 56/160 [00:39<00:31,  3.30it/s] 36%|███▌      | 57/160 [00:39<00:30,  3.33it/s] 36%|███▋      | 58/160 [00:39<00:30,  3.36it/s] 37%|███▋      | 59/160 [00:39<00:29,  3.37it/s] 38%|███▊      | 60/160 [00:40<00:29,  3.38it/s] 38%|███▊      | 61/160 [00:40<00:29,  3.39it/s] 39%|███▉      | 62/160 [00:40<00:28,  3.40it/s] 39%|███▉      | 63/160 [00:41<00:28,  3.40it/s] 40%|████      | 64/160 [00:41<00:26,  3.67it/s][INFO|trainer.py:2140] 2023-08-29 00:14:49,417 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:14:49,417 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:14:49,417 >>   Batch size = 8
{'eval_loss': 1.0832808017730713, 'eval_runtime': 9.8778, 'eval_samples_per_second': 352.507, 'eval_steps_per_second': 44.139, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.34it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.25it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.63it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.70it/s][A
  6%|▌         | 27/436 [00:00<00:09, 42.98it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.69it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.97it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.38it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.60it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.60it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.45it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.21it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.31it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.52it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.64it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.52it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.62it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.72it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.43it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.34it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.42it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.65it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.69it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.70it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.69it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.76it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.75it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.48it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 40.80it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 42.12it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 42.96it/s][A
 41%|████      | 177/436 [00:03<00:05, 43.64it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.00it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.21it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.20it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.98it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.09it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.28it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.63it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.72it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.86it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.38it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.30it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.23it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.46it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.68it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.76it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.86it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.88it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.70it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.41it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.33it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.43it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 35.89it/s][A
 70%|███████   | 307/436 [00:07<00:03, 38.31it/s][A
 72%|███████▏  | 312/436 [00:07<00:03, 40.13it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 41.54it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 42.59it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.36it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.88it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.29it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.97it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.78it/s][A
 81%|████████  | 352/436 [00:08<00:01, 43.85it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.27it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.54it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.76it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.86it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.82it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.59it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.27it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.18it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.18it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.36it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.58it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.71it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.87it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 41.76it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 42.71it/s][A                                                
                                                 [A 40%|████      | 64/160 [00:51<00:26,  3.67it/s]
100%|██████████| 436/436 [00:09<00:00, 42.71it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:14:59,613 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64
[INFO|configuration_utils.py:351] 2023-08-29 00:14:59,877 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:15:03,689 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:15:03,813 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:15:03,883 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64/special_tokens_map.json
 41%|████      | 65/160 [01:08<13:05,  8.27s/it] 41%|████▏     | 66/160 [01:08<09:14,  5.90s/it] 42%|████▏     | 67/160 [01:08<06:32,  4.22s/it] 42%|████▎     | 68/160 [01:09<04:39,  3.04s/it] 43%|████▎     | 69/160 [01:09<03:21,  2.22s/it] 44%|████▍     | 70/160 [01:09<02:27,  1.64s/it] 44%|████▍     | 71/160 [01:10<01:49,  1.24s/it] 45%|████▌     | 72/160 [01:10<01:23,  1.05it/s] 46%|████▌     | 73/160 [01:10<01:05,  1.32it/s] 46%|████▋     | 74/160 [01:10<00:53,  1.62it/s] 47%|████▋     | 75/160 [01:11<00:44,  1.93it/s] 48%|████▊     | 76/160 [01:11<00:39,  2.14it/s] 48%|████▊     | 77/160 [01:11<00:34,  2.40it/s] 49%|████▉     | 78/160 [01:12<00:31,  2.64it/s] 49%|████▉     | 79/160 [01:12<00:28,  2.83it/s] 50%|█████     | 80/160 [01:12<00:26,  2.99it/s] 51%|█████     | 81/160 [01:13<00:25,  3.12it/s] 51%|█████▏    | 82/160 [01:13<00:24,  3.21it/s] 52%|█████▏    | 83/160 [01:13<00:23,  3.29it/s] 52%|█████▎    | 84/160 [01:13<00:22,  3.33it/s] 53%|█████▎    | 85/160 [01:14<00:22,  3.37it/s] 54%|█████▍    | 86/160 [01:14<00:21,  3.40it/s] 54%|█████▍    | 87/160 [01:14<00:23,  3.10it/s] 55%|█████▌    | 88/160 [01:15<00:22,  3.20it/s] 56%|█████▌    | 89/160 [01:15<00:22,  3.19it/s] 56%|█████▋    | 90/160 [01:15<00:21,  3.27it/s] 57%|█████▋    | 91/160 [01:16<00:20,  3.32it/s] 57%|█████▊    | 92/160 [01:16<00:20,  3.36it/s] 58%|█████▊    | 93/160 [01:16<00:19,  3.39it/s] 59%|█████▉    | 94/160 [01:16<00:19,  3.41it/s] 59%|█████▉    | 95/160 [01:17<00:18,  3.43it/s] 60%|██████    | 96/160 [01:17<00:17,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 00:15:25,556 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:15:25,556 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:15:25,556 >>   Batch size = 8
{'eval_loss': 1.0884395837783813, 'eval_runtime': 9.9317, 'eval_samples_per_second': 350.596, 'eval_steps_per_second': 43.9, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.89it/s][A
  3%|▎         | 12/436 [00:00<00:09, 45.41it/s][A
  4%|▍         | 17/436 [00:00<00:09, 45.06it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.81it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.72it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.68it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.54it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.54it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.78it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.77it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.57it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.63it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.60it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.58it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.59it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.51it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.78it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.72it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.60it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.56it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.56it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.58it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.60it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.66it/s][A
 34%|███▎      | 147/436 [00:03<00:07, 39.92it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 41.44it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 42.48it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.23it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.61it/s][A
 39%|███▉      | 172/436 [00:03<00:06, 43.75it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.04it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.99it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.11it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.25it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.57it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.79it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.80it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.69it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.53it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.29it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.32it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.31it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.49it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.68it/s][A
 59%|█████▉    | 257/436 [00:05<00:03, 44.84it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.80it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.65it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.27it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.31it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.42it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.60it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.69it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.89it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.81it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.67it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.47it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.34it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.31it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.44it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.51it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.76it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.80it/s][A
 81%|████████  | 352/436 [00:07<00:01, 42.38it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.11it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.45it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.72it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.96it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.13it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.37it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.55it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.38it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.46it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.58it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.53it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.47it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.31it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.49it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.58it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.56it/s][A                                                
                                                 [A 60%|██████    | 96/160 [01:27<00:17,  3.70it/s]
100%|██████████| 436/436 [00:09<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:15:35,530 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-29 00:15:35,654 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:15:39,159 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:15:39,366 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:15:39,446 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96/special_tokens_map.json
 61%|██████    | 97/160 [01:38<06:50,  6.52s/it] 61%|██████▏   | 98/160 [01:38<04:49,  4.66s/it] 62%|██████▏   | 99/160 [01:39<03:24,  3.35s/it] 62%|██████▎   | 100/160 [01:39<02:26,  2.43s/it] 63%|██████▎   | 101/160 [01:39<01:45,  1.79s/it] 64%|██████▍   | 102/160 [01:40<01:17,  1.34s/it] 64%|██████▍   | 103/160 [01:40<00:58,  1.03s/it] 65%|██████▌   | 104/160 [01:40<00:45,  1.24it/s] 66%|██████▌   | 105/160 [01:40<00:35,  1.53it/s] 66%|██████▋   | 106/160 [01:41<00:29,  1.84it/s] 67%|██████▋   | 107/160 [01:41<00:24,  2.14it/s] 68%|██████▊   | 108/160 [01:41<00:21,  2.42it/s] 68%|██████▊   | 109/160 [01:42<00:19,  2.60it/s] 69%|██████▉   | 110/160 [01:42<00:17,  2.81it/s] 69%|██████▉   | 111/160 [01:42<00:16,  2.98it/s] 70%|███████   | 112/160 [01:43<00:15,  3.11it/s] 71%|███████   | 113/160 [01:43<00:14,  3.21it/s] 71%|███████▏  | 114/160 [01:43<00:14,  3.28it/s] 72%|███████▏  | 115/160 [01:43<00:13,  3.33it/s] 72%|███████▎  | 116/160 [01:44<00:13,  3.37it/s] 73%|███████▎  | 117/160 [01:44<00:12,  3.39it/s] 74%|███████▍  | 118/160 [01:44<00:12,  3.41it/s] 74%|███████▍  | 119/160 [01:45<00:11,  3.43it/s] 75%|███████▌  | 120/160 [01:45<00:11,  3.34it/s] 76%|███████▌  | 121/160 [01:45<00:11,  3.38it/s] 76%|███████▋  | 122/160 [01:45<00:11,  3.40it/s] 77%|███████▋  | 123/160 [01:46<00:10,  3.42it/s] 78%|███████▊  | 124/160 [01:46<00:10,  3.43it/s] 78%|███████▊  | 125/160 [01:46<00:10,  3.44it/s] 79%|███████▉  | 126/160 [01:47<00:09,  3.44it/s] 79%|███████▉  | 127/160 [01:47<00:09,  3.44it/s] 80%|████████  | 128/160 [01:47<00:08,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 00:15:55,679 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:15:55,679 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:15:55,680 >>   Batch size = 8
{'eval_loss': 1.0985291004180908, 'eval_runtime': 9.8424, 'eval_samples_per_second': 353.777, 'eval_steps_per_second': 44.298, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.87it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.38it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.58it/s][A
  5%|▌         | 22/436 [00:00<00:10, 40.26it/s][A
  6%|▌         | 27/436 [00:00<00:12, 32.83it/s][A
  7%|▋         | 32/436 [00:00<00:11, 36.21it/s][A
  8%|▊         | 37/436 [00:00<00:10, 38.67it/s][A
 10%|▉         | 42/436 [00:01<00:09, 40.50it/s][A
 11%|█         | 47/436 [00:01<00:09, 41.77it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 42.82it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 43.55it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.88it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.69it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.67it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.80it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.07it/s][A
 20%|█▉        | 87/436 [00:02<00:07, 44.30it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.57it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.80it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.94it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.82it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.49it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.18it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.25it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.28it/s][A
 30%|███       | 132/436 [00:03<00:06, 44.54it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.66it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.86it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.73it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.22it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.19it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.25it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.37it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.39it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.69it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.84it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.91it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.63it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.45it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.35it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.35it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.43it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.48it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.57it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.60it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.81it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 39.68it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 41.23it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 42.32it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.09it/s][A
 60%|██████    | 262/436 [00:06<00:03, 43.55it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.90it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.06it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.29it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.01it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.28it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.46it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.67it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.75it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.71it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.62it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.53it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.35it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.31it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.34it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.58it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.79it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.78it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.62it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.64it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.50it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.35it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 42.33it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.12it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 43.74it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.10it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.35it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.29it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.33it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.33it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.10it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.21it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.42it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.47it/s][A                                                 
                                                 [A 80%|████████  | 128/160 [01:57<00:08,  3.71it/s]
100%|██████████| 436/436 [00:09<00:00, 44.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:16:05,945 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128
[INFO|configuration_utils.py:351] 2023-08-29 00:16:06,222 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:16:10,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:16:11,197 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:16:11,362 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128/special_tokens_map.json
 81%|████████  | 129/160 [02:14<04:13,  8.16s/it] 81%|████████▏ | 130/160 [02:14<02:54,  5.81s/it] 82%|████████▏ | 131/160 [02:14<02:00,  4.15s/it] 82%|████████▎ | 132/160 [02:15<01:23,  3.00s/it] 83%|████████▎ | 133/160 [02:15<00:58,  2.18s/it] 84%|████████▍ | 134/160 [02:15<00:42,  1.63s/it] 84%|████████▍ | 135/160 [02:16<00:30,  1.23s/it] 85%|████████▌ | 136/160 [02:16<00:22,  1.05it/s] 86%|████████▌ | 137/160 [02:16<00:17,  1.33it/s] 86%|████████▋ | 138/160 [02:16<00:13,  1.63it/s] 87%|████████▋ | 139/160 [02:17<00:10,  1.93it/s] 88%|████████▊ | 140/160 [02:17<00:09,  2.22it/s] 88%|████████▊ | 141/160 [02:17<00:07,  2.43it/s] 89%|████████▉ | 142/160 [02:18<00:06,  2.66it/s] 89%|████████▉ | 143/160 [02:18<00:05,  2.85it/s] 90%|█████████ | 144/160 [02:18<00:05,  3.00it/s] 91%|█████████ | 145/160 [02:18<00:04,  3.11it/s] 91%|█████████▏| 146/160 [02:19<00:04,  3.20it/s] 92%|█████████▏| 147/160 [02:19<00:03,  3.26it/s] 92%|█████████▎| 148/160 [02:19<00:03,  3.31it/s] 93%|█████████▎| 149/160 [02:20<00:03,  3.34it/s] 94%|█████████▍| 150/160 [02:20<00:02,  3.36it/s] 94%|█████████▍| 151/160 [02:20<00:02,  3.37it/s] 95%|█████████▌| 152/160 [02:21<00:02,  3.28it/s] 96%|█████████▌| 153/160 [02:21<00:02,  3.32it/s] 96%|█████████▋| 154/160 [02:21<00:01,  3.34it/s] 97%|█████████▋| 155/160 [02:21<00:01,  3.36it/s] 98%|█████████▊| 156/160 [02:22<00:01,  3.37it/s] 98%|█████████▊| 157/160 [02:22<00:00,  3.38it/s] 99%|█████████▉| 158/160 [02:22<00:00,  3.39it/s] 99%|█████████▉| 159/160 [02:23<00:00,  3.40it/s]100%|██████████| 160/160 [02:23<00:00,  3.67it/s][INFO|trainer.py:2140] 2023-08-29 00:16:31,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:16:31,403 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:16:31,403 >>   Batch size = 8
{'eval_loss': 1.1065291166305542, 'eval_runtime': 9.9594, 'eval_samples_per_second': 349.62, 'eval_steps_per_second': 43.778, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.71it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.27it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.78it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.80it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.20it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.07it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.91it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.77it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.91it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.94it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.80it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.75it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.57it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.60it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.63it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.66it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.56it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.69it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.60it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.60it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.56it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.56it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.63it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.64it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.61it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.57it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.69it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.76it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.63it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.61it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.27it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.85it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.15it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.21it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.37it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.56it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.56it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.54it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.32it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.35it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.57it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.63it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.70it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.72it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.61it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.40it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.46it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.54it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.61it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.66it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.69it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.73it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.71it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.55it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.57it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 41.56it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 42.59it/s][A
 70%|███████   | 307/436 [00:06<00:02, 43.31it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 43.71it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 43.96it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.20it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.38it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.36it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.17it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.37it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.45it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.72it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.68it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.74it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.64it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.41it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.34it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.41it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.39it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.66it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.66it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.75it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.76it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.63it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.61it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.41it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 40.26it/s][A                                                 
                                                 [A100%|██████████| 160/160 [02:33<00:00,  3.67it/s]
100%|██████████| 436/436 [00:09<00:00, 40.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:16:41,482 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-29 00:16:41,720 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:16:45,436 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:16:45,622 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:16:45,688 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:16:54,075 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:16:54,120 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32 (score: 1.0832808017730713).
                                                 100%|██████████| 160/160 [02:57<00:00,  3.67it/s]100%|██████████| 160/160 [02:57<00:00,  1.11s/it]
[INFO|trainer.py:1894] 2023-08-29 00:17:05,368 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 00:17:05,545 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:17:09,709 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:17:10,108 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:17:10,259 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:17:11,241 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   train_loss               =     0.5735
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   train_runtime            = 0:02:57.04
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   train_samples            =       2030
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   train_samples_per_second =      57.33
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:11,241 >>   train_steps_per_second   =      0.904
{'eval_loss': 1.1096229553222656, 'eval_runtime': 9.8395, 'eval_samples_per_second': 353.88, 'eval_steps_per_second': 44.311, 'epoch': 5.0}
{'train_runtime': 177.0462, 'train_samples_per_second': 57.33, 'train_steps_per_second': 0.904, 'train_loss': 0.5735375881195068, 'epoch': 5.0}
08/29/2023 00:17:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:17:11,697 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:17:11,697 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:17:11,697 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.68it/s]  3%|▎         | 12/436 [00:00<00:08, 49.20it/s]  4%|▍         | 17/436 [00:00<00:08, 47.66it/s]  5%|▌         | 22/436 [00:00<00:08, 46.69it/s]  6%|▌         | 27/436 [00:00<00:08, 46.33it/s]  7%|▋         | 32/436 [00:00<00:08, 46.02it/s]  8%|▊         | 37/436 [00:00<00:08, 45.86it/s] 10%|▉         | 42/436 [00:00<00:08, 45.39it/s] 11%|█         | 47/436 [00:01<00:08, 44.69it/s] 12%|█▏        | 52/436 [00:01<00:11, 34.66it/s] 13%|█▎        | 57/436 [00:01<00:10, 37.35it/s] 14%|█▍        | 62/436 [00:01<00:09, 39.49it/s] 15%|█▌        | 67/436 [00:01<00:08, 41.12it/s] 17%|█▋        | 72/436 [00:01<00:08, 42.30it/s] 18%|█▊        | 77/436 [00:01<00:08, 43.22it/s] 19%|█▉        | 82/436 [00:01<00:08, 43.78it/s] 20%|█▉        | 87/436 [00:02<00:07, 44.11it/s] 21%|██        | 92/436 [00:02<00:07, 43.91it/s] 22%|██▏       | 97/436 [00:02<00:07, 43.81it/s] 23%|██▎       | 102/436 [00:02<00:07, 43.90it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.23it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.57it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.82it/s] 28%|██▊       | 122/436 [00:02<00:06, 44.95it/s] 29%|██▉       | 127/436 [00:02<00:06, 45.07it/s] 30%|███       | 132/436 [00:03<00:06, 44.85it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.55it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.30it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.22it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.36it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.57it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.79it/s] 38%|███▊      | 167/436 [00:03<00:05, 44.99it/s] 39%|███▉      | 172/436 [00:03<00:05, 45.07it/s] 41%|████      | 177/436 [00:04<00:05, 44.96it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.61it/s] 43%|████▎     | 187/436 [00:04<00:07, 35.19it/s] 44%|████▍     | 192/436 [00:04<00:06, 37.75it/s] 45%|████▌     | 197/436 [00:04<00:06, 39.77it/s] 46%|████▋     | 202/436 [00:04<00:05, 41.34it/s] 47%|████▋     | 207/436 [00:04<00:05, 42.49it/s] 49%|████▊     | 212/436 [00:04<00:05, 43.34it/s] 50%|████▉     | 217/436 [00:05<00:04, 43.88it/s] 51%|█████     | 222/436 [00:05<00:04, 44.13it/s] 52%|█████▏    | 227/436 [00:05<00:04, 43.86it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.66it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.94it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.33it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.67it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.89it/s] 59%|█████▉    | 257/436 [00:05<00:03, 45.03it/s] 60%|██████    | 262/436 [00:06<00:03, 44.98it/s] 61%|██████    | 267/436 [00:06<00:03, 44.76it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.43it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.27it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.22it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.54it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.76it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.90it/s] 69%|██████▉   | 302/436 [00:06<00:02, 44.92it/s] 70%|███████   | 307/436 [00:07<00:02, 44.99it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.70it/s] 73%|███████▎  | 317/436 [00:07<00:03, 37.40it/s] 74%|███████▍  | 322/436 [00:07<00:02, 39.47it/s] 75%|███████▌  | 327/436 [00:07<00:02, 41.13it/s] 76%|███████▌  | 332/436 [00:07<00:02, 42.23it/s] 77%|███████▋  | 337/436 [00:07<00:02, 43.10it/s] 78%|███████▊  | 342/436 [00:07<00:02, 43.67it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.11it/s] 81%|████████  | 352/436 [00:08<00:01, 44.28it/s] 82%|████████▏ | 357/436 [00:08<00:01, 43.97it/s] 83%|████████▎ | 362/436 [00:08<00:01, 43.81it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.07it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.35it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.62it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.81it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.86it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.95it/s] 91%|█████████ | 397/436 [00:09<00:00, 44.69it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.44it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.22it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.32it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.58it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.73it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.81it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.85it/s]100%|██████████| 436/436 [00:10<00:00, 42.92it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:17:21,906 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   eval_loss               =     1.0833
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   eval_runtime            = 0:00:10.17
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   eval_samples_per_second =    342.276
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   eval_steps_per_second   =     42.858
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:17:21,906 >>   perplexity              =     2.9544
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:32,062 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:32,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:32,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:32,096 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:32,096 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:17:32,565 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:17:32,566 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:17:32,875 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:17:34,033 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:17:34,033 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:35,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:35,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:35,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:35,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:17:35,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:17:36,306 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:17:36,308 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:17:36,597 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:17:36,824 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:17:36,824 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-160
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-32
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-64
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/checkpoint-128
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.62it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.44it/s]Extractor Predicting: 33it [00:21,  1.46it/s]Extractor Predicting: 34it [00:21,  1.46it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.49it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:33,  1.47it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.52it/s]Extractor Predicting: 70it [00:45,  1.56it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:46,  1.58it/s]Extractor Predicting: 73it [00:47,  1.64it/s]Extractor Predicting: 74it [00:48,  1.62it/s]Extractor Predicting: 75it [00:48,  1.59it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:50,  1.56it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:51,  1.53it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.53it/s]Extractor Predicting: 83it [00:53,  1.55it/s]Extractor Predicting: 84it [00:54,  1.50it/s]Extractor Predicting: 85it [00:55,  1.56it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.49it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:01,  1.49it/s]Extractor Predicting: 95it [01:01,  1.53it/s]Extractor Predicting: 96it [01:02,  1.54it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:04,  1.47it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:05,  1.49it/s]Extractor Predicting: 102it [01:06,  1.36it/s]Extractor Predicting: 103it [01:07,  1.39it/s]Extractor Predicting: 104it [01:08,  1.45it/s]Extractor Predicting: 105it [01:08,  1.47it/s]Extractor Predicting: 106it [01:09,  1.44it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:10,  1.50it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:12,  1.59it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:13,  1.57it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:15,  1.51it/s]Extractor Predicting: 117it [01:16,  1.51it/s]Extractor Predicting: 118it [01:17,  1.50it/s]Extractor Predicting: 119it [01:17,  1.51it/s]Extractor Predicting: 120it [01:18,  1.55it/s]Extractor Predicting: 121it [01:19,  1.51it/s]Extractor Predicting: 122it [01:19,  1.54it/s]Extractor Predicting: 123it [01:20,  1.51it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:21,  1.53it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:23,  1.52it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:25,  1.49it/s]Extractor Predicting: 131it [01:25,  1.51it/s]Extractor Predicting: 132it [01:26,  1.53it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:27,  1.45it/s]Extractor Predicting: 135it [01:28,  1.47it/s]Extractor Predicting: 136it [01:29,  1.48it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:31,  1.48it/s]Extractor Predicting: 140it [01:31,  1.46it/s]Extractor Predicting: 141it [01:32,  1.45it/s]Extractor Predicting: 142it [01:33,  1.49it/s]Extractor Predicting: 143it [01:33,  1.55it/s]Extractor Predicting: 143it [01:33,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:21,278 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:21,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:21,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:21,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:21,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:19:22,024 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:19:22,025 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:19:22,289 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:19:23,361 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:19:23,361 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:25,778 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:25,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:25,785 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:25,786 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:19:25,786 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:19:26,147 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:19:26,148 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:19:26,412 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:19:26,583 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:19:26,583 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2287157287157287,
  "recall": 0.0910396323951752,
  "score": 0.13023829087921115,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:13,  1.61it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:20,  1.54it/s]Extractor Predicting: 35it [00:21,  1.53it/s]Extractor Predicting: 36it [00:22,  1.52it/s]Extractor Predicting: 37it [00:22,  1.52it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:24,  1.53it/s]Extractor Predicting: 40it [00:24,  1.52it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:26,  1.50it/s]Extractor Predicting: 44it [00:27,  1.52it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:28,  1.52it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.54it/s]Extractor Predicting: 51it [00:31,  1.52it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:38,  1.52it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:41,  1.53it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.58it/s]Extractor Predicting: 71it [00:44,  1.53it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:49,  1.62it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:51,  1.58it/s]Extractor Predicting: 82it [00:51,  1.60it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:53,  1.55it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.42it/s]Extractor Predicting: 88it [00:55,  1.42it/s]Extractor Predicting: 89it [00:56,  1.50it/s]Extractor Predicting: 90it [00:57,  1.54it/s]Extractor Predicting: 91it [00:57,  1.56it/s]Extractor Predicting: 92it [00:58,  1.54it/s]Extractor Predicting: 93it [00:59,  1.52it/s]Extractor Predicting: 94it [00:59,  1.52it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:01,  1.53it/s]Extractor Predicting: 98it [01:02,  1.51it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.52it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:06,  1.53it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:08,  1.53it/s]Extractor Predicting: 108it [01:08,  1.53it/s]Extractor Predicting: 109it [01:09,  1.53it/s]Extractor Predicting: 110it [01:10,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:11,  1.51it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:12,  1.52it/s]Extractor Predicting: 115it [01:13,  1.52it/s]Extractor Predicting: 116it [01:14,  1.58it/s]Extractor Predicting: 117it [01:14,  1.67it/s]Extractor Predicting: 118it [01:15,  1.69it/s]Extractor Predicting: 119it [01:15,  1.68it/s]Extractor Predicting: 120it [01:16,  1.67it/s]Extractor Predicting: 121it [01:16,  1.65it/s]Extractor Predicting: 122it [01:17,  1.63it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:18,  1.63it/s]Extractor Predicting: 125it [01:19,  1.70it/s]Extractor Predicting: 126it [01:20,  1.68it/s]Extractor Predicting: 127it [01:20,  1.70it/s]Extractor Predicting: 128it [01:21,  1.71it/s]Extractor Predicting: 129it [01:21,  1.69it/s]Extractor Predicting: 130it [01:22,  1.68it/s]Extractor Predicting: 131it [01:22,  1.68it/s]Extractor Predicting: 132it [01:23,  1.69it/s]Extractor Predicting: 133it [01:24,  1.68it/s]Extractor Predicting: 134it [01:24,  1.70it/s]Extractor Predicting: 135it [01:25,  1.72it/s]Extractor Predicting: 136it [01:25,  1.75it/s]Extractor Predicting: 137it [01:26,  1.74it/s]Extractor Predicting: 138it [01:27,  1.71it/s]Extractor Predicting: 139it [01:27,  1.68it/s]Extractor Predicting: 140it [01:28,  1.70it/s]Extractor Predicting: 141it [01:28,  1.66it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:30,  1.67it/s]Extractor Predicting: 144it [01:30,  1.64it/s]Extractor Predicting: 145it [01:31,  1.65it/s]Extractor Predicting: 146it [01:31,  1.67it/s]Extractor Predicting: 147it [01:32,  1.65it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:33,  1.69it/s]Extractor Predicting: 150it [01:34,  1.67it/s]Extractor Predicting: 151it [01:34,  1.65it/s]Extractor Predicting: 152it [01:35,  1.66it/s]Extractor Predicting: 153it [01:36,  1.71it/s]Extractor Predicting: 154it [01:36,  1.72it/s]Extractor Predicting: 155it [01:37,  1.72it/s]Extractor Predicting: 156it [01:37,  1.70it/s]Extractor Predicting: 157it [01:38,  1.68it/s]Extractor Predicting: 158it [01:39,  1.68it/s]Extractor Predicting: 159it [01:39,  1.64it/s]Extractor Predicting: 160it [01:40,  1.65it/s]Extractor Predicting: 161it [01:40,  1.66it/s]Extractor Predicting: 162it [01:41,  1.65it/s]Extractor Predicting: 163it [01:42,  1.65it/s]Extractor Predicting: 164it [01:42,  1.65it/s]Extractor Predicting: 165it [01:43,  1.67it/s]Extractor Predicting: 166it [01:43,  1.67it/s]Extractor Predicting: 167it [01:44,  1.68it/s]Extractor Predicting: 168it [01:45,  1.64it/s]Extractor Predicting: 169it [01:45,  1.61it/s]Extractor Predicting: 170it [01:46,  1.59it/s]Extractor Predicting: 171it [01:47,  1.58it/s]Extractor Predicting: 172it [01:47,  1.65it/s]Extractor Predicting: 173it [01:48,  1.60it/s]Extractor Predicting: 174it [01:48,  1.56it/s]Extractor Predicting: 175it [01:49,  1.55it/s]Extractor Predicting: 176it [01:50,  1.56it/s]Extractor Predicting: 177it [01:50,  1.55it/s]Extractor Predicting: 178it [01:51,  1.53it/s]Extractor Predicting: 179it [01:52,  1.54it/s]Extractor Predicting: 180it [01:52,  1.58it/s]Extractor Predicting: 181it [01:53,  1.57it/s]Extractor Predicting: 182it [01:54,  1.50it/s]Extractor Predicting: 183it [01:54,  1.51it/s]Extractor Predicting: 184it [01:55,  1.50it/s]Extractor Predicting: 185it [01:56,  1.50it/s]Extractor Predicting: 186it [01:56,  1.50it/s]Extractor Predicting: 187it [01:57,  1.49it/s]Extractor Predicting: 188it [01:58,  1.53it/s]Extractor Predicting: 189it [01:58,  1.54it/s]Extractor Predicting: 190it [01:59,  1.56it/s]Extractor Predicting: 191it [02:00,  1.35it/s]Extractor Predicting: 192it [02:01,  1.36it/s]Extractor Predicting: 193it [02:01,  1.39it/s]Extractor Predicting: 194it [02:02,  1.43it/s]Extractor Predicting: 195it [02:03,  1.45it/s]Extractor Predicting: 196it [02:03,  1.49it/s]Extractor Predicting: 197it [02:04,  1.46it/s]Extractor Predicting: 198it [02:05,  1.48it/s]Extractor Predicting: 199it [02:05,  1.49it/s]Extractor Predicting: 200it [02:06,  1.49it/s]Extractor Predicting: 201it [02:07,  1.50it/s]Extractor Predicting: 202it [02:07,  1.50it/s]Extractor Predicting: 203it [02:08,  1.52it/s]Extractor Predicting: 204it [02:09,  1.53it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:10,  1.58it/s]Extractor Predicting: 207it [02:10,  1.54it/s]Extractor Predicting: 208it [02:11,  1.55it/s]Extractor Predicting: 209it [02:12,  1.54it/s]Extractor Predicting: 210it [02:12,  1.58it/s]Extractor Predicting: 211it [02:13,  1.61it/s]Extractor Predicting: 212it [02:14,  1.61it/s]Extractor Predicting: 213it [02:14,  1.64it/s]Extractor Predicting: 214it [02:15,  1.61it/s]Extractor Predicting: 215it [02:15,  1.57it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:17,  1.58it/s]Extractor Predicting: 218it [02:17,  1.62it/s]Extractor Predicting: 219it [02:18,  1.64it/s]Extractor Predicting: 220it [02:18,  1.61it/s]Extractor Predicting: 221it [02:19,  1.64it/s]Extractor Predicting: 222it [02:20,  1.64it/s]Extractor Predicting: 223it [02:20,  1.62it/s]Extractor Predicting: 224it [02:21,  1.62it/s]Extractor Predicting: 225it [02:22,  1.62it/s]Extractor Predicting: 226it [02:22,  1.57it/s]Extractor Predicting: 227it [02:23,  1.55it/s]Extractor Predicting: 228it [02:24,  1.56it/s]Extractor Predicting: 229it [02:24,  1.56it/s]Extractor Predicting: 230it [02:25,  1.55it/s]Extractor Predicting: 231it [02:25,  1.56it/s]Extractor Predicting: 232it [02:26,  1.56it/s]Extractor Predicting: 233it [02:27,  1.63it/s]Extractor Predicting: 234it [02:27,  1.64it/s]Extractor Predicting: 235it [02:28,  1.65it/s]Extractor Predicting: 236it [02:28,  1.70it/s]Extractor Predicting: 237it [02:29,  1.71it/s]Extractor Predicting: 238it [02:30,  1.70it/s]Extractor Predicting: 239it [02:30,  1.69it/s]Extractor Predicting: 240it [02:31,  1.72it/s]Extractor Predicting: 241it [02:31,  1.71it/s]Extractor Predicting: 242it [02:32,  1.72it/s]Extractor Predicting: 243it [02:32,  1.77it/s]Extractor Predicting: 244it [02:33,  1.74it/s]Extractor Predicting: 245it [02:34,  1.81it/s]Extractor Predicting: 246it [02:34,  1.81it/s]Extractor Predicting: 247it [02:35,  1.76it/s]Extractor Predicting: 248it [02:35,  1.71it/s]Extractor Predicting: 249it [02:36,  1.72it/s]Extractor Predicting: 250it [02:36,  1.73it/s]Extractor Predicting: 251it [02:37,  1.76it/s]Extractor Predicting: 252it [02:38,  1.76it/s]Extractor Predicting: 253it [02:38,  1.75it/s]Extractor Predicting: 254it [02:39,  1.72it/s]Extractor Predicting: 255it [02:39,  1.75it/s]Extractor Predicting: 256it [02:40,  1.75it/s]Extractor Predicting: 257it [02:40,  1.77it/s]Extractor Predicting: 258it [02:41,  1.73it/s]Extractor Predicting: 259it [02:42,  1.76it/s]Extractor Predicting: 260it [02:42,  1.74it/s]Extractor Predicting: 261it [02:43,  1.66it/s]Extractor Predicting: 262it [02:43,  1.67it/s]Extractor Predicting: 263it [02:44,  1.60it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:45,  1.56it/s]Extractor Predicting: 266it [02:46,  1.57it/s]Extractor Predicting: 267it [02:47,  1.60it/s]Extractor Predicting: 268it [02:47,  1.57it/s]Extractor Predicting: 269it [02:48,  1.55it/s]Extractor Predicting: 270it [02:49,  1.52it/s]Extractor Predicting: 271it [02:49,  1.52it/s]Extractor Predicting: 272it [02:50,  1.55it/s]Extractor Predicting: 273it [02:51,  1.49it/s]Extractor Predicting: 274it [02:51,  1.50it/s]Extractor Predicting: 275it [02:52,  1.51it/s]Extractor Predicting: 276it [02:53,  1.52it/s]Extractor Predicting: 277it [02:53,  1.51it/s]Extractor Predicting: 278it [02:54,  1.48it/s]Extractor Predicting: 279it [02:55,  1.52it/s]Extractor Predicting: 280it [02:55,  1.51it/s]Extractor Predicting: 281it [02:56,  1.51it/s]Extractor Predicting: 282it [02:57,  1.51it/s]Extractor Predicting: 283it [02:57,  1.46it/s]Extractor Predicting: 284it [02:58,  1.47it/s]Extractor Predicting: 285it [02:59,  1.50it/s]Extractor Predicting: 286it [02:59,  1.51it/s]Extractor Predicting: 287it [03:00,  1.50it/s]Extractor Predicting: 288it [03:01,  1.50it/s]Extractor Predicting: 289it [03:01,  1.52it/s]Extractor Predicting: 290it [03:02,  1.55it/s]Extractor Predicting: 291it [03:03,  1.57it/s]Extractor Predicting: 292it [03:03,  1.58it/s]Extractor Predicting: 293it [03:04,  1.58it/s]Extractor Predicting: 294it [03:04,  1.60it/s]Extractor Predicting: 295it [03:05,  1.59it/s]Extractor Predicting: 296it [03:06,  1.57it/s]Extractor Predicting: 297it [03:06,  1.59it/s]Extractor Predicting: 298it [03:07,  1.64it/s]Extractor Predicting: 299it [03:08,  1.60it/s]Extractor Predicting: 300it [03:08,  1.59it/s]Extractor Predicting: 301it [03:09,  1.58it/s]Extractor Predicting: 302it [03:09,  1.57it/s]Extractor Predicting: 303it [03:10,  1.57it/s]Extractor Predicting: 304it [03:11,  1.56it/s]Extractor Predicting: 305it [03:11,  1.54it/s]Extractor Predicting: 306it [03:12,  1.57it/s]Extractor Predicting: 307it [03:13,  1.57it/s]Extractor Predicting: 308it [03:13,  1.57it/s]Extractor Predicting: 309it [03:14,  1.54it/s]Extractor Predicting: 310it [03:15,  1.55it/s]Extractor Predicting: 311it [03:15,  1.54it/s]Extractor Predicting: 312it [03:16,  1.38it/s]Extractor Predicting: 313it [03:17,  1.44it/s]Extractor Predicting: 314it [03:17,  1.47it/s]Extractor Predicting: 315it [03:18,  1.52it/s]Extractor Predicting: 316it [03:19,  1.51it/s]Extractor Predicting: 317it [03:19,  1.55it/s]Extractor Predicting: 318it [03:20,  1.60it/s]Extractor Predicting: 319it [03:21,  1.60it/s]Extractor Predicting: 320it [03:21,  1.59it/s]Extractor Predicting: 321it [03:22,  1.56it/s]Extractor Predicting: 322it [03:22,  1.56it/s]Extractor Predicting: 323it [03:23,  1.59it/s]Extractor Predicting: 324it [03:24,  1.60it/s]Extractor Predicting: 325it [03:24,  1.59it/s]Extractor Predicting: 326it [03:25,  1.59it/s]Extractor Predicting: 327it [03:26,  1.59it/s]Extractor Predicting: 328it [03:26,  1.58it/s]Extractor Predicting: 329it [03:27,  1.59it/s]Extractor Predicting: 330it [03:27,  1.58it/s]Extractor Predicting: 331it [03:28,  1.56it/s]Extractor Predicting: 332it [03:29,  1.59it/s]Extractor Predicting: 333it [03:29,  1.56it/s]Extractor Predicting: 334it [03:30,  1.60it/s]Extractor Predicting: 335it [03:31,  1.58it/s]Extractor Predicting: 336it [03:31,  1.59it/s]Extractor Predicting: 337it [03:32,  1.58it/s]Extractor Predicting: 338it [03:33,  1.59it/s]Extractor Predicting: 339it [03:33,  1.59it/s]Extractor Predicting: 340it [03:34,  1.58it/s]Extractor Predicting: 341it [03:34,  1.58it/s]Extractor Predicting: 342it [03:35,  1.60it/s]Extractor Predicting: 343it [03:36,  1.60it/s]Extractor Predicting: 344it [03:36,  1.64it/s]Extractor Predicting: 345it [03:37,  1.60it/s]Extractor Predicting: 346it [03:38,  1.58it/s]Extractor Predicting: 347it [03:38,  1.60it/s]Extractor Predicting: 348it [03:39,  1.57it/s]Extractor Predicting: 349it [03:39,  1.58it/s]Extractor Predicting: 350it [03:40,  1.57it/s]Extractor Predicting: 351it [03:41,  1.55it/s]Extractor Predicting: 352it [03:41,  1.55it/s]Extractor Predicting: 353it [03:42,  1.56it/s]Extractor Predicting: 354it [03:43,  1.57it/s]Extractor Predicting: 355it [03:43,  1.56it/s]Extractor Predicting: 356it [03:44,  1.58it/s]Extractor Predicting: 357it [03:45,  1.55it/s]Extractor Predicting: 358it [03:45,  1.56it/s]Extractor Predicting: 359it [03:46,  1.56it/s]Extractor Predicting: 360it [03:46,  1.57it/s]Extractor Predicting: 361it [03:47,  1.57it/s]Extractor Predicting: 362it [03:48,  1.61it/s]Extractor Predicting: 363it [03:48,  1.60it/s]Extractor Predicting: 364it [03:49,  1.60it/s]Extractor Predicting: 365it [03:50,  1.58it/s]Extractor Predicting: 366it [03:50,  1.60it/s]Extractor Predicting: 367it [03:51,  1.59it/s]Extractor Predicting: 368it [03:51,  1.60it/s]Extractor Predicting: 369it [03:52,  1.55it/s]Extractor Predicting: 370it [03:53,  1.55it/s]Extractor Predicting: 371it [03:53,  1.56it/s]Extractor Predicting: 372it [03:54,  1.58it/s]Extractor Predicting: 373it [03:55,  1.57it/s]Extractor Predicting: 374it [03:55,  1.58it/s]Extractor Predicting: 375it [03:56,  1.57it/s]Extractor Predicting: 376it [03:57,  1.56it/s]Extractor Predicting: 377it [03:57,  1.62it/s]Extractor Predicting: 378it [03:58,  1.62it/s]Extractor Predicting: 379it [03:58,  1.62it/s]Extractor Predicting: 380it [03:59,  1.60it/s]Extractor Predicting: 381it [04:00,  1.60it/s]Extractor Predicting: 382it [04:00,  1.59it/s]Extractor Predicting: 383it [04:01,  1.57it/s]Extractor Predicting: 384it [04:02,  1.54it/s]Extractor Predicting: 385it [04:02,  1.56it/s]Extractor Predicting: 386it [04:03,  1.55it/s]Extractor Predicting: 387it [04:04,  1.54it/s]Extractor Predicting: 388it [04:04,  1.50it/s]Extractor Predicting: 389it [04:05,  1.46it/s]Extractor Predicting: 390it [04:06,  1.49it/s]Extractor Predicting: 391it [04:06,  1.53it/s]Extractor Predicting: 392it [04:07,  1.55it/s]Extractor Predicting: 393it [04:08,  1.55it/s]Extractor Predicting: 394it [04:08,  1.56it/s]Extractor Predicting: 395it [04:09,  1.57it/s]Extractor Predicting: 396it [04:09,  1.55it/s]Extractor Predicting: 397it [04:10,  1.56it/s]Extractor Predicting: 398it [04:11,  1.54it/s]Extractor Predicting: 399it [04:11,  1.58it/s]Extractor Predicting: 400it [04:12,  1.56it/s]Extractor Predicting: 401it [04:13,  1.57it/s]Extractor Predicting: 402it [04:13,  1.56it/s]Extractor Predicting: 403it [04:14,  1.55it/s]Extractor Predicting: 404it [04:15,  1.58it/s]Extractor Predicting: 405it [04:15,  1.59it/s]Extractor Predicting: 406it [04:16,  1.61it/s]Extractor Predicting: 407it [04:16,  1.60it/s]Extractor Predicting: 408it [04:17,  1.57it/s]Extractor Predicting: 409it [04:18,  1.59it/s]Extractor Predicting: 410it [04:18,  1.60it/s]Extractor Predicting: 411it [04:19,  1.59it/s]Extractor Predicting: 412it [04:20,  1.62it/s]Extractor Predicting: 413it [04:20,  1.60it/s]Extractor Predicting: 414it [04:21,  1.59it/s]Extractor Predicting: 415it [04:21,  1.57it/s]Extractor Predicting: 416it [04:22,  1.42it/s]Extractor Predicting: 417it [04:23,  1.47it/s]Extractor Predicting: 418it [04:24,  1.52it/s]Extractor Predicting: 419it [04:24,  1.56it/s]Extractor Predicting: 420it [04:25,  1.59it/s]Extractor Predicting: 421it [04:25,  1.57it/s]Extractor Predicting: 422it [04:26,  1.58it/s]Extractor Predicting: 423it [04:27,  1.58it/s]Extractor Predicting: 424it [04:27,  1.59it/s]Extractor Predicting: 425it [04:28,  1.58it/s]Extractor Predicting: 426it [04:29,  1.62it/s]Extractor Predicting: 427it [04:29,  1.61it/s]Extractor Predicting: 428it [04:30,  1.54it/s]Extractor Predicting: 429it [04:30,  1.78it/s]Extractor Predicting: 429it [04:30,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:07,999 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:08,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:08,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:08,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:08,041 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:24:08,345 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:24:08,346 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:08,630 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:09,825 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:09,825 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:12,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:12,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:12,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:12,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:12,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:24:13,055 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:24:13,056 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:13,750 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:13,924 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:13,924 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2031466113416321,
  "recall": 0.11427737794203463,
  "score": 0.1462716295281962,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.75it/s]Extractor Predicting: 5it [00:03,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:24:18,626 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:24:18,628 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:24:18,720 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:24:18,721 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:24:18,792 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:24:33,817 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:24:33,850 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:24:34,096 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:24:34,096 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:24:34,228 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:24:34,310 >> loading file outputs/wrapper/fewrel/unseen_15_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1568627450980392,
  "recall": 0.03669724770642202,
  "score": 0.05947955390334572,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:24:34,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:35,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:35,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:37,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:37,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:38,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:38,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:39,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:40,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:40,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:41,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:42,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:42,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:43,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:43,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:44,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:44,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:45,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:46,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:46,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:47,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:47,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:16, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-29 00:24:48,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:48,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:49,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:49,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:50,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:51,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:51,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:52,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:52,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:53,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:53,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:54,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:55,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:55,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:56,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:56,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:57,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:58,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:58,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:59,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:24:59,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:25<03:47, 12.64s/it][WARNING|generation_utils.py:914] 2023-08-29 00:25:00,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:01,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:01,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:02,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:03,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:03,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:04,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:04,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:05,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:06,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:06,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:07,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:08,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:09,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:10,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:10,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:11,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:11,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:13,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:13,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:14,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:15,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:15,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:16,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:42<04:05, 14.42s/it][WARNING|generation_utils.py:914] 2023-08-29 00:25:16,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:17,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:18,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:18,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:19,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:20,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:20,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:21,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:21,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:22,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:23,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:23,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:24,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:24,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:25,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:26,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:26,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:27,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:27,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:28,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:29,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:54<03:40, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 00:25:29,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:30,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:30,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:31,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:32,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:32,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:33,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:34,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:34,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:35,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:36,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:37,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:38,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:38,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:39,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:39,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:40,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:41,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:41,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:42,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:43,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:43,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:09<03:31, 14.09s/it][WARNING|generation_utils.py:914] 2023-08-29 00:25:44,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:45,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:45,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:46,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:46,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:47,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:48,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:48,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:49,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:50,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:50,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:51,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:52,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:52,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:54,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:54,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:55,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:55,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:56,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:57,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:23<03:15, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-29 00:25:58,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:58,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:59,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:25:59,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:00,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:01,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:01,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:02,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:03,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:03,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:04,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:05,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:05,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:06,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:08,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:09,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:09,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:10,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:10,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:11,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:12,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:12,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:13,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:13,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:14,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:40<03:14, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-29 00:26:14,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:15,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:16,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:17,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:17,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:18,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:18,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:19,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:20,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:20,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:21,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:21,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:22,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:23,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:24,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:24,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:25,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:25,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:26,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:27,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:28,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:28,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:54<02:58, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-29 00:26:29,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:30,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:30,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:31,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:32,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:32,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:33,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:34,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:34,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:35,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:36,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:36,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:37,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:37,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:38,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:39,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:40,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:40,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:41,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:42,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:42,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:43,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:43,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:09<02:43, 14.88s/it][WARNING|generation_utils.py:914] 2023-08-29 00:26:44,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:45,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:45,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:46,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:46,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:47,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:48,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:48,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:49,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:49,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:50,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:51,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:51,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:52,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:52,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:53,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:54,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:54,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:55,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:56,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:56,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:22<02:22, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-29 00:26:57,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:57,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:58,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:59,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:26:59,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:00,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:00,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:01,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:02,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:02,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:03,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:03,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:04,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:05,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:05,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:06,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:06,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:07,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:07,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:08,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:09,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:09,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:35<02:05, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-29 00:27:10,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:11,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:11,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:12,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:13,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:13,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:14,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:15,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:15,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:16,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:17,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:18,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:18,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:19,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:20,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:21,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:21,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:22,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:23,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:23,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:49<01:51, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-29 00:27:24,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:24,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:25,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:26,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:27,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:27,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:28,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:29,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:29,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:30,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:31,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:31,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:32,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:33,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:33,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:34,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:35,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:35,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:36,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:36,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:03<01:37, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-29 00:27:38,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:39,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:39,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:40,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:41,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:41,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:42,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:43,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:43,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:44,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:44,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:45,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:46,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:46,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:47,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:48,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:48,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:49,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:50,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:50,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:51,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:17<01:23, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-29 00:27:52,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:52,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:53,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:53,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:54,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:55,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:55,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:56,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:56,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:57,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:58,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:58,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:27:59,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:00,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:00,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:01,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:02,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:02,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:03,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:03,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:04,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:05,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:05,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:31<01:10, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-29 00:28:06,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:07,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:07,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:08,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:08,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:09,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:10,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:10,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:11,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:12,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:12,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:13,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:14,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:14,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:15,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:16,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:16,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:17,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:18,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:18,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:19,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:20,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:20,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:21,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:47<00:57, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-29 00:28:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:22,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:23,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:23,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:24,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:25,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:25,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:26,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:26,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:27,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:28,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:28,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:29,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:30,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:30,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:31,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:31,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:32,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:33,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:33,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:34,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:00<00:42, 14.17s/it][WARNING|generation_utils.py:914] 2023-08-29 00:28:35,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:36,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:36,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:37,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:37,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:38,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:39,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:39,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:40,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:41,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:41,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:42,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:43,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:43,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:44,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:44,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:45,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:46,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:46,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:47,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:48,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:13<00:27, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-29 00:28:48,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:49,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:49,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:50,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:50,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:51,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:52,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:52,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:53,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:53,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:54,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:55,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:55,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:56,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:56,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:57,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:58,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:58,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:59,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:28:59,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:00,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:01,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:01,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:27<00:13, 13.84s/it][WARNING|generation_utils.py:914] 2023-08-29 00:29:02,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:03,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:03,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:04,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:05,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:05,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:06,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:06,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:07,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:08,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:08,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:09,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:10,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:10,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:11,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:12,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:12,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:13,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:14,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:14,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:29:15,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:41<00:00, 13.74s/it]Generating: 100%|██████████| 20/20 [04:41<00:00, 14.05s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:24,607 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:24,653 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:24,653 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:24,653 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:24,653 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:29:25,316 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:29:25,317 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:29:25,692 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:29:26,911 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:29:26,911 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:29,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:29,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:29,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:29,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:29:29,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:29:30,346 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:29:30,347 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:29:30,715 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:29:31,033 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:29:31,033 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9151785714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 181, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 253, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 365, 'raw': 512}
{'target': 600, 'success': 389, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 458, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 530, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 575, 'raw': 800}
{'target': 600, 'success': 597, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7118055555555556, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Rafael Nadal', 'competition class', '', 'He won the French Open in 1973 and 1994 by beating Rafael Nadal on the same date .')"}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an automatic leave to leave the UK pending the decision on his decision on the death sentence . Head Entity : David Berenstain , Tail Entity : UK .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8328804347826086, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9345238095238095, 'errors': {''}}
['Relation : mouth of the watercourse . Context : The Cottages Creek Bridge is a bridge over a river at the mouth of Cottages Creek , New York state , United States . Head Entity : Cottages Creek , Tail Entity : River .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8396739130434783, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : On 1 March 2014 , the municipality announced that it would be given back control of a public land reserve under the authority of Parliament . Head Entity : Parliament , Tail Entity : Municipality .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8260869565217391, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 12272
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12372, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.74it/s]Extractor Estimating: 4it [00:02,  1.80it/s]Extractor Estimating: 5it [00:02,  1.76it/s]Extractor Estimating: 6it [00:03,  1.74it/s]Extractor Estimating: 7it [00:04,  1.73it/s]Extractor Estimating: 8it [00:04,  1.74it/s]Extractor Estimating: 9it [00:05,  1.69it/s]Extractor Estimating: 10it [00:05,  1.76it/s]Extractor Estimating: 11it [00:06,  1.64it/s]Extractor Estimating: 12it [00:07,  1.70it/s]Extractor Estimating: 13it [00:07,  1.72it/s]Extractor Estimating: 14it [00:08,  1.76it/s]Extractor Estimating: 15it [00:08,  1.77it/s]Extractor Estimating: 16it [00:09,  1.80it/s]Extractor Estimating: 17it [00:09,  1.84it/s]Extractor Estimating: 18it [00:10,  1.77it/s]Extractor Estimating: 19it [00:10,  1.83it/s]Extractor Estimating: 20it [00:11,  1.83it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:12,  1.80it/s]Extractor Estimating: 23it [00:13,  1.79it/s]Extractor Estimating: 24it [00:13,  1.81it/s]Extractor Estimating: 25it [00:14,  1.73it/s]Extractor Estimating: 26it [00:14,  1.75it/s]Extractor Estimating: 27it [00:15,  1.76it/s]Extractor Estimating: 28it [00:15,  1.79it/s]Extractor Estimating: 29it [00:16,  1.73it/s]Extractor Estimating: 30it [00:17,  1.68it/s]Extractor Estimating: 31it [00:17,  1.67it/s]Extractor Estimating: 32it [00:18,  1.68it/s]Extractor Estimating: 33it [00:19,  1.67it/s]Extractor Estimating: 34it [00:19,  1.71it/s]Extractor Estimating: 35it [00:20,  1.72it/s]Extractor Estimating: 36it [00:20,  1.74it/s]Extractor Estimating: 37it [00:21,  1.74it/s]Extractor Estimating: 38it [00:21,  1.77it/s]Extractor Estimating: 39it [00:22,  1.76it/s]Extractor Estimating: 40it [00:23,  1.69it/s]Extractor Estimating: 41it [00:23,  1.65it/s]Extractor Estimating: 42it [00:24,  1.70it/s]Extractor Estimating: 43it [00:24,  1.73it/s]Extractor Estimating: 44it [00:25,  1.69it/s]Extractor Estimating: 45it [00:25,  1.69it/s]Extractor Estimating: 46it [00:26,  1.66it/s]Extractor Estimating: 47it [00:27,  1.62it/s]Extractor Estimating: 48it [00:27,  1.68it/s]Extractor Estimating: 49it [00:28,  1.68it/s]Extractor Estimating: 50it [00:29,  1.67it/s]Extractor Estimating: 51it [00:29,  1.66it/s]Extractor Estimating: 52it [00:30,  1.61it/s]Extractor Estimating: 53it [00:30,  1.63it/s]Extractor Estimating: 54it [00:31,  1.67it/s]Extractor Estimating: 55it [00:32,  1.63it/s]Extractor Estimating: 56it [00:32,  1.67it/s]Extractor Estimating: 57it [00:33,  1.69it/s]Extractor Estimating: 58it [00:33,  1.72it/s]Extractor Estimating: 59it [00:34,  1.75it/s]Extractor Estimating: 60it [00:34,  1.76it/s]Extractor Estimating: 61it [00:35,  1.69it/s]Extractor Estimating: 62it [00:36,  1.68it/s]Extractor Estimating: 63it [00:36,  1.62it/s]Extractor Estimating: 64it [00:37,  1.58it/s]Extractor Estimating: 65it [00:38,  1.61it/s]Extractor Estimating: 66it [00:38,  1.67it/s]Extractor Estimating: 67it [00:39,  1.67it/s]Extractor Estimating: 68it [00:39,  1.64it/s]Extractor Estimating: 69it [00:40,  1.67it/s]Extractor Estimating: 70it [00:41,  1.63it/s]Extractor Estimating: 71it [00:41,  1.66it/s]Extractor Estimating: 72it [00:42,  1.67it/s]Extractor Estimating: 73it [00:42,  1.68it/s]Extractor Estimating: 74it [00:43,  1.70it/s]Extractor Estimating: 75it [00:43,  1.73it/s]Extractor Estimating: 76it [00:44,  1.52it/s]Extractor Estimating: 77it [00:45,  1.51it/s]Extractor Estimating: 78it [00:46,  1.53it/s]Extractor Estimating: 79it [00:46,  1.55it/s]Extractor Estimating: 80it [00:47,  1.52it/s]Extractor Estimating: 81it [00:48,  1.56it/s]Extractor Estimating: 82it [00:48,  1.60it/s]Extractor Estimating: 83it [00:49,  1.57it/s]Extractor Estimating: 84it [00:49,  1.61it/s]Extractor Estimating: 85it [00:50,  1.64it/s]Extractor Estimating: 86it [00:51,  1.68it/s]Extractor Estimating: 87it [00:51,  1.72it/s]Extractor Estimating: 88it [00:52,  1.73it/s]Extractor Estimating: 89it [00:52,  1.72it/s]Extractor Estimating: 90it [00:53,  1.71it/s]Extractor Estimating: 91it [00:53,  1.69it/s]Extractor Estimating: 92it [00:54,  1.66it/s]Extractor Estimating: 93it [00:55,  1.68it/s]Extractor Estimating: 94it [00:55,  1.65it/s]Extractor Estimating: 95it [00:56,  1.64it/s]Extractor Estimating: 96it [00:56,  1.64it/s]Extractor Estimating: 97it [00:57,  1.56it/s]Extractor Estimating: 98it [00:58,  1.59it/s]Extractor Estimating: 99it [00:58,  1.56it/s]Extractor Estimating: 100it [00:59,  1.60it/s]Extractor Estimating: 101it [01:00,  1.53it/s]Extractor Estimating: 102it [01:00,  1.60it/s]Extractor Estimating: 103it [01:01,  1.64it/s]Extractor Estimating: 104it [01:01,  1.68it/s]Extractor Estimating: 105it [01:02,  1.65it/s]Extractor Estimating: 106it [01:03,  1.70it/s]Extractor Estimating: 107it [01:03,  1.71it/s]Extractor Estimating: 108it [01:04,  1.76it/s]Extractor Estimating: 109it [01:04,  1.75it/s]Extractor Estimating: 110it [01:05,  1.78it/s]Extractor Estimating: 111it [01:05,  1.72it/s]Extractor Estimating: 112it [01:06,  1.63it/s]Extractor Estimating: 113it [01:07,  1.72it/s]Extractor Estimating: 114it [01:07,  1.74it/s]Extractor Estimating: 115it [01:08,  1.75it/s]Extractor Estimating: 116it [01:08,  1.71it/s]Extractor Estimating: 117it [01:09,  1.73it/s]Extractor Estimating: 118it [01:10,  1.65it/s]Extractor Estimating: 119it [01:10,  1.64it/s]Extractor Estimating: 120it [01:11,  1.70it/s]Extractor Estimating: 121it [01:11,  1.69it/s]Extractor Estimating: 122it [01:12,  1.73it/s]Extractor Estimating: 123it [01:13,  1.75it/s]Extractor Estimating: 124it [01:13,  1.68it/s]Extractor Estimating: 125it [01:14,  1.70it/s]Extractor Estimating: 126it [01:14,  1.58it/s]Extractor Estimating: 127it [01:15,  1.61it/s]Extractor Estimating: 128it [01:16,  1.63it/s]Extractor Estimating: 129it [01:16,  1.67it/s]Extractor Estimating: 130it [01:17,  1.67it/s]Extractor Estimating: 131it [01:17,  1.68it/s]Extractor Estimating: 132it [01:18,  1.68it/s]Extractor Estimating: 133it [01:19,  1.71it/s]Extractor Estimating: 134it [01:19,  1.70it/s]Extractor Estimating: 135it [01:20,  1.70it/s]Extractor Estimating: 136it [01:20,  1.69it/s]Extractor Estimating: 137it [01:21,  1.60it/s]Extractor Estimating: 138it [01:22,  1.62it/s]Extractor Estimating: 139it [01:22,  1.56it/s]Extractor Estimating: 140it [01:23,  1.52it/s]Extractor Estimating: 141it [01:24,  1.57it/s]Extractor Estimating: 142it [01:24,  1.51it/s]Extractor Estimating: 143it [01:25,  1.54it/s]Extractor Estimating: 144it [01:26,  1.59it/s]Extractor Estimating: 145it [01:26,  1.65it/s]Extractor Estimating: 146it [01:27,  1.63it/s]Extractor Estimating: 147it [01:27,  1.61it/s]Extractor Estimating: 148it [01:28,  1.55it/s]Extractor Estimating: 149it [01:29,  1.51it/s]Extractor Estimating: 150it [01:29,  1.52it/s]Extractor Estimating: 151it [01:30,  1.61it/s]Extractor Estimating: 152it [01:31,  1.65it/s]Extractor Estimating: 153it [01:31,  1.69it/s]Extractor Estimating: 154it [01:32,  1.53it/s]Extractor Estimating: 155it [01:32,  1.61it/s]Extractor Estimating: 156it [01:33,  1.48it/s]Extractor Estimating: 157it [01:34,  1.55it/s]Extractor Estimating: 158it [01:34,  1.63it/s]Extractor Estimating: 159it [01:35,  1.68it/s]Extractor Estimating: 160it [01:36,  1.66it/s]Extractor Estimating: 161it [01:36,  1.70it/s]Extractor Estimating: 162it [01:37,  1.75it/s]Extractor Estimating: 163it [01:37,  1.74it/s]Extractor Estimating: 164it [01:38,  1.76it/s]Extractor Estimating: 165it [01:38,  1.72it/s]Extractor Estimating: 166it [01:39,  1.81it/s]Extractor Estimating: 167it [01:39,  1.84it/s]Extractor Estimating: 168it [01:40,  1.84it/s]Extractor Estimating: 169it [01:41,  1.78it/s]Extractor Estimating: 170it [01:41,  1.76it/s]Extractor Estimating: 171it [01:42,  1.76it/s]Extractor Estimating: 172it [01:42,  1.73it/s]Extractor Estimating: 173it [01:43,  1.78it/s]Extractor Estimating: 174it [01:43,  1.76it/s]Extractor Estimating: 175it [01:44,  1.71it/s]Extractor Estimating: 176it [01:45,  1.77it/s]Extractor Estimating: 177it [01:45,  1.74it/s]Extractor Estimating: 178it [01:46,  1.79it/s]Extractor Estimating: 179it [01:46,  1.81it/s]Extractor Estimating: 180it [01:47,  1.82it/s]Extractor Estimating: 181it [01:47,  1.78it/s]Extractor Estimating: 182it [01:48,  1.70it/s]Extractor Estimating: 183it [01:48,  1.77it/s]Extractor Estimating: 184it [01:49,  1.78it/s]Extractor Estimating: 185it [01:50,  1.81it/s]Extractor Estimating: 186it [01:50,  1.84it/s]Extractor Estimating: 187it [01:51,  1.75it/s]Extractor Estimating: 188it [01:51,  1.83it/s]Extractor Estimating: 189it [01:52,  1.83it/s]Extractor Estimating: 190it [01:52,  1.77it/s]Extractor Estimating: 191it [01:53,  1.79it/s]Extractor Estimating: 192it [01:53,  1.81it/s]Extractor Estimating: 193it [01:54,  1.84it/s]Extractor Estimating: 194it [01:55,  1.80it/s]Extractor Estimating: 195it [01:55,  1.77it/s]Extractor Estimating: 196it [01:56,  1.70it/s]Extractor Estimating: 197it [01:56,  1.77it/s]Extractor Estimating: 198it [01:57,  1.80it/s]Extractor Estimating: 199it [01:57,  1.81it/s]Extractor Estimating: 200it [01:58,  1.82it/s]Extractor Estimating: 201it [01:59,  1.68it/s]Extractor Estimating: 202it [01:59,  1.68it/s]Extractor Estimating: 203it [02:00,  1.65it/s]Extractor Estimating: 204it [02:00,  1.65it/s]Extractor Estimating: 205it [02:01,  1.67it/s]Extractor Estimating: 206it [02:02,  1.51it/s]Extractor Estimating: 207it [02:02,  1.56it/s]Extractor Estimating: 208it [02:03,  1.54it/s]Extractor Estimating: 209it [02:04,  1.52it/s]Extractor Estimating: 210it [02:04,  1.51it/s]Extractor Estimating: 211it [02:05,  1.50it/s]Extractor Estimating: 212it [02:06,  1.54it/s]Extractor Estimating: 213it [02:06,  1.55it/s]Extractor Estimating: 214it [02:07,  1.61it/s]Extractor Estimating: 215it [02:08,  1.62it/s]Extractor Estimating: 216it [02:08,  1.60it/s]Extractor Estimating: 217it [02:09,  1.58it/s]Extractor Estimating: 218it [02:10,  1.52it/s]Extractor Estimating: 219it [02:10,  1.53it/s]Extractor Estimating: 220it [02:11,  1.53it/s]Extractor Estimating: 221it [02:12,  1.50it/s]Extractor Estimating: 222it [02:12,  1.55it/s]Extractor Estimating: 223it [02:13,  1.57it/s]Extractor Estimating: 224it [02:13,  1.62it/s]Extractor Estimating: 225it [02:14,  1.62it/s]Extractor Estimating: 226it [02:15,  1.64it/s]Extractor Estimating: 227it [02:15,  1.69it/s]Extractor Estimating: 228it [02:16,  1.71it/s]Extractor Estimating: 229it [02:16,  1.67it/s]Extractor Estimating: 230it [02:17,  1.64it/s]Extractor Estimating: 231it [02:18,  1.66it/s]Extractor Estimating: 232it [02:18,  1.63it/s]Extractor Estimating: 233it [02:19,  1.65it/s]Extractor Estimating: 234it [02:19,  1.69it/s]Extractor Estimating: 235it [02:20,  1.66it/s]Extractor Estimating: 236it [02:21,  1.62it/s]Extractor Estimating: 237it [02:21,  1.61it/s]Extractor Estimating: 238it [02:22,  1.67it/s]Extractor Estimating: 239it [02:22,  1.67it/s]Extractor Estimating: 240it [02:23,  1.60it/s]Extractor Estimating: 241it [02:24,  1.68it/s]Extractor Estimating: 242it [02:24,  1.72it/s]Extractor Estimating: 243it [02:25,  1.65it/s]Extractor Estimating: 244it [02:25,  1.68it/s]Extractor Estimating: 245it [02:26,  1.62it/s]Extractor Estimating: 246it [02:27,  1.47it/s]Extractor Estimating: 247it [02:27,  1.53it/s]Extractor Estimating: 248it [02:28,  1.53it/s]Extractor Estimating: 249it [02:29,  1.58it/s]Extractor Estimating: 250it [02:29,  1.58it/s]Extractor Estimating: 251it [02:30,  1.56it/s]Extractor Estimating: 252it [02:31,  1.59it/s]Extractor Estimating: 253it [02:31,  1.57it/s]Extractor Estimating: 254it [02:32,  1.56it/s]Extractor Estimating: 255it [02:33,  1.53it/s]Extractor Estimating: 256it [02:33,  1.58it/s]Extractor Estimating: 257it [02:34,  1.63it/s]Extractor Estimating: 258it [02:34,  1.70it/s]Extractor Estimating: 259it [02:35,  1.76it/s]Extractor Estimating: 260it [02:35,  1.69it/s]Extractor Estimating: 261it [02:36,  1.66it/s]Extractor Estimating: 262it [02:37,  1.64it/s]Extractor Estimating: 263it [02:37,  1.66it/s]Extractor Estimating: 264it [02:38,  1.69it/s]Extractor Estimating: 265it [02:38,  1.75it/s]Extractor Estimating: 266it [02:39,  1.73it/s]Extractor Estimating: 267it [02:40,  1.71it/s]Extractor Estimating: 268it [02:40,  1.71it/s]Extractor Estimating: 269it [02:41,  1.72it/s]Extractor Estimating: 270it [02:41,  1.73it/s]Extractor Estimating: 271it [02:42,  1.72it/s]Extractor Estimating: 272it [02:42,  1.70it/s]Extractor Estimating: 273it [02:43,  1.66it/s]Extractor Estimating: 274it [02:44,  1.64it/s]Extractor Estimating: 275it [02:44,  1.67it/s]Extractor Estimating: 276it [02:45,  1.66it/s]Extractor Estimating: 277it [02:46,  1.59it/s]Extractor Estimating: 278it [02:46,  1.60it/s]Extractor Estimating: 279it [02:47,  1.62it/s]Extractor Estimating: 280it [02:47,  1.62it/s]Extractor Estimating: 281it [02:48,  1.62it/s]Extractor Estimating: 282it [02:49,  1.61it/s]Extractor Estimating: 283it [02:49,  1.60it/s]Extractor Estimating: 284it [02:50,  1.59it/s]Extractor Estimating: 285it [02:50,  1.64it/s]Extractor Estimating: 286it [02:51,  1.63it/s]Extractor Estimating: 287it [02:52,  1.64it/s]Extractor Estimating: 288it [02:52,  1.59it/s]Extractor Estimating: 289it [02:53,  1.58it/s]Extractor Estimating: 290it [02:54,  1.62it/s]Extractor Estimating: 291it [02:54,  1.67it/s]Extractor Estimating: 292it [02:55,  1.67it/s]Extractor Estimating: 293it [02:55,  1.65it/s]Extractor Estimating: 294it [02:56,  1.64it/s]Extractor Estimating: 295it [02:57,  1.59it/s]Extractor Estimating: 296it [02:57,  1.60it/s]Extractor Estimating: 297it [02:58,  1.60it/s]Extractor Estimating: 298it [02:59,  1.63it/s]Extractor Estimating: 299it [02:59,  1.67it/s]Extractor Estimating: 300it [03:00,  1.59it/s]Extractor Estimating: 301it [03:00,  1.62it/s]Extractor Estimating: 302it [03:01,  1.65it/s]Extractor Estimating: 303it [03:02,  1.68it/s]Extractor Estimating: 304it [03:02,  1.68it/s]Extractor Estimating: 305it [03:03,  1.65it/s]Extractor Estimating: 306it [03:03,  1.70it/s]Extractor Estimating: 307it [03:04,  1.71it/s]Extractor Estimating: 308it [03:04,  1.78it/s]Extractor Estimating: 309it [03:05,  1.82it/s]Extractor Estimating: 310it [03:05,  1.77it/s]Extractor Estimating: 311it [03:06,  1.76it/s]Extractor Estimating: 312it [03:07,  1.76it/s]Extractor Estimating: 313it [03:07,  1.74it/s]Extractor Estimating: 314it [03:08,  1.76it/s]Extractor Estimating: 315it [03:08,  1.76it/s]Extractor Estimating: 316it [03:09,  1.76it/s]Extractor Estimating: 317it [03:10,  1.72it/s]Extractor Estimating: 318it [03:10,  1.72it/s]Extractor Estimating: 319it [03:11,  1.75it/s]Extractor Estimating: 320it [03:11,  1.66it/s]Extractor Estimating: 321it [03:12,  1.68it/s]Extractor Estimating: 322it [03:12,  1.74it/s]Extractor Estimating: 323it [03:13,  1.71it/s]Extractor Estimating: 324it [03:14,  1.71it/s]Extractor Estimating: 325it [03:14,  1.71it/s]Extractor Estimating: 326it [03:15,  1.79it/s]Extractor Estimating: 327it [03:15,  1.80it/s]Extractor Estimating: 328it [03:16,  1.77it/s]Extractor Estimating: 329it [03:16,  1.77it/s]Extractor Estimating: 330it [03:17,  1.79it/s]Extractor Estimating: 331it [03:18,  1.74it/s]Extractor Estimating: 332it [03:18,  1.69it/s]Extractor Estimating: 333it [03:19,  1.73it/s]Extractor Estimating: 334it [03:19,  1.75it/s]Extractor Estimating: 335it [03:20,  1.73it/s]Extractor Estimating: 336it [03:21,  1.59it/s]Extractor Estimating: 337it [03:21,  1.64it/s]Extractor Estimating: 338it [03:22,  1.64it/s]Extractor Estimating: 339it [03:22,  1.68it/s]Extractor Estimating: 340it [03:23,  1.65it/s]Extractor Estimating: 341it [03:24,  1.70it/s]Extractor Estimating: 342it [03:24,  1.74it/s]Extractor Estimating: 343it [03:25,  1.76it/s]Extractor Estimating: 344it [03:25,  1.76it/s]Extractor Estimating: 345it [03:26,  1.76it/s]Extractor Estimating: 346it [03:26,  1.77it/s]Extractor Estimating: 347it [03:27,  1.72it/s]Extractor Estimating: 348it [03:28,  1.73it/s]Extractor Estimating: 349it [03:28,  1.81it/s]Extractor Estimating: 350it [03:29,  1.79it/s]Extractor Estimating: 351it [03:29,  1.69it/s]Extractor Estimating: 352it [03:30,  1.63it/s]Extractor Estimating: 353it [03:31,  1.63it/s]Extractor Estimating: 354it [03:31,  1.63it/s]Extractor Estimating: 355it [03:32,  1.69it/s]Extractor Estimating: 356it [03:32,  1.71it/s]Extractor Estimating: 357it [03:33,  1.69it/s]Extractor Estimating: 358it [03:33,  1.71it/s]Extractor Estimating: 359it [03:34,  1.67it/s]Extractor Estimating: 360it [03:35,  1.62it/s]Extractor Estimating: 361it [03:35,  1.64it/s]Extractor Estimating: 362it [03:36,  1.69it/s]Extractor Estimating: 363it [03:36,  1.67it/s]Extractor Estimating: 364it [03:37,  1.65it/s]Extractor Estimating: 365it [03:38,  1.68it/s]Extractor Estimating: 366it [03:38,  1.67it/s]Extractor Estimating: 367it [03:39,  1.70it/s]Extractor Estimating: 368it [03:39,  1.71it/s]Extractor Estimating: 369it [03:40,  1.79it/s]Extractor Estimating: 370it [03:41,  1.76it/s]Extractor Estimating: 371it [03:41,  1.77it/s]Extractor Estimating: 372it [03:42,  1.74it/s]Extractor Estimating: 373it [03:42,  1.76it/s]Extractor Estimating: 374it [03:43,  1.81it/s]Extractor Estimating: 375it [03:43,  1.79it/s]Extractor Estimating: 376it [03:44,  1.71it/s]Extractor Estimating: 377it [03:45,  1.74it/s]Extractor Estimating: 378it [03:45,  1.71it/s]Extractor Estimating: 379it [03:46,  1.74it/s]Extractor Estimating: 380it [03:46,  1.76it/s]Extractor Estimating: 381it [03:47,  1.79it/s]Extractor Estimating: 382it [03:47,  1.78it/s]Extractor Estimating: 383it [03:48,  1.79it/s]Extractor Estimating: 384it [03:48,  1.76it/s]Extractor Estimating: 385it [03:49,  1.78it/s]Extractor Estimating: 386it [03:50,  1.75it/s]Extractor Estimating: 387it [03:50,  1.70it/s]Extractor Estimating: 388it [03:51,  1.65it/s]Extractor Estimating: 389it [03:52,  1.61it/s]Extractor Estimating: 390it [03:52,  1.56it/s]Extractor Estimating: 391it [03:53,  1.62it/s]Extractor Estimating: 392it [03:53,  1.69it/s]Extractor Estimating: 393it [03:54,  1.63it/s]Extractor Estimating: 394it [03:55,  1.61it/s]Extractor Estimating: 395it [03:55,  1.58it/s]Extractor Estimating: 396it [03:56,  1.62it/s]Extractor Estimating: 397it [03:56,  1.62it/s]Extractor Estimating: 398it [03:57,  1.60it/s]Extractor Estimating: 399it [03:58,  1.62it/s]Extractor Estimating: 400it [03:58,  1.64it/s]Extractor Estimating: 401it [03:59,  1.69it/s]Extractor Estimating: 402it [03:59,  1.71it/s]Extractor Estimating: 403it [04:00,  1.77it/s]Extractor Estimating: 404it [04:01,  1.75it/s]Extractor Estimating: 405it [04:01,  1.66it/s]Extractor Estimating: 406it [04:02,  1.64it/s]Extractor Estimating: 407it [04:02,  1.69it/s]Extractor Estimating: 408it [04:03,  1.75it/s]Extractor Estimating: 409it [04:03,  1.77it/s]Extractor Estimating: 410it [04:04,  1.77it/s]Extractor Estimating: 411it [04:05,  1.69it/s]Extractor Estimating: 412it [04:05,  1.72it/s]Extractor Estimating: 413it [04:06,  1.71it/s]Extractor Estimating: 414it [04:06,  1.73it/s]Extractor Estimating: 415it [04:07,  1.73it/s]Extractor Estimating: 416it [04:07,  1.80it/s]Extractor Estimating: 417it [04:08,  1.75it/s]Extractor Estimating: 418it [04:09,  1.71it/s]Extractor Estimating: 419it [04:09,  1.76it/s]Extractor Estimating: 420it [04:10,  1.77it/s]Extractor Estimating: 421it [04:10,  1.82it/s]Extractor Estimating: 422it [04:11,  1.76it/s]Extractor Estimating: 423it [04:12,  1.61it/s]Extractor Estimating: 424it [04:12,  1.65it/s]Extractor Estimating: 425it [04:13,  1.61it/s]Extractor Estimating: 426it [04:14,  1.57it/s]Extractor Estimating: 427it [04:14,  1.53it/s]Extractor Estimating: 428it [04:15,  1.52it/s]Extractor Estimating: 429it [04:16,  1.51it/s]Extractor Estimating: 430it [04:16,  1.57it/s]Extractor Estimating: 431it [04:17,  1.61it/s]Extractor Estimating: 432it [04:17,  1.61it/s]Extractor Estimating: 433it [04:18,  1.57it/s]Extractor Estimating: 434it [04:19,  1.58it/s]Extractor Estimating: 435it [04:19,  1.58it/s]Extractor Estimating: 436it [04:20,  1.61it/s]Extractor Estimating: 437it [04:20,  1.67it/s]Extractor Estimating: 438it [04:21,  1.59it/s]Extractor Estimating: 439it [04:22,  1.61it/s]Extractor Estimating: 440it [04:22,  1.62it/s]Extractor Estimating: 441it [04:23,  1.61it/s]Extractor Estimating: 442it [04:24,  1.62it/s]Extractor Estimating: 443it [04:24,  1.58it/s]Extractor Estimating: 444it [04:25,  1.61it/s]Extractor Estimating: 445it [04:26,  1.57it/s]Extractor Estimating: 446it [04:26,  1.56it/s]Extractor Estimating: 447it [04:27,  1.46it/s]Extractor Estimating: 448it [04:28,  1.53it/s]Extractor Estimating: 449it [04:28,  1.59it/s]Extractor Estimating: 450it [04:29,  1.57it/s]Extractor Estimating: 451it [04:29,  1.60it/s]Extractor Estimating: 452it [04:30,  1.65it/s]Extractor Estimating: 453it [04:30,  1.71it/s]Extractor Estimating: 454it [04:31,  1.79it/s]Extractor Estimating: 455it [04:31,  1.84it/s]Extractor Estimating: 456it [04:32,  1.80it/s]Extractor Estimating: 457it [04:33,  1.79it/s]Extractor Estimating: 458it [04:33,  1.79it/s]Extractor Estimating: 459it [04:34,  1.78it/s]Extractor Estimating: 460it [04:34,  1.83it/s]Extractor Estimating: 461it [04:35,  1.76it/s]Extractor Estimating: 462it [04:35,  1.78it/s]Extractor Estimating: 463it [04:36,  1.76it/s]Extractor Estimating: 464it [04:37,  1.77it/s]Extractor Estimating: 465it [04:37,  1.76it/s]Extractor Estimating: 466it [04:38,  1.75it/s]Extractor Estimating: 467it [04:38,  1.77it/s]Extractor Estimating: 468it [04:39,  1.75it/s]Extractor Estimating: 469it [04:39,  1.81it/s]Extractor Estimating: 470it [04:40,  1.82it/s]Extractor Estimating: 471it [04:40,  1.80it/s]Extractor Estimating: 472it [04:41,  1.80it/s]Extractor Estimating: 473it [04:42,  1.79it/s]Extractor Estimating: 474it [04:42,  1.76it/s]Extractor Estimating: 475it [04:43,  1.72it/s]Extractor Estimating: 476it [04:44,  1.61it/s]Extractor Estimating: 477it [04:44,  1.58it/s]Extractor Estimating: 478it [04:45,  1.58it/s]Extractor Estimating: 479it [04:46,  1.51it/s]Extractor Estimating: 480it [04:46,  1.54it/s]Extractor Estimating: 481it [04:47,  1.54it/s]Extractor Estimating: 482it [04:47,  1.54it/s]Extractor Estimating: 483it [04:48,  1.51it/s]Extractor Estimating: 484it [04:49,  1.51it/s]Extractor Estimating: 485it [04:49,  1.57it/s]Extractor Estimating: 486it [04:50,  1.56it/s]Extractor Estimating: 487it [04:51,  1.58it/s]Extractor Estimating: 488it [04:51,  1.62it/s]Extractor Estimating: 489it [04:52,  1.59it/s]Extractor Estimating: 490it [04:53,  1.61it/s]Extractor Estimating: 491it [04:53,  1.56it/s]Extractor Estimating: 492it [04:54,  1.52it/s]Extractor Estimating: 493it [04:55,  1.50it/s]Extractor Estimating: 494it [04:55,  1.55it/s]Extractor Estimating: 495it [04:56,  1.51it/s]Extractor Estimating: 496it [04:56,  1.54it/s]Extractor Estimating: 497it [04:57,  1.56it/s]Extractor Estimating: 498it [04:58,  1.56it/s]Extractor Estimating: 499it [04:58,  1.58it/s]Extractor Estimating: 500it [04:59,  1.66it/s]Extractor Estimating: 500it [04:59,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:54,373 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:54,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:54,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:54,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:54,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:34:55,252 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:34:55,253 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:34:55,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:34:57,051 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:34:57,051 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:35:00,156 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:35:00,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:35:00,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:35:00,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:35:00,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:35:01,036 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:35:01,037 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:35:01,921 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:35:02,155 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:35:02,155 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:41:22,813 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:41:23,006 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 4009 mean pseudo reward: 0.9775900053262682
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 16047
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16147, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16147, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.956, loss:515.0822
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 32, avg_time 0.976, loss:415.0004
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 132, avg_time 0.951, loss:420.0847
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 64, avg_time 0.966, loss:341.6478
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 164, avg_time 0.947, loss:370.3995
>> valid entity prec:0.5689, rec:0.5143, f1:0.5403
>> valid relation prec:0.1259, rec:0.0528, f1:0.0744
>> valid relation with NER prec:0.1259, rec:0.0528, f1:0.0744
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 96, avg_time 2.252, loss:299.3221
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 28, avg_time 0.955, loss:342.8279
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 128, avg_time 0.960, loss:296.5722
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.955, loss:289.7039
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 0.965, loss:309.5830
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5486, rec:0.5643, f1:0.5563
>> valid relation prec:0.1398, rec:0.0666, f1:0.0902
>> valid relation with NER prec:0.1398, rec:0.0666, f1:0.0902
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 92, avg_time 2.241, loss:266.7883
g_step 1200, step 24, avg_time 0.949, loss:291.5610
g_step 1300, step 124, avg_time 0.963, loss:274.1164
g_step 1400, step 56, avg_time 0.946, loss:265.2542
g_step 1500, step 156, avg_time 0.958, loss:243.3184
>> valid entity prec:0.5695, rec:0.4990, f1:0.5319
>> valid relation prec:0.1245, rec:0.0580, f1:0.0791
>> valid relation with NER prec:0.1245, rec:0.0580, f1:0.0791
g_step 1600, step 88, avg_time 2.243, loss:211.2524
g_step 1700, step 20, avg_time 0.947, loss:234.5864
g_step 1800, step 120, avg_time 0.957, loss:211.9454
g_step 1900, step 52, avg_time 0.961, loss:220.1637
g_step 2000, step 152, avg_time 0.963, loss:202.0889
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6117, rec:0.4476, f1:0.5169
>> valid relation prec:0.1361, rec:0.0574, f1:0.0808
>> valid relation with NER prec:0.1361, rec:0.0574, f1:0.0808
g_step 2100, step 84, avg_time 2.232, loss:181.0861
g_step 2200, step 16, avg_time 0.938, loss:185.7742
g_step 2300, step 116, avg_time 0.953, loss:193.1266
g_step 2400, step 48, avg_time 0.943, loss:166.0218
g_step 2500, step 148, avg_time 0.958, loss:177.4340
>> valid entity prec:0.5427, rec:0.5201, f1:0.5312
>> valid relation prec:0.1071, rec:0.0620, f1:0.0786
>> valid relation with NER prec:0.1071, rec:0.0620, f1:0.0786
g_step 2600, step 80, avg_time 2.228, loss:157.3443
g_step 2700, step 12, avg_time 0.946, loss:171.0031
g_step 2800, step 112, avg_time 0.950, loss:152.8812
g_step 2900, step 44, avg_time 0.941, loss:149.0917
g_step 3000, step 144, avg_time 0.955, loss:145.7582
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5825, rec:0.5368, f1:0.5587
>> valid relation prec:0.1292, rec:0.0744, f1:0.0944
>> valid relation with NER prec:0.1292, rec:0.0744, f1:0.0944
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 76, avg_time 2.234, loss:144.6767
g_step 3200, step 8, avg_time 0.957, loss:135.2473
g_step 3300, step 108, avg_time 0.951, loss:133.8064
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:41:23 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:41:23 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-41-22_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:41:24 - WARNING - datasets.builder -   Using custom data configuration default-26394e92b8a4d3dc
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-26394e92b8a4d3dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:41:25,742 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:41:25,743 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:41:25,744 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:41:25,745 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:41:25,860 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,962 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,962 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,962 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,962 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,963 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:41:25,963 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:41:26,287 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:41:29,383 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:41:29,410 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-26394e92b8a4d3dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.01ba/s] 40%|████      | 2/5 [00:00<00:00,  3.95ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.67ba/s]100%|██████████| 5/5 [00:00<00:00,  5.39ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.45ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.02ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.24ba/s]100%|██████████| 4/4 [00:00<00:00,  5.35ba/s]100%|██████████| 4/4 [00:00<00:00,  4.75ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.14ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.76ba/s]100%|██████████| 5/5 [00:00<00:00,  9.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.59ba/s]100%|██████████| 4/4 [00:00<00:00,  8.42ba/s]
[INFO|trainer.py:414] 2023-08-29 01:41:33,844 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:41:34,010 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:41:34,011 >>   Num examples = 4009
[INFO|trainer.py:1149] 2023-08-29 01:41:34,011 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:41:34,011 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:41:34,011 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:41:34,011 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:41:34,011 >>   Total optimization steps = 315
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 1/315 [00:00<01:50,  2.84it/s]  1%|          | 2/315 [00:00<01:38,  3.17it/s]  1%|          | 3/315 [00:00<01:34,  3.29it/s]  1%|▏         | 4/315 [00:01<01:32,  3.36it/s]  2%|▏         | 5/315 [00:01<01:31,  3.39it/s]  2%|▏         | 6/315 [00:01<01:30,  3.41it/s]  2%|▏         | 7/315 [00:02<01:32,  3.33it/s]  3%|▎         | 8/315 [00:02<01:31,  3.37it/s]  3%|▎         | 9/315 [00:02<01:30,  3.39it/s]  3%|▎         | 10/315 [00:02<01:29,  3.41it/s]  3%|▎         | 11/315 [00:03<01:28,  3.42it/s]  4%|▍         | 12/315 [00:03<01:28,  3.43it/s]  4%|▍         | 13/315 [00:03<01:27,  3.44it/s]  4%|▍         | 14/315 [00:04<01:27,  3.45it/s]  5%|▍         | 15/315 [00:04<01:26,  3.45it/s]  5%|▌         | 16/315 [00:04<01:26,  3.45it/s]  5%|▌         | 17/315 [00:05<01:26,  3.46it/s]  6%|▌         | 18/315 [00:05<01:27,  3.38it/s]  6%|▌         | 19/315 [00:05<01:26,  3.41it/s]  6%|▋         | 20/315 [00:05<01:26,  3.42it/s]  7%|▋         | 21/315 [00:06<01:25,  3.43it/s]  7%|▋         | 22/315 [00:06<01:25,  3.44it/s]  7%|▋         | 23/315 [00:06<01:24,  3.44it/s]  8%|▊         | 24/315 [00:07<01:24,  3.45it/s]  8%|▊         | 25/315 [00:07<01:24,  3.45it/s]  8%|▊         | 26/315 [00:07<01:23,  3.46it/s]  9%|▊         | 27/315 [00:07<01:23,  3.46it/s]  9%|▉         | 28/315 [00:08<01:22,  3.46it/s]  9%|▉         | 29/315 [00:08<01:24,  3.38it/s] 10%|▉         | 30/315 [00:08<01:23,  3.41it/s] 10%|▉         | 31/315 [00:09<01:22,  3.42it/s] 10%|█         | 32/315 [00:09<01:22,  3.43it/s] 10%|█         | 33/315 [00:09<01:22,  3.44it/s] 11%|█         | 34/315 [00:09<01:21,  3.45it/s] 11%|█         | 35/315 [00:10<01:21,  3.45it/s] 11%|█▏        | 36/315 [00:10<01:20,  3.45it/s] 12%|█▏        | 37/315 [00:10<01:20,  3.45it/s] 12%|█▏        | 38/315 [00:11<01:20,  3.45it/s] 12%|█▏        | 39/315 [00:11<01:19,  3.46it/s] 13%|█▎        | 40/315 [00:11<01:21,  3.38it/s] 13%|█▎        | 41/315 [00:12<01:20,  3.40it/s] 13%|█▎        | 42/315 [00:12<01:19,  3.42it/s] 14%|█▎        | 43/315 [00:12<01:19,  3.43it/s] 14%|█▍        | 44/315 [00:12<01:18,  3.44it/s] 14%|█▍        | 45/315 [00:13<01:18,  3.45it/s] 15%|█▍        | 46/315 [00:13<01:17,  3.45it/s] 15%|█▍        | 47/315 [00:13<01:17,  3.46it/s] 15%|█▌        | 48/315 [00:14<01:17,  3.46it/s] 16%|█▌        | 49/315 [00:14<01:16,  3.46it/s] 16%|█▌        | 50/315 [00:14<01:16,  3.46it/s] 16%|█▌        | 51/315 [00:14<01:17,  3.40it/s] 17%|█▋        | 52/315 [00:15<01:16,  3.42it/s] 17%|█▋        | 53/315 [00:15<01:16,  3.43it/s] 17%|█▋        | 54/315 [00:15<01:15,  3.44it/s] 17%|█▋        | 55/315 [00:16<01:15,  3.45it/s] 18%|█▊        | 56/315 [00:16<01:15,  3.45it/s] 18%|█▊        | 57/315 [00:16<01:14,  3.45it/s] 18%|█▊        | 58/315 [00:16<01:14,  3.45it/s] 19%|█▊        | 59/315 [00:17<01:14,  3.45it/s] 19%|█▉        | 60/315 [00:17<01:13,  3.46it/s] 19%|█▉        | 61/315 [00:17<01:13,  3.46it/s] 20%|█▉        | 62/315 [00:18<01:14,  3.40it/s] 20%|██        | 63/315 [00:18<01:07,  3.76it/s][INFO|trainer.py:2140] 2023-08-29 01:41:52,331 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:41:52,331 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:41:52,331 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.90it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.79it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.07it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.68it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.16it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.92it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.86it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.69it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.68it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.76it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.89it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.84it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.60it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.51it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.45it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.46it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.37it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.56it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.62it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.74it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.60it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.64it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.52it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.47it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.40it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.48it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.57it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.76it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.63it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.61it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.54it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.52it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.50it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.56it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.64it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.69it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.10it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.23it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.31it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.32it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.31it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.48it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.57it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.38it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.63it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.66it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.58it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.53it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.57it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.63it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.53it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.57it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.52it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.64it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.65it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.58it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.53it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.42it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.62it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 44.60it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.55it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 43.49it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 43.98it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.15it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.26it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.45it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.40it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.40it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.40it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.48it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.59it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.61it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.58it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.56it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.59it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.58it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.58it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.49it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.46it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.58it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.62it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.60it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.53it/s][A                                                
                                                 [A 20%|██        | 63/315 [00:28<01:07,  3.76it/s]
100%|██████████| 436/436 [00:09<00:00, 44.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:42:02,142 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63
[INFO|configuration_utils.py:351] 2023-08-29 01:42:02,211 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:42:08,183 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:42:08,741 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:42:08,870 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63/special_tokens_map.json
 20%|██        | 64/315 [00:42<31:37,  7.56s/it] 21%|██        | 65/315 [00:43<22:26,  5.39s/it] 21%|██        | 66/315 [00:43<16:00,  3.86s/it] 21%|██▏       | 67/315 [00:43<11:31,  2.79s/it] 22%|██▏       | 68/315 [00:44<08:23,  2.04s/it] 22%|██▏       | 69/315 [00:44<06:12,  1.52s/it] 22%|██▏       | 70/315 [00:44<04:41,  1.15s/it] 23%|██▎       | 71/315 [00:44<03:37,  1.12it/s] 23%|██▎       | 72/315 [00:45<02:53,  1.40it/s] 23%|██▎       | 73/315 [00:45<02:21,  1.71it/s] 23%|██▎       | 74/315 [00:45<02:00,  2.01it/s] 24%|██▍       | 75/315 [00:46<01:44,  2.29it/s] 24%|██▍       | 76/315 [00:46<01:35,  2.51it/s] 24%|██▍       | 77/315 [00:46<01:27,  2.73it/s] 25%|██▍       | 78/315 [00:47<01:21,  2.90it/s] 25%|██▌       | 79/315 [00:47<01:17,  3.04it/s] 25%|██▌       | 80/315 [00:47<01:15,  3.13it/s] 26%|██▌       | 81/315 [00:47<01:12,  3.21it/s] 26%|██▌       | 82/315 [00:48<01:11,  3.27it/s] 26%|██▋       | 83/315 [00:48<01:09,  3.32it/s] 27%|██▋       | 84/315 [00:48<01:09,  3.34it/s] 27%|██▋       | 85/315 [00:49<01:10,  3.26it/s] 27%|██▋       | 86/315 [00:49<01:09,  3.31it/s] 28%|██▊       | 87/315 [00:49<01:08,  3.31it/s] 28%|██▊       | 88/315 [00:49<01:07,  3.34it/s] 28%|██▊       | 89/315 [00:50<01:07,  3.37it/s] 29%|██▊       | 90/315 [00:50<01:06,  3.38it/s] 29%|██▉       | 91/315 [00:50<01:06,  3.39it/s] 29%|██▉       | 92/315 [00:51<01:05,  3.39it/s] 30%|██▉       | 93/315 [00:51<01:05,  3.39it/s] 30%|██▉       | 94/315 [00:51<01:04,  3.40it/s] 30%|███       | 95/315 [00:52<01:04,  3.41it/s] 30%|███       | 96/315 [00:52<01:04,  3.41it/s] 31%|███       | 97/315 [00:52<01:03,  3.41it/s] 31%|███       | 98/315 [00:52<01:03,  3.41it/s] 31%|███▏      | 99/315 [00:53<01:03,  3.41it/s] 32%|███▏      | 100/315 [00:53<01:03,  3.40it/s] 32%|███▏      | 101/315 [00:53<01:02,  3.40it/s] 32%|███▏      | 102/315 [00:54<01:02,  3.40it/s] 33%|███▎      | 103/315 [00:54<01:02,  3.41it/s] 33%|███▎      | 104/315 [00:54<01:01,  3.41it/s] 33%|███▎      | 105/315 [00:54<01:01,  3.41it/s] 34%|███▎      | 106/315 [00:55<01:01,  3.41it/s] 34%|███▍      | 107/315 [00:55<01:01,  3.41it/s] 34%|███▍      | 108/315 [00:55<01:00,  3.41it/s] 35%|███▍      | 109/315 [00:56<01:00,  3.41it/s] 35%|███▍      | 110/315 [00:56<01:00,  3.41it/s] 35%|███▌      | 111/315 [00:56<01:00,  3.40it/s] 36%|███▌      | 112/315 [00:57<00:59,  3.40it/s] 36%|███▌      | 113/315 [00:57<00:59,  3.40it/s] 36%|███▌      | 114/315 [00:57<00:59,  3.41it/s] 37%|███▋      | 115/315 [00:57<00:58,  3.40it/s] 37%|███▋      | 116/315 [00:58<00:58,  3.40it/s] 37%|███▋      | 117/315 [00:58<00:58,  3.41it/s] 37%|███▋      | 118/315 [00:58<00:57,  3.41it/s] 38%|███▊      | 119/315 [00:59<00:57,  3.41it/s] 38%|███▊      | 120/315 [00:59<00:57,  3.41it/s] 38%|███▊      | 121/315 [00:59<00:56,  3.41it/s] 39%|███▊      | 122/315 [00:59<00:56,  3.40it/s] 39%|███▉      | 123/315 [01:00<00:56,  3.41it/s] 39%|███▉      | 124/315 [01:00<00:56,  3.41it/s] 40%|███▉      | 125/315 [01:00<00:55,  3.41it/s] 40%|████      | 126/315 [01:01<00:50,  3.75it/s][INFO|trainer.py:2140] 2023-08-29 01:42:35,080 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:42:35,081 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:42:35,081 >>   Batch size = 8
{'eval_loss': 1.1262720823287964, 'eval_runtime': 9.7876, 'eval_samples_per_second': 355.755, 'eval_steps_per_second': 44.546, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.17it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.84it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.17it/s][A
  5%|▌         | 22/436 [00:00<00:08, 46.12it/s][A
  6%|▌         | 27/436 [00:00<00:08, 45.52it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.13it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.80it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.58it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.68it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.82it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.92it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.85it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.68it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.67it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.53it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.44it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.37it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.45it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.71it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.73it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.83it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.73it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.64it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.53it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.39it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.37it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.46it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.59it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.67it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.75it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.79it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.69it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.53it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.43it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.42it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.52it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.63it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.81it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.81it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.53it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.60it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.49it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.41it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.47it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.59it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.72it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.81it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.74it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.61it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.47it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.40it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.58it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.55it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.76it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.77it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.72it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.49it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.34it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 44.29it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.47it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.46it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.60it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.72it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.67it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.50it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.41it/s][A
 82%|████████▏ | 357/436 [00:07<00:01, 44.44it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.46it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.57it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.59it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.72it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.67it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.58it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.52it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.45it/s][A
 92%|█████████▏| 402/436 [00:08<00:00, 44.40it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.50it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.56it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.58it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.61it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.66it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.62it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.62it/s][A 40%|████      | 126/315 [01:10<00:50,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:42:44,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126
[INFO|configuration_utils.py:351] 2023-08-29 01:42:44,947 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:42:47,254 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:42:47,334 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:42:47,371 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126/special_tokens_map.json
 40%|████      | 127/315 [01:20<18:31,  5.91s/it] 41%|████      | 128/315 [01:20<13:11,  4.23s/it] 41%|████      | 129/315 [01:20<09:27,  3.05s/it] 41%|████▏     | 130/315 [01:21<06:51,  2.22s/it] 42%|████▏     | 131/315 [01:21<05:02,  1.64s/it] 42%|████▏     | 132/315 [01:21<03:46,  1.24s/it] 42%|████▏     | 133/315 [01:21<02:53,  1.05it/s] 43%|████▎     | 134/315 [01:22<02:17,  1.32it/s] 43%|████▎     | 135/315 [01:22<01:51,  1.62it/s] 43%|████▎     | 136/315 [01:22<01:33,  1.92it/s] 43%|████▎     | 137/315 [01:23<01:20,  2.21it/s] 44%|████▍     | 138/315 [01:23<01:11,  2.47it/s] 44%|████▍     | 139/315 [01:23<01:05,  2.69it/s] 44%|████▍     | 140/315 [01:23<01:00,  2.87it/s] 45%|████▍     | 141/315 [01:24<00:58,  2.96it/s] 45%|████▌     | 142/315 [01:24<00:56,  3.08it/s] 45%|████▌     | 143/315 [01:24<00:54,  3.17it/s] 46%|████▌     | 144/315 [01:25<00:52,  3.24it/s] 46%|████▌     | 145/315 [01:25<00:51,  3.29it/s] 46%|████▋     | 146/315 [01:25<00:50,  3.32it/s] 47%|████▋     | 147/315 [01:26<00:50,  3.35it/s] 47%|████▋     | 148/315 [01:26<00:49,  3.36it/s] 47%|████▋     | 149/315 [01:26<00:49,  3.37it/s] 48%|████▊     | 150/315 [01:26<00:48,  3.38it/s] 48%|████▊     | 151/315 [01:27<00:48,  3.38it/s] 48%|████▊     | 152/315 [01:27<00:49,  3.30it/s] 49%|████▊     | 153/315 [01:27<00:48,  3.33it/s] 49%|████▉     | 154/315 [01:28<00:47,  3.36it/s] 49%|████▉     | 155/315 [01:28<00:47,  3.37it/s] 50%|████▉     | 156/315 [01:28<00:47,  3.38it/s] 50%|████▉     | 157/315 [01:29<00:46,  3.39it/s] 50%|█████     | 158/315 [01:29<00:46,  3.39it/s] 50%|█████     | 159/315 [01:29<00:45,  3.39it/s] 51%|█████     | 160/315 [01:29<00:45,  3.40it/s] 51%|█████     | 161/315 [01:30<00:45,  3.40it/s] 51%|█████▏    | 162/315 [01:30<00:45,  3.40it/s] 52%|█████▏    | 163/315 [01:30<00:46,  3.26it/s] 52%|█████▏    | 164/315 [01:31<00:45,  3.31it/s] 52%|█████▏    | 165/315 [01:31<00:45,  3.33it/s] 53%|█████▎    | 166/315 [01:31<00:44,  3.35it/s] 53%|█████▎    | 167/315 [01:32<00:43,  3.37it/s] 53%|█████▎    | 168/315 [01:32<00:43,  3.38it/s] 54%|█████▎    | 169/315 [01:32<00:43,  3.38it/s] 54%|█████▍    | 170/315 [01:32<00:42,  3.39it/s] 54%|█████▍    | 171/315 [01:33<00:42,  3.39it/s] 55%|█████▍    | 172/315 [01:33<00:42,  3.39it/s] 55%|█████▍    | 173/315 [01:33<00:41,  3.39it/s] 55%|█████▌    | 174/315 [01:34<00:42,  3.31it/s] 56%|█████▌    | 175/315 [01:34<00:41,  3.34it/s] 56%|█████▌    | 176/315 [01:34<00:41,  3.36it/s] 56%|█████▌    | 177/315 [01:34<00:40,  3.37it/s] 57%|█████▋    | 178/315 [01:35<00:40,  3.38it/s] 57%|█████▋    | 179/315 [01:35<00:40,  3.39it/s] 57%|█████▋    | 180/315 [01:35<00:39,  3.39it/s] 57%|█████▋    | 181/315 [01:36<00:39,  3.39it/s] 58%|█████▊    | 182/315 [01:36<00:39,  3.40it/s] 58%|█████▊    | 183/315 [01:36<00:38,  3.40it/s] 58%|█████▊    | 184/315 [01:37<00:38,  3.40it/s] 59%|█████▊    | 185/315 [01:37<00:39,  3.27it/s] 59%|█████▉    | 186/315 [01:37<00:39,  3.31it/s] 59%|█████▉    | 187/315 [01:37<00:38,  3.33it/s] 60%|█████▉    | 188/315 [01:38<00:37,  3.36it/s] 60%|██████    | 189/315 [01:38<00:33,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 01:43:12,472 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:43:12,473 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:43:12,473 >>   Batch size = 8
{'eval_loss': 1.1419553756713867, 'eval_runtime': 9.7689, 'eval_samples_per_second': 356.436, 'eval_steps_per_second': 44.631, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.16it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.77it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.02it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.81it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.28it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.95it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.72it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.59it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.51it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.69it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.76it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.78it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.61it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.56it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 41.95it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 42.76it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 43.28it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.76it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.13it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.34it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.33it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.45it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.12it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.29it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.49it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.56it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.71it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.72it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.59it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.53it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.37it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.32it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.31it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.66it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.57it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.60it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.56it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.49it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.33it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 38.53it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 40.40it/s][A
 51%|█████     | 222/436 [00:05<00:05, 41.71it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 42.74it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.43it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.98it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.28it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.17it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.86it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.73it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.94it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.22it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.40it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.60it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.74it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.72it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.41it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.11it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.03it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.03it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.33it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.47it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.74it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.90it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.79it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.55it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.19it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 43.37it/s][A
 81%|████████  | 352/436 [00:07<00:01, 43.71it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.03it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.39it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.60it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.62it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.36it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.14it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.24it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.31it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.47it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 42.87it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.69it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.10it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.22it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.93it/s][A 60%|██████    | 189/315 [01:48<00:33,  3.71it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:43:22,455 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189
[INFO|configuration_utils.py:351] 2023-08-29 01:43:22,696 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:43:25,451 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:43:25,542 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:43:25,594 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189/special_tokens_map.json
 60%|██████    | 190/315 [01:58<13:02,  6.26s/it] 61%|██████    | 191/315 [01:59<09:15,  4.48s/it] 61%|██████    | 192/315 [01:59<06:36,  3.23s/it] 61%|██████▏   | 193/315 [01:59<04:46,  2.35s/it] 62%|██████▏   | 194/315 [01:59<03:29,  1.73s/it] 62%|██████▏   | 195/315 [02:00<02:35,  1.30s/it] 62%|██████▏   | 196/315 [02:00<01:58,  1.00it/s] 63%|██████▎   | 197/315 [02:00<01:32,  1.27it/s] 63%|██████▎   | 198/315 [02:01<01:14,  1.57it/s] 63%|██████▎   | 199/315 [02:01<01:02,  1.87it/s] 63%|██████▎   | 200/315 [02:01<00:53,  2.16it/s] 64%|██████▍   | 201/315 [02:01<00:46,  2.43it/s] 64%|██████▍   | 202/315 [02:02<00:43,  2.61it/s] 64%|██████▍   | 203/315 [02:02<00:39,  2.81it/s] 65%|██████▍   | 204/315 [02:02<00:37,  2.96it/s] 65%|██████▌   | 205/315 [02:03<00:35,  3.08it/s] 65%|██████▌   | 206/315 [02:03<00:34,  3.17it/s] 66%|██████▌   | 207/315 [02:03<00:33,  3.24it/s] 66%|██████▌   | 208/315 [02:04<00:32,  3.28it/s] 66%|██████▋   | 209/315 [02:04<00:31,  3.32it/s] 67%|██████▋   | 210/315 [02:04<00:31,  3.34it/s] 67%|██████▋   | 211/315 [02:04<00:30,  3.36it/s] 67%|██████▋   | 212/315 [02:05<00:30,  3.37it/s] 68%|██████▊   | 213/315 [02:05<00:31,  3.28it/s] 68%|██████▊   | 214/315 [02:05<00:30,  3.31it/s] 68%|██████▊   | 215/315 [02:06<00:29,  3.34it/s] 69%|██████▊   | 216/315 [02:06<00:29,  3.36it/s] 69%|██████▉   | 217/315 [02:06<00:29,  3.37it/s] 69%|██████▉   | 218/315 [02:07<00:28,  3.38it/s] 70%|██████▉   | 219/315 [02:07<00:28,  3.39it/s] 70%|██████▉   | 220/315 [02:07<00:28,  3.39it/s] 70%|███████   | 221/315 [02:07<00:27,  3.40it/s] 70%|███████   | 222/315 [02:08<00:27,  3.40it/s] 71%|███████   | 223/315 [02:08<00:27,  3.40it/s] 71%|███████   | 224/315 [02:08<00:27,  3.31it/s] 71%|███████▏  | 225/315 [02:09<00:26,  3.34it/s] 72%|███████▏  | 226/315 [02:09<00:26,  3.36it/s] 72%|███████▏  | 227/315 [02:09<00:26,  3.37it/s] 72%|███████▏  | 228/315 [02:09<00:25,  3.38it/s] 73%|███████▎  | 229/315 [02:10<00:25,  3.39it/s] 73%|███████▎  | 230/315 [02:10<00:25,  3.39it/s] 73%|███████▎  | 231/315 [02:10<00:24,  3.40it/s] 74%|███████▎  | 232/315 [02:11<00:24,  3.40it/s] 74%|███████▍  | 233/315 [02:11<00:24,  3.40it/s] 74%|███████▍  | 234/315 [02:11<00:23,  3.40it/s] 75%|███████▍  | 235/315 [02:12<00:24,  3.33it/s] 75%|███████▍  | 236/315 [02:12<00:23,  3.35it/s] 75%|███████▌  | 237/315 [02:12<00:23,  3.37it/s] 76%|███████▌  | 238/315 [02:12<00:22,  3.38it/s] 76%|███████▌  | 239/315 [02:13<00:22,  3.39it/s] 76%|███████▌  | 240/315 [02:13<00:22,  3.39it/s] 77%|███████▋  | 241/315 [02:13<00:21,  3.39it/s] 77%|███████▋  | 242/315 [02:14<00:21,  3.40it/s] 77%|███████▋  | 243/315 [02:14<00:21,  3.40it/s] 77%|███████▋  | 244/315 [02:14<00:20,  3.40it/s] 78%|███████▊  | 245/315 [02:14<00:20,  3.40it/s] 78%|███████▊  | 246/315 [02:15<00:20,  3.35it/s] 78%|███████▊  | 247/315 [02:15<00:20,  3.36it/s] 79%|███████▊  | 248/315 [02:15<00:19,  3.38it/s] 79%|███████▉  | 249/315 [02:16<00:19,  3.38it/s] 79%|███████▉  | 250/315 [02:16<00:19,  3.39it/s] 80%|███████▉  | 251/315 [02:16<00:18,  3.39it/s] 80%|████████  | 252/315 [02:16<00:16,  3.74it/s][INFO|trainer.py:2140] 2023-08-29 01:43:50,997 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:43:50,997 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:43:50,997 >>   Batch size = 8
{'eval_loss': 1.1545298099517822, 'eval_runtime': 9.8841, 'eval_samples_per_second': 352.283, 'eval_steps_per_second': 44.111, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.04it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.32it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.63it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.58it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.05it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.80it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.75it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.61it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.64it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 43.17it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 43.76it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.07it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.11it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.14it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.35it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.30it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.15it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.14it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.34it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.49it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.63it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.55it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.42it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.54it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.44it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.39it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.36it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.49it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.52it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.62it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.54it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.61it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.54it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.37it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.30it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.40it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.84it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.08it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.27it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.40it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.39it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.29it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.25it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.35it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.41it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.45it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 40.43it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 42.02it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 42.96it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 43.53it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.79it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.00it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.11it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.20it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.88it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.94it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.20it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.52it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.67it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 44.71it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.70it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.58it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.39it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.13it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.14it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.31it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.50it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.62it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.79it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.71it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.62it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.40it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.18it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 42.29it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.15it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 43.73it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.13it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.36it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.47it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.37it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.06it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.96it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.08it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.22it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.37it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.60it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.60it/s][A 80%|████████  | 252/315 [02:26<00:16,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:44:01,020 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252
[INFO|configuration_utils.py:351] 2023-08-29 01:44:01,207 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:44:04,475 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:44:04,621 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:44:04,691 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252/special_tokens_map.json
 80%|████████  | 253/315 [02:38<06:55,  6.71s/it] 81%|████████  | 254/315 [02:39<04:53,  4.81s/it] 81%|████████  | 255/315 [02:39<03:27,  3.45s/it] 81%|████████▏ | 256/315 [02:39<02:27,  2.50s/it] 82%|████████▏ | 257/315 [02:39<01:46,  1.84s/it] 82%|████████▏ | 258/315 [02:40<01:18,  1.38s/it] 82%|████████▏ | 259/315 [02:40<00:58,  1.05s/it] 83%|████████▎ | 260/315 [02:40<00:45,  1.21it/s] 83%|████████▎ | 261/315 [02:41<00:35,  1.50it/s] 83%|████████▎ | 262/315 [02:41<00:29,  1.80it/s] 83%|████████▎ | 263/315 [02:41<00:24,  2.10it/s] 84%|████████▍ | 264/315 [02:42<00:22,  2.31it/s] 84%|████████▍ | 265/315 [02:42<00:19,  2.55it/s] 84%|████████▍ | 266/315 [02:42<00:17,  2.76it/s] 85%|████████▍ | 267/315 [02:42<00:16,  2.93it/s] 85%|████████▌ | 268/315 [02:43<00:15,  3.05it/s] 85%|████████▌ | 269/315 [02:43<00:14,  3.15it/s] 86%|████████▌ | 270/315 [02:43<00:13,  3.22it/s] 86%|████████▌ | 271/315 [02:44<00:13,  3.28it/s] 86%|████████▋ | 272/315 [02:44<00:12,  3.31it/s] 87%|████████▋ | 273/315 [02:44<00:12,  3.34it/s] 87%|████████▋ | 274/315 [02:45<00:12,  3.36it/s] 87%|████████▋ | 275/315 [02:45<00:12,  3.17it/s] 88%|████████▊ | 276/315 [02:45<00:12,  3.23it/s] 88%|████████▊ | 277/315 [02:45<00:11,  3.28it/s] 88%|████████▊ | 278/315 [02:46<00:11,  3.32it/s] 89%|████████▊ | 279/315 [02:46<00:10,  3.34it/s] 89%|████████▉ | 280/315 [02:46<00:10,  3.36it/s] 89%|████████▉ | 281/315 [02:47<00:10,  3.38it/s] 90%|████████▉ | 282/315 [02:47<00:09,  3.38it/s] 90%|████████▉ | 283/315 [02:47<00:09,  3.28it/s] 90%|█████████ | 284/315 [02:48<00:09,  3.31it/s] 90%|█████████ | 285/315 [02:48<00:09,  3.22it/s] 91%|█████████ | 286/315 [02:48<00:08,  3.27it/s] 91%|█████████ | 287/315 [02:48<00:08,  3.31it/s] 91%|█████████▏| 288/315 [02:49<00:08,  3.33it/s] 92%|█████████▏| 289/315 [02:49<00:07,  3.35it/s] 92%|█████████▏| 290/315 [02:49<00:07,  3.37it/s] 92%|█████████▏| 291/315 [02:50<00:07,  3.38it/s] 93%|█████████▎| 292/315 [02:50<00:06,  3.38it/s] 93%|█████████▎| 293/315 [02:50<00:06,  3.39it/s] 93%|█████████▎| 294/315 [02:51<00:06,  3.39it/s] 94%|█████████▎| 295/315 [02:51<00:05,  3.41it/s] 94%|█████████▍| 296/315 [02:51<00:05,  3.32it/s] 94%|█████████▍| 297/315 [02:51<00:05,  3.35it/s] 95%|█████████▍| 298/315 [02:52<00:06,  2.69it/s] 95%|█████████▍| 299/315 [02:52<00:05,  2.88it/s] 95%|█████████▌| 300/315 [02:53<00:04,  3.04it/s] 96%|█████████▌| 301/315 [02:53<00:04,  3.15it/s] 96%|█████████▌| 302/315 [02:53<00:04,  3.23it/s] 96%|█████████▌| 303/315 [02:53<00:03,  3.29it/s] 97%|█████████▋| 304/315 [02:54<00:03,  3.34it/s] 97%|█████████▋| 305/315 [02:54<00:02,  3.37it/s] 97%|█████████▋| 306/315 [02:54<00:02,  3.40it/s] 97%|█████████▋| 307/315 [02:55<00:02,  3.41it/s] 98%|█████████▊| 308/315 [02:55<00:02,  3.42it/s] 98%|█████████▊| 309/315 [02:55<00:01,  3.43it/s] 98%|█████████▊| 310/315 [02:55<00:01,  3.44it/s] 99%|█████████▊| 311/315 [02:56<00:01,  3.35it/s] 99%|█████████▉| 312/315 [02:56<00:00,  3.38it/s] 99%|█████████▉| 313/315 [02:56<00:00,  3.40it/s]100%|█████████▉| 314/315 [02:57<00:00,  3.42it/s]100%|██████████| 315/315 [02:57<00:00,  3.77it/s][INFO|trainer.py:2140] 2023-08-29 01:44:31,340 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:44:31,340 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:44:31,340 >>   Batch size = 8
{'eval_loss': 1.1663248538970947, 'eval_runtime': 9.8573, 'eval_samples_per_second': 353.241, 'eval_steps_per_second': 44.231, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.91it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.01it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.83it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.37it/s][A
  7%|▋         | 32/436 [00:00<00:08, 45.00it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.66it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.42it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.64it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.73it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.87it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.79it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.73it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.32it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 42.88it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 43.44it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.83it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.07it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.36it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.44it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.44it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.39it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.14it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.28it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.38it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.59it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.74it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.76it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.50it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.33it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.26it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.33it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.42it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.47it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.65it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.75it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.71it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.64it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.41it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.29it/s][A
 50%|████▉     | 217/436 [00:04<00:05, 41.96it/s][A
 51%|█████     | 222/436 [00:04<00:04, 42.91it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.35it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.90it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.34it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.46it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.42it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.29it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 43.92it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.17it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.34it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.42it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.68it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.86it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.78it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.60it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.34it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.17it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.24it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.35it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.48it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.62it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.83it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.82it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.60it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.42it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.36it/s][A
 81%|████████  | 352/436 [00:07<00:01, 42.52it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.16it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.71it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.04it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.33it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.41it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.34it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.20it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.04it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.11it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.22it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.42it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.69it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.81it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.69it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.59it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.38it/s][A                                                 
                                                 [A100%|██████████| 315/315 [03:07<00:00,  3.77it/s]
100%|██████████| 436/436 [00:09<00:00, 44.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:44:41,280 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315
[INFO|configuration_utils.py:351] 2023-08-29 01:44:41,463 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:44:44,595 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:44:44,722 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:44:44,804 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:44:51,926 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:44:51,947 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63 (score: 1.1262720823287964).
                                                 100%|██████████| 315/315 [03:26<00:00,  3.77it/s]100%|██████████| 315/315 [03:26<00:00,  1.52it/s]
[INFO|trainer.py:1894] 2023-08-29 01:45:00,746 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 01:45:00,921 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:45:04,343 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:45:04,471 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:45:04,544 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:45:05,054 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   train_loss               =     0.4675
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   train_runtime            = 0:03:26.67
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   train_samples            =       4009
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   train_samples_per_second =      96.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:05,055 >>   train_steps_per_second   =      1.524
{'eval_loss': 1.171911358833313, 'eval_runtime': 9.8315, 'eval_samples_per_second': 354.167, 'eval_steps_per_second': 44.347, 'epoch': 5.0}
{'train_runtime': 206.6702, 'train_samples_per_second': 96.99, 'train_steps_per_second': 1.524, 'train_loss': 0.4674993121434772, 'epoch': 5.0}
08/29/2023 01:45:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:45:05,367 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:45:05,367 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 01:45:05,367 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.52it/s]  3%|▎         | 12/436 [00:00<00:08, 49.21it/s]  4%|▍         | 17/436 [00:00<00:08, 47.50it/s]  5%|▌         | 22/436 [00:00<00:08, 46.68it/s]  6%|▌         | 27/436 [00:00<00:08, 46.16it/s]  7%|▋         | 32/436 [00:00<00:08, 45.70it/s]  8%|▊         | 37/436 [00:00<00:08, 45.59it/s] 10%|▉         | 42/436 [00:00<00:08, 45.30it/s] 11%|█         | 47/436 [00:01<00:08, 44.61it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.38it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.52it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.74it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.87it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.78it/s] 18%|█▊        | 77/436 [00:01<00:07, 44.92it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.00it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.82it/s] 21%|██        | 92/436 [00:02<00:07, 44.46it/s] 22%|██▏       | 97/436 [00:02<00:07, 42.86it/s] 23%|██▎       | 102/436 [00:02<00:07, 43.64it/s] 25%|██▍       | 107/436 [00:02<00:07, 43.93it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.20it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.53it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.70it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.74it/s] 30%|███       | 132/436 [00:02<00:06, 44.55it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.21it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.33it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.40it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.60it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.72it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.88it/s] 38%|███▊      | 167/436 [00:03<00:05, 44.95it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.85it/s] 41%|████      | 177/436 [00:03<00:05, 44.31it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.29it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.40it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.43it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.56it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.73it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.87it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.75it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.70it/s] 51%|█████     | 222/436 [00:04<00:04, 44.54it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.28it/s] 53%|█████▎    | 232/436 [00:05<00:04, 43.27it/s] 54%|█████▍    | 237/436 [00:05<00:04, 43.86it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.08it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.35it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.50it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.65it/s] 60%|██████    | 262/436 [00:05<00:03, 44.67it/s] 61%|██████    | 267/436 [00:05<00:03, 44.48it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.32it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.39it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.62it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.83it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.84it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.79it/s] 69%|██████▉   | 302/436 [00:06<00:02, 44.81it/s] 70%|███████   | 307/436 [00:06<00:02, 44.77it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.52it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.30it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.35it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.57it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.66it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.79it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.84it/s] 80%|███████▉  | 347/436 [00:07<00:01, 44.88it/s] 81%|████████  | 352/436 [00:07<00:01, 44.72it/s] 82%|████████▏ | 357/436 [00:07<00:01, 44.60it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.39it/s] 84%|████████▍ | 367/436 [00:08<00:01, 40.13it/s] 85%|████████▌ | 372/436 [00:08<00:01, 41.65it/s] 86%|████████▋ | 377/436 [00:08<00:01, 42.66it/s] 88%|████████▊ | 382/436 [00:08<00:01, 43.47it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.03it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.33it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.36it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.35it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.08it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.01it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.30it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.58it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.78it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.93it/s]100%|██████████| 436/436 [00:09<00:00, 44.54it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:45:15,175 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   eval_loss               =     1.1263
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   eval_runtime            = 0:00:09.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   eval_samples_per_second =    355.044
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   eval_steps_per_second   =     44.457
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:45:15,175 >>   perplexity              =     3.0841
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:30,052 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:30,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:30,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:30,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:30,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:45:30,956 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:45:30,957 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:45:31,571 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:45:32,718 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:45:32,718 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:35,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:35,843 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:35,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:35,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:45:35,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:45:36,627 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:45:36,628 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:45:37,253 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:45:37,478 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:45:37,478 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-252
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-126
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-315
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-189
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-63
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:25,  1.42it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:26,  1.47it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:33,  1.50it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.51it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.48it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.49it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:45,  1.58it/s]Extractor Predicting: 71it [00:46,  1.56it/s]Extractor Predicting: 72it [00:47,  1.57it/s]Extractor Predicting: 73it [00:47,  1.63it/s]Extractor Predicting: 74it [00:48,  1.63it/s]Extractor Predicting: 75it [00:48,  1.59it/s]Extractor Predicting: 76it [00:49,  1.53it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:52,  1.55it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:53,  1.52it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.56it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:57,  1.52it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:58,  1.49it/s]Extractor Predicting: 91it [00:59,  1.48it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:00,  1.51it/s]Extractor Predicting: 94it [01:01,  1.49it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:02,  1.54it/s]Extractor Predicting: 97it [01:03,  1.52it/s]Extractor Predicting: 98it [01:04,  1.51it/s]Extractor Predicting: 99it [01:04,  1.47it/s]Extractor Predicting: 100it [01:05,  1.45it/s]Extractor Predicting: 101it [01:06,  1.49it/s]Extractor Predicting: 102it [01:06,  1.46it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:08,  1.50it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:10,  1.48it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:11,  1.56it/s]Extractor Predicting: 110it [01:12,  1.56it/s]Extractor Predicting: 111it [01:12,  1.61it/s]Extractor Predicting: 112it [01:13,  1.57it/s]Extractor Predicting: 113it [01:13,  1.56it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:15,  1.52it/s]Extractor Predicting: 117it [01:16,  1.50it/s]Extractor Predicting: 118it [01:17,  1.50it/s]Extractor Predicting: 119it [01:17,  1.51it/s]Extractor Predicting: 120it [01:18,  1.55it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.41it/s]Extractor Predicting: 123it [01:20,  1.41it/s]Extractor Predicting: 124it [01:21,  1.45it/s]Extractor Predicting: 125it [01:22,  1.47it/s]Extractor Predicting: 126it [01:22,  1.49it/s]Extractor Predicting: 127it [01:23,  1.49it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:24,  1.48it/s]Extractor Predicting: 130it [01:25,  1.47it/s]Extractor Predicting: 131it [01:26,  1.49it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:28,  1.44it/s]Extractor Predicting: 135it [01:28,  1.46it/s]Extractor Predicting: 136it [01:29,  1.48it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:30,  1.50it/s]Extractor Predicting: 139it [01:31,  1.46it/s]Extractor Predicting: 140it [01:32,  1.46it/s]Extractor Predicting: 141it [01:32,  1.46it/s]Extractor Predicting: 142it [01:33,  1.50it/s]Extractor Predicting: 143it [01:34,  1.55it/s]Extractor Predicting: 143it [01:34,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:27,439 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:27,473 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:27,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:27,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:27,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:47:28,343 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:47:28,344 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:47:29,043 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:47:30,147 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:47:30,147 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:33,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:33,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:33,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:33,200 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:33,201 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:47:33,995 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:47:33,996 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:47:34,739 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:47:34,950 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:47:34,950 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.23067776456599287,
  "recall": 0.11143021252153934,
  "score": 0.15027110766847407,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.69it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.52it/s]Extractor Predicting: 35it [00:21,  1.51it/s]Extractor Predicting: 36it [00:22,  1.52it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:23,  1.50it/s]Extractor Predicting: 39it [00:24,  1.52it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:27,  1.52it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.51it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.53it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.52it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:36,  1.56it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:38,  1.55it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.55it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.58it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:47,  1.58it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.59it/s]Extractor Predicting: 78it [00:49,  1.64it/s]Extractor Predicting: 79it [00:50,  1.63it/s]Extractor Predicting: 80it [00:50,  1.55it/s]Extractor Predicting: 81it [00:51,  1.55it/s]Extractor Predicting: 82it [00:52,  1.58it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.57it/s]Extractor Predicting: 89it [00:56,  1.62it/s]Extractor Predicting: 90it [00:57,  1.61it/s]Extractor Predicting: 91it [00:57,  1.60it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:59,  1.56it/s]Extractor Predicting: 94it [00:59,  1.54it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:01,  1.53it/s]Extractor Predicting: 98it [01:02,  1.52it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.52it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.52it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:09,  1.53it/s]Extractor Predicting: 110it [01:10,  1.51it/s]Extractor Predicting: 111it [01:11,  1.51it/s]Extractor Predicting: 112it [01:12,  1.34it/s]Extractor Predicting: 113it [01:12,  1.40it/s]Extractor Predicting: 114it [01:13,  1.43it/s]Extractor Predicting: 115it [01:14,  1.43it/s]Extractor Predicting: 116it [01:14,  1.51it/s]Extractor Predicting: 117it [01:15,  1.60it/s]Extractor Predicting: 118it [01:15,  1.65it/s]Extractor Predicting: 119it [01:16,  1.64it/s]Extractor Predicting: 120it [01:16,  1.64it/s]Extractor Predicting: 121it [01:17,  1.63it/s]Extractor Predicting: 122it [01:18,  1.60it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:19,  1.63it/s]Extractor Predicting: 125it [01:19,  1.68it/s]Extractor Predicting: 126it [01:20,  1.67it/s]Extractor Predicting: 127it [01:21,  1.67it/s]Extractor Predicting: 128it [01:21,  1.70it/s]Extractor Predicting: 129it [01:22,  1.68it/s]Extractor Predicting: 130it [01:22,  1.67it/s]Extractor Predicting: 131it [01:23,  1.69it/s]Extractor Predicting: 132it [01:24,  1.69it/s]Extractor Predicting: 133it [01:24,  1.66it/s]Extractor Predicting: 134it [01:25,  1.68it/s]Extractor Predicting: 135it [01:25,  1.70it/s]Extractor Predicting: 136it [01:26,  1.74it/s]Extractor Predicting: 137it [01:27,  1.74it/s]Extractor Predicting: 138it [01:27,  1.71it/s]Extractor Predicting: 139it [01:28,  1.66it/s]Extractor Predicting: 140it [01:28,  1.68it/s]Extractor Predicting: 141it [01:29,  1.65it/s]Extractor Predicting: 142it [01:30,  1.65it/s]Extractor Predicting: 143it [01:30,  1.68it/s]Extractor Predicting: 144it [01:31,  1.63it/s]Extractor Predicting: 145it [01:31,  1.64it/s]Extractor Predicting: 146it [01:32,  1.66it/s]Extractor Predicting: 147it [01:33,  1.66it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:34,  1.69it/s]Extractor Predicting: 150it [01:34,  1.67it/s]Extractor Predicting: 151it [01:35,  1.64it/s]Extractor Predicting: 152it [01:36,  1.67it/s]Extractor Predicting: 153it [01:36,  1.72it/s]Extractor Predicting: 154it [01:37,  1.73it/s]Extractor Predicting: 155it [01:37,  1.72it/s]Extractor Predicting: 156it [01:38,  1.70it/s]Extractor Predicting: 157it [01:39,  1.69it/s]Extractor Predicting: 158it [01:39,  1.70it/s]Extractor Predicting: 159it [01:40,  1.65it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:41,  1.64it/s]Extractor Predicting: 162it [01:42,  1.65it/s]Extractor Predicting: 163it [01:42,  1.66it/s]Extractor Predicting: 164it [01:43,  1.66it/s]Extractor Predicting: 165it [01:43,  1.65it/s]Extractor Predicting: 166it [01:44,  1.66it/s]Extractor Predicting: 167it [01:45,  1.67it/s]Extractor Predicting: 168it [01:45,  1.62it/s]Extractor Predicting: 169it [01:46,  1.63it/s]Extractor Predicting: 170it [01:46,  1.61it/s]Extractor Predicting: 171it [01:47,  1.60it/s]Extractor Predicting: 172it [01:48,  1.66it/s]Extractor Predicting: 173it [01:48,  1.56it/s]Extractor Predicting: 174it [01:49,  1.56it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:50,  1.56it/s]Extractor Predicting: 177it [01:51,  1.55it/s]Extractor Predicting: 178it [01:52,  1.51it/s]Extractor Predicting: 179it [01:52,  1.53it/s]Extractor Predicting: 180it [01:53,  1.57it/s]Extractor Predicting: 181it [01:54,  1.56it/s]Extractor Predicting: 182it [01:54,  1.54it/s]Extractor Predicting: 183it [01:55,  1.54it/s]Extractor Predicting: 184it [01:56,  1.52it/s]Extractor Predicting: 185it [01:56,  1.52it/s]Extractor Predicting: 186it [01:57,  1.52it/s]Extractor Predicting: 187it [01:58,  1.51it/s]Extractor Predicting: 188it [01:58,  1.54it/s]Extractor Predicting: 189it [01:59,  1.54it/s]Extractor Predicting: 190it [01:59,  1.56it/s]Extractor Predicting: 191it [02:00,  1.52it/s]Extractor Predicting: 192it [02:01,  1.52it/s]Extractor Predicting: 193it [02:01,  1.49it/s]Extractor Predicting: 194it [02:02,  1.49it/s]Extractor Predicting: 195it [02:03,  1.49it/s]Extractor Predicting: 196it [02:03,  1.51it/s]Extractor Predicting: 197it [02:04,  1.50it/s]Extractor Predicting: 198it [02:05,  1.50it/s]Extractor Predicting: 199it [02:05,  1.50it/s]Extractor Predicting: 200it [02:06,  1.49it/s]Extractor Predicting: 201it [02:07,  1.51it/s]Extractor Predicting: 202it [02:07,  1.51it/s]Extractor Predicting: 203it [02:08,  1.51it/s]Extractor Predicting: 204it [02:09,  1.54it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:10,  1.58it/s]Extractor Predicting: 207it [02:11,  1.57it/s]Extractor Predicting: 208it [02:11,  1.56it/s]Extractor Predicting: 209it [02:12,  1.55it/s]Extractor Predicting: 210it [02:13,  1.58it/s]Extractor Predicting: 211it [02:13,  1.61it/s]Extractor Predicting: 212it [02:14,  1.62it/s]Extractor Predicting: 213it [02:14,  1.62it/s]Extractor Predicting: 214it [02:15,  1.61it/s]Extractor Predicting: 215it [02:16,  1.56it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:17,  1.58it/s]Extractor Predicting: 218it [02:17,  1.62it/s]Extractor Predicting: 219it [02:18,  1.64it/s]Extractor Predicting: 220it [02:19,  1.61it/s]Extractor Predicting: 221it [02:19,  1.62it/s]Extractor Predicting: 222it [02:20,  1.45it/s]Extractor Predicting: 223it [02:21,  1.48it/s]Extractor Predicting: 224it [02:21,  1.52it/s]Extractor Predicting: 225it [02:22,  1.54it/s]Extractor Predicting: 226it [02:23,  1.49it/s]Extractor Predicting: 227it [02:23,  1.51it/s]Extractor Predicting: 228it [02:24,  1.53it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:25,  1.52it/s]Extractor Predicting: 231it [02:26,  1.53it/s]Extractor Predicting: 232it [02:27,  1.57it/s]Extractor Predicting: 233it [02:27,  1.64it/s]Extractor Predicting: 234it [02:28,  1.65it/s]Extractor Predicting: 235it [02:28,  1.66it/s]Extractor Predicting: 236it [02:29,  1.71it/s]Extractor Predicting: 237it [02:30,  1.69it/s]Extractor Predicting: 238it [02:30,  1.70it/s]Extractor Predicting: 239it [02:31,  1.69it/s]Extractor Predicting: 240it [02:31,  1.71it/s]Extractor Predicting: 241it [02:32,  1.70it/s]Extractor Predicting: 242it [02:32,  1.70it/s]Extractor Predicting: 243it [02:33,  1.74it/s]Extractor Predicting: 244it [02:34,  1.71it/s]Extractor Predicting: 245it [02:34,  1.78it/s]Extractor Predicting: 246it [02:35,  1.81it/s]Extractor Predicting: 247it [02:35,  1.76it/s]Extractor Predicting: 248it [02:36,  1.71it/s]Extractor Predicting: 249it [02:36,  1.71it/s]Extractor Predicting: 250it [02:37,  1.72it/s]Extractor Predicting: 251it [02:38,  1.75it/s]Extractor Predicting: 252it [02:38,  1.76it/s]Extractor Predicting: 253it [02:39,  1.75it/s]Extractor Predicting: 254it [02:39,  1.73it/s]Extractor Predicting: 255it [02:40,  1.71it/s]Extractor Predicting: 256it [02:40,  1.72it/s]Extractor Predicting: 257it [02:41,  1.75it/s]Extractor Predicting: 258it [02:42,  1.76it/s]Extractor Predicting: 259it [02:42,  1.79it/s]Extractor Predicting: 260it [02:43,  1.77it/s]Extractor Predicting: 261it [02:43,  1.63it/s]Extractor Predicting: 262it [02:44,  1.65it/s]Extractor Predicting: 263it [02:45,  1.61it/s]Extractor Predicting: 264it [02:45,  1.57it/s]Extractor Predicting: 265it [02:46,  1.56it/s]Extractor Predicting: 266it [02:47,  1.57it/s]Extractor Predicting: 267it [02:47,  1.60it/s]Extractor Predicting: 268it [02:48,  1.60it/s]Extractor Predicting: 269it [02:49,  1.57it/s]Extractor Predicting: 270it [02:49,  1.54it/s]Extractor Predicting: 271it [02:50,  1.48it/s]Extractor Predicting: 272it [02:51,  1.52it/s]Extractor Predicting: 273it [02:51,  1.53it/s]Extractor Predicting: 274it [02:52,  1.53it/s]Extractor Predicting: 275it [02:53,  1.54it/s]Extractor Predicting: 276it [02:53,  1.51it/s]Extractor Predicting: 277it [02:54,  1.50it/s]Extractor Predicting: 278it [02:55,  1.51it/s]Extractor Predicting: 279it [02:55,  1.55it/s]Extractor Predicting: 280it [02:56,  1.53it/s]Extractor Predicting: 281it [02:57,  1.51it/s]Extractor Predicting: 282it [02:57,  1.51it/s]Extractor Predicting: 283it [02:58,  1.53it/s]Extractor Predicting: 284it [02:58,  1.52it/s]Extractor Predicting: 285it [02:59,  1.53it/s]Extractor Predicting: 286it [03:00,  1.52it/s]Extractor Predicting: 287it [03:00,  1.51it/s]Extractor Predicting: 288it [03:01,  1.56it/s]Extractor Predicting: 289it [03:02,  1.56it/s]Extractor Predicting: 290it [03:02,  1.58it/s]Extractor Predicting: 291it [03:03,  1.58it/s]Extractor Predicting: 292it [03:04,  1.59it/s]Extractor Predicting: 293it [03:04,  1.59it/s]Extractor Predicting: 294it [03:05,  1.61it/s]Extractor Predicting: 295it [03:05,  1.61it/s]Extractor Predicting: 296it [03:06,  1.58it/s]Extractor Predicting: 297it [03:07,  1.60it/s]Extractor Predicting: 298it [03:07,  1.64it/s]Extractor Predicting: 299it [03:08,  1.61it/s]Extractor Predicting: 300it [03:09,  1.61it/s]Extractor Predicting: 301it [03:09,  1.58it/s]Extractor Predicting: 302it [03:10,  1.56it/s]Extractor Predicting: 303it [03:10,  1.57it/s]Extractor Predicting: 304it [03:11,  1.56it/s]Extractor Predicting: 305it [03:12,  1.56it/s]Extractor Predicting: 306it [03:12,  1.55it/s]Extractor Predicting: 307it [03:13,  1.56it/s]Extractor Predicting: 308it [03:14,  1.56it/s]Extractor Predicting: 309it [03:14,  1.54it/s]Extractor Predicting: 310it [03:15,  1.55it/s]Extractor Predicting: 311it [03:16,  1.55it/s]Extractor Predicting: 312it [03:16,  1.56it/s]Extractor Predicting: 313it [03:17,  1.57it/s]Extractor Predicting: 314it [03:18,  1.56it/s]Extractor Predicting: 315it [03:18,  1.58it/s]Extractor Predicting: 316it [03:19,  1.57it/s]Extractor Predicting: 317it [03:19,  1.59it/s]Extractor Predicting: 318it [03:20,  1.63it/s]Extractor Predicting: 319it [03:21,  1.60it/s]Extractor Predicting: 320it [03:21,  1.59it/s]Extractor Predicting: 321it [03:22,  1.57it/s]Extractor Predicting: 322it [03:23,  1.57it/s]Extractor Predicting: 323it [03:23,  1.41it/s]Extractor Predicting: 324it [03:24,  1.44it/s]Extractor Predicting: 325it [03:25,  1.47it/s]Extractor Predicting: 326it [03:25,  1.51it/s]Extractor Predicting: 327it [03:26,  1.54it/s]Extractor Predicting: 328it [03:27,  1.54it/s]Extractor Predicting: 329it [03:27,  1.54it/s]Extractor Predicting: 330it [03:28,  1.54it/s]Extractor Predicting: 331it [03:29,  1.55it/s]Extractor Predicting: 332it [03:29,  1.58it/s]Extractor Predicting: 333it [03:30,  1.55it/s]Extractor Predicting: 334it [03:30,  1.58it/s]Extractor Predicting: 335it [03:31,  1.57it/s]Extractor Predicting: 336it [03:32,  1.60it/s]Extractor Predicting: 337it [03:32,  1.58it/s]Extractor Predicting: 338it [03:33,  1.59it/s]Extractor Predicting: 339it [03:34,  1.58it/s]Extractor Predicting: 340it [03:34,  1.56it/s]Extractor Predicting: 341it [03:35,  1.59it/s]Extractor Predicting: 342it [03:35,  1.61it/s]Extractor Predicting: 343it [03:36,  1.61it/s]Extractor Predicting: 344it [03:37,  1.62it/s]Extractor Predicting: 345it [03:37,  1.59it/s]Extractor Predicting: 346it [03:38,  1.59it/s]Extractor Predicting: 347it [03:39,  1.60it/s]Extractor Predicting: 348it [03:39,  1.57it/s]Extractor Predicting: 349it [03:40,  1.57it/s]Extractor Predicting: 350it [03:41,  1.56it/s]Extractor Predicting: 351it [03:41,  1.58it/s]Extractor Predicting: 352it [03:42,  1.57it/s]Extractor Predicting: 353it [03:42,  1.57it/s]Extractor Predicting: 354it [03:43,  1.52it/s]Extractor Predicting: 355it [03:44,  1.53it/s]Extractor Predicting: 356it [03:44,  1.55it/s]Extractor Predicting: 357it [03:45,  1.54it/s]Extractor Predicting: 358it [03:46,  1.56it/s]Extractor Predicting: 359it [03:46,  1.54it/s]Extractor Predicting: 360it [03:47,  1.56it/s]Extractor Predicting: 361it [03:48,  1.56it/s]Extractor Predicting: 362it [03:48,  1.60it/s]Extractor Predicting: 363it [03:49,  1.60it/s]Extractor Predicting: 364it [03:49,  1.62it/s]Extractor Predicting: 365it [03:50,  1.61it/s]Extractor Predicting: 366it [03:51,  1.62it/s]Extractor Predicting: 367it [03:51,  1.57it/s]Extractor Predicting: 368it [03:52,  1.58it/s]Extractor Predicting: 369it [03:53,  1.55it/s]Extractor Predicting: 370it [03:53,  1.56it/s]Extractor Predicting: 371it [03:54,  1.56it/s]Extractor Predicting: 372it [03:55,  1.56it/s]Extractor Predicting: 373it [03:55,  1.56it/s]Extractor Predicting: 374it [03:56,  1.60it/s]Extractor Predicting: 375it [03:56,  1.59it/s]Extractor Predicting: 376it [03:57,  1.57it/s]Extractor Predicting: 377it [03:58,  1.61it/s]Extractor Predicting: 378it [03:58,  1.62it/s]Extractor Predicting: 379it [03:59,  1.65it/s]Extractor Predicting: 380it [04:00,  1.61it/s]Extractor Predicting: 381it [04:00,  1.62it/s]Extractor Predicting: 382it [04:01,  1.58it/s]Extractor Predicting: 383it [04:01,  1.57it/s]Extractor Predicting: 384it [04:02,  1.56it/s]Extractor Predicting: 385it [04:03,  1.57it/s]Extractor Predicting: 386it [04:03,  1.56it/s]Extractor Predicting: 387it [04:04,  1.54it/s]Extractor Predicting: 388it [04:05,  1.50it/s]Extractor Predicting: 389it [04:05,  1.52it/s]Extractor Predicting: 390it [04:06,  1.53it/s]Extractor Predicting: 391it [04:07,  1.56it/s]Extractor Predicting: 392it [04:07,  1.57it/s]Extractor Predicting: 393it [04:08,  1.56it/s]Extractor Predicting: 394it [04:09,  1.57it/s]Extractor Predicting: 395it [04:09,  1.58it/s]Extractor Predicting: 396it [04:10,  1.56it/s]Extractor Predicting: 397it [04:11,  1.56it/s]Extractor Predicting: 398it [04:11,  1.55it/s]Extractor Predicting: 399it [04:12,  1.59it/s]Extractor Predicting: 400it [04:12,  1.56it/s]Extractor Predicting: 401it [04:13,  1.57it/s]Extractor Predicting: 402it [04:14,  1.54it/s]Extractor Predicting: 403it [04:14,  1.53it/s]Extractor Predicting: 404it [04:15,  1.56it/s]Extractor Predicting: 405it [04:16,  1.57it/s]Extractor Predicting: 406it [04:16,  1.59it/s]Extractor Predicting: 407it [04:17,  1.58it/s]Extractor Predicting: 408it [04:18,  1.56it/s]Extractor Predicting: 409it [04:18,  1.59it/s]Extractor Predicting: 410it [04:19,  1.57it/s]Extractor Predicting: 411it [04:19,  1.58it/s]Extractor Predicting: 412it [04:20,  1.60it/s]Extractor Predicting: 413it [04:21,  1.60it/s]Extractor Predicting: 414it [04:21,  1.59it/s]Extractor Predicting: 415it [04:22,  1.55it/s]Extractor Predicting: 416it [04:23,  1.59it/s]Extractor Predicting: 417it [04:23,  1.59it/s]Extractor Predicting: 418it [04:24,  1.62it/s]Extractor Predicting: 419it [04:24,  1.63it/s]Extractor Predicting: 420it [04:25,  1.63it/s]Extractor Predicting: 421it [04:26,  1.60it/s]Extractor Predicting: 422it [04:26,  1.60it/s]Extractor Predicting: 423it [04:27,  1.62it/s]Extractor Predicting: 424it [04:28,  1.61it/s]Extractor Predicting: 425it [04:28,  1.58it/s]Extractor Predicting: 426it [04:29,  1.62it/s]Extractor Predicting: 427it [04:29,  1.61it/s]Extractor Predicting: 428it [04:30,  1.57it/s]Extractor Predicting: 429it [04:30,  1.80it/s]Extractor Predicting: 429it [04:30,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:17,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:17,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:17,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:17,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:17,799 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:52:18,262 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:52:18,263 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:52:18,541 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:52:19,659 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:52:19,659 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:22,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:22,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:22,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:22,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:52:22,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:52:23,369 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:52:23,370 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:52:23,694 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:52:23,917 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:52:23,917 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2019704433497537,
  "recall": 0.11962653180315114,
  "score": 0.1502565355484974,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.73it/s]Extractor Predicting: 5it [00:03,  1.62it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:52:29,241 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:52:29,242 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:52:29,319 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:52:29,319 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:52:29,355 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:52:38,608 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:52:38,622 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:52:38,711 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:52:38,712 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:52:38,768 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:52:38,802 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.08333333333333333,
  "recall": 0.022935779816513763,
  "score": 0.03597122302158273,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:52:39,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:39,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:40,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:40,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:41,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:42,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:42,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:43,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:43,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:44,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:45,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:45,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:46,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:46,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:47,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:47,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:48,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:49,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:49,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:50,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:50,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:50, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-29 01:52:51,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:51,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:52,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:52,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:53,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:53,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:55,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:55,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:56,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:56,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:57,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:58,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:58,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:59,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:52:59,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:00,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:00,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:01,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:01,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:02,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:24<03:35, 11.98s/it][WARNING|generation_utils.py:914] 2023-08-29 01:53:03,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:03,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:04,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:05,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:05,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:06,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:06,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:07,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:08,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:09,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:10,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:10,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:11,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:12,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:12,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:13,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:14,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:15,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:15,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:16,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:17,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:17,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:39<03:52, 13.70s/it][WARNING|generation_utils.py:914] 2023-08-29 01:53:18,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:19,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:20,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:20,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:21,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:22,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:22,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:23,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:23,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:24,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:25,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:25,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:26,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:27,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:27,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:28,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:28,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:29,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:30,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:30,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:52<03:30, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-29 01:53:31,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:31,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:32,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:32,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:33,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:34,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:35,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:35,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:36,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:37,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:38,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:38,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:39,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:40,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:41,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:41,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:42,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:43,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:43,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:44,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:45,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:06<03:26, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-29 01:53:45,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:46,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:47,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:47,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:48,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:49,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:49,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:50,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:50,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:51,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:52,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:52,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:53,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:54,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:54,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:55,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:55,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:56,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:57,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:58,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:19<03:07, 13.38s/it][WARNING|generation_utils.py:914] 2023-08-29 01:53:58,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:59,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:53:59,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:00,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:00,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:01,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:01,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:02,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:02,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:03,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:04,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:04,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:05,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:06,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:06,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:07,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:07,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:08,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:08,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:09,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:10,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:10,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:11,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:11,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:33<02:55, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 01:54:12,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:12,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:13,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:13,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:14,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:15,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:15,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:16,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:16,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:17,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:17,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:18,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:18,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:19,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:19,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:20,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:21,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:21,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:22,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:22,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:23,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:24,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:24,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:46<02:40, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-29 01:54:25,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:25,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:26,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:27,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:27,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:28,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:28,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:29,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:30,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:30,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:31,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:31,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:32,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:32,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:33,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:34,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:34,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:35,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:36,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:36,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:37,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:38,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:59<02:26, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-29 01:54:38,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:39,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:39,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:41,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:41,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:42,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:42,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:43,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:44,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:44,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:45,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:45,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:46,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:46,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:47,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:48,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:48,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:49,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:49,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:50,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:11<02:09, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-29 01:54:50,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:51,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:52,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:52,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:53,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:53,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:54,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:55,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:55,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:56,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:56,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:57,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:58,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:58,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:54:59,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:00,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:00,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:01,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:02,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:02,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:03,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:24<01:57, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-29 01:55:04,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:04,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:05,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:05,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:06,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:06,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:07,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:08,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:08,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:09,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:09,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:10,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:11,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:11,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:12,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:13,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:13,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:14,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:15,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:15,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:16,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:38<01:44, 13.09s/it][WARNING|generation_utils.py:914] 2023-08-29 01:55:17,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:17,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:18,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:19,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:19,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:20,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:20,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:21,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:22,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:22,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:23,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:23,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:24,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:25,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:25,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:26,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:27,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:27,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:28,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:29,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:50<01:30, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-29 01:55:29,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:30,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:31,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:31,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:32,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:33,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:34,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:34,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:35,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:36,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:36,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:37,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:37,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:38,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:39,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:39,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:40,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:41,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:41,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:42,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:04<01:18, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-29 01:55:43,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:43,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:44,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:46,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:46,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:47,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:48,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:48,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:49,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:50,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:50,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:51,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:51,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:52,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:52,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:53,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:54,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:54,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:55,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:55,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:56,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:18<01:06, 13.33s/it][WARNING|generation_utils.py:914] 2023-08-29 01:55:57,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:57,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:58,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:58,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:59,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:55:59,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:00,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:01,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:01,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:02,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:03,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:03,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:04,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:04,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:05,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:06,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:06,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:07,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:08,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:09,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:09,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:31<00:53, 13.38s/it][WARNING|generation_utils.py:914] 2023-08-29 01:56:10,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:11,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:11,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:12,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:13,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:13,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:14,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:14,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:15,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:16,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:16,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:17,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:18,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:18,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:19,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:20,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:20,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:21,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:22,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:22,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:23,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:45<00:40, 13.45s/it][WARNING|generation_utils.py:914] 2023-08-29 01:56:24,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:24,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:25,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:26,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:26,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:27,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:27,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:28,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:29,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:29,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:30,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:31,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:31,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:32,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:32,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:33,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:34,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:34,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:35,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:35,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:57<00:26, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-29 01:56:36,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:36,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:37,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:38,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:38,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:39,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:39,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:40,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:40,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:41,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:42,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:42,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:43,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:44,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:44,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:45,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:45,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:47,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:47,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:48,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:09<00:12, 12.86s/it][WARNING|generation_utils.py:914] 2023-08-29 01:56:48,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:49,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:49,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:50,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:51,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:51,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:52,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:53,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:53,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:54,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:55,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:55,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:56,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:56,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:57,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:58,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:58,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:59,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:56:59,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:57:00,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:22<00:00, 12.71s/it]Generating: 100%|██████████| 20/20 [04:22<00:00, 13.10s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:09,324 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:09,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:09,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:09,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:09,342 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:57:09,961 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:57:09,962 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:57:10,546 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:57:11,644 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:57:11,645 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:14,788 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:14,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:14,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:14,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:57:14,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:57:15,507 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:57:15,508 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:57:16,121 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:57:16,297 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:57:16,298 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
["Relation : head of government . Context : On 1 January 2014 , the government announced that it would be given an additional two days to plan and respond to the government 's response to the situation in Ukraine . Head Entity : Prime Minister , Tail Entity : Ukrainian government .\n"]
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.940625, 'errors': {'', "('British Empire Artists Society', 'after a work by', '', 'He was a member of the British Empire Artists Society until his death in 1876 .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 496, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7838541666666666, 'errors': {''}}
['Relation : country of citizenship . Context : On 31 March 2014 , the Swedish government appointed him a Vice President of the Social Democratic Party . Head Entity : Vice President of the Social Democratic Party , Tail Entity : Sweden .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.959375, 'errors': {''}}
['Relation : mouth of the watercourse . Context : The Cottages Creek Bridge is a bridge over a river at the mouth of Cottages Creek , New York , United States . Head Entity : Cottages Creek Bridge , Tail Entity : River .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year , the house was reconstructed , and the owners of the family moved to the new house , in downtown Edmonton . Head Entity : rebuilt , Tail Entity : Edmonton .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.95625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 10378
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10478, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.64it/s]Extractor Estimating: 4it [00:02,  1.64it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:03,  1.71it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.76it/s]Extractor Estimating: 9it [00:05,  1.77it/s]Extractor Estimating: 10it [00:05,  1.70it/s]Extractor Estimating: 11it [00:06,  1.73it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.76it/s]Extractor Estimating: 14it [00:08,  1.78it/s]Extractor Estimating: 15it [00:08,  1.75it/s]Extractor Estimating: 16it [00:09,  1.77it/s]Extractor Estimating: 17it [00:09,  1.77it/s]Extractor Estimating: 18it [00:10,  1.78it/s]Extractor Estimating: 19it [00:10,  1.83it/s]Extractor Estimating: 20it [00:11,  1.85it/s]Extractor Estimating: 21it [00:11,  1.90it/s]Extractor Estimating: 22it [00:12,  1.88it/s]Extractor Estimating: 23it [00:13,  1.79it/s]Extractor Estimating: 24it [00:13,  1.73it/s]Extractor Estimating: 25it [00:14,  1.78it/s]Extractor Estimating: 26it [00:14,  1.76it/s]Extractor Estimating: 27it [00:15,  1.73it/s]Extractor Estimating: 28it [00:16,  1.73it/s]Extractor Estimating: 29it [00:16,  1.75it/s]Extractor Estimating: 30it [00:17,  1.71it/s]Extractor Estimating: 31it [00:17,  1.72it/s]Extractor Estimating: 32it [00:18,  1.77it/s]Extractor Estimating: 33it [00:19,  1.64it/s]Extractor Estimating: 34it [00:19,  1.67it/s]Extractor Estimating: 35it [00:20,  1.69it/s]Extractor Estimating: 36it [00:20,  1.62it/s]Extractor Estimating: 37it [00:21,  1.61it/s]Extractor Estimating: 38it [00:22,  1.60it/s]Extractor Estimating: 39it [00:22,  1.55it/s]Extractor Estimating: 40it [00:23,  1.53it/s]Extractor Estimating: 41it [00:24,  1.50it/s]Extractor Estimating: 42it [00:24,  1.56it/s]Extractor Estimating: 43it [00:25,  1.62it/s]Extractor Estimating: 44it [00:25,  1.63it/s]Extractor Estimating: 45it [00:26,  1.62it/s]Extractor Estimating: 46it [00:27,  1.65it/s]Extractor Estimating: 47it [00:27,  1.65it/s]Extractor Estimating: 48it [00:28,  1.66it/s]Extractor Estimating: 49it [00:28,  1.69it/s]Extractor Estimating: 50it [00:29,  1.71it/s]Extractor Estimating: 51it [00:29,  1.77it/s]Extractor Estimating: 52it [00:30,  1.75it/s]Extractor Estimating: 53it [00:31,  1.79it/s]Extractor Estimating: 54it [00:31,  1.82it/s]Extractor Estimating: 55it [00:32,  1.74it/s]Extractor Estimating: 56it [00:32,  1.77it/s]Extractor Estimating: 57it [00:33,  1.78it/s]Extractor Estimating: 58it [00:33,  1.79it/s]Extractor Estimating: 59it [00:34,  1.79it/s]Extractor Estimating: 60it [00:34,  1.81it/s]Extractor Estimating: 61it [00:35,  1.81it/s]Extractor Estimating: 62it [00:36,  1.81it/s]Extractor Estimating: 63it [00:36,  1.82it/s]Extractor Estimating: 64it [00:37,  1.82it/s]Extractor Estimating: 65it [00:37,  1.79it/s]Extractor Estimating: 66it [00:38,  1.77it/s]Extractor Estimating: 67it [00:38,  1.82it/s]Extractor Estimating: 68it [00:39,  1.80it/s]Extractor Estimating: 69it [00:40,  1.78it/s]Extractor Estimating: 70it [00:40,  1.76it/s]Extractor Estimating: 71it [00:41,  1.78it/s]Extractor Estimating: 72it [00:41,  1.69it/s]Extractor Estimating: 73it [00:42,  1.69it/s]Extractor Estimating: 74it [00:42,  1.74it/s]Extractor Estimating: 75it [00:43,  1.73it/s]Extractor Estimating: 76it [00:44,  1.67it/s]Extractor Estimating: 77it [00:44,  1.62it/s]Extractor Estimating: 78it [00:45,  1.59it/s]Extractor Estimating: 79it [00:46,  1.53it/s]Extractor Estimating: 80it [00:46,  1.55it/s]Extractor Estimating: 81it [00:47,  1.54it/s]Extractor Estimating: 82it [00:48,  1.54it/s]Extractor Estimating: 83it [00:48,  1.53it/s]Extractor Estimating: 84it [00:49,  1.61it/s]Extractor Estimating: 85it [00:49,  1.63it/s]Extractor Estimating: 86it [00:50,  1.60it/s]Extractor Estimating: 87it [00:51,  1.56it/s]Extractor Estimating: 88it [00:51,  1.55it/s]Extractor Estimating: 89it [00:52,  1.53it/s]Extractor Estimating: 90it [00:53,  1.59it/s]Extractor Estimating: 91it [00:53,  1.60it/s]Extractor Estimating: 92it [00:54,  1.60it/s]Extractor Estimating: 93it [00:55,  1.57it/s]Extractor Estimating: 94it [00:55,  1.59it/s]Extractor Estimating: 95it [00:56,  1.59it/s]Extractor Estimating: 96it [00:56,  1.59it/s]Extractor Estimating: 97it [00:57,  1.59it/s]Extractor Estimating: 98it [00:58,  1.60it/s]Extractor Estimating: 99it [00:58,  1.60it/s]Extractor Estimating: 100it [00:59,  1.57it/s]Extractor Estimating: 101it [00:59,  1.68it/s]Extractor Estimating: 102it [01:00,  1.72it/s]Extractor Estimating: 103it [01:01,  1.71it/s]Extractor Estimating: 104it [01:01,  1.66it/s]Extractor Estimating: 105it [01:02,  1.70it/s]Extractor Estimating: 106it [01:02,  1.72it/s]Extractor Estimating: 107it [01:03,  1.74it/s]Extractor Estimating: 108it [01:04,  1.70it/s]Extractor Estimating: 109it [01:04,  1.79it/s]Extractor Estimating: 110it [01:05,  1.77it/s]Extractor Estimating: 111it [01:05,  1.79it/s]Extractor Estimating: 112it [01:06,  1.72it/s]Extractor Estimating: 113it [01:06,  1.76it/s]Extractor Estimating: 114it [01:07,  1.77it/s]Extractor Estimating: 115it [01:08,  1.72it/s]Extractor Estimating: 116it [01:08,  1.75it/s]Extractor Estimating: 117it [01:09,  1.75it/s]Extractor Estimating: 118it [01:09,  1.73it/s]Extractor Estimating: 119it [01:10,  1.59it/s]Extractor Estimating: 120it [01:11,  1.63it/s]Extractor Estimating: 121it [01:11,  1.69it/s]Extractor Estimating: 122it [01:12,  1.77it/s]Extractor Estimating: 123it [01:12,  1.83it/s]Extractor Estimating: 124it [01:13,  1.89it/s]Extractor Estimating: 125it [01:13,  1.88it/s]Extractor Estimating: 126it [01:14,  1.80it/s]Extractor Estimating: 127it [01:14,  1.73it/s]Extractor Estimating: 128it [01:15,  1.66it/s]Extractor Estimating: 129it [01:16,  1.65it/s]Extractor Estimating: 130it [01:16,  1.64it/s]Extractor Estimating: 131it [01:17,  1.57it/s]Extractor Estimating: 132it [01:18,  1.58it/s]Extractor Estimating: 133it [01:18,  1.54it/s]Extractor Estimating: 134it [01:19,  1.56it/s]Extractor Estimating: 135it [01:20,  1.55it/s]Extractor Estimating: 136it [01:20,  1.60it/s]Extractor Estimating: 137it [01:21,  1.64it/s]Extractor Estimating: 138it [01:21,  1.67it/s]Extractor Estimating: 139it [01:22,  1.64it/s]Extractor Estimating: 140it [01:23,  1.60it/s]Extractor Estimating: 141it [01:23,  1.58it/s]Extractor Estimating: 142it [01:24,  1.59it/s]Extractor Estimating: 143it [01:24,  1.66it/s]Extractor Estimating: 144it [01:25,  1.64it/s]Extractor Estimating: 145it [01:26,  1.67it/s]Extractor Estimating: 146it [01:26,  1.62it/s]Extractor Estimating: 147it [01:27,  1.62it/s]Extractor Estimating: 148it [01:28,  1.52it/s]Extractor Estimating: 149it [01:28,  1.52it/s]Extractor Estimating: 150it [01:29,  1.57it/s]Extractor Estimating: 151it [01:29,  1.68it/s]Extractor Estimating: 152it [01:30,  1.76it/s]Extractor Estimating: 153it [01:30,  1.84it/s]Extractor Estimating: 154it [01:31,  1.87it/s]Extractor Estimating: 155it [01:31,  1.85it/s]Extractor Estimating: 156it [01:32,  1.86it/s]Extractor Estimating: 157it [01:32,  1.88it/s]Extractor Estimating: 158it [01:33,  1.84it/s]Extractor Estimating: 159it [01:34,  1.82it/s]Extractor Estimating: 160it [01:34,  1.86it/s]Extractor Estimating: 161it [01:35,  1.82it/s]Extractor Estimating: 162it [01:35,  1.79it/s]Extractor Estimating: 163it [01:36,  1.75it/s]Extractor Estimating: 164it [01:36,  1.84it/s]Extractor Estimating: 165it [01:37,  1.82it/s]Extractor Estimating: 166it [01:37,  1.83it/s]Extractor Estimating: 167it [01:38,  1.81it/s]Extractor Estimating: 168it [01:39,  1.82it/s]Extractor Estimating: 169it [01:39,  1.84it/s]Extractor Estimating: 170it [01:40,  1.89it/s]Extractor Estimating: 171it [01:40,  1.88it/s]Extractor Estimating: 172it [01:41,  1.89it/s]Extractor Estimating: 173it [01:41,  1.83it/s]Extractor Estimating: 174it [01:42,  1.85it/s]Extractor Estimating: 175it [01:42,  1.85it/s]Extractor Estimating: 176it [01:43,  1.87it/s]Extractor Estimating: 177it [01:43,  1.87it/s]Extractor Estimating: 178it [01:44,  1.88it/s]Extractor Estimating: 179it [01:45,  1.73it/s]Extractor Estimating: 180it [01:45,  1.80it/s]Extractor Estimating: 181it [01:46,  1.87it/s]Extractor Estimating: 182it [01:46,  1.88it/s]Extractor Estimating: 183it [01:47,  1.92it/s]Extractor Estimating: 184it [01:47,  1.94it/s]Extractor Estimating: 185it [01:48,  1.88it/s]Extractor Estimating: 186it [01:48,  1.89it/s]Extractor Estimating: 187it [01:49,  1.96it/s]Extractor Estimating: 188it [01:49,  1.99it/s]Extractor Estimating: 189it [01:50,  1.93it/s]Extractor Estimating: 190it [01:50,  1.88it/s]Extractor Estimating: 191it [01:51,  1.92it/s]Extractor Estimating: 192it [01:51,  1.92it/s]Extractor Estimating: 193it [01:52,  1.89it/s]Extractor Estimating: 194it [01:52,  1.82it/s]Extractor Estimating: 195it [01:53,  1.88it/s]Extractor Estimating: 196it [01:53,  1.84it/s]Extractor Estimating: 197it [01:54,  1.82it/s]Extractor Estimating: 198it [01:55,  1.84it/s]Extractor Estimating: 199it [01:55,  1.86it/s]Extractor Estimating: 200it [01:56,  1.80it/s]Extractor Estimating: 201it [01:56,  1.77it/s]Extractor Estimating: 202it [01:57,  1.70it/s]Extractor Estimating: 203it [01:58,  1.64it/s]Extractor Estimating: 204it [01:58,  1.66it/s]Extractor Estimating: 205it [01:59,  1.58it/s]Extractor Estimating: 206it [01:59,  1.60it/s]Extractor Estimating: 207it [02:00,  1.61it/s]Extractor Estimating: 208it [02:01,  1.62it/s]Extractor Estimating: 209it [02:01,  1.62it/s]Extractor Estimating: 210it [02:02,  1.60it/s]Extractor Estimating: 211it [02:03,  1.63it/s]Extractor Estimating: 212it [02:03,  1.67it/s]Extractor Estimating: 213it [02:04,  1.68it/s]Extractor Estimating: 214it [02:04,  1.62it/s]Extractor Estimating: 215it [02:05,  1.57it/s]Extractor Estimating: 216it [02:06,  1.57it/s]Extractor Estimating: 217it [02:06,  1.65it/s]Extractor Estimating: 218it [02:07,  1.63it/s]Extractor Estimating: 219it [02:08,  1.45it/s]Extractor Estimating: 220it [02:08,  1.45it/s]Extractor Estimating: 221it [02:09,  1.49it/s]Extractor Estimating: 222it [02:10,  1.56it/s]Extractor Estimating: 223it [02:10,  1.57it/s]Extractor Estimating: 224it [02:11,  1.53it/s]Extractor Estimating: 225it [02:12,  1.50it/s]Extractor Estimating: 226it [02:12,  1.56it/s]Extractor Estimating: 227it [02:13,  1.60it/s]Extractor Estimating: 228it [02:13,  1.63it/s]Extractor Estimating: 229it [02:14,  1.65it/s]Extractor Estimating: 230it [02:15,  1.61it/s]Extractor Estimating: 231it [02:15,  1.66it/s]Extractor Estimating: 232it [02:16,  1.72it/s]Extractor Estimating: 233it [02:16,  1.68it/s]Extractor Estimating: 234it [02:17,  1.74it/s]Extractor Estimating: 235it [02:17,  1.72it/s]Extractor Estimating: 236it [02:18,  1.62it/s]Extractor Estimating: 237it [02:19,  1.63it/s]Extractor Estimating: 238it [02:19,  1.68it/s]Extractor Estimating: 239it [02:20,  1.67it/s]Extractor Estimating: 240it [02:20,  1.69it/s]Extractor Estimating: 241it [02:21,  1.73it/s]Extractor Estimating: 242it [02:22,  1.70it/s]Extractor Estimating: 243it [02:22,  1.71it/s]Extractor Estimating: 244it [02:23,  1.62it/s]Extractor Estimating: 245it [02:24,  1.59it/s]Extractor Estimating: 246it [02:24,  1.63it/s]Extractor Estimating: 247it [02:25,  1.66it/s]Extractor Estimating: 248it [02:25,  1.73it/s]Extractor Estimating: 249it [02:26,  1.63it/s]Extractor Estimating: 250it [02:27,  1.66it/s]Extractor Estimating: 251it [02:27,  1.65it/s]Extractor Estimating: 252it [02:28,  1.68it/s]Extractor Estimating: 253it [02:28,  1.69it/s]Extractor Estimating: 254it [02:29,  1.67it/s]Extractor Estimating: 255it [02:30,  1.65it/s]Extractor Estimating: 256it [02:30,  1.66it/s]Extractor Estimating: 257it [02:31,  1.69it/s]Extractor Estimating: 258it [02:31,  1.61it/s]Extractor Estimating: 259it [02:32,  1.61it/s]Extractor Estimating: 260it [02:33,  1.62it/s]Extractor Estimating: 261it [02:33,  1.65it/s]Extractor Estimating: 262it [02:34,  1.72it/s]Extractor Estimating: 263it [02:34,  1.66it/s]Extractor Estimating: 264it [02:35,  1.68it/s]Extractor Estimating: 265it [02:36,  1.65it/s]Extractor Estimating: 266it [02:36,  1.65it/s]Extractor Estimating: 267it [02:37,  1.67it/s]Extractor Estimating: 268it [02:37,  1.69it/s]Extractor Estimating: 269it [02:38,  1.70it/s]Extractor Estimating: 270it [02:39,  1.66it/s]Extractor Estimating: 271it [02:39,  1.63it/s]Extractor Estimating: 272it [02:40,  1.68it/s]Extractor Estimating: 273it [02:40,  1.66it/s]Extractor Estimating: 274it [02:41,  1.69it/s]Extractor Estimating: 275it [02:42,  1.69it/s]Extractor Estimating: 276it [02:42,  1.67it/s]Extractor Estimating: 277it [02:43,  1.67it/s]Extractor Estimating: 278it [02:43,  1.73it/s]Extractor Estimating: 279it [02:44,  1.73it/s]Extractor Estimating: 280it [02:44,  1.73it/s]Extractor Estimating: 281it [02:45,  1.73it/s]Extractor Estimating: 282it [02:46,  1.75it/s]Extractor Estimating: 283it [02:46,  1.65it/s]Extractor Estimating: 284it [02:47,  1.67it/s]Extractor Estimating: 285it [02:47,  1.66it/s]Extractor Estimating: 286it [02:48,  1.68it/s]Extractor Estimating: 287it [02:49,  1.71it/s]Extractor Estimating: 288it [02:49,  1.68it/s]Extractor Estimating: 289it [02:50,  1.68it/s]Extractor Estimating: 290it [02:50,  1.67it/s]Extractor Estimating: 291it [02:51,  1.59it/s]Extractor Estimating: 292it [02:52,  1.61it/s]Extractor Estimating: 293it [02:52,  1.68it/s]Extractor Estimating: 294it [02:53,  1.70it/s]Extractor Estimating: 295it [02:53,  1.72it/s]Extractor Estimating: 296it [02:54,  1.70it/s]Extractor Estimating: 297it [02:55,  1.70it/s]Extractor Estimating: 298it [02:55,  1.71it/s]Extractor Estimating: 299it [02:56,  1.68it/s]Extractor Estimating: 300it [02:56,  1.71it/s]Extractor Estimating: 301it [02:57,  1.76it/s]Extractor Estimating: 302it [02:57,  1.74it/s]Extractor Estimating: 303it [02:58,  1.77it/s]Extractor Estimating: 304it [02:59,  1.75it/s]Extractor Estimating: 305it [02:59,  1.81it/s]Extractor Estimating: 306it [03:00,  1.83it/s]Extractor Estimating: 307it [03:00,  1.86it/s]Extractor Estimating: 308it [03:01,  1.77it/s]Extractor Estimating: 309it [03:01,  1.75it/s]Extractor Estimating: 310it [03:02,  1.83it/s]Extractor Estimating: 311it [03:02,  1.84it/s]Extractor Estimating: 312it [03:03,  1.81it/s]Extractor Estimating: 313it [03:03,  1.86it/s]Extractor Estimating: 314it [03:04,  1.82it/s]Extractor Estimating: 315it [03:05,  1.79it/s]Extractor Estimating: 316it [03:05,  1.81it/s]Extractor Estimating: 317it [03:06,  1.82it/s]Extractor Estimating: 318it [03:06,  1.85it/s]Extractor Estimating: 319it [03:07,  1.83it/s]Extractor Estimating: 320it [03:07,  1.73it/s]Extractor Estimating: 321it [03:08,  1.60it/s]Extractor Estimating: 322it [03:09,  1.62it/s]Extractor Estimating: 323it [03:09,  1.60it/s]Extractor Estimating: 324it [03:10,  1.68it/s]Extractor Estimating: 325it [03:10,  1.71it/s]Extractor Estimating: 326it [03:11,  1.74it/s]Extractor Estimating: 327it [03:12,  1.71it/s]Extractor Estimating: 328it [03:12,  1.71it/s]Extractor Estimating: 329it [03:13,  1.76it/s]Extractor Estimating: 330it [03:13,  1.81it/s]Extractor Estimating: 331it [03:14,  1.79it/s]Extractor Estimating: 332it [03:14,  1.79it/s]Extractor Estimating: 333it [03:15,  1.75it/s]Extractor Estimating: 334it [03:15,  1.82it/s]Extractor Estimating: 335it [03:16,  1.87it/s]Extractor Estimating: 336it [03:17,  1.89it/s]Extractor Estimating: 337it [03:17,  1.88it/s]Extractor Estimating: 338it [03:18,  1.79it/s]Extractor Estimating: 339it [03:18,  1.81it/s]Extractor Estimating: 340it [03:19,  1.81it/s]Extractor Estimating: 341it [03:19,  1.85it/s]Extractor Estimating: 342it [03:20,  1.86it/s]Extractor Estimating: 343it [03:20,  1.81it/s]Extractor Estimating: 344it [03:21,  1.74it/s]Extractor Estimating: 345it [03:22,  1.73it/s]Extractor Estimating: 346it [03:22,  1.73it/s]Extractor Estimating: 347it [03:23,  1.74it/s]Extractor Estimating: 348it [03:23,  1.75it/s]Extractor Estimating: 349it [03:24,  1.73it/s]Extractor Estimating: 350it [03:24,  1.74it/s]Extractor Estimating: 351it [03:25,  1.76it/s]Extractor Estimating: 352it [03:26,  1.73it/s]Extractor Estimating: 353it [03:26,  1.75it/s]Extractor Estimating: 354it [03:27,  1.77it/s]Extractor Estimating: 355it [03:27,  1.76it/s]Extractor Estimating: 356it [03:28,  1.69it/s]Extractor Estimating: 357it [03:29,  1.68it/s]Extractor Estimating: 358it [03:29,  1.64it/s]Extractor Estimating: 359it [03:30,  1.62it/s]Extractor Estimating: 360it [03:30,  1.64it/s]Extractor Estimating: 361it [03:31,  1.69it/s]Extractor Estimating: 362it [03:32,  1.71it/s]Extractor Estimating: 363it [03:32,  1.71it/s]Extractor Estimating: 364it [03:33,  1.74it/s]Extractor Estimating: 365it [03:33,  1.72it/s]Extractor Estimating: 366it [03:34,  1.73it/s]Extractor Estimating: 367it [03:34,  1.71it/s]Extractor Estimating: 368it [03:35,  1.75it/s]Extractor Estimating: 369it [03:36,  1.73it/s]Extractor Estimating: 370it [03:36,  1.74it/s]Extractor Estimating: 371it [03:37,  1.73it/s]Extractor Estimating: 372it [03:37,  1.72it/s]Extractor Estimating: 373it [03:38,  1.73it/s]Extractor Estimating: 374it [03:38,  1.73it/s]Extractor Estimating: 375it [03:39,  1.73it/s]Extractor Estimating: 376it [03:40,  1.78it/s]Extractor Estimating: 377it [03:40,  1.74it/s]Extractor Estimating: 378it [03:41,  1.74it/s]Extractor Estimating: 379it [03:41,  1.69it/s]Extractor Estimating: 380it [03:42,  1.70it/s]Extractor Estimating: 381it [03:43,  1.71it/s]Extractor Estimating: 382it [03:43,  1.72it/s]Extractor Estimating: 383it [03:44,  1.73it/s]Extractor Estimating: 384it [03:44,  1.76it/s]Extractor Estimating: 385it [03:45,  1.73it/s]Extractor Estimating: 386it [03:45,  1.71it/s]Extractor Estimating: 387it [03:46,  1.78it/s]Extractor Estimating: 388it [03:47,  1.78it/s]Extractor Estimating: 389it [03:47,  1.78it/s]Extractor Estimating: 390it [03:48,  1.70it/s]Extractor Estimating: 391it [03:48,  1.75it/s]Extractor Estimating: 392it [03:49,  1.74it/s]Extractor Estimating: 393it [03:49,  1.72it/s]Extractor Estimating: 394it [03:50,  1.72it/s]Extractor Estimating: 395it [03:51,  1.71it/s]Extractor Estimating: 396it [03:51,  1.74it/s]Extractor Estimating: 397it [03:52,  1.79it/s]Extractor Estimating: 398it [03:52,  1.76it/s]Extractor Estimating: 399it [03:53,  1.70it/s]Extractor Estimating: 400it [03:53,  1.72it/s]Extractor Estimating: 401it [03:54,  1.78it/s]Extractor Estimating: 402it [03:55,  1.80it/s]Extractor Estimating: 403it [03:55,  1.75it/s]Extractor Estimating: 404it [03:56,  1.79it/s]Extractor Estimating: 405it [03:56,  1.85it/s]Extractor Estimating: 406it [03:57,  1.81it/s]Extractor Estimating: 407it [03:57,  1.90it/s]Extractor Estimating: 408it [03:58,  1.80it/s]Extractor Estimating: 409it [03:58,  1.73it/s]Extractor Estimating: 410it [03:59,  1.73it/s]Extractor Estimating: 411it [04:00,  1.73it/s]Extractor Estimating: 412it [04:00,  1.56it/s]Extractor Estimating: 413it [04:01,  1.62it/s]Extractor Estimating: 414it [04:02,  1.57it/s]Extractor Estimating: 415it [04:02,  1.63it/s]Extractor Estimating: 416it [04:03,  1.67it/s]Extractor Estimating: 417it [04:03,  1.65it/s]Extractor Estimating: 418it [04:04,  1.68it/s]Extractor Estimating: 419it [04:05,  1.68it/s]Extractor Estimating: 420it [04:05,  1.60it/s]Extractor Estimating: 421it [04:06,  1.61it/s]Extractor Estimating: 422it [04:06,  1.63it/s]Extractor Estimating: 423it [04:07,  1.67it/s]Extractor Estimating: 424it [04:08,  1.72it/s]Extractor Estimating: 425it [04:08,  1.67it/s]Extractor Estimating: 426it [04:09,  1.66it/s]Extractor Estimating: 427it [04:09,  1.65it/s]Extractor Estimating: 428it [04:10,  1.60it/s]Extractor Estimating: 429it [04:11,  1.59it/s]Extractor Estimating: 430it [04:11,  1.54it/s]Extractor Estimating: 431it [04:12,  1.58it/s]Extractor Estimating: 432it [04:13,  1.63it/s]Extractor Estimating: 433it [04:13,  1.61it/s]Extractor Estimating: 434it [04:14,  1.53it/s]Extractor Estimating: 435it [04:15,  1.52it/s]Extractor Estimating: 436it [04:15,  1.54it/s]Extractor Estimating: 437it [04:16,  1.56it/s]Extractor Estimating: 438it [04:16,  1.59it/s]Extractor Estimating: 439it [04:17,  1.55it/s]Extractor Estimating: 440it [04:18,  1.54it/s]Extractor Estimating: 441it [04:18,  1.61it/s]Extractor Estimating: 442it [04:19,  1.60it/s]Extractor Estimating: 443it [04:20,  1.60it/s]Extractor Estimating: 444it [04:20,  1.64it/s]Extractor Estimating: 445it [04:21,  1.59it/s]Extractor Estimating: 446it [04:22,  1.60it/s]Extractor Estimating: 447it [04:22,  1.60it/s]Extractor Estimating: 448it [04:23,  1.62it/s]Extractor Estimating: 449it [04:23,  1.60it/s]Extractor Estimating: 450it [04:24,  1.65it/s]Extractor Estimating: 451it [04:25,  1.66it/s]Extractor Estimating: 452it [04:25,  1.72it/s]Extractor Estimating: 453it [04:26,  1.74it/s]Extractor Estimating: 454it [04:26,  1.79it/s]Extractor Estimating: 455it [04:27,  1.81it/s]Extractor Estimating: 456it [04:27,  1.76it/s]Extractor Estimating: 457it [04:28,  1.79it/s]Extractor Estimating: 458it [04:28,  1.84it/s]Extractor Estimating: 459it [04:29,  1.86it/s]Extractor Estimating: 460it [04:29,  1.84it/s]Extractor Estimating: 461it [04:30,  1.80it/s]Extractor Estimating: 462it [04:31,  1.75it/s]Extractor Estimating: 463it [04:31,  1.79it/s]Extractor Estimating: 464it [04:32,  1.79it/s]Extractor Estimating: 465it [04:32,  1.80it/s]Extractor Estimating: 466it [04:33,  1.83it/s]Extractor Estimating: 467it [04:33,  1.84it/s]Extractor Estimating: 468it [04:34,  1.81it/s]Extractor Estimating: 469it [04:34,  1.81it/s]Extractor Estimating: 470it [04:35,  1.79it/s]Extractor Estimating: 471it [04:36,  1.83it/s]Extractor Estimating: 472it [04:36,  1.79it/s]Extractor Estimating: 473it [04:37,  1.82it/s]Extractor Estimating: 474it [04:37,  1.85it/s]Extractor Estimating: 475it [04:38,  1.84it/s]Extractor Estimating: 476it [04:38,  1.81it/s]Extractor Estimating: 477it [04:39,  1.69it/s]Extractor Estimating: 478it [04:40,  1.66it/s]Extractor Estimating: 479it [04:40,  1.58it/s]Extractor Estimating: 480it [04:41,  1.54it/s]Extractor Estimating: 481it [04:42,  1.47it/s]Extractor Estimating: 482it [04:42,  1.51it/s]Extractor Estimating: 483it [04:43,  1.43it/s]Extractor Estimating: 484it [04:44,  1.50it/s]Extractor Estimating: 485it [04:44,  1.55it/s]Extractor Estimating: 486it [04:45,  1.51it/s]Extractor Estimating: 487it [04:46,  1.55it/s]Extractor Estimating: 488it [04:46,  1.60it/s]Extractor Estimating: 489it [04:47,  1.60it/s]Extractor Estimating: 490it [04:47,  1.63it/s]Extractor Estimating: 491it [04:48,  1.61it/s]Extractor Estimating: 492it [04:49,  1.61it/s]Extractor Estimating: 493it [04:49,  1.60it/s]Extractor Estimating: 494it [04:50,  1.59it/s]Extractor Estimating: 495it [04:51,  1.62it/s]Extractor Estimating: 496it [04:51,  1.47it/s]Extractor Estimating: 497it [04:52,  1.52it/s]Extractor Estimating: 498it [04:53,  1.50it/s]Extractor Estimating: 499it [04:53,  1.47it/s]Extractor Estimating: 500it [04:54,  1.74it/s]Extractor Estimating: 500it [04:54,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:27,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:27,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:27,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:27,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:27,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:02:28,537 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:02:28,538 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:02:29,133 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:02:30,228 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:02:30,229 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:33,274 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:33,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:33,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:33,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:02:33,301 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:02:33,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:02:33,955 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:02:34,547 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:02:34,725 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:02:34,725 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:43:53,161 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:43:53,185 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 6010 mean pseudo reward: 0.9802578970094166
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 16493
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16593, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16593, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.966, loss:522.2561
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.956, loss:469.4193
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 49, avg_time 0.972, loss:447.3201
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 149, avg_time 0.963, loss:407.9991
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 249, avg_time 0.963, loss:413.1044
>> valid entity prec:0.5549, rec:0.5615, f1:0.5582
>> valid relation prec:0.1201, rec:0.0517, f1:0.0723
>> valid relation with NER prec:0.1201, rec:0.0517, f1:0.0723
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 98, avg_time 2.227, loss:348.9326
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 198, avg_time 0.944, loss:395.6181
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 47, avg_time 0.962, loss:358.1200
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 147, avg_time 0.950, loss:356.4020
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 247, avg_time 0.970, loss:413.9471
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5620, rec:0.5495, f1:0.5557
>> valid relation prec:0.1274, rec:0.0551, f1:0.0770
>> valid relation with NER prec:0.1274, rec:0.0551, f1:0.0770
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 96, avg_time 2.241, loss:337.3332
g_step 1200, step 196, avg_time 0.965, loss:372.8181
g_step 1300, step 45, avg_time 0.936, loss:326.8278
g_step 1400, step 145, avg_time 0.964, loss:335.1452
g_step 1500, step 245, avg_time 0.977, loss:349.8077
>> valid entity prec:0.5464, rec:0.5237, f1:0.5349
>> valid relation prec:0.1476, rec:0.0583, f1:0.0836
>> valid relation with NER prec:0.1476, rec:0.0583, f1:0.0836
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 94, avg_time 2.230, loss:310.9165
g_step 1700, step 194, avg_time 0.952, loss:309.2100
g_step 1800, step 43, avg_time 0.951, loss:308.3042
g_step 1900, step 143, avg_time 0.967, loss:295.9029
g_step 2000, step 243, avg_time 0.953, loss:302.7655
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5427, rec:0.5735, f1:0.5577
>> valid relation prec:0.1136, rec:0.0526, f1:0.0719
>> valid relation with NER prec:0.1136, rec:0.0526, f1:0.0719
g_step 2100, step 92, avg_time 2.225, loss:264.5522
g_step 2200, step 192, avg_time 0.966, loss:289.6684
g_step 2300, step 41, avg_time 0.952, loss:257.3440
g_step 2400, step 141, avg_time 0.950, loss:254.6849
g_step 2500, step 241, avg_time 0.962, loss:273.1289
>> valid entity prec:0.5768, rec:0.5127, f1:0.5429
>> valid relation prec:0.0823, rec:0.0402, f1:0.0540
>> valid relation with NER prec:0.0823, rec:0.0402, f1:0.0540
g_step 2600, step 90, avg_time 2.216, loss:255.4963
g_step 2700, step 190, avg_time 0.956, loss:245.2510
g_step 2800, step 39, avg_time 0.946, loss:227.0078
g_step 2900, step 139, avg_time 0.954, loss:223.3590
g_step 3000, step 239, avg_time 0.958, loss:237.6792
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5677, rec:0.5282, f1:0.5472
>> valid relation prec:0.1392, rec:0.0744, f1:0.0969
>> valid relation with NER prec:0.1392, rec:0.0744, f1:0.0969
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 88, avg_time 2.236, loss:233.1702
g_step 3200, step 188, avg_time 0.945, loss:215.3628
g_step 3300, step 37, avg_time 0.952, loss:227.3107
g_step 3400, step 137, avg_time 0.954, loss:198.4628
g_step 3500, step 237, avg_time 0.955, loss:227.7885
>> valid entity prec:0.5709, rec:0.5292, f1:0.5493
>> valid relation prec:0.0926, rec:0.0408, f1:0.0566
>> valid relation with NER prec:0.0926, rec:0.0408, f1:0.0566
g_step 3600, step 86, avg_time 2.218, loss:210.5133
g_step 3700, step 186, avg_time 0.944, loss:215.2793
g_step 3800, step 35, avg_time 0.964, loss:207.3937
g_step 3900, step 135, avg_time 0.953, loss:179.2963
g_step 4000, step 235, avg_time 0.958, loss:188.8991
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5611, rec:0.5191, f1:0.5393
>> valid relation prec:0.1237, rec:0.0629, f1:0.0834
>> valid relation with NER prec:0.1237, rec:0.0629, f1:0.0834
g_step 4100, step 84, avg_time 2.210, loss:186.2366
g_step 4200, step 184, avg_time 0.954, loss:182.8193
g_step 4300, step 33, avg_time 0.952, loss:189.8427
g_step 4400, step 133, avg_time 0.950, loss:175.6938
g_step 4500, step 233, avg_time 0.953, loss:197.0617
>> valid entity prec:0.5669, rec:0.5116, f1:0.5378
>> valid relation prec:0.1047, rec:0.0540, f1:0.0712
>> valid relation with NER prec:0.1047, rec:0.0540, f1:0.0712
g_step 4600, step 82, avg_time 2.230, loss:162.5927
g_step 4700, step 182, avg_time 0.966, loss:178.0003
g_step 4800, step 31, avg_time 0.949, loss:186.5355
g_step 4900, step 131, avg_time 0.956, loss:168.9546
g_step 5000, step 231, avg_time 0.962, loss:171.2285
learning rate was adjusted to 0.0008
>> valid entity prec:0.5832, rec:0.4867, f1:0.5306
>> valid relation prec:0.1194, rec:0.0594, f1:0.0794
>> valid relation with NER prec:0.1194, rec:0.0594, f1:0.0794
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:43:53 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:43:53 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-43-53_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:43:54 - WARNING - datasets.builder -   Using custom data configuration default-a6c22338776cfcb5
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a6c22338776cfcb5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:43:56,251 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:43:56,272 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:43:56,272 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:43:56,273 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:43:56,370 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:43:56,422 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:43:56,731 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:43:59,861 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:43:59,891 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a6c22338776cfcb5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.59ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.63ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.18ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.50ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.72ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.86ba/s]100%|██████████| 7/7 [00:01<00:00,  5.10ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.44ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.34ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.80ba/s]100%|██████████| 4/4 [00:00<00:00,  4.92ba/s]100%|██████████| 4/4 [00:00<00:00,  4.15ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.13ba/s] 29%|██▊       | 2/7 [00:00<00:00,  5.61ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  8.20ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.41ba/s]100%|██████████| 7/7 [00:00<00:00,  9.55ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.31ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.98ba/s]100%|██████████| 4/4 [00:00<00:00,  8.85ba/s]
[INFO|trainer.py:414] 2023-08-29 03:44:04,861 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:44:04,988 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:44:04,988 >>   Num examples = 6010
[INFO|trainer.py:1149] 2023-08-29 03:44:04,988 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:44:04,988 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:44:04,988 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:44:04,988 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:44:04,988 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:20,  3.33it/s]  0%|          | 2/470 [00:00<02:17,  3.41it/s]  1%|          | 3/470 [00:00<02:16,  3.43it/s]  1%|          | 4/470 [00:01<02:15,  3.44it/s]  1%|          | 5/470 [00:01<02:22,  3.26it/s]  1%|▏         | 6/470 [00:01<02:20,  3.31it/s]  1%|▏         | 7/470 [00:02<02:18,  3.34it/s]  2%|▏         | 8/470 [00:02<02:17,  3.36it/s]  2%|▏         | 9/470 [00:02<02:16,  3.38it/s]  2%|▏         | 10/470 [00:02<02:15,  3.39it/s]  2%|▏         | 11/470 [00:03<02:15,  3.39it/s]  3%|▎         | 12/470 [00:03<02:14,  3.40it/s]  3%|▎         | 13/470 [00:03<02:14,  3.40it/s]  3%|▎         | 14/470 [00:04<02:14,  3.40it/s]  3%|▎         | 15/470 [00:04<02:13,  3.40it/s]  3%|▎         | 16/470 [00:04<02:15,  3.35it/s]  4%|▎         | 17/470 [00:05<02:14,  3.36it/s]  4%|▍         | 18/470 [00:05<02:13,  3.38it/s]  4%|▍         | 19/470 [00:05<02:13,  3.39it/s]  4%|▍         | 20/470 [00:05<02:12,  3.39it/s]  4%|▍         | 21/470 [00:06<02:12,  3.39it/s]  5%|▍         | 22/470 [00:06<02:11,  3.40it/s]  5%|▍         | 23/470 [00:06<02:11,  3.40it/s]  5%|▌         | 24/470 [00:07<02:11,  3.40it/s]  5%|▌         | 25/470 [00:07<02:10,  3.40it/s]  6%|▌         | 26/470 [00:07<02:13,  3.33it/s]  6%|▌         | 27/470 [00:08<02:14,  3.29it/s]  6%|▌         | 28/470 [00:08<02:12,  3.33it/s]  6%|▌         | 29/470 [00:08<02:11,  3.35it/s]  6%|▋         | 30/470 [00:08<02:10,  3.36it/s]  7%|▋         | 31/470 [00:09<02:10,  3.38it/s]  7%|▋         | 32/470 [00:09<02:09,  3.39it/s]  7%|▋         | 33/470 [00:09<02:08,  3.41it/s]  7%|▋         | 34/470 [00:10<02:15,  3.23it/s]  7%|▋         | 35/470 [00:10<02:11,  3.30it/s]  8%|▊         | 36/470 [00:10<02:09,  3.34it/s]  8%|▊         | 37/470 [00:10<02:08,  3.37it/s]  8%|▊         | 38/470 [00:11<02:08,  3.36it/s]  8%|▊         | 39/470 [00:11<02:07,  3.38it/s]  9%|▊         | 40/470 [00:11<02:06,  3.41it/s]  9%|▊         | 41/470 [00:12<02:05,  3.42it/s]  9%|▉         | 42/470 [00:12<02:04,  3.44it/s]  9%|▉         | 43/470 [00:12<02:04,  3.44it/s]  9%|▉         | 44/470 [00:13<02:03,  3.45it/s] 10%|▉         | 45/470 [00:13<02:03,  3.45it/s] 10%|▉         | 46/470 [00:13<02:02,  3.45it/s] 10%|█         | 47/470 [00:13<02:02,  3.46it/s] 10%|█         | 48/470 [00:14<02:02,  3.46it/s] 10%|█         | 49/470 [00:14<02:03,  3.40it/s] 11%|█         | 50/470 [00:14<02:02,  3.42it/s] 11%|█         | 51/470 [00:15<02:02,  3.43it/s] 11%|█         | 52/470 [00:15<02:02,  3.42it/s] 11%|█▏        | 53/470 [00:15<02:01,  3.44it/s] 11%|█▏        | 54/470 [00:15<02:00,  3.44it/s] 12%|█▏        | 55/470 [00:16<02:00,  3.45it/s] 12%|█▏        | 56/470 [00:16<02:00,  3.45it/s] 12%|█▏        | 57/470 [00:16<01:59,  3.45it/s] 12%|█▏        | 58/470 [00:17<01:59,  3.45it/s] 13%|█▎        | 59/470 [00:17<01:58,  3.45it/s] 13%|█▎        | 60/470 [00:17<02:00,  3.41it/s] 13%|█▎        | 61/470 [00:17<01:59,  3.42it/s] 13%|█▎        | 62/470 [00:18<01:58,  3.43it/s] 13%|█▎        | 63/470 [00:18<01:58,  3.44it/s] 14%|█▎        | 64/470 [00:18<01:57,  3.44it/s] 14%|█▍        | 65/470 [00:19<01:57,  3.45it/s] 14%|█▍        | 66/470 [00:19<01:57,  3.45it/s] 14%|█▍        | 67/470 [00:19<01:56,  3.45it/s] 14%|█▍        | 68/470 [00:20<01:56,  3.45it/s] 15%|█▍        | 69/470 [00:20<01:56,  3.45it/s] 15%|█▍        | 70/470 [00:20<01:55,  3.45it/s] 15%|█▌        | 71/470 [00:20<01:55,  3.45it/s] 15%|█▌        | 72/470 [00:21<01:55,  3.45it/s] 16%|█▌        | 73/470 [00:21<01:54,  3.45it/s] 16%|█▌        | 74/470 [00:21<01:54,  3.45it/s] 16%|█▌        | 75/470 [00:22<01:56,  3.39it/s] 16%|█▌        | 76/470 [00:22<01:55,  3.41it/s] 16%|█▋        | 77/470 [00:22<01:54,  3.42it/s] 17%|█▋        | 78/470 [00:22<01:54,  3.43it/s] 17%|█▋        | 79/470 [00:23<01:53,  3.44it/s] 17%|█▋        | 80/470 [00:23<01:53,  3.44it/s] 17%|█▋        | 81/470 [00:23<01:52,  3.45it/s] 17%|█▋        | 82/470 [00:24<01:52,  3.45it/s] 18%|█▊        | 83/470 [00:24<01:52,  3.45it/s] 18%|█▊        | 84/470 [00:24<01:52,  3.45it/s] 18%|█▊        | 85/470 [00:24<01:51,  3.45it/s] 18%|█▊        | 86/470 [00:25<01:58,  3.25it/s] 19%|█▊        | 87/470 [00:25<01:55,  3.31it/s] 19%|█▊        | 88/470 [00:25<01:54,  3.35it/s] 19%|█▉        | 89/470 [00:26<01:52,  3.38it/s] 19%|█▉        | 90/470 [00:26<01:51,  3.40it/s] 19%|█▉        | 91/470 [00:26<01:50,  3.42it/s] 20%|█▉        | 92/470 [00:27<01:50,  3.42it/s] 20%|█▉        | 93/470 [00:27<01:49,  3.44it/s] 20%|██        | 94/470 [00:27<01:46,  3.53it/s][INFO|trainer.py:2140] 2023-08-29 03:44:32,582 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:44:32,582 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:44:32,582 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.96it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.59it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.69it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.85it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.82it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.49it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.51it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.39it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.54it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.71it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.77it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.48it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.52it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.59it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.43it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.46it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.42it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.55it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.69it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.75it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.51it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.60it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.50it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.43it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.45it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.48it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.61it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.66it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.52it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.86it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.97it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.19it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.30it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.45it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.47it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.52it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.58it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.45it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.44it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.48it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.48it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.50it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.56it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.50it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.44it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.35it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.41it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.48it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.37it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.43it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.49it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.47it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.46it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.56it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.52it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.51it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.76it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.04it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.19it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.29it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.38it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.42it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.44it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.36it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.39it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.31it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.47it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.60it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.63it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.56it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.57it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.43it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.29it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.47it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.57it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.56it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.52it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.58it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.61it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.51it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.29it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.35it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.93it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.21it/s][A                                                
                                                 [A 20%|██        | 94/470 [00:37<01:46,  3.53it/s]
100%|██████████| 436/436 [00:09<00:00, 44.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:44:42,508 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 03:44:42,613 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:44:45,640 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:44:45,820 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:44:45,961 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:49<41:50,  6.69s/it] 20%|██        | 96/470 [00:49<29:50,  4.79s/it] 21%|██        | 97/470 [00:49<21:23,  3.44s/it] 21%|██        | 98/470 [00:50<15:28,  2.50s/it] 21%|██        | 99/470 [00:50<11:20,  1.83s/it] 21%|██▏       | 100/470 [00:50<08:27,  1.37s/it] 21%|██▏       | 101/470 [00:51<06:26,  1.05s/it] 22%|██▏       | 102/470 [00:51<05:01,  1.22it/s] 22%|██▏       | 103/470 [00:51<04:02,  1.51it/s] 22%|██▏       | 104/470 [00:51<03:21,  1.82it/s] 22%|██▏       | 105/470 [00:52<02:52,  2.12it/s] 23%|██▎       | 106/470 [00:52<02:31,  2.40it/s] 23%|██▎       | 107/470 [00:52<02:20,  2.59it/s] 23%|██▎       | 108/470 [00:53<02:09,  2.80it/s] 23%|██▎       | 109/470 [00:53<02:01,  2.97it/s] 23%|██▎       | 110/470 [00:53<01:56,  3.10it/s] 24%|██▎       | 111/470 [00:53<01:52,  3.20it/s] 24%|██▍       | 112/470 [00:54<01:49,  3.27it/s] 24%|██▍       | 113/470 [00:54<01:47,  3.33it/s] 24%|██▍       | 114/470 [00:54<01:45,  3.37it/s] 24%|██▍       | 115/470 [00:55<01:44,  3.39it/s] 25%|██▍       | 116/470 [00:55<01:43,  3.41it/s] 25%|██▍       | 117/470 [00:55<01:43,  3.43it/s] 25%|██▌       | 118/470 [00:56<01:46,  3.30it/s] 25%|██▌       | 119/470 [00:56<01:44,  3.35it/s] 26%|██▌       | 120/470 [00:56<01:43,  3.38it/s] 26%|██▌       | 121/470 [00:56<01:42,  3.40it/s] 26%|██▌       | 122/470 [00:57<01:41,  3.42it/s] 26%|██▌       | 123/470 [00:57<01:41,  3.43it/s] 26%|██▋       | 124/470 [00:57<01:40,  3.43it/s] 27%|██▋       | 125/470 [00:58<01:40,  3.44it/s] 27%|██▋       | 126/470 [00:58<01:39,  3.44it/s] 27%|██▋       | 127/470 [00:58<01:39,  3.45it/s] 27%|██▋       | 128/470 [00:58<01:39,  3.45it/s] 27%|██▋       | 129/470 [00:59<01:44,  3.27it/s] 28%|██▊       | 130/470 [00:59<01:42,  3.33it/s] 28%|██▊       | 131/470 [00:59<01:40,  3.36it/s] 28%|██▊       | 132/470 [01:00<01:39,  3.39it/s] 28%|██▊       | 133/470 [01:00<01:38,  3.41it/s] 29%|██▊       | 134/470 [01:00<01:38,  3.42it/s] 29%|██▊       | 135/470 [01:01<01:37,  3.43it/s] 29%|██▉       | 136/470 [01:01<01:37,  3.44it/s] 29%|██▉       | 137/470 [01:01<01:36,  3.44it/s] 29%|██▉       | 138/470 [01:01<01:36,  3.45it/s] 30%|██▉       | 139/470 [01:02<01:36,  3.45it/s] 30%|██▉       | 140/470 [01:02<01:40,  3.29it/s] 30%|███       | 141/470 [01:02<01:38,  3.34it/s] 30%|███       | 142/470 [01:03<01:37,  3.37it/s] 30%|███       | 143/470 [01:03<01:36,  3.40it/s] 31%|███       | 144/470 [01:03<01:35,  3.41it/s] 31%|███       | 145/470 [01:03<01:34,  3.43it/s] 31%|███       | 146/470 [01:04<01:34,  3.44it/s] 31%|███▏      | 147/470 [01:04<01:33,  3.44it/s] 31%|███▏      | 148/470 [01:04<01:33,  3.45it/s] 32%|███▏      | 149/470 [01:05<01:33,  3.45it/s] 32%|███▏      | 150/470 [01:05<01:32,  3.45it/s] 32%|███▏      | 151/470 [01:05<01:41,  3.16it/s] 32%|███▏      | 152/470 [01:06<01:38,  3.24it/s] 33%|███▎      | 153/470 [01:06<01:36,  3.30it/s] 33%|███▎      | 154/470 [01:06<01:34,  3.34it/s] 33%|███▎      | 155/470 [01:06<01:33,  3.38it/s] 33%|███▎      | 156/470 [01:07<01:32,  3.40it/s] 33%|███▎      | 157/470 [01:07<01:31,  3.41it/s] 34%|███▎      | 158/470 [01:07<01:37,  3.20it/s] 34%|███▍      | 159/470 [01:08<01:34,  3.28it/s] 34%|███▍      | 160/470 [01:08<01:33,  3.33it/s] 34%|███▍      | 161/470 [01:08<01:38,  3.13it/s] 34%|███▍      | 162/470 [01:09<01:35,  3.22it/s] 35%|███▍      | 163/470 [01:09<01:33,  3.29it/s] 35%|███▍      | 164/470 [01:09<01:31,  3.33it/s] 35%|███▌      | 165/470 [01:10<01:52,  2.71it/s] 35%|███▌      | 166/470 [01:10<01:44,  2.90it/s] 36%|███▌      | 167/470 [01:10<01:39,  3.05it/s] 36%|███▌      | 168/470 [01:11<01:35,  3.16it/s] 36%|███▌      | 169/470 [01:11<01:32,  3.24it/s] 36%|███▌      | 170/470 [01:11<01:30,  3.30it/s] 36%|███▋      | 171/470 [01:11<01:31,  3.28it/s] 37%|███▋      | 172/470 [01:12<01:29,  3.34it/s] 37%|███▋      | 173/470 [01:12<01:28,  3.37it/s] 37%|███▋      | 174/470 [01:12<01:27,  3.39it/s] 37%|███▋      | 175/470 [01:13<01:26,  3.41it/s] 37%|███▋      | 176/470 [01:13<01:25,  3.42it/s] 38%|███▊      | 177/470 [01:13<01:25,  3.43it/s] 38%|███▊      | 178/470 [01:13<01:24,  3.44it/s] 38%|███▊      | 179/470 [01:14<01:24,  3.44it/s] 38%|███▊      | 180/470 [01:14<01:24,  3.44it/s] 39%|███▊      | 181/470 [01:14<01:24,  3.44it/s] 39%|███▊      | 182/470 [01:15<01:25,  3.36it/s] 39%|███▉      | 183/470 [01:15<01:24,  3.40it/s] 39%|███▉      | 184/470 [01:15<01:23,  3.41it/s] 39%|███▉      | 185/470 [01:16<01:23,  3.42it/s] 40%|███▉      | 186/470 [01:16<01:22,  3.43it/s] 40%|███▉      | 187/470 [01:16<01:22,  3.43it/s] 40%|████      | 188/470 [01:16<01:19,  3.53it/s][INFO|trainer.py:2140] 2023-08-29 03:45:21,883 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:45:21,883 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:45:21,883 >>   Batch size = 8
{'eval_loss': 1.1687309741973877, 'eval_runtime': 9.8049, 'eval_samples_per_second': 355.129, 'eval_steps_per_second': 44.468, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.16it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.03it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.05it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.61it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.28it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.86it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.75it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.63it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.75it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 43.60it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 43.90it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.01it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.23it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.14it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.32it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.35it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.38it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.39it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.41it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.50it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.46it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.42it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.39it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.41it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.38it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.29it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.35it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 42.88it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.59it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.71it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.97it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.23it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.19it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.29it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.23it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.15it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.42it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.48it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.48it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.54it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.49it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.23it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.38it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.30it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.36it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.45it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.45it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.55it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.57it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.50it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.45it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.32it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.39it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.37it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.07it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.59it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.00it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.14it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.27it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.25it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.25it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.23it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.17it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.15it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.36it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.43it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.59it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.50it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.45it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.45it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.35it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.28it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.22it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.35it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.57it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.69it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.59it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.59it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.50it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.41it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.27it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 42.85it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.51it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.93it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.23it/s][A                                                 
                                                 [A 40%|████      | 188/470 [01:26<01:19,  3.53it/s]
100%|██████████| 436/436 [00:09<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:45:31,898 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 03:45:32,104 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:45:35,334 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:45:35,481 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:45:35,547 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:37<29:30,  6.30s/it] 40%|████      | 190/470 [01:37<21:02,  4.51s/it] 41%|████      | 191/470 [01:37<15:04,  3.24s/it] 41%|████      | 192/470 [01:38<10:55,  2.36s/it] 41%|████      | 193/470 [01:38<08:01,  1.74s/it] 41%|████▏     | 194/470 [01:38<06:00,  1.31s/it] 41%|████▏     | 195/470 [01:39<04:35,  1.00s/it] 42%|████▏     | 196/470 [01:39<03:36,  1.27it/s] 42%|████▏     | 197/470 [01:39<02:54,  1.56it/s] 42%|████▏     | 198/470 [01:39<02:25,  1.86it/s] 42%|████▏     | 199/470 [01:40<02:05,  2.16it/s] 43%|████▎     | 200/470 [01:40<01:51,  2.42it/s] 43%|████▎     | 201/470 [01:40<01:44,  2.56it/s] 43%|████▎     | 202/470 [01:41<01:36,  2.77it/s] 43%|████▎     | 203/470 [01:41<01:31,  2.93it/s] 43%|████▎     | 204/470 [01:41<01:26,  3.06it/s] 44%|████▎     | 205/470 [01:42<01:24,  3.15it/s] 44%|████▍     | 206/470 [01:42<01:21,  3.23it/s] 44%|████▍     | 207/470 [01:42<01:20,  3.27it/s] 44%|████▍     | 208/470 [01:42<01:19,  3.31it/s] 44%|████▍     | 209/470 [01:43<01:18,  3.34it/s] 45%|████▍     | 210/470 [01:43<01:17,  3.35it/s] 45%|████▍     | 211/470 [01:43<01:17,  3.36it/s] 45%|████▌     | 212/470 [01:44<01:18,  3.30it/s] 45%|████▌     | 213/470 [01:44<01:17,  3.33it/s] 46%|████▌     | 214/470 [01:44<01:16,  3.35it/s] 46%|████▌     | 215/470 [01:44<01:15,  3.37it/s] 46%|████▌     | 216/470 [01:45<01:15,  3.38it/s] 46%|████▌     | 217/470 [01:45<01:14,  3.39it/s] 46%|████▋     | 218/470 [01:45<01:14,  3.39it/s] 47%|████▋     | 219/470 [01:46<01:13,  3.39it/s] 47%|████▋     | 220/470 [01:46<01:13,  3.40it/s] 47%|████▋     | 221/470 [01:46<01:12,  3.42it/s] 47%|████▋     | 222/470 [01:47<01:12,  3.43it/s] 47%|████▋     | 223/470 [01:47<01:14,  3.32it/s] 48%|████▊     | 224/470 [01:47<01:13,  3.36it/s] 48%|████▊     | 225/470 [01:47<01:12,  3.39it/s] 48%|████▊     | 226/470 [01:48<01:11,  3.40it/s] 48%|████▊     | 227/470 [01:48<01:11,  3.42it/s] 49%|████▊     | 228/470 [01:48<01:10,  3.43it/s] 49%|████▊     | 229/470 [01:49<01:10,  3.43it/s] 49%|████▉     | 230/470 [01:49<01:09,  3.44it/s] 49%|████▉     | 231/470 [01:49<01:09,  3.44it/s] 49%|████▉     | 232/470 [01:49<01:09,  3.44it/s] 50%|████▉     | 233/470 [01:50<01:08,  3.44it/s] 50%|████▉     | 234/470 [01:50<01:09,  3.37it/s] 50%|█████     | 235/470 [01:50<01:09,  3.40it/s] 50%|█████     | 236/470 [01:51<01:08,  3.41it/s] 50%|█████     | 237/470 [01:51<01:07,  3.43it/s] 51%|█████     | 238/470 [01:51<01:07,  3.43it/s] 51%|█████     | 239/470 [01:51<01:07,  3.44it/s] 51%|█████     | 240/470 [01:52<01:06,  3.44it/s] 51%|█████▏    | 241/470 [01:52<01:06,  3.44it/s] 51%|█████▏    | 242/470 [01:52<01:06,  3.45it/s] 52%|█████▏    | 243/470 [01:53<01:05,  3.45it/s] 52%|█████▏    | 244/470 [01:53<01:05,  3.44it/s] 52%|█████▏    | 245/470 [01:53<01:05,  3.45it/s] 52%|█████▏    | 246/470 [01:54<01:04,  3.45it/s] 53%|█████▎    | 247/470 [01:54<01:07,  3.32it/s] 53%|█████▎    | 248/470 [01:54<01:06,  3.36it/s] 53%|█████▎    | 249/470 [01:54<01:05,  3.39it/s] 53%|█████▎    | 250/470 [01:55<01:04,  3.40it/s] 53%|█████▎    | 251/470 [01:55<01:04,  3.42it/s] 54%|█████▎    | 252/470 [01:55<01:03,  3.42it/s] 54%|█████▍    | 253/470 [01:56<01:03,  3.43it/s] 54%|█████▍    | 254/470 [01:56<01:02,  3.43it/s] 54%|█████▍    | 255/470 [01:56<01:02,  3.44it/s] 54%|█████▍    | 256/470 [01:56<01:02,  3.44it/s] 55%|█████▍    | 257/470 [01:57<01:01,  3.45it/s] 55%|█████▍    | 258/470 [01:57<01:05,  3.25it/s] 55%|█████▌    | 259/470 [01:57<01:03,  3.31it/s] 55%|█████▌    | 260/470 [01:58<01:02,  3.35it/s] 56%|█████▌    | 261/470 [01:58<01:01,  3.38it/s] 56%|█████▌    | 262/470 [01:58<01:01,  3.40it/s] 56%|█████▌    | 263/470 [01:59<01:00,  3.42it/s] 56%|█████▌    | 264/470 [01:59<00:59,  3.43it/s] 56%|█████▋    | 265/470 [01:59<00:59,  3.44it/s] 57%|█████▋    | 266/470 [01:59<00:59,  3.44it/s] 57%|█████▋    | 267/470 [02:00<00:58,  3.45it/s] 57%|█████▋    | 268/470 [02:00<00:58,  3.45it/s] 57%|█████▋    | 269/470 [02:00<01:00,  3.31it/s] 57%|█████▋    | 270/470 [02:01<00:59,  3.36it/s] 58%|█████▊    | 271/470 [02:01<00:58,  3.38it/s] 58%|█████▊    | 272/470 [02:01<00:58,  3.41it/s] 58%|█████▊    | 273/470 [02:01<00:57,  3.42it/s] 58%|█████▊    | 274/470 [02:02<00:57,  3.43it/s] 59%|█████▊    | 275/470 [02:02<00:56,  3.44it/s] 59%|█████▊    | 276/470 [02:02<00:56,  3.44it/s] 59%|█████▉    | 277/470 [02:03<00:56,  3.44it/s] 59%|█████▉    | 278/470 [02:03<00:55,  3.45it/s] 59%|█████▉    | 279/470 [02:03<00:55,  3.45it/s] 60%|█████▉    | 280/470 [02:04<00:56,  3.34it/s] 60%|█████▉    | 281/470 [02:04<00:56,  3.37it/s] 60%|██████    | 282/470 [02:04<00:53,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 03:46:09,595 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:46:09,595 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:46:09,595 >>   Batch size = 8
{'eval_loss': 1.188925862312317, 'eval_runtime': 9.8416, 'eval_samples_per_second': 353.803, 'eval_steps_per_second': 44.302, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.28it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.29it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.33it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.23it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.12it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.93it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.83it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.50it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.83it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.77it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.69it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.35it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.38it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.38it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.36it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.37it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.56it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.70it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.75it/s][A
 23%|██▎       | 102/436 [00:02<00:08, 41.21it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 42.32it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 42.89it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.24it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 43.50it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.84it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.26it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.33it/s][A
 33%|███▎      | 142/436 [00:03<00:07, 40.57it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 42.33it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.16it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.62it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.91it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.09it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.33it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.42it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 43.91it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.07it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.32it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.52it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.52it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.47it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.51it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.24it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.00it/s][A
 54%|█████▍    | 237/436 [00:05<00:10, 19.28it/s][A
 56%|█████▌    | 242/436 [00:05<00:08, 23.29it/s][A
 57%|█████▋    | 247/436 [00:06<00:06, 27.22it/s][A
 58%|█████▊    | 252/436 [00:06<00:05, 30.88it/s][A
 59%|█████▉    | 257/436 [00:06<00:05, 34.11it/s][A
 60%|██████    | 262/436 [00:06<00:04, 36.75it/s][A
 61%|██████    | 267/436 [00:06<00:04, 38.88it/s][A
 62%|██████▏   | 272/436 [00:06<00:04, 40.48it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 41.22it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 41.77it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 42.50it/s][A
 67%|██████▋   | 292/436 [00:07<00:03, 43.07it/s][A
 68%|██████▊   | 297/436 [00:07<00:03, 43.35it/s][A
 69%|██████▉   | 302/436 [00:07<00:03, 43.95it/s][A
 70%|███████   | 307/436 [00:07<00:02, 44.23it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.47it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.37it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.15it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.02it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.06it/s][A
 77%|███████▋  | 337/436 [00:08<00:02, 44.10it/s][A
 78%|███████▊  | 342/436 [00:08<00:02, 44.35it/s][A
 80%|███████▉  | 347/436 [00:08<00:01, 44.54it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.38it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.53it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.49it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.18it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.99it/s][A
 86%|████████▋ | 377/436 [00:09<00:01, 44.00it/s][A
 88%|████████▊ | 382/436 [00:09<00:01, 44.19it/s][A
 89%|████████▉ | 387/436 [00:09<00:01, 44.43it/s][A
 90%|████████▉ | 392/436 [00:09<00:00, 44.52it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 44.68it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.68it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.51it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.38it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.18it/s][A
 97%|█████████▋| 422/436 [00:10<00:00, 44.13it/s][A
 98%|█████████▊| 427/436 [00:10<00:00, 44.21it/s][A
 99%|█████████▉| 432/436 [00:10<00:00, 44.38it/s][A                                                 
                                                 [A 60%|██████    | 282/470 [02:14<00:53,  3.48it/s]
100%|██████████| 436/436 [00:10<00:00, 44.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:46:20,070 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 03:46:20,322 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:46:23,189 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:46:23,321 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:46:23,384 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:25<19:53,  6.38s/it] 60%|██████    | 284/470 [02:25<14:09,  4.57s/it] 61%|██████    | 285/470 [02:25<10:07,  3.28s/it] 61%|██████    | 286/470 [02:26<07:19,  2.39s/it] 61%|██████    | 287/470 [02:26<05:22,  1.76s/it] 61%|██████▏   | 288/470 [02:26<04:00,  1.32s/it] 61%|██████▏   | 289/470 [02:27<03:03,  1.01s/it] 62%|██████▏   | 290/470 [02:27<02:23,  1.25it/s] 62%|██████▏   | 291/470 [02:27<01:55,  1.55it/s] 62%|██████▏   | 292/470 [02:27<01:36,  1.85it/s] 62%|██████▏   | 293/470 [02:28<01:22,  2.14it/s] 63%|██████▎   | 294/470 [02:28<01:13,  2.41it/s] 63%|██████▎   | 295/470 [02:28<01:07,  2.58it/s] 63%|██████▎   | 296/470 [02:29<01:02,  2.78it/s] 63%|██████▎   | 297/470 [02:29<00:58,  2.94it/s] 63%|██████▎   | 298/470 [02:29<00:56,  3.06it/s] 64%|██████▎   | 299/470 [02:29<00:54,  3.16it/s] 64%|██████▍   | 300/470 [02:30<00:52,  3.23it/s] 64%|██████▍   | 301/470 [02:30<00:51,  3.28it/s] 64%|██████▍   | 302/470 [02:30<00:50,  3.32it/s] 64%|██████▍   | 303/470 [02:31<00:49,  3.34it/s] 65%|██████▍   | 304/470 [02:31<00:49,  3.36it/s] 65%|██████▍   | 305/470 [02:31<00:48,  3.37it/s] 65%|██████▌   | 306/470 [02:32<00:49,  3.33it/s] 65%|██████▌   | 307/470 [02:32<00:48,  3.35it/s] 66%|██████▌   | 308/470 [02:32<00:48,  3.36it/s] 66%|██████▌   | 309/470 [02:32<00:47,  3.37it/s] 66%|██████▌   | 310/470 [02:33<00:47,  3.38it/s] 66%|██████▌   | 311/470 [02:33<00:47,  3.38it/s] 66%|██████▋   | 312/470 [02:33<00:46,  3.38it/s] 67%|██████▋   | 313/470 [02:34<00:46,  3.38it/s] 67%|██████▋   | 314/470 [02:34<00:45,  3.39it/s] 67%|██████▋   | 315/470 [02:34<00:45,  3.39it/s] 67%|██████▋   | 316/470 [02:34<00:45,  3.39it/s] 67%|██████▋   | 317/470 [02:35<00:46,  3.30it/s] 68%|██████▊   | 318/470 [02:35<00:45,  3.33it/s] 68%|██████▊   | 319/470 [02:35<00:45,  3.35it/s] 68%|██████▊   | 320/470 [02:36<00:44,  3.36it/s] 68%|██████▊   | 321/470 [02:36<00:44,  3.37it/s] 69%|██████▊   | 322/470 [02:36<00:43,  3.38it/s] 69%|██████▊   | 323/470 [02:37<00:43,  3.38it/s] 69%|██████▉   | 324/470 [02:37<00:43,  3.39it/s] 69%|██████▉   | 325/470 [02:37<00:42,  3.39it/s] 69%|██████▉   | 326/470 [02:37<00:42,  3.39it/s] 70%|██████▉   | 327/470 [02:38<00:42,  3.39it/s] 70%|██████▉   | 328/470 [02:38<00:42,  3.31it/s] 70%|███████   | 329/470 [02:38<00:42,  3.34it/s] 70%|███████   | 330/470 [02:39<00:41,  3.35it/s] 70%|███████   | 331/470 [02:39<00:41,  3.37it/s] 71%|███████   | 332/470 [02:39<00:40,  3.38it/s] 71%|███████   | 333/470 [02:40<00:40,  3.38it/s] 71%|███████   | 334/470 [02:40<00:40,  3.39it/s] 71%|███████▏  | 335/470 [02:40<00:39,  3.40it/s] 71%|███████▏  | 336/470 [02:40<00:39,  3.41it/s] 72%|███████▏  | 337/470 [02:41<00:38,  3.42it/s] 72%|███████▏  | 338/470 [02:41<00:38,  3.43it/s] 72%|███████▏  | 339/470 [02:41<00:38,  3.37it/s] 72%|███████▏  | 340/470 [02:42<00:38,  3.39it/s] 73%|███████▎  | 341/470 [02:42<00:37,  3.41it/s] 73%|███████▎  | 342/470 [02:42<00:37,  3.42it/s] 73%|███████▎  | 343/470 [02:42<00:37,  3.43it/s] 73%|███████▎  | 344/470 [02:43<00:36,  3.43it/s] 73%|███████▎  | 345/470 [02:43<00:36,  3.44it/s] 74%|███████▎  | 346/470 [02:43<00:36,  3.44it/s] 74%|███████▍  | 347/470 [02:44<00:35,  3.45it/s] 74%|███████▍  | 348/470 [02:44<00:35,  3.45it/s] 74%|███████▍  | 349/470 [02:44<00:35,  3.45it/s] 74%|███████▍  | 350/470 [02:45<00:35,  3.37it/s] 75%|███████▍  | 351/470 [02:45<00:35,  3.40it/s] 75%|███████▍  | 352/470 [02:45<00:34,  3.41it/s] 75%|███████▌  | 353/470 [02:45<00:34,  3.42it/s] 75%|███████▌  | 354/470 [02:46<00:33,  3.43it/s] 76%|███████▌  | 355/470 [02:46<00:33,  3.44it/s] 76%|███████▌  | 356/470 [02:46<00:33,  3.43it/s] 76%|███████▌  | 357/470 [02:47<00:32,  3.44it/s] 76%|███████▌  | 358/470 [02:47<00:32,  3.44it/s] 76%|███████▋  | 359/470 [02:47<00:32,  3.44it/s] 77%|███████▋  | 360/470 [02:47<00:31,  3.44it/s] 77%|███████▋  | 361/470 [02:48<00:32,  3.35it/s] 77%|███████▋  | 362/470 [02:48<00:31,  3.38it/s] 77%|███████▋  | 363/470 [02:48<00:31,  3.40it/s] 77%|███████▋  | 364/470 [02:49<00:31,  3.41it/s] 78%|███████▊  | 365/470 [02:49<00:30,  3.42it/s] 78%|███████▊  | 366/470 [02:49<00:30,  3.43it/s] 78%|███████▊  | 367/470 [02:49<00:30,  3.43it/s] 78%|███████▊  | 368/470 [02:50<00:29,  3.42it/s] 79%|███████▊  | 369/470 [02:50<00:29,  3.41it/s] 79%|███████▊  | 370/470 [02:50<00:29,  3.41it/s] 79%|███████▉  | 371/470 [02:51<00:29,  3.41it/s] 79%|███████▉  | 372/470 [02:51<00:30,  3.23it/s] 79%|███████▉  | 373/470 [02:51<00:29,  3.28it/s] 80%|███████▉  | 374/470 [02:52<00:28,  3.31it/s] 80%|███████▉  | 375/470 [02:52<00:28,  3.34it/s] 80%|████████  | 376/470 [02:52<00:27,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 03:46:57,667 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:46:57,667 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:46:57,667 >>   Batch size = 8
{'eval_loss': 1.2054401636123657, 'eval_runtime': 10.3453, 'eval_samples_per_second': 336.579, 'eval_steps_per_second': 42.145, 'epoch': 3.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.24it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.38it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.71it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.69it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.35it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.55it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.47it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.47it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.59it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.77it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.66it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.52it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.32it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.35it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.32it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.38it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.44it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.60it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.58it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.54it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.42it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 42.52it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.16it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.60it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 43.93it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.26it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.34it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.10it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.11it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.30it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.44it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.57it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.55it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.54it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.60it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.37it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.21it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.26it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.31it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.47it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.55it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.68it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.61it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.52it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.37it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.25it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 40.30it/s][A
 60%|██████    | 262/436 [00:05<00:04, 41.73it/s][A
 61%|██████    | 267/436 [00:06<00:03, 42.62it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.33it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.83it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.99it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.13it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.80it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 43.87it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.11it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.37it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.61it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.71it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.53it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.42it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.17it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.01it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.05it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.12it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.40it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.59it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.75it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.61it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.49it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.35it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.16it/s][A
 90%|████████▉ | 392/436 [00:08<00:01, 40.20it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 41.56it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 42.56it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.26it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 43.74it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.15it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.29it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.08it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.81it/s][A                                                 
                                                 [A 80%|████████  | 376/470 [03:02<00:27,  3.44it/s]
100%|██████████| 436/436 [00:09<00:00, 43.81it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:47:07,832 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 03:47:08,126 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:47:13,434 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:47:13,704 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:47:13,793 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:19<12:53,  8.31s/it] 80%|████████  | 378/470 [03:20<09:04,  5.92s/it] 81%|████████  | 379/470 [03:20<06:24,  4.23s/it] 81%|████████  | 380/470 [03:20<04:34,  3.05s/it] 81%|████████  | 381/470 [03:20<03:17,  2.22s/it] 81%|████████▏ | 382/470 [03:21<02:24,  1.64s/it] 81%|████████▏ | 383/470 [03:21<01:47,  1.24s/it] 82%|████████▏ | 384/470 [03:21<01:22,  1.05it/s] 82%|████████▏ | 385/470 [03:22<01:04,  1.32it/s] 82%|████████▏ | 386/470 [03:22<00:51,  1.62it/s] 82%|████████▏ | 387/470 [03:22<00:43,  1.92it/s] 83%|████████▎ | 388/470 [03:22<00:37,  2.21it/s] 83%|████████▎ | 389/470 [03:23<00:33,  2.42it/s] 83%|████████▎ | 390/470 [03:23<00:30,  2.65it/s] 83%|████████▎ | 391/470 [03:23<00:27,  2.84it/s] 83%|████████▎ | 392/470 [03:24<00:26,  2.99it/s] 84%|████████▎ | 393/470 [03:24<00:24,  3.10it/s] 84%|████████▍ | 394/470 [03:24<00:23,  3.18it/s] 84%|████████▍ | 395/470 [03:25<00:23,  3.25it/s] 84%|████████▍ | 396/470 [03:25<00:22,  3.29it/s] 84%|████████▍ | 397/470 [03:25<00:21,  3.32it/s] 85%|████████▍ | 398/470 [03:25<00:21,  3.34it/s] 85%|████████▍ | 399/470 [03:26<00:21,  3.36it/s] 85%|████████▌ | 400/470 [03:26<00:21,  3.26it/s] 85%|████████▌ | 401/470 [03:26<00:20,  3.31it/s] 86%|████████▌ | 402/470 [03:27<00:20,  3.33it/s] 86%|████████▌ | 403/470 [03:27<00:20,  3.35it/s] 86%|████████▌ | 404/470 [03:27<00:19,  3.36it/s] 86%|████████▌ | 405/470 [03:28<00:19,  3.37it/s] 86%|████████▋ | 406/470 [03:28<00:18,  3.38it/s] 87%|████████▋ | 407/470 [03:28<00:18,  3.38it/s] 87%|████████▋ | 408/470 [03:28<00:18,  3.39it/s] 87%|████████▋ | 409/470 [03:29<00:17,  3.39it/s] 87%|████████▋ | 410/470 [03:29<00:17,  3.39it/s] 87%|████████▋ | 411/470 [03:29<00:18,  3.19it/s] 88%|████████▊ | 412/470 [03:30<00:17,  3.25it/s] 88%|████████▊ | 413/470 [03:30<00:17,  3.30it/s] 88%|████████▊ | 414/470 [03:30<00:16,  3.32it/s] 88%|████████▊ | 415/470 [03:31<00:16,  3.35it/s] 89%|████████▊ | 416/470 [03:31<00:16,  3.37it/s] 89%|████████▊ | 417/470 [03:31<00:15,  3.38it/s] 89%|████████▉ | 418/470 [03:31<00:15,  3.39it/s] 89%|████████▉ | 419/470 [03:32<00:15,  3.39it/s] 89%|████████▉ | 420/470 [03:32<00:14,  3.40it/s] 90%|████████▉ | 421/470 [03:32<00:14,  3.34it/s] 90%|████████▉ | 422/470 [03:33<00:14,  3.36it/s] 90%|█████████ | 423/470 [03:33<00:13,  3.37it/s] 90%|█████████ | 424/470 [03:33<00:13,  3.38it/s] 90%|█████████ | 425/470 [03:33<00:13,  3.38it/s] 91%|█████████ | 426/470 [03:34<00:12,  3.39it/s] 91%|█████████ | 427/470 [03:34<00:12,  3.39it/s] 91%|█████████ | 428/470 [03:34<00:12,  3.39it/s] 91%|█████████▏| 429/470 [03:35<00:12,  3.40it/s] 91%|█████████▏| 430/470 [03:35<00:11,  3.40it/s] 92%|█████████▏| 431/470 [03:35<00:11,  3.40it/s] 92%|█████████▏| 432/470 [03:36<00:11,  3.29it/s] 92%|█████████▏| 433/470 [03:36<00:11,  3.32it/s] 92%|█████████▏| 434/470 [03:36<00:10,  3.34it/s] 93%|█████████▎| 435/470 [03:36<00:10,  3.36it/s] 93%|█████████▎| 436/470 [03:37<00:10,  3.37it/s] 93%|█████████▎| 437/470 [03:37<00:09,  3.38it/s] 93%|█████████▎| 438/470 [03:37<00:09,  3.39it/s] 93%|█████████▎| 439/470 [03:38<00:09,  3.39it/s] 94%|█████████▎| 440/470 [03:38<00:08,  3.39it/s] 94%|█████████▍| 441/470 [03:38<00:08,  3.39it/s] 94%|█████████▍| 442/470 [03:39<00:08,  3.40it/s] 94%|█████████▍| 443/470 [03:39<00:08,  3.34it/s] 94%|█████████▍| 444/470 [03:39<00:07,  3.36it/s] 95%|█████████▍| 445/470 [03:39<00:07,  3.37it/s] 95%|█████████▍| 446/470 [03:40<00:07,  3.38it/s] 95%|█████████▌| 447/470 [03:40<00:06,  3.38it/s] 95%|█████████▌| 448/470 [03:40<00:06,  3.39it/s] 96%|█████████▌| 449/470 [03:41<00:06,  3.39it/s] 96%|█████████▌| 450/470 [03:41<00:05,  3.39it/s] 96%|█████████▌| 451/470 [03:41<00:05,  3.40it/s] 96%|█████████▌| 452/470 [03:41<00:05,  3.40it/s] 96%|█████████▋| 453/470 [03:42<00:05,  3.40it/s] 97%|█████████▋| 454/470 [03:42<00:04,  3.28it/s] 97%|█████████▋| 455/470 [03:42<00:04,  3.31it/s] 97%|█████████▋| 456/470 [03:43<00:04,  3.34it/s] 97%|█████████▋| 457/470 [03:43<00:03,  3.35it/s] 97%|█████████▋| 458/470 [03:43<00:03,  3.37it/s] 98%|█████████▊| 459/470 [03:44<00:03,  3.38it/s] 98%|█████████▊| 460/470 [03:44<00:02,  3.38it/s] 98%|█████████▊| 461/470 [03:44<00:02,  3.39it/s] 98%|█████████▊| 462/470 [03:44<00:02,  3.39it/s] 99%|█████████▊| 463/470 [03:45<00:02,  3.39it/s] 99%|█████████▊| 464/470 [03:45<00:01,  3.40it/s] 99%|█████████▉| 465/470 [03:45<00:01,  3.28it/s] 99%|█████████▉| 466/470 [03:46<00:01,  3.31it/s] 99%|█████████▉| 467/470 [03:46<00:00,  3.34it/s]100%|█████████▉| 468/470 [03:46<00:00,  3.36it/s]100%|█████████▉| 469/470 [03:47<00:00,  3.37it/s]100%|██████████| 470/470 [03:47<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 03:47:52,318 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:47:52,318 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:47:52,318 >>   Batch size = 8
{'eval_loss': 1.2166857719421387, 'eval_runtime': 9.891, 'eval_samples_per_second': 352.039, 'eval_steps_per_second': 44.081, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.42it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.52it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.88it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.87it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.33it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.90it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.74it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.56it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.71it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.78it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.82it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.27it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.61it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.89it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.94it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.21it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.41it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.47it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.38it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.43it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.52it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.54it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.47it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.40it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.47it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.59it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.52it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.50it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.49it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.44it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.52it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.51it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.49it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.43it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.43it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.49it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.51it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 41.94it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.84it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.38it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.74it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 43.99it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.13it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.39it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.35it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.09it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.40it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.50it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.52it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.38it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.43it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.56it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.44it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.27it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.21it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.68it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.00it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.12it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.12it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.36it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.39it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.29it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.06it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.29it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.49it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.55it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.47it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.49it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.43it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.58it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.38it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.28it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.43it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.51it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.53it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.55it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.50it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.53it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.43it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.33it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.39it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 42.61it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.31it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.79it/s][A                                                 
                                                 [A100%|██████████| 470/470 [03:57<00:00,  3.47it/s]
100%|██████████| 436/436 [00:09<00:00, 43.79it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:48:02,239 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 03:48:02,382 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:48:05,818 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:48:06,086 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:48:06,216 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:48:15,871 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:48:15,915 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94 (score: 1.1687309741973877).
                                                 100%|██████████| 470/470 [04:20<00:00,  3.47it/s]100%|██████████| 470/470 [04:20<00:00,  1.80it/s]
[INFO|trainer.py:1894] 2023-08-29 03:48:25,966 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 03:48:26,107 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:48:29,073 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:48:29,213 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:48:29,283 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:48:29,773 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   train_loss               =     0.4185
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   train_runtime            = 0:04:20.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   train_samples            =       6010
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   train_samples_per_second =    115.275
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:29,774 >>   train_steps_per_second   =      1.803
{'eval_loss': 1.2223551273345947, 'eval_runtime': 9.8472, 'eval_samples_per_second': 353.602, 'eval_steps_per_second': 44.276, 'epoch': 5.0}
{'train_runtime': 260.6809, 'train_samples_per_second': 115.275, 'train_steps_per_second': 1.803, 'train_loss': 0.41848261407081117, 'epoch': 5.0}
08/29/2023 03:48:30 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:48:30,045 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:48:30,046 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 03:48:30,046 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.65it/s]  3%|▎         | 12/436 [00:00<00:08, 49.23it/s]  4%|▍         | 17/436 [00:00<00:08, 47.63it/s]  5%|▌         | 22/436 [00:00<00:08, 46.78it/s]  6%|▌         | 27/436 [00:00<00:08, 46.35it/s]  7%|▋         | 32/436 [00:00<00:08, 46.07it/s]  8%|▊         | 37/436 [00:00<00:08, 45.80it/s] 10%|▉         | 42/436 [00:00<00:08, 45.33it/s] 11%|█         | 47/436 [00:01<00:08, 44.74it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.29it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.33it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.53it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.71it/s] 17%|█▋        | 72/436 [00:01<00:08, 44.88it/s] 18%|█▊        | 77/436 [00:01<00:07, 44.92it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.11it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.95it/s] 21%|██        | 92/436 [00:02<00:07, 44.58it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.37it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.32it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.36it/s] 26%|██▌       | 112/436 [00:02<00:07, 43.35it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.01it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.33it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.69it/s] 30%|███       | 132/436 [00:02<00:06, 44.53it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.34it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.35it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.30it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.21it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.34it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.61it/s] 38%|███▊      | 167/436 [00:03<00:05, 44.90it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.94it/s] 41%|████      | 177/436 [00:03<00:05, 44.80it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.57it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.46it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.42it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.40it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.50it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.64it/s] 49%|████▊     | 212/436 [00:04<00:04, 44.89it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.99it/s] 51%|█████     | 222/436 [00:04<00:04, 44.82it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.71it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.47it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.35it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.26it/s] 57%|█████▋    | 247/436 [00:05<00:04, 42.31it/s] 58%|█████▊    | 252/436 [00:05<00:04, 43.07it/s] 59%|█████▉    | 257/436 [00:05<00:04, 43.79it/s] 60%|██████    | 262/436 [00:05<00:03, 44.15it/s] 61%|██████    | 267/436 [00:05<00:03, 44.48it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.53it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.43it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.42it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.03it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.12it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.30it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.55it/s] 70%|███████   | 307/436 [00:06<00:02, 44.79it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.91it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.82it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.70it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.53it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.39it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.37it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.39it/s] 80%|███████▉  | 347/436 [00:07<00:01, 44.60it/s] 81%|████████  | 352/436 [00:07<00:01, 44.73it/s] 82%|████████▏ | 357/436 [00:07<00:01, 44.85it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.79it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.71it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.62it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.37it/s] 88%|████████▊ | 382/436 [00:08<00:01, 43.61it/s] 89%|████████▉ | 387/436 [00:08<00:01, 43.99it/s] 90%|████████▉ | 392/436 [00:08<00:00, 44.23it/s] 91%|█████████ | 397/436 [00:08<00:00, 44.36it/s] 92%|█████████▏| 402/436 [00:09<00:00, 44.40it/s] 93%|█████████▎| 407/436 [00:09<00:00, 44.50it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.64it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.50it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.17it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.38it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.49it/s]100%|██████████| 436/436 [00:09<00:00, 44.61it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:48:39,837 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   eval_loss               =     1.1687
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   eval_runtime            = 0:00:09.79
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   eval_samples_per_second =    355.619
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   eval_steps_per_second   =     44.529
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:48:39,837 >>   perplexity              =     3.2179
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:49,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:49,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:49,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:49,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:49,477 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:48:49,784 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:48:49,785 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:48:50,057 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:48:51,121 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:48:51,121 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:53,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:53,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:53,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:53,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:53,881 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:48:54,624 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:48:54,625 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:48:54,890 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:48:55,061 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:48:55,061 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.83it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:02,  1.65it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.54it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.51it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:21,  1.42it/s]Extractor Predicting: 35it [00:22,  1.46it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:30,  1.51it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.48it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:43,  1.52it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.54it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:46,  1.59it/s]Extractor Predicting: 72it [00:46,  1.59it/s]Extractor Predicting: 73it [00:47,  1.64it/s]Extractor Predicting: 74it [00:47,  1.65it/s]Extractor Predicting: 75it [00:48,  1.61it/s]Extractor Predicting: 76it [00:49,  1.57it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:51,  1.55it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:53,  1.53it/s]Extractor Predicting: 83it [00:53,  1.55it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.57it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:57,  1.51it/s]Extractor Predicting: 90it [00:58,  1.50it/s]Extractor Predicting: 91it [00:59,  1.49it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [01:00,  1.53it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:01,  1.55it/s]Extractor Predicting: 96it [01:02,  1.56it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:04,  1.48it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:05,  1.50it/s]Extractor Predicting: 102it [01:06,  1.48it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:07,  1.50it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:09,  1.49it/s]Extractor Predicting: 108it [01:10,  1.52it/s]Extractor Predicting: 109it [01:11,  1.54it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:12,  1.59it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:13,  1.57it/s]Extractor Predicting: 114it [01:14,  1.52it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:15,  1.51it/s]Extractor Predicting: 117it [01:16,  1.39it/s]Extractor Predicting: 118it [01:17,  1.42it/s]Extractor Predicting: 119it [01:17,  1.43it/s]Extractor Predicting: 120it [01:18,  1.49it/s]Extractor Predicting: 121it [01:19,  1.48it/s]Extractor Predicting: 122it [01:19,  1.51it/s]Extractor Predicting: 123it [01:20,  1.48it/s]Extractor Predicting: 124it [01:21,  1.50it/s]Extractor Predicting: 125it [01:21,  1.50it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.51it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:25,  1.50it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.48it/s]Extractor Predicting: 134it [01:27,  1.44it/s]Extractor Predicting: 135it [01:28,  1.46it/s]Extractor Predicting: 136it [01:29,  1.48it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.50it/s]Extractor Predicting: 139it [01:31,  1.48it/s]Extractor Predicting: 140it [01:31,  1.46it/s]Extractor Predicting: 141it [01:32,  1.47it/s]Extractor Predicting: 142it [01:33,  1.51it/s]Extractor Predicting: 143it [01:33,  1.55it/s]Extractor Predicting: 143it [01:33,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:41,051 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:41,090 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:41,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:41,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:41,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:50:41,707 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:50:41,708 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:42,311 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:43,357 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:43,357 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:46,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:46,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:46,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:46,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:46,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:50:47,038 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:50:47,039 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:47,650 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:47,808 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:47,808 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2245762711864407,
  "recall": 0.10654796094198736,
  "score": 0.14452668484612388,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:06,  1.52it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:17,  1.70it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.56it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.50it/s]Extractor Predicting: 36it [00:22,  1.51it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:23,  1.51it/s]Extractor Predicting: 39it [00:24,  1.53it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:25,  1.54it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:26,  1.52it/s]Extractor Predicting: 44it [00:27,  1.53it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:32,  1.53it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.54it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.56it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:37,  1.56it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:39,  1.60it/s]Extractor Predicting: 64it [00:40,  1.61it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.56it/s]Extractor Predicting: 69it [00:43,  1.59it/s]Extractor Predicting: 70it [00:44,  1.58it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.64it/s]Extractor Predicting: 79it [00:49,  1.63it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:51,  1.58it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:53,  1.55it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.58it/s]Extractor Predicting: 88it [00:55,  1.58it/s]Extractor Predicting: 89it [00:56,  1.62it/s]Extractor Predicting: 90it [00:56,  1.62it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:58,  1.56it/s]Extractor Predicting: 94it [00:59,  1.54it/s]Extractor Predicting: 95it [01:00,  1.54it/s]Extractor Predicting: 96it [01:00,  1.53it/s]Extractor Predicting: 97it [01:01,  1.54it/s]Extractor Predicting: 98it [01:02,  1.52it/s]Extractor Predicting: 99it [01:02,  1.54it/s]Extractor Predicting: 100it [01:03,  1.53it/s]Extractor Predicting: 101it [01:04,  1.52it/s]Extractor Predicting: 102it [01:04,  1.53it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:06,  1.54it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:07,  1.52it/s]Extractor Predicting: 108it [01:08,  1.54it/s]Extractor Predicting: 109it [01:09,  1.54it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:11,  1.51it/s]Extractor Predicting: 113it [01:11,  1.53it/s]Extractor Predicting: 114it [01:12,  1.53it/s]Extractor Predicting: 115it [01:13,  1.52it/s]Extractor Predicting: 116it [01:13,  1.58it/s]Extractor Predicting: 117it [01:14,  1.67it/s]Extractor Predicting: 118it [01:14,  1.71it/s]Extractor Predicting: 119it [01:15,  1.69it/s]Extractor Predicting: 120it [01:16,  1.68it/s]Extractor Predicting: 121it [01:16,  1.64it/s]Extractor Predicting: 122it [01:17,  1.62it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:18,  1.64it/s]Extractor Predicting: 125it [01:19,  1.50it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:21,  1.64it/s]Extractor Predicting: 130it [01:22,  1.63it/s]Extractor Predicting: 131it [01:23,  1.62it/s]Extractor Predicting: 132it [01:23,  1.64it/s]Extractor Predicting: 133it [01:24,  1.65it/s]Extractor Predicting: 134it [01:24,  1.67it/s]Extractor Predicting: 135it [01:25,  1.69it/s]Extractor Predicting: 136it [01:25,  1.74it/s]Extractor Predicting: 137it [01:26,  1.72it/s]Extractor Predicting: 138it [01:27,  1.69it/s]Extractor Predicting: 139it [01:27,  1.66it/s]Extractor Predicting: 140it [01:28,  1.68it/s]Extractor Predicting: 141it [01:28,  1.65it/s]Extractor Predicting: 142it [01:29,  1.58it/s]Extractor Predicting: 143it [01:30,  1.62it/s]Extractor Predicting: 144it [01:30,  1.61it/s]Extractor Predicting: 145it [01:31,  1.62it/s]Extractor Predicting: 146it [01:32,  1.65it/s]Extractor Predicting: 147it [01:32,  1.62it/s]Extractor Predicting: 148it [01:33,  1.60it/s]Extractor Predicting: 149it [01:33,  1.67it/s]Extractor Predicting: 150it [01:34,  1.65it/s]Extractor Predicting: 151it [01:35,  1.64it/s]Extractor Predicting: 152it [01:35,  1.66it/s]Extractor Predicting: 153it [01:36,  1.71it/s]Extractor Predicting: 154it [01:36,  1.72it/s]Extractor Predicting: 155it [01:37,  1.72it/s]Extractor Predicting: 156it [01:38,  1.70it/s]Extractor Predicting: 157it [01:38,  1.69it/s]Extractor Predicting: 158it [01:39,  1.69it/s]Extractor Predicting: 159it [01:39,  1.64it/s]Extractor Predicting: 160it [01:40,  1.65it/s]Extractor Predicting: 161it [01:41,  1.65it/s]Extractor Predicting: 162it [01:41,  1.66it/s]Extractor Predicting: 163it [01:42,  1.65it/s]Extractor Predicting: 164it [01:42,  1.65it/s]Extractor Predicting: 165it [01:43,  1.67it/s]Extractor Predicting: 166it [01:44,  1.68it/s]Extractor Predicting: 167it [01:44,  1.68it/s]Extractor Predicting: 168it [01:45,  1.64it/s]Extractor Predicting: 169it [01:45,  1.65it/s]Extractor Predicting: 170it [01:46,  1.62it/s]Extractor Predicting: 171it [01:47,  1.59it/s]Extractor Predicting: 172it [01:47,  1.66it/s]Extractor Predicting: 173it [01:48,  1.60it/s]Extractor Predicting: 174it [01:49,  1.59it/s]Extractor Predicting: 175it [01:49,  1.57it/s]Extractor Predicting: 176it [01:50,  1.56it/s]Extractor Predicting: 177it [01:50,  1.55it/s]Extractor Predicting: 178it [01:51,  1.53it/s]Extractor Predicting: 179it [01:52,  1.55it/s]Extractor Predicting: 180it [01:52,  1.58it/s]Extractor Predicting: 181it [01:53,  1.56it/s]Extractor Predicting: 182it [01:54,  1.54it/s]Extractor Predicting: 183it [01:54,  1.54it/s]Extractor Predicting: 184it [01:55,  1.52it/s]Extractor Predicting: 185it [01:56,  1.52it/s]Extractor Predicting: 186it [01:56,  1.50it/s]Extractor Predicting: 187it [01:57,  1.50it/s]Extractor Predicting: 188it [01:58,  1.54it/s]Extractor Predicting: 189it [01:58,  1.54it/s]Extractor Predicting: 190it [01:59,  1.56it/s]Extractor Predicting: 191it [02:00,  1.51it/s]Extractor Predicting: 192it [02:00,  1.51it/s]Extractor Predicting: 193it [02:01,  1.49it/s]Extractor Predicting: 194it [02:02,  1.49it/s]Extractor Predicting: 195it [02:02,  1.49it/s]Extractor Predicting: 196it [02:03,  1.50it/s]Extractor Predicting: 197it [02:04,  1.49it/s]Extractor Predicting: 198it [02:04,  1.50it/s]Extractor Predicting: 199it [02:05,  1.50it/s]Extractor Predicting: 200it [02:06,  1.49it/s]Extractor Predicting: 201it [02:06,  1.50it/s]Extractor Predicting: 202it [02:07,  1.51it/s]Extractor Predicting: 203it [02:08,  1.52it/s]Extractor Predicting: 204it [02:08,  1.54it/s]Extractor Predicting: 205it [02:09,  1.60it/s]Extractor Predicting: 206it [02:09,  1.58it/s]Extractor Predicting: 207it [02:10,  1.57it/s]Extractor Predicting: 208it [02:11,  1.57it/s]Extractor Predicting: 209it [02:11,  1.56it/s]Extractor Predicting: 210it [02:12,  1.59it/s]Extractor Predicting: 211it [02:13,  1.62it/s]Extractor Predicting: 212it [02:13,  1.62it/s]Extractor Predicting: 213it [02:14,  1.65it/s]Extractor Predicting: 214it [02:14,  1.63it/s]Extractor Predicting: 215it [02:15,  1.58it/s]Extractor Predicting: 216it [02:16,  1.60it/s]Extractor Predicting: 217it [02:16,  1.60it/s]Extractor Predicting: 218it [02:17,  1.63it/s]Extractor Predicting: 219it [02:18,  1.63it/s]Extractor Predicting: 220it [02:18,  1.60it/s]Extractor Predicting: 221it [02:19,  1.64it/s]Extractor Predicting: 222it [02:19,  1.66it/s]Extractor Predicting: 223it [02:20,  1.63it/s]Extractor Predicting: 224it [02:21,  1.60it/s]Extractor Predicting: 225it [02:21,  1.61it/s]Extractor Predicting: 226it [02:22,  1.57it/s]Extractor Predicting: 227it [02:23,  1.57it/s]Extractor Predicting: 228it [02:23,  1.57it/s]Extractor Predicting: 229it [02:24,  1.55it/s]Extractor Predicting: 230it [02:25,  1.55it/s]Extractor Predicting: 231it [02:25,  1.57it/s]Extractor Predicting: 232it [02:26,  1.61it/s]Extractor Predicting: 233it [02:26,  1.67it/s]Extractor Predicting: 234it [02:27,  1.66it/s]Extractor Predicting: 235it [02:27,  1.67it/s]Extractor Predicting: 236it [02:28,  1.72it/s]Extractor Predicting: 237it [02:29,  1.72it/s]Extractor Predicting: 238it [02:29,  1.73it/s]Extractor Predicting: 239it [02:30,  1.72it/s]Extractor Predicting: 240it [02:30,  1.71it/s]Extractor Predicting: 241it [02:31,  1.50it/s]Extractor Predicting: 242it [02:32,  1.57it/s]Extractor Predicting: 243it [02:32,  1.65it/s]Extractor Predicting: 244it [02:33,  1.66it/s]Extractor Predicting: 245it [02:33,  1.73it/s]Extractor Predicting: 246it [02:34,  1.78it/s]Extractor Predicting: 247it [02:35,  1.73it/s]Extractor Predicting: 248it [02:35,  1.69it/s]Extractor Predicting: 249it [02:36,  1.70it/s]Extractor Predicting: 250it [02:36,  1.71it/s]Extractor Predicting: 251it [02:37,  1.72it/s]Extractor Predicting: 252it [02:37,  1.74it/s]Extractor Predicting: 253it [02:38,  1.73it/s]Extractor Predicting: 254it [02:39,  1.71it/s]Extractor Predicting: 255it [02:39,  1.74it/s]Extractor Predicting: 256it [02:40,  1.74it/s]Extractor Predicting: 257it [02:40,  1.74it/s]Extractor Predicting: 258it [02:41,  1.75it/s]Extractor Predicting: 259it [02:41,  1.77it/s]Extractor Predicting: 260it [02:42,  1.75it/s]Extractor Predicting: 261it [02:43,  1.66it/s]Extractor Predicting: 262it [02:43,  1.66it/s]Extractor Predicting: 263it [02:44,  1.61it/s]Extractor Predicting: 264it [02:45,  1.56it/s]Extractor Predicting: 265it [02:45,  1.55it/s]Extractor Predicting: 266it [02:46,  1.56it/s]Extractor Predicting: 267it [02:47,  1.59it/s]Extractor Predicting: 268it [02:47,  1.59it/s]Extractor Predicting: 269it [02:48,  1.55it/s]Extractor Predicting: 270it [02:49,  1.52it/s]Extractor Predicting: 271it [02:49,  1.51it/s]Extractor Predicting: 272it [02:50,  1.54it/s]Extractor Predicting: 273it [02:51,  1.54it/s]Extractor Predicting: 274it [02:51,  1.53it/s]Extractor Predicting: 275it [02:52,  1.52it/s]Extractor Predicting: 276it [02:52,  1.53it/s]Extractor Predicting: 277it [02:53,  1.51it/s]Extractor Predicting: 278it [02:54,  1.52it/s]Extractor Predicting: 279it [02:54,  1.55it/s]Extractor Predicting: 280it [02:55,  1.52it/s]Extractor Predicting: 281it [02:56,  1.51it/s]Extractor Predicting: 282it [02:56,  1.51it/s]Extractor Predicting: 283it [02:57,  1.53it/s]Extractor Predicting: 284it [02:58,  1.51it/s]Extractor Predicting: 285it [02:58,  1.51it/s]Extractor Predicting: 286it [02:59,  1.52it/s]Extractor Predicting: 287it [03:00,  1.51it/s]Extractor Predicting: 288it [03:00,  1.55it/s]Extractor Predicting: 289it [03:01,  1.56it/s]Extractor Predicting: 290it [03:02,  1.57it/s]Extractor Predicting: 291it [03:02,  1.59it/s]Extractor Predicting: 292it [03:03,  1.58it/s]Extractor Predicting: 293it [03:03,  1.58it/s]Extractor Predicting: 294it [03:04,  1.60it/s]Extractor Predicting: 295it [03:05,  1.60it/s]Extractor Predicting: 296it [03:05,  1.58it/s]Extractor Predicting: 297it [03:06,  1.60it/s]Extractor Predicting: 298it [03:07,  1.64it/s]Extractor Predicting: 299it [03:07,  1.61it/s]Extractor Predicting: 300it [03:08,  1.60it/s]Extractor Predicting: 301it [03:08,  1.59it/s]Extractor Predicting: 302it [03:09,  1.57it/s]Extractor Predicting: 303it [03:10,  1.57it/s]Extractor Predicting: 304it [03:10,  1.56it/s]Extractor Predicting: 305it [03:11,  1.54it/s]Extractor Predicting: 306it [03:12,  1.57it/s]Extractor Predicting: 307it [03:12,  1.57it/s]Extractor Predicting: 308it [03:13,  1.57it/s]Extractor Predicting: 309it [03:14,  1.53it/s]Extractor Predicting: 310it [03:14,  1.54it/s]Extractor Predicting: 311it [03:15,  1.55it/s]Extractor Predicting: 312it [03:16,  1.56it/s]Extractor Predicting: 313it [03:16,  1.57it/s]Extractor Predicting: 314it [03:17,  1.56it/s]Extractor Predicting: 315it [03:17,  1.57it/s]Extractor Predicting: 316it [03:18,  1.57it/s]Extractor Predicting: 317it [03:19,  1.58it/s]Extractor Predicting: 318it [03:19,  1.62it/s]Extractor Predicting: 319it [03:20,  1.61it/s]Extractor Predicting: 320it [03:21,  1.60it/s]Extractor Predicting: 321it [03:21,  1.59it/s]Extractor Predicting: 322it [03:22,  1.56it/s]Extractor Predicting: 323it [03:22,  1.59it/s]Extractor Predicting: 324it [03:23,  1.60it/s]Extractor Predicting: 325it [03:24,  1.59it/s]Extractor Predicting: 326it [03:24,  1.61it/s]Extractor Predicting: 327it [03:25,  1.59it/s]Extractor Predicting: 328it [03:26,  1.58it/s]Extractor Predicting: 329it [03:26,  1.59it/s]Extractor Predicting: 330it [03:27,  1.58it/s]Extractor Predicting: 331it [03:28,  1.58it/s]Extractor Predicting: 332it [03:28,  1.57it/s]Extractor Predicting: 333it [03:29,  1.55it/s]Extractor Predicting: 334it [03:29,  1.59it/s]Extractor Predicting: 335it [03:30,  1.58it/s]Extractor Predicting: 336it [03:31,  1.60it/s]Extractor Predicting: 337it [03:31,  1.55it/s]Extractor Predicting: 338it [03:32,  1.57it/s]Extractor Predicting: 339it [03:33,  1.58it/s]Extractor Predicting: 340it [03:33,  1.56it/s]Extractor Predicting: 341it [03:34,  1.59it/s]Extractor Predicting: 342it [03:35,  1.59it/s]Extractor Predicting: 343it [03:35,  1.60it/s]Extractor Predicting: 344it [03:36,  1.64it/s]Extractor Predicting: 345it [03:36,  1.60it/s]Extractor Predicting: 346it [03:37,  1.60it/s]Extractor Predicting: 347it [03:38,  1.60it/s]Extractor Predicting: 348it [03:38,  1.56it/s]Extractor Predicting: 349it [03:39,  1.58it/s]Extractor Predicting: 350it [03:40,  1.56it/s]Extractor Predicting: 351it [03:40,  1.57it/s]Extractor Predicting: 352it [03:41,  1.54it/s]Extractor Predicting: 353it [03:41,  1.55it/s]Extractor Predicting: 354it [03:42,  1.56it/s]Extractor Predicting: 355it [03:43,  1.55it/s]Extractor Predicting: 356it [03:43,  1.57it/s]Extractor Predicting: 357it [03:44,  1.54it/s]Extractor Predicting: 358it [03:45,  1.38it/s]Extractor Predicting: 359it [03:46,  1.44it/s]Extractor Predicting: 360it [03:46,  1.47it/s]Extractor Predicting: 361it [03:47,  1.49it/s]Extractor Predicting: 362it [03:47,  1.54it/s]Extractor Predicting: 363it [03:48,  1.55it/s]Extractor Predicting: 364it [03:49,  1.58it/s]Extractor Predicting: 365it [03:49,  1.56it/s]Extractor Predicting: 366it [03:50,  1.56it/s]Extractor Predicting: 367it [03:51,  1.56it/s]Extractor Predicting: 368it [03:51,  1.58it/s]Extractor Predicting: 369it [03:52,  1.54it/s]Extractor Predicting: 370it [03:53,  1.55it/s]Extractor Predicting: 371it [03:53,  1.54it/s]Extractor Predicting: 372it [03:54,  1.56it/s]Extractor Predicting: 373it [03:55,  1.56it/s]Extractor Predicting: 374it [03:55,  1.59it/s]Extractor Predicting: 375it [03:56,  1.58it/s]Extractor Predicting: 376it [03:56,  1.55it/s]Extractor Predicting: 377it [03:57,  1.61it/s]Extractor Predicting: 378it [03:58,  1.61it/s]Extractor Predicting: 379it [03:58,  1.64it/s]Extractor Predicting: 380it [03:59,  1.61it/s]Extractor Predicting: 381it [04:00,  1.60it/s]Extractor Predicting: 382it [04:00,  1.59it/s]Extractor Predicting: 383it [04:01,  1.58it/s]Extractor Predicting: 384it [04:01,  1.57it/s]Extractor Predicting: 385it [04:02,  1.58it/s]Extractor Predicting: 386it [04:03,  1.54it/s]Extractor Predicting: 387it [04:03,  1.54it/s]Extractor Predicting: 388it [04:04,  1.49it/s]Extractor Predicting: 389it [04:05,  1.52it/s]Extractor Predicting: 390it [04:05,  1.53it/s]Extractor Predicting: 391it [04:06,  1.54it/s]Extractor Predicting: 392it [04:07,  1.57it/s]Extractor Predicting: 393it [04:07,  1.56it/s]Extractor Predicting: 394it [04:08,  1.57it/s]Extractor Predicting: 395it [04:09,  1.57it/s]Extractor Predicting: 396it [04:09,  1.54it/s]Extractor Predicting: 397it [04:10,  1.56it/s]Extractor Predicting: 398it [04:11,  1.54it/s]Extractor Predicting: 399it [04:11,  1.58it/s]Extractor Predicting: 400it [04:12,  1.56it/s]Extractor Predicting: 401it [04:12,  1.56it/s]Extractor Predicting: 402it [04:13,  1.56it/s]Extractor Predicting: 403it [04:14,  1.54it/s]Extractor Predicting: 404it [04:14,  1.57it/s]Extractor Predicting: 405it [04:15,  1.58it/s]Extractor Predicting: 406it [04:16,  1.59it/s]Extractor Predicting: 407it [04:16,  1.58it/s]Extractor Predicting: 408it [04:17,  1.57it/s]Extractor Predicting: 409it [04:17,  1.59it/s]Extractor Predicting: 410it [04:18,  1.59it/s]Extractor Predicting: 411it [04:19,  1.58it/s]Extractor Predicting: 412it [04:19,  1.61it/s]Extractor Predicting: 413it [04:20,  1.61it/s]Extractor Predicting: 414it [04:21,  1.58it/s]Extractor Predicting: 415it [04:21,  1.56it/s]Extractor Predicting: 416it [04:22,  1.60it/s]Extractor Predicting: 417it [04:22,  1.60it/s]Extractor Predicting: 418it [04:23,  1.64it/s]Extractor Predicting: 419it [04:24,  1.61it/s]Extractor Predicting: 420it [04:24,  1.63it/s]Extractor Predicting: 421it [04:25,  1.60it/s]Extractor Predicting: 422it [04:26,  1.61it/s]Extractor Predicting: 423it [04:26,  1.62it/s]Extractor Predicting: 424it [04:27,  1.59it/s]Extractor Predicting: 425it [04:27,  1.58it/s]Extractor Predicting: 426it [04:28,  1.62it/s]Extractor Predicting: 427it [04:29,  1.61it/s]Extractor Predicting: 428it [04:29,  1.57it/s]Extractor Predicting: 429it [04:30,  1.77it/s]Extractor Predicting: 429it [04:30,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:33,419 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:33,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:33,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:33,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:33,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:55:34,222 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:55:34,223 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:55:34,952 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:55:36,072 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:55:36,072 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:39,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:39,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:39,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:39,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:39,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:55:40,023 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:55:40,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:55:40,647 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:55:40,885 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:55:40,886 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.19577146546158813,
  "recall": 0.11797315697335149,
  "score": 0.1472266051705304,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 5it [00:03,  1.60it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:55:45,502 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:55:45,503 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:55:45,560 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:55:45,586 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:55:45,614 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:55:54,555 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:55:54,575 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:55:54,679 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:55:54,680 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:55:54,730 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:55:54,769 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.19607843137254902,
  "recall": 0.045871559633027525,
  "score": 0.07434944237918216,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:55:55,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:55,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:56,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:56,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:57,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:57,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:58,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:58,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:59,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:55:59,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:00,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:00,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:01,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:01,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:02,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:02,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:03,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:04,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:04,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:05,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:05,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:11<03:32, 11.17s/it][WARNING|generation_utils.py:914] 2023-08-29 03:56:06,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:06,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:07,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:07,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:08,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:08,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:09,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:09,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:10,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:11,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:11,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:12,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:12,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:13,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:13,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:14,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:14,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:15,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:15,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:16,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:16,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:22<03:20, 11.16s/it][WARNING|generation_utils.py:914] 2023-08-29 03:56:17,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:18,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:18,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:19,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:20,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:20,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:21,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:22,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:23,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:24,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:24,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:25,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:26,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:26,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:27,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:27,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:28,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:29,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:29,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:30,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:31,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:36<03:33, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-29 03:56:31,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:32,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:32,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:33,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:34,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:34,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:35,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:35,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:36,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:37,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:37,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:38,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:39,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:40,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:40,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:41,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:41,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:42,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:42,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:43,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:48<03:19, 12.47s/it][WARNING|generation_utils.py:914] 2023-08-29 03:56:44,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:44,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:45,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:46,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:47,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:47,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:48,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:48,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:50,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:50,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:51,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:51,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:53,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:53,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:54,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:55,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:56,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:56,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:57,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:58,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:59,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:56:59,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:05<03:28, 13.87s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:00,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:01,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:01,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:02,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:02,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:03,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:04,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:04,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:05,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:05,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:06,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:06,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:07,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:08,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:08,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:09,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:09,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:10,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:10,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:11,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:16<03:03, 13.11s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:11,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:12,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:13,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:13,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:14,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:14,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:15,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:15,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:16,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:16,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:17,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:17,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:18,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:18,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:19,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:20,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:20,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:21,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:21,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:22,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:22,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:23,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:23,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:24,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:29<02:50, 13.08s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:25,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:25,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:26,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:26,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:27,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:28,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:28,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:29,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:30,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:30,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:31,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:32,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:33,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:33,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:34,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:35,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:36,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:36,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:37,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:37,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:38,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:43<02:40, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:38,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:39,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:40,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:40,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:41,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:42,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:42,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:43,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:44,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:44,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:45,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:45,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:46,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:46,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:47,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:48,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:48,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:49,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:49,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:50,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:51,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:56<02:25, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:51,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:52,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:52,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:53,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:54,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:54,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:55,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:55,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:56,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:56,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:57,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:58,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:58,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:59,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:00,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:00,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:01,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:02,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:02,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:03,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:08<02:07, 12.79s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:03,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:04,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:04,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:05,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:06,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:07,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:07,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:08,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:09,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:09,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:10,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:10,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:11,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:12,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:12,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:13,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:13,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:14,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:15,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:15,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:21<01:55, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:16,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:17,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:17,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:18,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:18,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:19,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:20,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:20,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:21,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:21,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:22,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:22,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:23,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:24,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:24,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:25,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:25,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:26,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:26,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:27,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:28,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:33<01:40, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:28,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:29,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:29,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:30,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:30,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:31,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:32,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:32,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:33,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:33,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:34,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:35,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:36,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:37,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:37,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:38,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:38,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:39,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:39,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:45<01:26, 12.40s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:40,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:41,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:41,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:42,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:42,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:43,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:44,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:44,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:45,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:45,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:46,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:47,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:48,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:48,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:49,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:50,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:51,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:51,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:57<01:13, 12.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:52,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:52,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:53,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:54,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:54,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:55,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:55,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:56,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:56,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:57,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:58,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:58,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:59,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:59,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:00,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:01,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:01,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:02,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:02,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:03,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:04,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:09<01:01, 12.23s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:04,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:05,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:05,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:06,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:07,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:07,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:08,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:08,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:09,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:09,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:10,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:11,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:11,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:12,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:12,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:13,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:13,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:14,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:15,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:16,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:16,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:22<00:49, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:17,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:17,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:18,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:18,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:19,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:20,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:20,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:21,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:22,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:22,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:23,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:23,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:24,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:25,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:25,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:26,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:27,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:27,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:28,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:28,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:29,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:34<00:37, 12.50s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:30,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:30,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:31,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:31,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:32,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:32,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:33,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:33,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:34,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:35,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:35,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:36,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:36,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:37,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:37,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:38,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:39,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:39,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:40,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:41,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:46<00:24, 12.25s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:41,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:42,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:43,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:43,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:44,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:45,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:45,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:46,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:46,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:47,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:48,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:48,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:49,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:49,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:50,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:50,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:51,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:51,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:52,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:52,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:53,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:58<00:12, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:54,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:54,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:55,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:55,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:56,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:56,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:57,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:58,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:59,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:00,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:00,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:01,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:01,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:02,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:02,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:03,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:03,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:04,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:05,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:10<00:00, 12.15s/it]Generating: 100%|██████████| 20/20 [04:10<00:00, 12.54s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:14,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:14,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:14,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:14,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:14,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:00:15,498 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:00:15,500 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:00:16,243 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:00:17,362 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:00:17,362 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:19,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:19,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:19,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:19,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:00:19,626 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:00:20,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:00:20,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:00:20,416 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:00:20,633 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:00:20,633 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : head of government . Context : On 1 January 2014 , the Prime Minister appointed him as Minister of State for Foreign Affairs and Trade . Head Entity : Prime Minister , Tail Entity : Prime Minister of State for Foreign Affairs and Trade .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in life , he married his second wife , a young French lawyer who had a son in the First World War . Head Entity : First World War , Tail Entity : Marie Bélanger .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : spouse . Context : On 31 March 2014 , the couple announced that they had split and remarried at the end of 2015 . Head Entity : divorce , Tail Entity : husband .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.95625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 9016
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9116, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.69it/s]Extractor Estimating: 3it [00:01,  1.81it/s]Extractor Estimating: 4it [00:02,  1.79it/s]Extractor Estimating: 5it [00:02,  1.89it/s]Extractor Estimating: 6it [00:03,  1.83it/s]Extractor Estimating: 7it [00:03,  1.81it/s]Extractor Estimating: 8it [00:04,  1.82it/s]Extractor Estimating: 9it [00:05,  1.79it/s]Extractor Estimating: 10it [00:05,  1.85it/s]Extractor Estimating: 11it [00:06,  1.86it/s]Extractor Estimating: 12it [00:06,  1.89it/s]Extractor Estimating: 13it [00:07,  1.79it/s]Extractor Estimating: 14it [00:07,  1.84it/s]Extractor Estimating: 15it [00:08,  1.88it/s]Extractor Estimating: 16it [00:08,  1.94it/s]Extractor Estimating: 17it [00:09,  1.92it/s]Extractor Estimating: 18it [00:09,  1.93it/s]Extractor Estimating: 19it [00:10,  1.92it/s]Extractor Estimating: 20it [00:10,  1.76it/s]Extractor Estimating: 21it [00:11,  1.81it/s]Extractor Estimating: 22it [00:11,  1.85it/s]Extractor Estimating: 23it [00:12,  1.89it/s]Extractor Estimating: 24it [00:13,  1.84it/s]Extractor Estimating: 25it [00:13,  1.82it/s]Extractor Estimating: 26it [00:14,  1.73it/s]Extractor Estimating: 27it [00:14,  1.81it/s]Extractor Estimating: 28it [00:15,  1.77it/s]Extractor Estimating: 29it [00:15,  1.78it/s]Extractor Estimating: 30it [00:16,  1.78it/s]Extractor Estimating: 31it [00:16,  1.79it/s]Extractor Estimating: 32it [00:17,  1.72it/s]Extractor Estimating: 33it [00:18,  1.76it/s]Extractor Estimating: 34it [00:18,  1.69it/s]Extractor Estimating: 35it [00:19,  1.62it/s]Extractor Estimating: 36it [00:20,  1.66it/s]Extractor Estimating: 37it [00:20,  1.67it/s]Extractor Estimating: 38it [00:21,  1.71it/s]Extractor Estimating: 39it [00:21,  1.74it/s]Extractor Estimating: 40it [00:22,  1.77it/s]Extractor Estimating: 41it [00:22,  1.80it/s]Extractor Estimating: 42it [00:23,  1.79it/s]Extractor Estimating: 43it [00:23,  1.76it/s]Extractor Estimating: 44it [00:24,  1.75it/s]Extractor Estimating: 45it [00:25,  1.75it/s]Extractor Estimating: 46it [00:25,  1.79it/s]Extractor Estimating: 47it [00:26,  1.81it/s]Extractor Estimating: 48it [00:26,  1.80it/s]Extractor Estimating: 49it [00:27,  1.66it/s]Extractor Estimating: 50it [00:27,  1.73it/s]Extractor Estimating: 51it [00:28,  1.69it/s]Extractor Estimating: 52it [00:29,  1.67it/s]Extractor Estimating: 53it [00:29,  1.68it/s]Extractor Estimating: 54it [00:30,  1.67it/s]Extractor Estimating: 55it [00:31,  1.68it/s]Extractor Estimating: 56it [00:31,  1.71it/s]Extractor Estimating: 57it [00:32,  1.74it/s]Extractor Estimating: 58it [00:32,  1.77it/s]Extractor Estimating: 59it [00:33,  1.60it/s]Extractor Estimating: 60it [00:34,  1.62it/s]Extractor Estimating: 61it [00:34,  1.62it/s]Extractor Estimating: 62it [00:35,  1.60it/s]Extractor Estimating: 63it [00:35,  1.67it/s]Extractor Estimating: 64it [00:36,  1.62it/s]Extractor Estimating: 65it [00:37,  1.64it/s]Extractor Estimating: 66it [00:37,  1.66it/s]Extractor Estimating: 67it [00:38,  1.75it/s]Extractor Estimating: 68it [00:38,  1.76it/s]Extractor Estimating: 69it [00:39,  1.80it/s]Extractor Estimating: 70it [00:39,  1.81it/s]Extractor Estimating: 71it [00:40,  1.83it/s]Extractor Estimating: 72it [00:40,  1.82it/s]Extractor Estimating: 73it [00:41,  1.84it/s]Extractor Estimating: 74it [00:42,  1.76it/s]Extractor Estimating: 75it [00:42,  1.83it/s]Extractor Estimating: 76it [00:43,  1.77it/s]Extractor Estimating: 77it [00:43,  1.78it/s]Extractor Estimating: 78it [00:44,  1.70it/s]Extractor Estimating: 79it [00:44,  1.66it/s]Extractor Estimating: 80it [00:45,  1.64it/s]Extractor Estimating: 81it [00:46,  1.65it/s]Extractor Estimating: 82it [00:46,  1.62it/s]Extractor Estimating: 83it [00:47,  1.59it/s]Extractor Estimating: 84it [00:48,  1.60it/s]Extractor Estimating: 85it [00:48,  1.48it/s]Extractor Estimating: 86it [00:49,  1.49it/s]Extractor Estimating: 87it [00:50,  1.50it/s]Extractor Estimating: 88it [00:50,  1.52it/s]Extractor Estimating: 89it [00:51,  1.44it/s]Extractor Estimating: 90it [00:52,  1.46it/s]Extractor Estimating: 91it [00:52,  1.47it/s]Extractor Estimating: 92it [00:53,  1.55it/s]Extractor Estimating: 93it [00:54,  1.57it/s]Extractor Estimating: 94it [00:54,  1.57it/s]Extractor Estimating: 95it [00:55,  1.61it/s]Extractor Estimating: 96it [00:55,  1.61it/s]Extractor Estimating: 97it [00:56,  1.59it/s]Extractor Estimating: 98it [00:57,  1.66it/s]Extractor Estimating: 99it [00:57,  1.62it/s]Extractor Estimating: 100it [00:58,  1.62it/s]Extractor Estimating: 101it [00:59,  1.59it/s]Extractor Estimating: 102it [00:59,  1.64it/s]Extractor Estimating: 103it [01:00,  1.64it/s]Extractor Estimating: 104it [01:00,  1.66it/s]Extractor Estimating: 105it [01:01,  1.77it/s]Extractor Estimating: 106it [01:01,  1.78it/s]Extractor Estimating: 107it [01:02,  1.79it/s]Extractor Estimating: 108it [01:03,  1.76it/s]Extractor Estimating: 109it [01:03,  1.83it/s]Extractor Estimating: 110it [01:04,  1.80it/s]Extractor Estimating: 111it [01:04,  1.75it/s]Extractor Estimating: 112it [01:05,  1.73it/s]Extractor Estimating: 113it [01:05,  1.78it/s]Extractor Estimating: 114it [01:06,  1.75it/s]Extractor Estimating: 115it [01:06,  1.76it/s]Extractor Estimating: 116it [01:07,  1.77it/s]Extractor Estimating: 117it [01:08,  1.82it/s]Extractor Estimating: 118it [01:08,  1.85it/s]Extractor Estimating: 119it [01:09,  1.86it/s]Extractor Estimating: 120it [01:09,  1.82it/s]Extractor Estimating: 121it [01:10,  1.90it/s]Extractor Estimating: 122it [01:10,  1.90it/s]Extractor Estimating: 123it [01:11,  1.85it/s]Extractor Estimating: 124it [01:11,  1.77it/s]Extractor Estimating: 125it [01:12,  1.74it/s]Extractor Estimating: 126it [01:13,  1.67it/s]Extractor Estimating: 127it [01:13,  1.69it/s]Extractor Estimating: 128it [01:14,  1.66it/s]Extractor Estimating: 129it [01:14,  1.64it/s]Extractor Estimating: 130it [01:15,  1.62it/s]Extractor Estimating: 131it [01:16,  1.61it/s]Extractor Estimating: 132it [01:16,  1.66it/s]Extractor Estimating: 133it [01:17,  1.60it/s]Extractor Estimating: 134it [01:18,  1.61it/s]Extractor Estimating: 135it [01:18,  1.66it/s]Extractor Estimating: 136it [01:19,  1.65it/s]Extractor Estimating: 137it [01:19,  1.63it/s]Extractor Estimating: 138it [01:20,  1.63it/s]Extractor Estimating: 139it [01:21,  1.61it/s]Extractor Estimating: 140it [01:21,  1.60it/s]Extractor Estimating: 141it [01:22,  1.65it/s]Extractor Estimating: 142it [01:22,  1.65it/s]Extractor Estimating: 143it [01:23,  1.64it/s]Extractor Estimating: 144it [01:24,  1.65it/s]Extractor Estimating: 145it [01:24,  1.63it/s]Extractor Estimating: 146it [01:25,  1.70it/s]Extractor Estimating: 147it [01:26,  1.61it/s]Extractor Estimating: 148it [01:26,  1.63it/s]Extractor Estimating: 149it [01:27,  1.60it/s]Extractor Estimating: 150it [01:27,  1.56it/s]Extractor Estimating: 151it [01:28,  1.67it/s]Extractor Estimating: 152it [01:28,  1.75it/s]Extractor Estimating: 153it [01:29,  1.82it/s]Extractor Estimating: 154it [01:29,  1.83it/s]Extractor Estimating: 155it [01:30,  1.85it/s]Extractor Estimating: 156it [01:31,  1.91it/s]Extractor Estimating: 157it [01:31,  1.95it/s]Extractor Estimating: 158it [01:31,  1.98it/s]Extractor Estimating: 159it [01:32,  1.95it/s]Extractor Estimating: 160it [01:33,  1.92it/s]Extractor Estimating: 161it [01:33,  1.92it/s]Extractor Estimating: 162it [01:34,  1.96it/s]Extractor Estimating: 163it [01:34,  1.99it/s]Extractor Estimating: 164it [01:35,  2.00it/s]Extractor Estimating: 165it [01:35,  1.99it/s]Extractor Estimating: 166it [01:36,  2.02it/s]Extractor Estimating: 167it [01:36,  1.97it/s]Extractor Estimating: 168it [01:37,  1.94it/s]Extractor Estimating: 169it [01:37,  2.00it/s]Extractor Estimating: 170it [01:38,  1.98it/s]Extractor Estimating: 171it [01:38,  2.04it/s]Extractor Estimating: 172it [01:38,  2.12it/s]Extractor Estimating: 173it [01:39,  1.86it/s]Extractor Estimating: 174it [01:40,  1.86it/s]Extractor Estimating: 175it [01:40,  1.93it/s]Extractor Estimating: 176it [01:41,  1.96it/s]Extractor Estimating: 177it [01:41,  2.04it/s]Extractor Estimating: 178it [01:42,  1.97it/s]Extractor Estimating: 179it [01:42,  1.94it/s]Extractor Estimating: 180it [01:43,  1.95it/s]Extractor Estimating: 181it [01:43,  1.96it/s]Extractor Estimating: 182it [01:44,  1.92it/s]Extractor Estimating: 183it [01:44,  1.93it/s]Extractor Estimating: 184it [01:45,  1.93it/s]Extractor Estimating: 185it [01:45,  1.87it/s]Extractor Estimating: 186it [01:46,  1.95it/s]Extractor Estimating: 187it [01:46,  1.92it/s]Extractor Estimating: 188it [01:47,  1.89it/s]Extractor Estimating: 189it [01:47,  1.94it/s]Extractor Estimating: 190it [01:48,  1.92it/s]Extractor Estimating: 191it [01:49,  1.84it/s]Extractor Estimating: 192it [01:49,  1.86it/s]Extractor Estimating: 193it [01:50,  1.91it/s]Extractor Estimating: 194it [01:50,  1.94it/s]Extractor Estimating: 195it [01:51,  1.91it/s]Extractor Estimating: 196it [01:51,  1.95it/s]Extractor Estimating: 197it [01:52,  1.90it/s]Extractor Estimating: 198it [01:52,  1.86it/s]Extractor Estimating: 199it [01:53,  1.91it/s]Extractor Estimating: 200it [01:53,  1.89it/s]Extractor Estimating: 201it [01:54,  1.83it/s]Extractor Estimating: 202it [01:55,  1.64it/s]Extractor Estimating: 203it [01:55,  1.61it/s]Extractor Estimating: 204it [01:56,  1.61it/s]Extractor Estimating: 205it [01:56,  1.65it/s]Extractor Estimating: 206it [01:57,  1.65it/s]Extractor Estimating: 207it [01:58,  1.67it/s]Extractor Estimating: 208it [01:58,  1.65it/s]Extractor Estimating: 209it [01:59,  1.61it/s]Extractor Estimating: 210it [01:59,  1.64it/s]Extractor Estimating: 211it [02:00,  1.66it/s]Extractor Estimating: 212it [02:01,  1.67it/s]Extractor Estimating: 213it [02:01,  1.67it/s]Extractor Estimating: 214it [02:02,  1.71it/s]Extractor Estimating: 215it [02:02,  1.77it/s]Extractor Estimating: 216it [02:03,  1.70it/s]Extractor Estimating: 217it [02:04,  1.65it/s]Extractor Estimating: 218it [02:04,  1.65it/s]Extractor Estimating: 219it [02:05,  1.61it/s]Extractor Estimating: 220it [02:06,  1.55it/s]Extractor Estimating: 221it [02:06,  1.59it/s]Extractor Estimating: 222it [02:07,  1.62it/s]Extractor Estimating: 223it [02:07,  1.66it/s]Extractor Estimating: 224it [02:08,  1.65it/s]Extractor Estimating: 225it [02:08,  1.70it/s]Extractor Estimating: 226it [02:09,  1.73it/s]Extractor Estimating: 227it [02:10,  1.66it/s]Extractor Estimating: 228it [02:10,  1.60it/s]Extractor Estimating: 229it [02:11,  1.69it/s]Extractor Estimating: 230it [02:11,  1.71it/s]Extractor Estimating: 231it [02:12,  1.74it/s]Extractor Estimating: 232it [02:13,  1.71it/s]Extractor Estimating: 233it [02:13,  1.71it/s]Extractor Estimating: 234it [02:14,  1.78it/s]Extractor Estimating: 235it [02:14,  1.75it/s]Extractor Estimating: 236it [02:15,  1.77it/s]Extractor Estimating: 237it [02:15,  1.79it/s]Extractor Estimating: 238it [02:16,  1.72it/s]Extractor Estimating: 239it [02:17,  1.64it/s]Extractor Estimating: 240it [02:17,  1.69it/s]Extractor Estimating: 241it [02:18,  1.70it/s]Extractor Estimating: 242it [02:18,  1.75it/s]Extractor Estimating: 243it [02:19,  1.74it/s]Extractor Estimating: 244it [02:19,  1.76it/s]Extractor Estimating: 245it [02:20,  1.72it/s]Extractor Estimating: 246it [02:21,  1.69it/s]Extractor Estimating: 247it [02:21,  1.64it/s]Extractor Estimating: 248it [02:22,  1.71it/s]Extractor Estimating: 249it [02:23,  1.53it/s]Extractor Estimating: 250it [02:23,  1.58it/s]Extractor Estimating: 251it [02:24,  1.57it/s]Extractor Estimating: 252it [02:25,  1.60it/s]Extractor Estimating: 253it [02:25,  1.62it/s]Extractor Estimating: 254it [02:26,  1.64it/s]Extractor Estimating: 255it [02:26,  1.69it/s]Extractor Estimating: 256it [02:27,  1.67it/s]Extractor Estimating: 257it [02:28,  1.63it/s]Extractor Estimating: 258it [02:28,  1.68it/s]Extractor Estimating: 259it [02:29,  1.67it/s]Extractor Estimating: 260it [02:29,  1.68it/s]Extractor Estimating: 261it [02:30,  1.59it/s]Extractor Estimating: 262it [02:31,  1.61it/s]Extractor Estimating: 263it [02:31,  1.59it/s]Extractor Estimating: 264it [02:32,  1.65it/s]Extractor Estimating: 265it [02:32,  1.66it/s]Extractor Estimating: 266it [02:33,  1.61it/s]Extractor Estimating: 267it [02:34,  1.62it/s]Extractor Estimating: 268it [02:34,  1.67it/s]Extractor Estimating: 269it [02:35,  1.73it/s]Extractor Estimating: 270it [02:35,  1.73it/s]Extractor Estimating: 271it [02:36,  1.73it/s]Extractor Estimating: 272it [02:37,  1.68it/s]Extractor Estimating: 273it [02:37,  1.66it/s]Extractor Estimating: 274it [02:38,  1.66it/s]Extractor Estimating: 275it [02:38,  1.67it/s]Extractor Estimating: 276it [02:39,  1.65it/s]Extractor Estimating: 277it [02:40,  1.63it/s]Extractor Estimating: 278it [02:40,  1.67it/s]Extractor Estimating: 279it [02:41,  1.65it/s]Extractor Estimating: 280it [02:41,  1.66it/s]Extractor Estimating: 281it [02:42,  1.70it/s]Extractor Estimating: 282it [02:43,  1.70it/s]Extractor Estimating: 283it [02:43,  1.70it/s]Extractor Estimating: 284it [02:44,  1.64it/s]Extractor Estimating: 285it [02:44,  1.65it/s]Extractor Estimating: 286it [02:45,  1.67it/s]Extractor Estimating: 287it [02:46,  1.71it/s]Extractor Estimating: 288it [02:46,  1.69it/s]Extractor Estimating: 289it [02:47,  1.73it/s]Extractor Estimating: 290it [02:47,  1.72it/s]Extractor Estimating: 291it [02:48,  1.75it/s]Extractor Estimating: 292it [02:48,  1.77it/s]Extractor Estimating: 293it [02:49,  1.75it/s]Extractor Estimating: 294it [02:49,  1.76it/s]Extractor Estimating: 295it [02:50,  1.75it/s]Extractor Estimating: 296it [02:51,  1.73it/s]Extractor Estimating: 297it [02:51,  1.75it/s]Extractor Estimating: 298it [02:52,  1.75it/s]Extractor Estimating: 299it [02:52,  1.74it/s]Extractor Estimating: 300it [02:53,  1.69it/s]Extractor Estimating: 301it [02:54,  1.70it/s]Extractor Estimating: 302it [02:54,  1.72it/s]Extractor Estimating: 303it [02:55,  1.76it/s]Extractor Estimating: 304it [02:55,  1.83it/s]Extractor Estimating: 305it [02:56,  1.80it/s]Extractor Estimating: 306it [02:56,  1.79it/s]Extractor Estimating: 307it [02:57,  1.87it/s]Extractor Estimating: 308it [02:57,  1.76it/s]Extractor Estimating: 309it [02:58,  1.83it/s]Extractor Estimating: 310it [02:58,  1.89it/s]Extractor Estimating: 311it [02:59,  1.90it/s]Extractor Estimating: 312it [03:00,  1.85it/s]Extractor Estimating: 313it [03:00,  1.89it/s]Extractor Estimating: 314it [03:01,  1.87it/s]Extractor Estimating: 315it [03:01,  1.93it/s]Extractor Estimating: 316it [03:02,  1.94it/s]Extractor Estimating: 317it [03:02,  1.94it/s]Extractor Estimating: 318it [03:03,  1.92it/s]Extractor Estimating: 319it [03:03,  1.94it/s]Extractor Estimating: 320it [03:04,  1.86it/s]Extractor Estimating: 321it [03:04,  1.89it/s]Extractor Estimating: 322it [03:05,  1.90it/s]Extractor Estimating: 323it [03:05,  1.90it/s]Extractor Estimating: 324it [03:06,  1.90it/s]Extractor Estimating: 325it [03:06,  1.95it/s]Extractor Estimating: 326it [03:07,  1.96it/s]Extractor Estimating: 327it [03:07,  2.03it/s]Extractor Estimating: 328it [03:08,  1.91it/s]Extractor Estimating: 329it [03:08,  1.91it/s]Extractor Estimating: 330it [03:09,  1.99it/s]Extractor Estimating: 331it [03:09,  2.02it/s]Extractor Estimating: 332it [03:10,  1.99it/s]Extractor Estimating: 333it [03:10,  1.97it/s]Extractor Estimating: 334it [03:11,  1.96it/s]Extractor Estimating: 335it [03:11,  1.90it/s]Extractor Estimating: 336it [03:12,  1.91it/s]Extractor Estimating: 337it [03:12,  1.93it/s]Extractor Estimating: 338it [03:13,  1.99it/s]Extractor Estimating: 339it [03:13,  1.94it/s]Extractor Estimating: 340it [03:14,  1.91it/s]Extractor Estimating: 341it [03:14,  1.95it/s]Extractor Estimating: 342it [03:15,  1.95it/s]Extractor Estimating: 343it [03:15,  2.02it/s]Extractor Estimating: 344it [03:16,  1.98it/s]Extractor Estimating: 345it [03:17,  1.78it/s]Extractor Estimating: 346it [03:17,  1.88it/s]Extractor Estimating: 347it [03:18,  1.82it/s]Extractor Estimating: 348it [03:18,  1.86it/s]Extractor Estimating: 349it [03:19,  1.91it/s]Extractor Estimating: 350it [03:19,  1.89it/s]Extractor Estimating: 351it [03:20,  1.90it/s]Extractor Estimating: 352it [03:20,  1.86it/s]Extractor Estimating: 353it [03:21,  1.83it/s]Extractor Estimating: 354it [03:21,  1.84it/s]Extractor Estimating: 355it [03:22,  1.84it/s]Extractor Estimating: 356it [03:23,  1.83it/s]Extractor Estimating: 357it [03:23,  1.82it/s]Extractor Estimating: 358it [03:24,  1.88it/s]Extractor Estimating: 359it [03:24,  1.82it/s]Extractor Estimating: 360it [03:25,  1.79it/s]Extractor Estimating: 361it [03:25,  1.84it/s]Extractor Estimating: 362it [03:26,  1.80it/s]Extractor Estimating: 363it [03:26,  1.74it/s]Extractor Estimating: 364it [03:27,  1.70it/s]Extractor Estimating: 365it [03:28,  1.71it/s]Extractor Estimating: 366it [03:28,  1.76it/s]Extractor Estimating: 367it [03:29,  1.72it/s]Extractor Estimating: 368it [03:29,  1.76it/s]Extractor Estimating: 369it [03:30,  1.79it/s]Extractor Estimating: 370it [03:30,  1.74it/s]Extractor Estimating: 371it [03:31,  1.75it/s]Extractor Estimating: 372it [03:32,  1.74it/s]Extractor Estimating: 373it [03:32,  1.71it/s]Extractor Estimating: 374it [03:33,  1.70it/s]Extractor Estimating: 375it [03:33,  1.69it/s]Extractor Estimating: 376it [03:34,  1.66it/s]Extractor Estimating: 377it [03:35,  1.68it/s]Extractor Estimating: 378it [03:35,  1.69it/s]Extractor Estimating: 379it [03:36,  1.68it/s]Extractor Estimating: 380it [03:36,  1.69it/s]Extractor Estimating: 381it [03:37,  1.72it/s]Extractor Estimating: 382it [03:38,  1.75it/s]Extractor Estimating: 383it [03:38,  1.76it/s]Extractor Estimating: 384it [03:39,  1.75it/s]Extractor Estimating: 385it [03:39,  1.75it/s]Extractor Estimating: 386it [03:40,  1.77it/s]Extractor Estimating: 387it [03:40,  1.78it/s]Extractor Estimating: 388it [03:41,  1.79it/s]Extractor Estimating: 389it [03:41,  1.83it/s]Extractor Estimating: 390it [03:42,  1.77it/s]Extractor Estimating: 391it [03:43,  1.77it/s]Extractor Estimating: 392it [03:43,  1.77it/s]Extractor Estimating: 393it [03:44,  1.79it/s]Extractor Estimating: 394it [03:44,  1.80it/s]Extractor Estimating: 395it [03:45,  1.76it/s]Extractor Estimating: 396it [03:45,  1.71it/s]Extractor Estimating: 397it [03:46,  1.67it/s]Extractor Estimating: 398it [03:47,  1.67it/s]Extractor Estimating: 399it [03:47,  1.70it/s]Extractor Estimating: 400it [03:48,  1.75it/s]Extractor Estimating: 401it [03:48,  1.78it/s]Extractor Estimating: 402it [03:49,  1.78it/s]Extractor Estimating: 403it [03:49,  1.84it/s]Extractor Estimating: 404it [03:50,  1.83it/s]Extractor Estimating: 405it [03:51,  1.79it/s]Extractor Estimating: 406it [03:51,  1.73it/s]Extractor Estimating: 407it [03:52,  1.74it/s]Extractor Estimating: 408it [03:52,  1.70it/s]Extractor Estimating: 409it [03:53,  1.72it/s]Extractor Estimating: 410it [03:53,  1.79it/s]Extractor Estimating: 411it [03:54,  1.85it/s]Extractor Estimating: 412it [03:54,  1.80it/s]Extractor Estimating: 413it [03:55,  1.80it/s]Extractor Estimating: 414it [03:56,  1.82it/s]Extractor Estimating: 415it [03:56,  1.82it/s]Extractor Estimating: 416it [03:57,  1.77it/s]Extractor Estimating: 417it [03:57,  1.79it/s]Extractor Estimating: 418it [03:58,  1.77it/s]Extractor Estimating: 419it [03:59,  1.61it/s]Extractor Estimating: 420it [03:59,  1.70it/s]Extractor Estimating: 421it [04:00,  1.79it/s]Extractor Estimating: 422it [04:00,  1.81it/s]Extractor Estimating: 423it [04:01,  1.81it/s]Extractor Estimating: 424it [04:01,  1.88it/s]Extractor Estimating: 425it [04:02,  1.73it/s]Extractor Estimating: 426it [04:03,  1.64it/s]Extractor Estimating: 427it [04:03,  1.66it/s]Extractor Estimating: 428it [04:04,  1.64it/s]Extractor Estimating: 429it [04:04,  1.63it/s]Extractor Estimating: 430it [04:05,  1.63it/s]Extractor Estimating: 431it [04:06,  1.58it/s]Extractor Estimating: 432it [04:06,  1.57it/s]Extractor Estimating: 433it [04:07,  1.60it/s]Extractor Estimating: 434it [04:07,  1.65it/s]Extractor Estimating: 435it [04:08,  1.66it/s]Extractor Estimating: 436it [04:09,  1.63it/s]Extractor Estimating: 437it [04:09,  1.59it/s]Extractor Estimating: 438it [04:10,  1.57it/s]Extractor Estimating: 439it [04:11,  1.63it/s]Extractor Estimating: 440it [04:11,  1.67it/s]Extractor Estimating: 441it [04:12,  1.57it/s]Extractor Estimating: 442it [04:12,  1.60it/s]Extractor Estimating: 443it [04:13,  1.50it/s]Extractor Estimating: 444it [04:14,  1.52it/s]Extractor Estimating: 445it [04:14,  1.58it/s]Extractor Estimating: 446it [04:15,  1.57it/s]Extractor Estimating: 447it [04:16,  1.53it/s]Extractor Estimating: 448it [04:17,  1.42it/s]Extractor Estimating: 449it [04:17,  1.46it/s]Extractor Estimating: 450it [04:18,  1.53it/s]Extractor Estimating: 451it [04:18,  1.60it/s]Extractor Estimating: 452it [04:19,  1.69it/s]Extractor Estimating: 453it [04:19,  1.74it/s]Extractor Estimating: 454it [04:20,  1.80it/s]Extractor Estimating: 455it [04:20,  1.82it/s]Extractor Estimating: 456it [04:21,  1.82it/s]Extractor Estimating: 457it [04:22,  1.84it/s]Extractor Estimating: 458it [04:22,  1.86it/s]Extractor Estimating: 459it [04:23,  1.89it/s]Extractor Estimating: 460it [04:23,  1.92it/s]Extractor Estimating: 461it [04:24,  1.92it/s]Extractor Estimating: 462it [04:24,  1.93it/s]Extractor Estimating: 463it [04:25,  1.98it/s]Extractor Estimating: 464it [04:25,  1.98it/s]Extractor Estimating: 465it [04:26,  1.98it/s]Extractor Estimating: 466it [04:26,  2.01it/s]Extractor Estimating: 467it [04:27,  1.97it/s]Extractor Estimating: 468it [04:27,  1.92it/s]Extractor Estimating: 469it [04:28,  1.91it/s]Extractor Estimating: 470it [04:28,  1.95it/s]Extractor Estimating: 471it [04:29,  1.88it/s]Extractor Estimating: 472it [04:29,  1.85it/s]Extractor Estimating: 473it [04:30,  1.89it/s]Extractor Estimating: 474it [04:30,  1.91it/s]Extractor Estimating: 475it [04:31,  1.84it/s]Extractor Estimating: 476it [04:32,  1.68it/s]Extractor Estimating: 477it [04:32,  1.61it/s]Extractor Estimating: 478it [04:33,  1.62it/s]Extractor Estimating: 479it [04:34,  1.59it/s]Extractor Estimating: 480it [04:34,  1.59it/s]Extractor Estimating: 481it [04:35,  1.58it/s]Extractor Estimating: 482it [04:36,  1.56it/s]Extractor Estimating: 483it [04:36,  1.58it/s]Extractor Estimating: 484it [04:37,  1.61it/s]Extractor Estimating: 485it [04:37,  1.56it/s]Extractor Estimating: 486it [04:38,  1.61it/s]Extractor Estimating: 487it [04:39,  1.57it/s]Extractor Estimating: 488it [04:39,  1.59it/s]Extractor Estimating: 489it [04:40,  1.62it/s]Extractor Estimating: 490it [04:40,  1.66it/s]Extractor Estimating: 491it [04:41,  1.67it/s]Extractor Estimating: 492it [04:42,  1.64it/s]Extractor Estimating: 493it [04:42,  1.61it/s]Extractor Estimating: 494it [04:43,  1.56it/s]Extractor Estimating: 495it [04:44,  1.57it/s]Extractor Estimating: 496it [04:44,  1.64it/s]Extractor Estimating: 497it [04:45,  1.62it/s]Extractor Estimating: 498it [04:45,  1.65it/s]Extractor Estimating: 499it [04:46,  1.66it/s]Extractor Estimating: 500it [04:46,  1.87it/s]Extractor Estimating: 500it [04:46,  1.74it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:23,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:23,662 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:23,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:23,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:23,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:05:24,224 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:05:24,225 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:05:24,513 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:05:25,658 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:05:25,658 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:28,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:28,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:28,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:28,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:05:28,709 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:05:29,650 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:05:29,651 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:05:30,321 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:05:30,600 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:05:30,600 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 06:18:49,838 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 06:18:50,023 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 7997 mean pseudo reward: 0.9690127542897915
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 16561
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16661, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16661, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.927, loss:523.5790
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.944, loss:478.2280
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.009, loss:422.1956
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 66, avg_time 0.946, loss:429.6021
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 166, avg_time 0.955, loss:429.8286
>> valid entity prec:0.5430, rec:0.5669, f1:0.5547
>> valid relation prec:0.1584, rec:0.0629, f1:0.0900
>> valid relation with NER prec:0.1584, rec:0.0629, f1:0.0900
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 266, avg_time 2.241, loss:399.3778
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 32, avg_time 0.974, loss:416.0939
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 132, avg_time 0.963, loss:380.4643
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 232, avg_time 0.960, loss:411.3423
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 332, avg_time 0.960, loss:439.5573
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5431, rec:0.5537, f1:0.5483
>> valid relation prec:0.1731, rec:0.0609, f1:0.0901
>> valid relation with NER prec:0.1731, rec:0.0609, f1:0.0901
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 98, avg_time 2.225, loss:375.7346
g_step 1200, step 198, avg_time 0.957, loss:397.8377
g_step 1300, step 298, avg_time 0.945, loss:391.6071
g_step 1400, step 64, avg_time 0.949, loss:362.9827
g_step 1500, step 164, avg_time 0.932, loss:378.4226
>> valid entity prec:0.5411, rec:0.5446, f1:0.5429
>> valid relation prec:0.1168, rec:0.0494, f1:0.0694
>> valid relation with NER prec:0.1168, rec:0.0494, f1:0.0694
g_step 1600, step 264, avg_time 2.220, loss:368.6949
g_step 1700, step 30, avg_time 0.932, loss:361.0172
g_step 1800, step 130, avg_time 0.948, loss:343.3360
g_step 1900, step 230, avg_time 0.940, loss:352.9772
g_step 2000, step 330, avg_time 0.954, loss:368.6779
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5507, rec:0.5870, f1:0.5682
>> valid relation prec:0.1086, rec:0.0503, f1:0.0687
>> valid relation with NER prec:0.1086, rec:0.0503, f1:0.0687
new max entity f1 on valid!
g_step 2100, step 96, avg_time 2.207, loss:293.8936
g_step 2200, step 196, avg_time 0.947, loss:316.0492
g_step 2300, step 296, avg_time 0.953, loss:360.1633
g_step 2400, step 62, avg_time 0.942, loss:305.9579
g_step 2500, step 162, avg_time 0.952, loss:295.8407
>> valid entity prec:0.5552, rec:0.5653, f1:0.5602
>> valid relation prec:0.1148, rec:0.0554, f1:0.0748
>> valid relation with NER prec:0.1148, rec:0.0554, f1:0.0748
g_step 2600, step 262, avg_time 2.205, loss:345.4652
g_step 2700, step 28, avg_time 0.944, loss:330.3600
g_step 2800, step 128, avg_time 0.950, loss:302.8267
g_step 2900, step 228, avg_time 0.939, loss:322.4039
g_step 3000, step 328, avg_time 0.945, loss:315.8087
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5534, rec:0.4608, f1:0.5028
>> valid relation prec:0.1177, rec:0.0434, f1:0.0634
>> valid relation with NER prec:0.1177, rec:0.0434, f1:0.0634
g_step 3100, step 94, avg_time 2.220, loss:276.7425
g_step 3200, step 194, avg_time 0.936, loss:299.8207
g_step 3300, step 294, avg_time 0.943, loss:301.8281
g_step 3400, step 60, avg_time 0.942, loss:277.4144
g_step 3500, step 160, avg_time 0.950, loss:267.3832
>> valid entity prec:0.5349, rec:0.5728, f1:0.5532
>> valid relation prec:0.1201, rec:0.0620, f1:0.0818
>> valid relation with NER prec:0.1201, rec:0.0620, f1:0.0818
g_step 3600, step 260, avg_time 2.230, loss:282.5661
g_step 3700, step 26, avg_time 0.937, loss:268.0776
g_step 3800, step 126, avg_time 0.953, loss:239.5148
g_step 3900, step 226, avg_time 0.932, loss:281.4493
g_step 4000, step 326, avg_time 0.955, loss:272.2341
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5527, rec:0.5056, f1:0.5281
>> valid relation prec:0.1085, rec:0.0460, f1:0.0646
>> valid relation with NER prec:0.1085, rec:0.0460, f1:0.0646
g_step 4100, step 92, avg_time 2.234, loss:263.3732
g_step 4200, step 192, avg_time 0.962, loss:257.3954
g_step 4300, step 292, avg_time 0.935, loss:259.1079
g_step 4400, step 58, avg_time 0.950, loss:242.6369
g_step 4500, step 158, avg_time 0.948, loss:238.4286
>> valid entity prec:0.5527, rec:0.5314, f1:0.5418
>> valid relation prec:0.0834, rec:0.0411, f1:0.0550
>> valid relation with NER prec:0.0834, rec:0.0411, f1:0.0550
g_step 4600, step 258, avg_time 2.226, loss:252.0368
g_step 4700, step 24, avg_time 0.939, loss:235.4576
g_step 4800, step 124, avg_time 0.952, loss:232.9098
g_step 4900, step 224, avg_time 0.957, loss:237.0240
g_step 5000, step 324, avg_time 0.959, loss:256.4561
learning rate was adjusted to 0.0008
>> valid entity prec:0.5415, rec:0.5732, f1:0.5569
>> valid relation prec:0.0859, rec:0.0437, f1:0.0579
>> valid relation with NER prec:0.0859, rec:0.0437, f1:0.0579
g_step 5100, step 90, avg_time 2.215, loss:213.8981
g_step 5200, step 190, avg_time 0.944, loss:238.6673
g_step 5300, step 290, avg_time 0.960, loss:234.7910
g_step 5400, step 56, avg_time 0.960, loss:210.7330
g_step 5500, step 156, avg_time 0.959, loss:207.8138
>> valid entity prec:0.5526, rec:0.5481, f1:0.5503
>> valid relation prec:0.1110, rec:0.0574, f1:0.0757
>> valid relation with NER prec:0.1110, rec:0.0574, f1:0.0757
g_step 5600, step 256, avg_time 2.244, loss:212.6682
g_step 5700, step 22, avg_time 0.935, loss:234.8416
g_step 5800, step 122, avg_time 0.933, loss:206.1678
g_step 5900, step 222, avg_time 0.937, loss:208.4554
g_step 6000, step 322, avg_time 0.967, loss:222.2129
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5530, rec:0.5162, f1:0.5340
>> valid relation prec:0.0770, rec:0.0388, f1:0.0516
>> valid relation with NER prec:0.0770, rec:0.0388, f1:0.0516
g_step 6100, step 88, avg_time 2.223, loss:212.1993
g_step 6200, step 188, avg_time 0.944, loss:213.8441
g_step 6300, step 288, avg_time 0.942, loss:214.9066
g_step 6400, step 54, avg_time 0.936, loss:202.7752
g_step 6500, step 154, avg_time 0.931, loss:200.6927
>> valid entity prec:0.5321, rec:0.5308, f1:0.5314
>> valid relation prec:0.1180, rec:0.0597, f1:0.0793
>> valid relation with NER prec:0.1180, rec:0.0597, f1:0.0793
g_step 6600, step 254, avg_time 2.210, loss:199.5122
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 06:18:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 06:18:50 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_06-18-49_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 06:18:51 - WARNING - datasets.builder -   Using custom data configuration default-3cc2b129ae90ba3c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3cc2b129ae90ba3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 06:18:52,539 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:18:52,541 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:18:52,541 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:18:52,542 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:18:52,590 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:18:52,622 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 06:18:52,877 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:18:55,959 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 06:18:55,973 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3cc2b129ae90ba3c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.06ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.18ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.84ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.27ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.55ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.73ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.88ba/s]100%|██████████| 8/8 [00:01<00:00,  4.90ba/s]100%|██████████| 8/8 [00:01<00:00,  4.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.27ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.90ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18ba/s]100%|██████████| 4/4 [00:00<00:00,  5.07ba/s]100%|██████████| 4/4 [00:00<00:00,  4.55ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.20ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.07ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.96ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.36ba/s]100%|██████████| 8/8 [00:00<00:00,  9.98ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.90ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.65ba/s]100%|██████████| 4/4 [00:00<00:00, 10.93ba/s]
[INFO|trainer.py:414] 2023-08-29 06:19:01,085 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 06:19:01,174 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 06:19:01,174 >>   Num examples = 8000
[INFO|trainer.py:1149] 2023-08-29 06:19:01,174 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 06:19:01,174 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 06:19:01,174 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 06:19:01,174 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 06:19:01,174 >>   Total optimization steps = 625
  0%|          | 0/625 [00:00<?, ?it/s]  0%|          | 1/625 [00:00<03:06,  3.35it/s]  0%|          | 2/625 [00:00<03:02,  3.42it/s]  0%|          | 3/625 [00:00<03:00,  3.44it/s]  1%|          | 4/625 [00:01<03:00,  3.45it/s]  1%|          | 5/625 [00:01<03:07,  3.31it/s]  1%|          | 6/625 [00:01<03:04,  3.35it/s]  1%|          | 7/625 [00:02<03:03,  3.37it/s]  1%|▏         | 8/625 [00:02<03:02,  3.38it/s]  1%|▏         | 9/625 [00:02<03:01,  3.40it/s]  2%|▏         | 10/625 [00:02<03:00,  3.40it/s]  2%|▏         | 11/625 [00:03<03:00,  3.40it/s]  2%|▏         | 12/625 [00:03<02:59,  3.41it/s]  2%|▏         | 13/625 [00:03<02:59,  3.41it/s]  2%|▏         | 14/625 [00:04<02:58,  3.42it/s]  2%|▏         | 15/625 [00:04<02:57,  3.43it/s]  3%|▎         | 16/625 [00:04<03:03,  3.33it/s]  3%|▎         | 17/625 [00:05<03:00,  3.37it/s]  3%|▎         | 18/625 [00:05<02:58,  3.39it/s]  3%|▎         | 19/625 [00:05<02:57,  3.41it/s]  3%|▎         | 20/625 [00:05<02:56,  3.43it/s]  3%|▎         | 21/625 [00:06<02:55,  3.44it/s]  4%|▎         | 22/625 [00:06<02:55,  3.44it/s]  4%|▎         | 23/625 [00:06<02:54,  3.45it/s]  4%|▍         | 24/625 [00:07<02:54,  3.45it/s]  4%|▍         | 25/625 [00:07<02:53,  3.45it/s]  4%|▍         | 26/625 [00:07<02:53,  3.45it/s]  4%|▍         | 27/625 [00:07<02:56,  3.38it/s]  4%|▍         | 28/625 [00:08<02:55,  3.40it/s]  5%|▍         | 29/625 [00:08<02:54,  3.42it/s]  5%|▍         | 30/625 [00:08<02:53,  3.43it/s]  5%|▍         | 31/625 [00:09<02:52,  3.44it/s]  5%|▌         | 32/625 [00:09<02:52,  3.44it/s]  5%|▌         | 33/625 [00:09<02:51,  3.45it/s]  5%|▌         | 34/625 [00:09<02:51,  3.45it/s]  6%|▌         | 35/625 [00:10<02:51,  3.45it/s]  6%|▌         | 36/625 [00:10<02:50,  3.45it/s]  6%|▌         | 37/625 [00:10<02:50,  3.45it/s]  6%|▌         | 38/625 [00:11<02:53,  3.39it/s]  6%|▌         | 39/625 [00:11<02:52,  3.41it/s]  6%|▋         | 40/625 [00:11<02:50,  3.42it/s]  7%|▋         | 41/625 [00:12<02:50,  3.43it/s]  7%|▋         | 42/625 [00:12<02:49,  3.44it/s]  7%|▋         | 43/625 [00:12<02:48,  3.45it/s]  7%|▋         | 44/625 [00:12<02:48,  3.45it/s]  7%|▋         | 45/625 [00:13<02:48,  3.45it/s]  7%|▋         | 46/625 [00:13<02:47,  3.45it/s]  8%|▊         | 47/625 [00:13<02:47,  3.45it/s]  8%|▊         | 48/625 [00:14<02:47,  3.45it/s]  8%|▊         | 49/625 [00:14<02:51,  3.35it/s]  8%|▊         | 50/625 [00:14<02:49,  3.39it/s]  8%|▊         | 51/625 [00:14<02:48,  3.40it/s]  8%|▊         | 52/625 [00:15<02:47,  3.42it/s]  8%|▊         | 53/625 [00:15<02:46,  3.43it/s]  9%|▊         | 54/625 [00:15<02:45,  3.44it/s]  9%|▉         | 55/625 [00:16<02:45,  3.45it/s]  9%|▉         | 56/625 [00:16<02:44,  3.45it/s]  9%|▉         | 57/625 [00:16<02:44,  3.45it/s]  9%|▉         | 58/625 [00:16<02:44,  3.45it/s]  9%|▉         | 59/625 [00:17<02:44,  3.45it/s] 10%|▉         | 60/625 [00:17<02:45,  3.41it/s] 10%|▉         | 61/625 [00:17<02:44,  3.42it/s] 10%|▉         | 62/625 [00:18<02:44,  3.43it/s] 10%|█         | 63/625 [00:18<02:43,  3.44it/s] 10%|█         | 64/625 [00:18<02:42,  3.44it/s] 10%|█         | 65/625 [00:18<02:42,  3.45it/s] 11%|█         | 66/625 [00:19<02:42,  3.45it/s] 11%|█         | 67/625 [00:19<02:41,  3.45it/s] 11%|█         | 68/625 [00:19<02:41,  3.45it/s] 11%|█         | 69/625 [00:20<02:41,  3.45it/s] 11%|█         | 70/625 [00:20<02:40,  3.45it/s] 11%|█▏        | 71/625 [00:20<02:40,  3.46it/s] 12%|█▏        | 72/625 [00:21<02:40,  3.45it/s] 12%|█▏        | 73/625 [00:21<02:39,  3.45it/s] 12%|█▏        | 74/625 [00:21<02:39,  3.45it/s] 12%|█▏        | 75/625 [00:21<02:39,  3.45it/s] 12%|█▏        | 76/625 [00:22<02:38,  3.45it/s] 12%|█▏        | 77/625 [00:22<02:38,  3.45it/s] 12%|█▏        | 78/625 [00:22<02:46,  3.28it/s] 13%|█▎        | 79/625 [00:23<02:43,  3.33it/s] 13%|█▎        | 80/625 [00:23<02:41,  3.37it/s] 13%|█▎        | 81/625 [00:23<02:40,  3.39it/s] 13%|█▎        | 82/625 [00:23<02:39,  3.41it/s] 13%|█▎        | 83/625 [00:24<02:38,  3.42it/s] 13%|█▎        | 84/625 [00:24<02:37,  3.43it/s] 14%|█▎        | 85/625 [00:24<02:37,  3.44it/s] 14%|█▍        | 86/625 [00:25<02:36,  3.44it/s] 14%|█▍        | 87/625 [00:25<02:36,  3.45it/s] 14%|█▍        | 88/625 [00:25<02:35,  3.44it/s] 14%|█▍        | 89/625 [00:26<02:40,  3.34it/s] 14%|█▍        | 90/625 [00:26<02:38,  3.38it/s] 15%|█▍        | 91/625 [00:26<02:37,  3.40it/s] 15%|█▍        | 92/625 [00:26<02:36,  3.41it/s] 15%|█▍        | 93/625 [00:27<02:35,  3.42it/s] 15%|█▌        | 94/625 [00:27<02:34,  3.43it/s] 15%|█▌        | 95/625 [00:27<02:34,  3.44it/s] 15%|█▌        | 96/625 [00:28<02:33,  3.44it/s] 16%|█▌        | 97/625 [00:28<02:33,  3.44it/s] 16%|█▌        | 98/625 [00:28<02:32,  3.45it/s] 16%|█▌        | 99/625 [00:28<02:32,  3.45it/s] 16%|█▌        | 100/625 [00:29<02:34,  3.39it/s] 16%|█▌        | 101/625 [00:29<02:33,  3.40it/s] 16%|█▋        | 102/625 [00:29<02:32,  3.42it/s] 16%|█▋        | 103/625 [00:30<02:32,  3.43it/s] 17%|█▋        | 104/625 [00:30<02:31,  3.44it/s] 17%|█▋        | 105/625 [00:30<02:31,  3.44it/s] 17%|█▋        | 106/625 [00:30<02:30,  3.44it/s] 17%|█▋        | 107/625 [00:31<02:30,  3.45it/s] 17%|█▋        | 108/625 [00:31<02:32,  3.38it/s] 17%|█▋        | 109/625 [00:31<02:31,  3.41it/s] 18%|█▊        | 110/625 [00:32<02:30,  3.42it/s] 18%|█▊        | 111/625 [00:32<02:32,  3.37it/s] 18%|█▊        | 112/625 [00:32<02:31,  3.39it/s] 18%|█▊        | 113/625 [00:33<02:30,  3.41it/s] 18%|█▊        | 114/625 [00:33<02:29,  3.42it/s] 18%|█▊        | 115/625 [00:33<02:28,  3.43it/s] 19%|█▊        | 116/625 [00:33<02:28,  3.44it/s] 19%|█▊        | 117/625 [00:34<02:27,  3.44it/s] 19%|█▉        | 118/625 [00:34<02:27,  3.44it/s] 19%|█▉        | 119/625 [00:34<02:26,  3.45it/s] 19%|█▉        | 120/625 [00:35<02:26,  3.45it/s] 19%|█▉        | 121/625 [00:35<02:26,  3.45it/s] 20%|█▉        | 122/625 [00:35<02:29,  3.36it/s] 20%|█▉        | 123/625 [00:35<02:28,  3.39it/s] 20%|█▉        | 124/625 [00:36<02:27,  3.41it/s] 20%|██        | 125/625 [00:36<02:26,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 06:19:37,708 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:19:37,708 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:19:37,708 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.63it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.00it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.49it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.74it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.27it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.85it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.74it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.68it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.73it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.52it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.49it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.46it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.47it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.37it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.36it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 42.01it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.85it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.43it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.76it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.99it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.19it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.18it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.13it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.27it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.51it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.58it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.55it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.41it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.32it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.43it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.37it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.25it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.36it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.43it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.63it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.65it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.42it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.49it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.37it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.30it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.33it/s][A
 51%|█████     | 222/436 [00:05<00:05, 42.57it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 43.41it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.83it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.14it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.16it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.26it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.35it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.17it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.07it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.17it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.60it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.67it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.59it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.60it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.44it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.34it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.24it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.26it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.47it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.67it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.65it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.55it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.53it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.49it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.39it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.28it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 40.94it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 42.05it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 42.98it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.64it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.93it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.09it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.06it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.13it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 43.88it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.95it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.17it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.33it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.48it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.68it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.79it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.70it/s][A                                                 
                                                 [A 20%|██        | 125/625 [00:46<02:26,  3.42it/s]
100%|██████████| 436/436 [00:09<00:00, 44.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:19:47,957 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125
[INFO|configuration_utils.py:351] 2023-08-29 06:19:48,644 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:19:52,013 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:19:52,226 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:19:52,254 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125/special_tokens_map.json
 20%|██        | 126/625 [00:57<53:52,  6.48s/it] 20%|██        | 127/625 [00:57<38:24,  4.63s/it] 20%|██        | 128/625 [00:58<27:33,  3.33s/it] 21%|██        | 129/625 [00:58<19:58,  2.42s/it] 21%|██        | 130/625 [00:58<14:41,  1.78s/it] 21%|██        | 131/625 [00:58<10:59,  1.33s/it] 21%|██        | 132/625 [00:59<08:23,  1.02s/it] 21%|██▏       | 133/625 [00:59<06:35,  1.24it/s] 21%|██▏       | 134/625 [00:59<05:19,  1.54it/s] 22%|██▏       | 135/625 [01:00<04:26,  1.84it/s] 22%|██▏       | 136/625 [01:00<03:49,  2.13it/s] 22%|██▏       | 137/625 [01:00<03:23,  2.40it/s] 22%|██▏       | 138/625 [01:01<03:09,  2.57it/s] 22%|██▏       | 139/625 [01:01<02:55,  2.78it/s] 22%|██▏       | 140/625 [01:01<02:44,  2.94it/s] 23%|██▎       | 141/625 [01:01<02:37,  3.06it/s] 23%|██▎       | 142/625 [01:02<02:32,  3.16it/s] 23%|██▎       | 143/625 [01:02<02:28,  3.24it/s] 23%|██▎       | 144/625 [01:02<02:26,  3.28it/s] 23%|██▎       | 145/625 [01:03<02:24,  3.32it/s] 23%|██▎       | 146/625 [01:03<02:23,  3.34it/s] 24%|██▎       | 147/625 [01:03<02:22,  3.35it/s] 24%|██▎       | 148/625 [01:03<02:21,  3.37it/s] 24%|██▍       | 149/625 [01:04<02:24,  3.29it/s] 24%|██▍       | 150/625 [01:04<02:22,  3.33it/s] 24%|██▍       | 151/625 [01:04<02:21,  3.35it/s] 24%|██▍       | 152/625 [01:05<02:20,  3.36it/s] 24%|██▍       | 153/625 [01:05<02:19,  3.38it/s] 25%|██▍       | 154/625 [01:05<02:19,  3.39it/s] 25%|██▍       | 155/625 [01:06<02:18,  3.39it/s] 25%|██▍       | 156/625 [01:06<02:18,  3.39it/s] 25%|██▌       | 157/625 [01:06<02:17,  3.40it/s] 25%|██▌       | 158/625 [01:06<02:17,  3.40it/s] 25%|██▌       | 159/625 [01:07<02:17,  3.40it/s] 26%|██▌       | 160/625 [01:07<02:18,  3.36it/s] 26%|██▌       | 161/625 [01:07<02:17,  3.37it/s] 26%|██▌       | 162/625 [01:08<02:16,  3.39it/s] 26%|██▌       | 163/625 [01:08<02:16,  3.39it/s] 26%|██▌       | 164/625 [01:08<02:15,  3.40it/s] 26%|██▋       | 165/625 [01:08<02:15,  3.40it/s] 27%|██▋       | 166/625 [01:09<02:14,  3.40it/s] 27%|██▋       | 167/625 [01:09<02:14,  3.40it/s] 27%|██▋       | 168/625 [01:09<02:14,  3.40it/s] 27%|██▋       | 169/625 [01:10<02:13,  3.41it/s] 27%|██▋       | 170/625 [01:10<02:13,  3.40it/s] 27%|██▋       | 171/625 [01:10<02:17,  3.31it/s] 28%|██▊       | 172/625 [01:11<02:15,  3.34it/s] 28%|██▊       | 173/625 [01:11<02:14,  3.36it/s] 28%|██▊       | 174/625 [01:11<02:13,  3.37it/s] 28%|██▊       | 175/625 [01:11<02:13,  3.38it/s] 28%|██▊       | 176/625 [01:12<02:12,  3.39it/s] 28%|██▊       | 177/625 [01:12<02:11,  3.40it/s] 28%|██▊       | 178/625 [01:12<02:11,  3.41it/s] 29%|██▊       | 179/625 [01:13<02:10,  3.42it/s] 29%|██▉       | 180/625 [01:13<02:09,  3.43it/s] 29%|██▉       | 181/625 [01:13<02:09,  3.43it/s] 29%|██▉       | 182/625 [01:13<02:10,  3.40it/s] 29%|██▉       | 183/625 [01:14<02:09,  3.41it/s] 29%|██▉       | 184/625 [01:14<02:08,  3.42it/s] 30%|██▉       | 185/625 [01:14<02:08,  3.43it/s] 30%|██▉       | 186/625 [01:15<02:07,  3.44it/s] 30%|██▉       | 187/625 [01:15<02:07,  3.44it/s] 30%|███       | 188/625 [01:15<02:06,  3.44it/s] 30%|███       | 189/625 [01:16<02:06,  3.44it/s] 30%|███       | 190/625 [01:16<02:06,  3.45it/s] 31%|███       | 191/625 [01:16<02:06,  3.44it/s] 31%|███       | 192/625 [01:16<02:05,  3.44it/s] 31%|███       | 193/625 [01:17<02:07,  3.38it/s] 31%|███       | 194/625 [01:17<02:06,  3.40it/s] 31%|███       | 195/625 [01:17<02:05,  3.42it/s] 31%|███▏      | 196/625 [01:18<02:05,  3.43it/s] 32%|███▏      | 197/625 [01:18<02:04,  3.43it/s] 32%|███▏      | 198/625 [01:18<02:04,  3.44it/s] 32%|███▏      | 199/625 [01:18<02:03,  3.44it/s] 32%|███▏      | 200/625 [01:19<02:03,  3.44it/s] 32%|███▏      | 201/625 [01:19<02:03,  3.44it/s] 32%|███▏      | 202/625 [01:19<02:02,  3.45it/s] 32%|███▏      | 203/625 [01:20<02:02,  3.45it/s] 33%|███▎      | 204/625 [01:20<02:04,  3.39it/s] 33%|███▎      | 205/625 [01:20<02:03,  3.41it/s] 33%|███▎      | 206/625 [01:20<02:02,  3.42it/s] 33%|███▎      | 207/625 [01:21<02:01,  3.43it/s] 33%|███▎      | 208/625 [01:21<02:01,  3.43it/s] 33%|███▎      | 209/625 [01:21<02:01,  3.44it/s] 34%|███▎      | 210/625 [01:22<02:00,  3.44it/s] 34%|███▍      | 211/625 [01:22<02:00,  3.44it/s] 34%|███▍      | 212/625 [01:22<01:59,  3.44it/s] 34%|███▍      | 213/625 [01:23<01:59,  3.44it/s] 34%|███▍      | 214/625 [01:23<01:59,  3.44it/s] 34%|███▍      | 215/625 [01:23<01:59,  3.44it/s] 35%|███▍      | 216/625 [01:23<01:58,  3.45it/s] 35%|███▍      | 217/625 [01:24<02:00,  3.38it/s] 35%|███▍      | 218/625 [01:24<01:59,  3.40it/s] 35%|███▌      | 219/625 [01:24<01:58,  3.42it/s] 35%|███▌      | 220/625 [01:25<01:58,  3.43it/s] 35%|███▌      | 221/625 [01:25<01:57,  3.43it/s] 36%|███▌      | 222/625 [01:25<01:57,  3.44it/s] 36%|███▌      | 223/625 [01:25<01:56,  3.44it/s] 36%|███▌      | 224/625 [01:26<01:56,  3.44it/s] 36%|███▌      | 225/625 [01:26<01:56,  3.44it/s] 36%|███▌      | 226/625 [01:26<01:55,  3.44it/s] 36%|███▋      | 227/625 [01:27<01:55,  3.44it/s] 36%|███▋      | 228/625 [01:27<01:57,  3.37it/s] 37%|███▋      | 229/625 [01:27<01:56,  3.39it/s] 37%|███▋      | 230/625 [01:27<01:55,  3.41it/s] 37%|███▋      | 231/625 [01:28<01:55,  3.42it/s] 37%|███▋      | 232/625 [01:28<01:54,  3.43it/s] 37%|███▋      | 233/625 [01:28<01:54,  3.43it/s] 37%|███▋      | 234/625 [01:29<01:53,  3.43it/s] 38%|███▊      | 235/625 [01:29<01:53,  3.44it/s] 38%|███▊      | 236/625 [01:29<01:53,  3.44it/s] 38%|███▊      | 237/625 [01:30<01:52,  3.44it/s] 38%|███▊      | 238/625 [01:30<01:52,  3.44it/s] 38%|███▊      | 239/625 [01:30<01:54,  3.36it/s] 38%|███▊      | 240/625 [01:30<01:53,  3.38it/s] 39%|███▊      | 241/625 [01:31<01:52,  3.41it/s] 39%|███▊      | 242/625 [01:31<01:52,  3.42it/s] 39%|███▉      | 243/625 [01:31<01:51,  3.43it/s] 39%|███▉      | 244/625 [01:32<01:51,  3.43it/s] 39%|███▉      | 245/625 [01:32<01:50,  3.44it/s] 39%|███▉      | 246/625 [01:32<01:50,  3.44it/s] 40%|███▉      | 247/625 [01:32<01:49,  3.44it/s] 40%|███▉      | 248/625 [01:33<01:49,  3.44it/s] 40%|███▉      | 249/625 [01:33<01:49,  3.44it/s] 40%|████      | 250/625 [01:33<01:51,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 06:20:35,025 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:20:35,025 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:20:35,025 >>   Batch size = 8
{'eval_loss': 1.207527995109558, 'eval_runtime': 9.859, 'eval_samples_per_second': 353.181, 'eval_steps_per_second': 44.224, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.33it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.70it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.77it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.87it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.24it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.66it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.39it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.36it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.54it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.90it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.75it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.53it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.25it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 44.18it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.22it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.42it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.40it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.66it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.72it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.63it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.37it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.32it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.63it/s][A
 30%|███       | 132/436 [00:02<00:06, 43.88it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.08it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.36it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.44it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.58it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.57it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.40it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.31it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.17it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.30it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.34it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.41it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.58it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.64it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.60it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.56it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.34it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.20it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.28it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.19it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.64it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.69it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.70it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.54it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.48it/s][A
 60%|██████    | 262/436 [00:05<00:04, 42.72it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.22it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 43.64it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 43.85it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.08it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.35it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.31it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.12it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.27it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.42it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.50it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.49it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.57it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.56it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.57it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.34it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.41it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.41it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.45it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.57it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.55it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.61it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.63it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.51it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.39it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.38it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 42.93it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.52it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 43.89it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.04it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.26it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.34it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.17it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.27it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.27it/s][A 40%|████      | 250/625 [01:43<01:51,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:20:45,046 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250
[INFO|configuration_utils.py:351] 2023-08-29 06:20:45,318 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:20:49,601 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:20:50,166 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:20:50,323 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250/special_tokens_map.json
 40%|████      | 251/625 [01:55<42:13,  6.77s/it] 40%|████      | 252/625 [01:56<30:04,  4.84s/it] 40%|████      | 253/625 [01:56<21:32,  3.47s/it] 41%|████      | 254/625 [01:56<15:34,  2.52s/it] 41%|████      | 255/625 [01:56<11:25,  1.85s/it] 41%|████      | 256/625 [01:57<08:30,  1.38s/it] 41%|████      | 257/625 [01:57<06:28,  1.06s/it] 41%|████▏     | 258/625 [01:57<05:03,  1.21it/s] 41%|████▏     | 259/625 [01:58<04:04,  1.50it/s] 42%|████▏     | 260/625 [01:58<03:22,  1.80it/s] 42%|████▏     | 261/625 [01:58<02:53,  2.10it/s] 42%|████▏     | 262/625 [01:58<02:32,  2.37it/s] 42%|████▏     | 263/625 [01:59<02:21,  2.56it/s] 42%|████▏     | 264/625 [01:59<02:10,  2.77it/s] 42%|████▏     | 265/625 [01:59<02:02,  2.93it/s] 43%|████▎     | 266/625 [02:00<01:57,  3.06it/s] 43%|████▎     | 267/625 [02:00<01:53,  3.16it/s] 43%|████▎     | 268/625 [02:00<01:50,  3.23it/s] 43%|████▎     | 269/625 [02:01<01:48,  3.28it/s] 43%|████▎     | 270/625 [02:01<01:46,  3.32it/s] 43%|████▎     | 271/625 [02:01<01:45,  3.35it/s] 44%|████▎     | 272/625 [02:01<01:44,  3.36it/s] 44%|████▎     | 273/625 [02:02<01:44,  3.37it/s] 44%|████▍     | 274/625 [02:02<01:45,  3.34it/s] 44%|████▍     | 275/625 [02:02<01:44,  3.36it/s] 44%|████▍     | 276/625 [02:03<01:43,  3.38it/s] 44%|████▍     | 277/625 [02:03<01:42,  3.39it/s] 44%|████▍     | 278/625 [02:03<01:42,  3.39it/s] 45%|████▍     | 279/625 [02:04<01:41,  3.40it/s] 45%|████▍     | 280/625 [02:04<01:41,  3.41it/s] 45%|████▍     | 281/625 [02:04<01:40,  3.43it/s] 45%|████▌     | 282/625 [02:04<01:39,  3.43it/s] 45%|████▌     | 283/625 [02:05<01:39,  3.44it/s] 45%|████▌     | 284/625 [02:05<01:38,  3.45it/s] 46%|████▌     | 285/625 [02:05<01:40,  3.39it/s] 46%|████▌     | 286/625 [02:06<01:39,  3.41it/s] 46%|████▌     | 287/625 [02:06<01:38,  3.42it/s] 46%|████▌     | 288/625 [02:06<01:38,  3.43it/s] 46%|████▌     | 289/625 [02:06<01:37,  3.44it/s] 46%|████▋     | 290/625 [02:07<01:37,  3.44it/s] 47%|████▋     | 291/625 [02:07<01:36,  3.45it/s] 47%|████▋     | 292/625 [02:07<01:36,  3.45it/s] 47%|████▋     | 293/625 [02:08<01:36,  3.45it/s] 47%|████▋     | 294/625 [02:08<01:35,  3.45it/s] 47%|████▋     | 295/625 [02:08<01:35,  3.45it/s] 47%|████▋     | 296/625 [02:08<01:37,  3.37it/s] 48%|████▊     | 297/625 [02:09<01:36,  3.40it/s] 48%|████▊     | 298/625 [02:09<01:35,  3.41it/s] 48%|████▊     | 299/625 [02:09<01:35,  3.42it/s] 48%|████▊     | 300/625 [02:10<01:34,  3.43it/s] 48%|████▊     | 301/625 [02:10<01:34,  3.44it/s] 48%|████▊     | 302/625 [02:10<01:33,  3.44it/s] 48%|████▊     | 303/625 [02:10<01:33,  3.44it/s] 49%|████▊     | 304/625 [02:11<01:33,  3.45it/s] 49%|████▉     | 305/625 [02:11<01:32,  3.45it/s] 49%|████▉     | 306/625 [02:11<01:32,  3.45it/s] 49%|████▉     | 307/625 [02:12<01:32,  3.44it/s] 49%|████▉     | 308/625 [02:12<01:32,  3.44it/s] 49%|████▉     | 309/625 [02:12<01:31,  3.44it/s] 50%|████▉     | 310/625 [02:13<01:31,  3.45it/s] 50%|████▉     | 311/625 [02:13<01:30,  3.45it/s] 50%|████▉     | 312/625 [02:13<01:30,  3.45it/s] 50%|█████     | 313/625 [02:13<01:30,  3.45it/s] 50%|█████     | 314/625 [02:14<01:30,  3.44it/s] 50%|█████     | 315/625 [02:14<01:30,  3.44it/s] 51%|█████     | 316/625 [02:14<01:29,  3.44it/s] 51%|█████     | 317/625 [02:15<01:29,  3.45it/s] 51%|█████     | 318/625 [02:15<01:31,  3.35it/s] 51%|█████     | 319/625 [02:15<01:30,  3.38it/s] 51%|█████     | 320/625 [02:15<01:29,  3.40it/s] 51%|█████▏    | 321/625 [02:16<01:29,  3.41it/s] 52%|█████▏    | 322/625 [02:16<01:28,  3.42it/s] 52%|█████▏    | 323/625 [02:16<01:28,  3.43it/s] 52%|█████▏    | 324/625 [02:17<01:27,  3.44it/s] 52%|█████▏    | 325/625 [02:17<01:27,  3.44it/s] 52%|█████▏    | 326/625 [02:17<01:26,  3.44it/s] 52%|█████▏    | 327/625 [02:17<01:26,  3.44it/s] 52%|█████▏    | 328/625 [02:18<01:26,  3.44it/s] 53%|█████▎    | 329/625 [02:18<01:28,  3.35it/s] 53%|█████▎    | 330/625 [02:18<01:27,  3.38it/s] 53%|█████▎    | 331/625 [02:19<01:26,  3.40it/s] 53%|█████▎    | 332/625 [02:19<01:25,  3.41it/s] 53%|█████▎    | 333/625 [02:19<01:25,  3.42it/s] 53%|█████▎    | 334/625 [02:20<01:24,  3.43it/s] 54%|█████▎    | 335/625 [02:20<01:24,  3.44it/s] 54%|█████▍    | 336/625 [02:20<01:23,  3.44it/s] 54%|█████▍    | 337/625 [02:20<01:23,  3.44it/s] 54%|█████▍    | 338/625 [02:21<01:23,  3.44it/s] 54%|█████▍    | 339/625 [02:21<01:23,  3.45it/s] 54%|█████▍    | 340/625 [02:21<01:24,  3.35it/s] 55%|█████▍    | 341/625 [02:22<01:23,  3.38it/s] 55%|█████▍    | 342/625 [02:22<01:23,  3.40it/s] 55%|█████▍    | 343/625 [02:22<01:22,  3.41it/s] 55%|█████▌    | 344/625 [02:22<01:22,  3.42it/s] 55%|█████▌    | 345/625 [02:23<01:21,  3.43it/s] 55%|█████▌    | 346/625 [02:23<01:21,  3.44it/s] 56%|█████▌    | 347/625 [02:23<01:20,  3.44it/s] 56%|█████▌    | 348/625 [02:24<01:20,  3.45it/s] 56%|█████▌    | 349/625 [02:24<01:20,  3.45it/s] 56%|█████▌    | 350/625 [02:24<01:19,  3.45it/s] 56%|█████▌    | 351/625 [02:25<01:19,  3.45it/s] 56%|█████▋    | 352/625 [02:25<01:19,  3.45it/s] 56%|█████▋    | 353/625 [02:25<01:19,  3.41it/s] 57%|█████▋    | 354/625 [02:25<01:19,  3.42it/s] 57%|█████▋    | 355/625 [02:26<01:18,  3.43it/s] 57%|█████▋    | 356/625 [02:26<01:18,  3.44it/s] 57%|█████▋    | 357/625 [02:26<01:17,  3.44it/s] 57%|█████▋    | 358/625 [02:27<01:17,  3.44it/s] 57%|█████▋    | 359/625 [02:27<01:17,  3.45it/s] 58%|█████▊    | 360/625 [02:27<01:16,  3.45it/s] 58%|█████▊    | 361/625 [02:27<01:16,  3.45it/s] 58%|█████▊    | 362/625 [02:28<01:16,  3.45it/s] 58%|█████▊    | 363/625 [02:28<01:15,  3.45it/s] 58%|█████▊    | 364/625 [02:28<01:17,  3.38it/s] 58%|█████▊    | 365/625 [02:29<01:16,  3.41it/s] 59%|█████▊    | 366/625 [02:29<01:15,  3.42it/s] 59%|█████▊    | 367/625 [02:29<01:15,  3.43it/s] 59%|█████▉    | 368/625 [02:29<01:14,  3.44it/s] 59%|█████▉    | 369/625 [02:30<01:14,  3.44it/s] 59%|█████▉    | 370/625 [02:30<01:14,  3.44it/s] 59%|█████▉    | 371/625 [02:30<01:13,  3.44it/s] 60%|█████▉    | 372/625 [02:31<01:13,  3.45it/s] 60%|█████▉    | 373/625 [02:31<01:13,  3.45it/s] 60%|█████▉    | 374/625 [02:31<01:12,  3.45it/s] 60%|██████    | 375/625 [02:32<01:13,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 06:21:33,181 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:21:33,181 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:21:33,181 >>   Batch size = 8
{'eval_loss': 1.2215527296066284, 'eval_runtime': 9.8282, 'eval_samples_per_second': 354.288, 'eval_steps_per_second': 44.362, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.66it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.95it/s][A
  4%|▍         | 17/436 [00:00<00:08, 46.94it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.85it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.37it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.69it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.53it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.41it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.51it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.61it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.81it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.67it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.28it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.28it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.32it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.37it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.51it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.57it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.78it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.75it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.46it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.35it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.04it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.21it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.27it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.54it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.61it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.45it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.36it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.29it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.46it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.48it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.63it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.66it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.53it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.39it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.27it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.24it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.39it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.46it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.51it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.61it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.69it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.67it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.46it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.33it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.79it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.12it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.21it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.38it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.54it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.56it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.31it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.28it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.34it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 44.36it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.58it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.66it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.69it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.54it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.31it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.32it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.25it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.51it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.57it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.60it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.63it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.46it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.46it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.35it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.23it/s][A
 91%|█████████ | 397/436 [00:08<00:01, 37.26it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 39.24it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 40.86it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 42.09it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 42.93it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.58it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.21it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.21it/s][A 60%|██████    | 375/625 [02:41<01:13,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:21:43,155 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375
[INFO|configuration_utils.py:351] 2023-08-29 06:21:43,382 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:21:46,366 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:21:46,605 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:21:46,703 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375/special_tokens_map.json
 60%|██████    | 376/625 [02:55<30:43,  7.40s/it] 60%|██████    | 377/625 [02:56<21:49,  5.28s/it] 60%|██████    | 378/625 [02:56<15:34,  3.78s/it] 61%|██████    | 379/625 [02:56<11:13,  2.74s/it] 61%|██████    | 380/625 [02:57<08:10,  2.00s/it] 61%|██████    | 381/625 [02:57<06:03,  1.49s/it] 61%|██████    | 382/625 [02:57<04:34,  1.13s/it] 61%|██████▏   | 383/625 [02:58<03:32,  1.14it/s] 61%|██████▏   | 384/625 [02:58<02:49,  1.42it/s] 62%|██████▏   | 385/625 [02:58<02:19,  1.72it/s] 62%|██████▏   | 386/625 [02:58<01:58,  2.02it/s] 62%|██████▏   | 387/625 [02:59<01:43,  2.30it/s] 62%|██████▏   | 388/625 [02:59<01:34,  2.50it/s] 62%|██████▏   | 389/625 [02:59<01:26,  2.72it/s] 62%|██████▏   | 390/625 [03:00<01:21,  2.89it/s] 63%|██████▎   | 391/625 [03:00<01:17,  3.03it/s] 63%|██████▎   | 392/625 [03:00<01:14,  3.14it/s] 63%|██████▎   | 393/625 [03:01<01:12,  3.21it/s] 63%|██████▎   | 394/625 [03:01<01:10,  3.27it/s] 63%|██████▎   | 395/625 [03:01<01:09,  3.31it/s] 63%|██████▎   | 396/625 [03:01<01:08,  3.34it/s] 64%|██████▎   | 397/625 [03:02<01:07,  3.36it/s] 64%|██████▎   | 398/625 [03:02<01:07,  3.37it/s] 64%|██████▍   | 399/625 [03:02<01:08,  3.30it/s] 64%|██████▍   | 400/625 [03:03<01:07,  3.33it/s] 64%|██████▍   | 401/625 [03:03<01:06,  3.36it/s] 64%|██████▍   | 402/625 [03:03<01:06,  3.37it/s] 64%|██████▍   | 403/625 [03:03<01:05,  3.38it/s] 65%|██████▍   | 404/625 [03:04<01:05,  3.39it/s] 65%|██████▍   | 405/625 [03:04<01:04,  3.39it/s] 65%|██████▍   | 406/625 [03:04<01:04,  3.40it/s] 65%|██████▌   | 407/625 [03:05<01:04,  3.40it/s] 65%|██████▌   | 408/625 [03:05<01:03,  3.40it/s] 65%|██████▌   | 409/625 [03:05<01:03,  3.40it/s] 66%|██████▌   | 410/625 [03:06<01:04,  3.33it/s] 66%|██████▌   | 411/625 [03:06<01:03,  3.35it/s] 66%|██████▌   | 412/625 [03:06<01:03,  3.37it/s] 66%|██████▌   | 413/625 [03:06<01:02,  3.38it/s] 66%|██████▌   | 414/625 [03:07<01:02,  3.39it/s] 66%|██████▋   | 415/625 [03:07<01:01,  3.39it/s] 67%|██████▋   | 416/625 [03:07<01:01,  3.40it/s] 67%|██████▋   | 417/625 [03:08<01:01,  3.40it/s] 67%|██████▋   | 418/625 [03:08<01:00,  3.40it/s] 67%|██████▋   | 419/625 [03:08<01:00,  3.40it/s] 67%|██████▋   | 420/625 [03:08<00:59,  3.42it/s] 67%|██████▋   | 421/625 [03:09<01:01,  3.34it/s] 68%|██████▊   | 422/625 [03:09<01:00,  3.38it/s] 68%|██████▊   | 423/625 [03:09<00:59,  3.40it/s] 68%|██████▊   | 424/625 [03:10<00:58,  3.42it/s] 68%|██████▊   | 425/625 [03:10<00:58,  3.43it/s] 68%|██████▊   | 426/625 [03:10<00:57,  3.44it/s] 68%|██████▊   | 427/625 [03:11<00:57,  3.44it/s] 68%|██████▊   | 428/625 [03:11<00:57,  3.45it/s] 69%|██████▊   | 429/625 [03:11<00:56,  3.45it/s] 69%|██████▉   | 430/625 [03:11<00:56,  3.45it/s] 69%|██████▉   | 431/625 [03:12<00:56,  3.45it/s] 69%|██████▉   | 432/625 [03:12<00:57,  3.35it/s] 69%|██████▉   | 433/625 [03:12<00:56,  3.38it/s] 69%|██████▉   | 434/625 [03:13<00:56,  3.40it/s] 70%|██████▉   | 435/625 [03:13<00:55,  3.42it/s] 70%|██████▉   | 436/625 [03:13<00:55,  3.43it/s] 70%|██████▉   | 437/625 [03:13<00:54,  3.44it/s] 70%|███████   | 438/625 [03:14<00:54,  3.44it/s] 70%|███████   | 439/625 [03:14<00:53,  3.44it/s] 70%|███████   | 440/625 [03:14<00:53,  3.45it/s] 71%|███████   | 441/625 [03:15<00:53,  3.45it/s] 71%|███████   | 442/625 [03:15<00:53,  3.45it/s] 71%|███████   | 443/625 [03:15<00:52,  3.45it/s] 71%|███████   | 444/625 [03:16<00:52,  3.45it/s] 71%|███████   | 445/625 [03:16<00:52,  3.45it/s] 71%|███████▏  | 446/625 [03:16<00:51,  3.46it/s] 72%|███████▏  | 447/625 [03:16<00:51,  3.42it/s] 72%|███████▏  | 448/625 [03:17<00:51,  3.43it/s] 72%|███████▏  | 449/625 [03:17<00:51,  3.44it/s] 72%|███████▏  | 450/625 [03:17<00:50,  3.44it/s] 72%|███████▏  | 451/625 [03:18<00:50,  3.45it/s] 72%|███████▏  | 452/625 [03:18<00:50,  3.45it/s] 72%|███████▏  | 453/625 [03:18<00:49,  3.45it/s] 73%|███████▎  | 454/625 [03:18<00:49,  3.45it/s] 73%|███████▎  | 455/625 [03:19<00:49,  3.45it/s] 73%|███████▎  | 456/625 [03:19<00:48,  3.45it/s] 73%|███████▎  | 457/625 [03:19<00:48,  3.46it/s] 73%|███████▎  | 458/625 [03:20<00:49,  3.38it/s] 73%|███████▎  | 459/625 [03:20<00:48,  3.41it/s] 74%|███████▎  | 460/625 [03:20<00:48,  3.42it/s] 74%|███████▍  | 461/625 [03:20<00:47,  3.43it/s] 74%|███████▍  | 462/625 [03:21<00:47,  3.44it/s] 74%|███████▍  | 463/625 [03:21<00:47,  3.44it/s] 74%|███████▍  | 464/625 [03:21<00:46,  3.45it/s] 74%|███████▍  | 465/625 [03:22<00:46,  3.45it/s] 75%|███████▍  | 466/625 [03:22<00:46,  3.45it/s] 75%|███████▍  | 467/625 [03:22<00:45,  3.45it/s] 75%|███████▍  | 468/625 [03:22<00:45,  3.45it/s] 75%|███████▌  | 469/625 [03:23<00:46,  3.39it/s] 75%|███████▌  | 470/625 [03:23<00:45,  3.41it/s] 75%|███████▌  | 471/625 [03:23<00:45,  3.42it/s] 76%|███████▌  | 472/625 [03:24<00:44,  3.43it/s] 76%|███████▌  | 473/625 [03:24<00:44,  3.43it/s] 76%|███████▌  | 474/625 [03:24<00:43,  3.44it/s] 76%|███████▌  | 475/625 [03:25<00:43,  3.44it/s] 76%|███████▌  | 476/625 [03:25<00:43,  3.45it/s] 76%|███████▋  | 477/625 [03:25<00:42,  3.45it/s] 76%|███████▋  | 478/625 [03:25<00:42,  3.45it/s] 77%|███████▋  | 479/625 [03:26<00:42,  3.45it/s] 77%|███████▋  | 480/625 [03:26<00:43,  3.32it/s] 77%|███████▋  | 481/625 [03:26<00:42,  3.36it/s] 77%|███████▋  | 482/625 [03:27<00:42,  3.39it/s] 77%|███████▋  | 483/625 [03:27<00:41,  3.41it/s] 77%|███████▋  | 484/625 [03:27<00:41,  3.42it/s] 78%|███████▊  | 485/625 [03:27<00:40,  3.43it/s] 78%|███████▊  | 486/625 [03:28<00:40,  3.44it/s] 78%|███████▊  | 487/625 [03:28<00:40,  3.44it/s] 78%|███████▊  | 488/625 [03:28<00:39,  3.45it/s] 78%|███████▊  | 489/625 [03:29<00:39,  3.45it/s] 78%|███████▊  | 490/625 [03:29<00:39,  3.45it/s] 79%|███████▊  | 491/625 [03:29<00:39,  3.38it/s] 79%|███████▊  | 492/625 [03:30<00:39,  3.40it/s] 79%|███████▉  | 493/625 [03:30<00:38,  3.42it/s] 79%|███████▉  | 494/625 [03:30<00:38,  3.43it/s] 79%|███████▉  | 495/625 [03:30<00:37,  3.44it/s] 79%|███████▉  | 496/625 [03:31<00:37,  3.44it/s] 80%|███████▉  | 497/625 [03:31<00:37,  3.44it/s] 80%|███████▉  | 498/625 [03:31<00:36,  3.45it/s] 80%|███████▉  | 499/625 [03:32<00:36,  3.45it/s] 80%|████████  | 500/625 [03:32<00:36,  3.45it/s]                                                  80%|████████  | 500/625 [03:32<00:36,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 06:22:33,496 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:22:33,496 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:22:33,496 >>   Batch size = 8
{'eval_loss': 1.2442994117736816, 'eval_runtime': 9.8619, 'eval_samples_per_second': 353.076, 'eval_steps_per_second': 44.211, 'epoch': 3.0}
{'loss': 0.3861, 'learning_rate': 7.5e-06, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.99it/s][A
  3%|▎         | 12/436 [00:00<00:08, 48.00it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.38it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.59it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.23it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.77it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.63it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.60it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.75it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.72it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.75it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.53it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.49it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.50it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.39it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.41it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.45it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.64it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.83it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.65it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.62it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.52it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.49it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.47it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.52it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.63it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.72it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.55it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.34it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.51it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.53it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.41it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.54it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.58it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.61it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.53it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.44it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.44it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.43it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.57it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.44it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.51it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.58it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.56it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.63it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.38it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.41it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.52it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.53it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.44it/s][A
 61%|██████    | 267/436 [00:05<00:03, 44.55it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.57it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.43it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.25it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.19it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.36it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.37it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.52it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 44.54it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.61it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.53it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.54it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.39it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.52it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.46it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.45it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.49it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.40it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.63it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.60it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.56it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.62it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.61it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.60it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.47it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.49it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.46it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.56it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.54it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 42.85it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.53it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.80it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.07it/s][A                                                 
                                                 [A 80%|████████  | 500/625 [03:42<00:36,  3.45it/s]
100%|██████████| 436/436 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:22:43,398 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500
[INFO|configuration_utils.py:351] 2023-08-29 06:22:43,647 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:22:47,355 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:22:47,606 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:22:47,699 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500/special_tokens_map.json
 80%|████████  | 501/625 [03:57<15:52,  7.68s/it] 80%|████████  | 502/625 [03:57<11:14,  5.49s/it] 80%|████████  | 503/625 [03:57<07:59,  3.93s/it] 81%|████████  | 504/625 [03:58<05:43,  2.84s/it] 81%|████████  | 505/625 [03:58<04:08,  2.07s/it] 81%|████████  | 506/625 [03:58<03:03,  1.54s/it] 81%|████████  | 507/625 [03:59<02:17,  1.17s/it] 81%|████████▏ | 508/625 [03:59<01:45,  1.11it/s] 81%|████████▏ | 509/625 [03:59<01:23,  1.39it/s] 82%|████████▏ | 510/625 [03:59<01:08,  1.69it/s] 82%|████████▏ | 511/625 [04:00<00:57,  1.99it/s] 82%|████████▏ | 512/625 [04:00<00:50,  2.25it/s] 82%|████████▏ | 513/625 [04:00<00:44,  2.50it/s] 82%|████████▏ | 514/625 [04:01<00:40,  2.72it/s] 82%|████████▏ | 515/625 [04:01<00:38,  2.89it/s] 83%|████████▎ | 516/625 [04:01<00:36,  3.03it/s] 83%|████████▎ | 517/625 [04:02<00:34,  3.13it/s] 83%|████████▎ | 518/625 [04:02<00:33,  3.21it/s] 83%|████████▎ | 519/625 [04:02<00:32,  3.27it/s] 83%|████████▎ | 520/625 [04:02<00:31,  3.31it/s] 83%|████████▎ | 521/625 [04:03<00:31,  3.34it/s] 84%|████████▎ | 522/625 [04:03<00:30,  3.36it/s] 84%|████████▎ | 523/625 [04:03<00:30,  3.29it/s] 84%|████████▍ | 524/625 [04:04<00:30,  3.33it/s] 84%|████████▍ | 525/625 [04:04<00:29,  3.36it/s] 84%|████████▍ | 526/625 [04:04<00:29,  3.37it/s] 84%|████████▍ | 527/625 [04:04<00:28,  3.38it/s] 84%|████████▍ | 528/625 [04:05<00:28,  3.39it/s] 85%|████████▍ | 529/625 [04:05<00:28,  3.39it/s] 85%|████████▍ | 530/625 [04:05<00:27,  3.40it/s] 85%|████████▍ | 531/625 [04:06<00:27,  3.40it/s] 85%|████████▌ | 532/625 [04:06<00:27,  3.41it/s] 85%|████████▌ | 533/625 [04:06<00:27,  3.41it/s] 85%|████████▌ | 534/625 [04:07<00:27,  3.32it/s] 86%|████████▌ | 535/625 [04:07<00:26,  3.35it/s] 86%|████████▌ | 536/625 [04:07<00:26,  3.36it/s] 86%|████████▌ | 537/625 [04:07<00:26,  3.38it/s] 86%|████████▌ | 538/625 [04:08<00:25,  3.39it/s] 86%|████████▌ | 539/625 [04:08<00:25,  3.39it/s] 86%|████████▋ | 540/625 [04:08<00:25,  3.40it/s] 87%|████████▋ | 541/625 [04:09<00:24,  3.40it/s] 87%|████████▋ | 542/625 [04:09<00:24,  3.40it/s] 87%|████████▋ | 543/625 [04:09<00:24,  3.40it/s] 87%|████████▋ | 544/625 [04:10<00:23,  3.41it/s] 87%|████████▋ | 545/625 [04:10<00:24,  3.32it/s] 87%|████████▋ | 546/625 [04:10<00:23,  3.34it/s] 88%|████████▊ | 547/625 [04:10<00:23,  3.36it/s] 88%|████████▊ | 548/625 [04:11<00:22,  3.38it/s] 88%|████████▊ | 549/625 [04:11<00:22,  3.39it/s] 88%|████████▊ | 550/625 [04:11<00:22,  3.39it/s] 88%|████████▊ | 551/625 [04:12<00:21,  3.40it/s] 88%|████████▊ | 552/625 [04:12<00:21,  3.40it/s] 88%|████████▊ | 553/625 [04:12<00:21,  3.40it/s] 89%|████████▊ | 554/625 [04:12<00:20,  3.41it/s] 89%|████████▉ | 555/625 [04:13<00:20,  3.41it/s] 89%|████████▉ | 556/625 [04:13<00:20,  3.30it/s] 89%|████████▉ | 557/625 [04:13<00:20,  3.33it/s] 89%|████████▉ | 558/625 [04:14<00:19,  3.35it/s] 89%|████████▉ | 559/625 [04:14<00:19,  3.37it/s] 90%|████████▉ | 560/625 [04:14<00:19,  3.38it/s] 90%|████████▉ | 561/625 [04:15<00:18,  3.39it/s] 90%|████████▉ | 562/625 [04:15<00:18,  3.39it/s] 90%|█████████ | 563/625 [04:15<00:18,  3.40it/s] 90%|█████████ | 564/625 [04:15<00:17,  3.40it/s] 90%|█████████ | 565/625 [04:16<00:17,  3.40it/s] 91%|█████████ | 566/625 [04:16<00:17,  3.40it/s] 91%|█████████ | 567/625 [04:16<00:17,  3.40it/s] 91%|█████████ | 568/625 [04:17<00:16,  3.40it/s] 91%|█████████ | 569/625 [04:17<00:16,  3.40it/s] 91%|█████████ | 570/625 [04:17<00:16,  3.40it/s] 91%|█████████▏| 571/625 [04:17<00:15,  3.40it/s] 92%|█████████▏| 572/625 [04:18<00:15,  3.32it/s] 92%|█████████▏| 573/625 [04:18<00:15,  3.34it/s] 92%|█████████▏| 574/625 [04:18<00:15,  3.36it/s] 92%|█████████▏| 575/625 [04:19<00:14,  3.37it/s] 92%|█████████▏| 576/625 [04:19<00:14,  3.38it/s] 92%|█████████▏| 577/625 [04:19<00:14,  3.39it/s] 92%|█████████▏| 578/625 [04:20<00:13,  3.39it/s] 93%|█████████▎| 579/625 [04:20<00:13,  3.40it/s] 93%|█████████▎| 580/625 [04:20<00:13,  3.40it/s] 93%|█████████▎| 581/625 [04:20<00:12,  3.40it/s] 93%|█████████▎| 582/625 [04:21<00:12,  3.40it/s] 93%|█████████▎| 583/625 [04:21<00:12,  3.34it/s] 93%|█████████▎| 584/625 [04:21<00:12,  3.36it/s] 94%|█████████▎| 585/625 [04:22<00:11,  3.37it/s] 94%|█████████▍| 586/625 [04:22<00:11,  3.38it/s] 94%|█████████▍| 587/625 [04:22<00:11,  3.39it/s] 94%|█████████▍| 588/625 [04:23<00:10,  3.39it/s] 94%|█████████▍| 589/625 [04:23<00:10,  3.39it/s] 94%|█████████▍| 590/625 [04:23<00:10,  3.40it/s] 95%|█████████▍| 591/625 [04:23<00:10,  3.40it/s] 95%|█████████▍| 592/625 [04:24<00:09,  3.40it/s] 95%|█████████▍| 593/625 [04:24<00:09,  3.40it/s] 95%|█████████▌| 594/625 [04:24<00:09,  3.38it/s] 95%|█████████▌| 595/625 [04:25<00:08,  3.38it/s] 95%|█████████▌| 596/625 [04:25<00:08,  3.39it/s] 96%|█████████▌| 597/625 [04:25<00:08,  3.40it/s] 96%|█████████▌| 598/625 [04:25<00:07,  3.40it/s] 96%|█████████▌| 599/625 [04:26<00:07,  3.40it/s] 96%|█████████▌| 600/625 [04:26<00:07,  3.40it/s] 96%|█████████▌| 601/625 [04:26<00:07,  3.40it/s] 96%|█████████▋| 602/625 [04:27<00:06,  3.40it/s] 96%|█████████▋| 603/625 [04:27<00:06,  3.40it/s] 97%|█████████▋| 604/625 [04:27<00:06,  3.41it/s] 97%|█████████▋| 605/625 [04:28<00:05,  3.33it/s] 97%|█████████▋| 606/625 [04:28<00:05,  3.35it/s] 97%|█████████▋| 607/625 [04:28<00:05,  3.37it/s] 97%|█████████▋| 608/625 [04:28<00:05,  3.38it/s] 97%|█████████▋| 609/625 [04:29<00:04,  3.39it/s] 98%|█████████▊| 610/625 [04:29<00:04,  3.39it/s] 98%|█████████▊| 611/625 [04:29<00:04,  3.40it/s] 98%|█████████▊| 612/625 [04:30<00:03,  3.40it/s] 98%|█████████▊| 613/625 [04:30<00:03,  3.40it/s] 98%|█████████▊| 614/625 [04:30<00:03,  3.40it/s] 98%|█████████▊| 615/625 [04:30<00:02,  3.40it/s] 99%|█████████▊| 616/625 [04:31<00:02,  3.33it/s] 99%|█████████▊| 617/625 [04:31<00:02,  3.35it/s] 99%|█████████▉| 618/625 [04:31<00:02,  3.37it/s] 99%|█████████▉| 619/625 [04:32<00:01,  3.38it/s] 99%|█████████▉| 620/625 [04:32<00:01,  3.38it/s] 99%|█████████▉| 621/625 [04:32<00:01,  3.39it/s]100%|█████████▉| 622/625 [04:33<00:00,  3.39it/s]100%|█████████▉| 623/625 [04:33<00:00,  3.40it/s]100%|█████████▉| 624/625 [04:33<00:00,  3.40it/s]100%|██████████| 625/625 [04:33<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 06:23:35,127 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:23:35,128 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:23:35,128 >>   Batch size = 8
{'eval_loss': 1.2508400678634644, 'eval_runtime': 9.7995, 'eval_samples_per_second': 355.323, 'eval_steps_per_second': 44.492, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.58it/s][A
  3%|▎         | 12/436 [00:00<00:09, 45.64it/s][A
  4%|▍         | 17/436 [00:00<00:09, 44.94it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.87it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.64it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.59it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.50it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.26it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.39it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.48it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.58it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.70it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.55it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.48it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.42it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.37it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.29it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.33it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.41it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.58it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.40it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.51it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.39it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.55it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.50it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.46it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.52it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 42.85it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.49it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.73it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.98it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.14it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.17it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.27it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.37it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.11it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.40it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.52it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.56it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.44it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.34it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.23it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.41it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.47it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.41it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.45it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.56it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.59it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.51it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.50it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.40it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.29it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.45it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.43it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 41.94it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 42.86it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 43.42it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 43.98it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.04it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.17it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.25it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.30it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.12it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.22it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.37it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.55it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.58it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 44.61it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.56it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.52it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.46it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.10it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.30it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.47it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.59it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.70it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.69it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.60it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.54it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.38it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.35it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 42.20it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 43.06it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.65it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.97it/s][A                                                 
                                                 [A100%|██████████| 625/625 [04:43<00:00,  3.40it/s]
100%|██████████| 436/436 [00:09<00:00, 43.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:23:45,101 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625
[INFO|configuration_utils.py:351] 2023-08-29 06:23:45,397 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:23:48,578 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:23:48,815 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:23:48,892 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 06:23:56,408 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 06:23:56,472 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125 (score: 1.207527995109558).
                                                 100%|██████████| 625/625 [05:03<00:00,  3.40it/s]100%|██████████| 625/625 [05:03<00:00,  2.06it/s]
[INFO|trainer.py:1894] 2023-08-29 06:24:04,931 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 06:24:05,034 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:24:07,973 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:24:08,085 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:24:08,138 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:24:08,600 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,600 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,600 >>   train_loss               =     0.3822
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,600 >>   train_runtime            = 0:05:03.58
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,600 >>   train_samples            =       8000
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,601 >>   train_samples_per_second =    131.757
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:08,601 >>   train_steps_per_second   =      2.059
{'eval_loss': 1.257163643836975, 'eval_runtime': 9.8562, 'eval_samples_per_second': 353.281, 'eval_steps_per_second': 44.236, 'epoch': 5.0}
{'train_runtime': 303.5897, 'train_samples_per_second': 131.757, 'train_steps_per_second': 2.059, 'train_loss': 0.3822362976074219, 'epoch': 5.0}
08/29/2023 06:24:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 06:24:08,793 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:24:08,793 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 06:24:08,793 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.81it/s]  3%|▎         | 12/436 [00:00<00:08, 49.11it/s]  4%|▍         | 17/436 [00:00<00:08, 47.64it/s]  5%|▌         | 22/436 [00:00<00:08, 46.80it/s]  6%|▌         | 27/436 [00:00<00:08, 46.32it/s]  7%|▋         | 32/436 [00:00<00:08, 46.02it/s]  8%|▊         | 37/436 [00:00<00:08, 45.82it/s] 10%|▉         | 42/436 [00:00<00:08, 45.17it/s] 11%|█         | 47/436 [00:01<00:08, 44.65it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.24it/s] 13%|█▎        | 57/436 [00:01<00:08, 44.30it/s] 14%|█▍        | 62/436 [00:01<00:08, 44.52it/s] 15%|█▌        | 67/436 [00:01<00:08, 44.64it/s] 17%|█▋        | 72/436 [00:01<00:08, 45.04it/s] 18%|█▊        | 77/436 [00:01<00:07, 45.01it/s] 19%|█▉        | 82/436 [00:01<00:07, 45.10it/s] 20%|█▉        | 87/436 [00:01<00:07, 44.69it/s] 21%|██        | 92/436 [00:02<00:07, 44.24it/s] 22%|██▏       | 97/436 [00:02<00:07, 44.07it/s] 23%|██▎       | 102/436 [00:02<00:07, 44.10it/s] 25%|██▍       | 107/436 [00:02<00:07, 44.36it/s] 26%|██▌       | 112/436 [00:02<00:07, 44.52it/s] 27%|██▋       | 117/436 [00:02<00:07, 44.75it/s] 28%|██▊       | 122/436 [00:02<00:07, 44.78it/s] 29%|██▉       | 127/436 [00:02<00:06, 44.81it/s] 30%|███       | 132/436 [00:02<00:06, 44.80it/s] 31%|███▏      | 137/436 [00:03<00:06, 44.40it/s] 33%|███▎      | 142/436 [00:03<00:06, 44.26it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.22it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.39it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.50it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.69it/s] 38%|███▊      | 167/436 [00:03<00:05, 44.91it/s] 39%|███▉      | 172/436 [00:03<00:05, 44.90it/s] 41%|████      | 177/436 [00:03<00:05, 44.74it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.48it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.32it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.35it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.38it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.70it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.63it/s] 49%|████▊     | 212/436 [00:04<00:04, 44.81it/s] 50%|████▉     | 217/436 [00:04<00:04, 44.80it/s] 51%|█████     | 222/436 [00:04<00:04, 44.71it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.49it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.27it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.41it/s] 56%|█████▌    | 242/436 [00:05<00:04, 43.92it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.36it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.41it/s] 59%|█████▉    | 257/436 [00:05<00:04, 44.69it/s] 60%|██████    | 262/436 [00:05<00:03, 44.64it/s] 61%|██████    | 267/436 [00:05<00:03, 44.61it/s] 62%|██████▏   | 272/436 [00:06<00:03, 44.43it/s] 64%|██████▎   | 277/436 [00:06<00:03, 44.33it/s] 65%|██████▍   | 282/436 [00:06<00:03, 44.45it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.42it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.60it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.59it/s] 69%|██████▉   | 302/436 [00:06<00:02, 44.71it/s] 70%|███████   | 307/436 [00:06<00:02, 44.79it/s] 72%|███████▏  | 312/436 [00:06<00:02, 44.59it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.49it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.46it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.51it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.58it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.46it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.57it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.44it/s] 81%|████████  | 352/436 [00:07<00:01, 44.60it/s] 82%|████████▏ | 357/436 [00:07<00:01, 44.55it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.44it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.40it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.48it/s] 86%|████████▋ | 377/436 [00:08<00:01, 43.61it/s] 88%|████████▊ | 382/436 [00:08<00:01, 43.96it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.12it/s] 90%|████████▉ | 392/436 [00:08<00:01, 42.09it/s] 91%|█████████ | 397/436 [00:08<00:00, 42.88it/s] 92%|█████████▏| 402/436 [00:09<00:00, 43.38it/s] 93%|█████████▎| 407/436 [00:09<00:00, 43.64it/s] 94%|█████████▍| 412/436 [00:09<00:00, 43.94it/s] 96%|█████████▌| 417/436 [00:09<00:00, 43.92it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.22it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.38it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.22it/s]100%|██████████| 436/436 [00:09<00:00, 44.57it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:24:18,596 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   eval_loss               =     1.2075
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   eval_runtime            = 0:00:09.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   eval_samples_per_second =    355.202
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   eval_steps_per_second   =     44.477
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:24:18,596 >>   perplexity              =     3.3452
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:27,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:27,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:24:27,708 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:24:27,709 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:24:28,302 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:24:29,371 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:24:29,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:32,343 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:32,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:32,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:32,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:24:32,369 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:24:33,017 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:24:33,018 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:24:33,623 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:24:33,782 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:24:33,782 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-125
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-625
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-250
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-500
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-375
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:08,  1.52it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.60it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.53it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:33,  1.51it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:35,  1.49it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:39,  1.50it/s]Extractor Predicting: 61it [00:39,  1.40it/s]Extractor Predicting: 62it [00:40,  1.45it/s]Extractor Predicting: 63it [00:41,  1.43it/s]Extractor Predicting: 64it [00:42,  1.44it/s]Extractor Predicting: 65it [00:42,  1.48it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.53it/s]Extractor Predicting: 70it [00:45,  1.57it/s]Extractor Predicting: 71it [00:46,  1.56it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:47,  1.61it/s]Extractor Predicting: 74it [00:48,  1.61it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:51,  1.53it/s]Extractor Predicting: 80it [00:52,  1.53it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.50it/s]Extractor Predicting: 85it [00:55,  1.55it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.51it/s]Extractor Predicting: 89it [00:58,  1.49it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:00,  1.50it/s]Extractor Predicting: 94it [01:01,  1.48it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:04,  1.50it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:05,  1.45it/s]Extractor Predicting: 101it [01:06,  1.47it/s]Extractor Predicting: 102it [01:07,  1.46it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:09,  1.45it/s]Extractor Predicting: 107it [01:10,  1.47it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.55it/s]Extractor Predicting: 111it [01:12,  1.57it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:16,  1.51it/s]Extractor Predicting: 118it [01:17,  1.48it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.54it/s]Extractor Predicting: 123it [01:20,  1.48it/s]Extractor Predicting: 124it [01:21,  1.51it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:25,  1.49it/s]Extractor Predicting: 131it [01:26,  1.52it/s]Extractor Predicting: 132it [01:26,  1.54it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:28,  1.48it/s]Extractor Predicting: 136it [01:29,  1.50it/s]Extractor Predicting: 137it [01:30,  1.50it/s]Extractor Predicting: 138it [01:30,  1.51it/s]Extractor Predicting: 139it [01:31,  1.36it/s]Extractor Predicting: 140it [01:32,  1.38it/s]Extractor Predicting: 141it [01:33,  1.41it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:34,  1.49it/s]Extractor Predicting: 143it [01:34,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:20,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:20,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:20,399 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:20,399 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:20,399 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:26:20,807 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:26:20,808 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:26:21,102 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:26:22,198 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:26:22,198 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:24,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:24,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:24,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:24,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:26:24,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:26:25,711 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:26:25,712 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:26:26,014 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:26:26,204 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:26:26,204 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26102610261026105,
  "recall": 0.0832854681217691,
  "score": 0.12627912040060962,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.57it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.51it/s]Extractor Predicting: 35it [00:21,  1.51it/s]Extractor Predicting: 36it [00:22,  1.52it/s]Extractor Predicting: 37it [00:23,  1.41it/s]Extractor Predicting: 38it [00:24,  1.43it/s]Extractor Predicting: 39it [00:24,  1.44it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:25,  1.49it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.48it/s]Extractor Predicting: 50it [00:31,  1.49it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.55it/s]Extractor Predicting: 53it [00:33,  1.52it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:41,  1.58it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.57it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:49,  1.59it/s]Extractor Predicting: 78it [00:49,  1.63it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.57it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.58it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:53,  1.54it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.55it/s]Extractor Predicting: 87it [00:55,  1.56it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:57,  1.61it/s]Extractor Predicting: 91it [00:58,  1.60it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:00,  1.52it/s]Extractor Predicting: 96it [01:01,  1.51it/s]Extractor Predicting: 97it [01:02,  1.52it/s]Extractor Predicting: 98it [01:02,  1.51it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.51it/s]Extractor Predicting: 101it [01:04,  1.50it/s]Extractor Predicting: 102it [01:05,  1.52it/s]Extractor Predicting: 103it [01:06,  1.53it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.52it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:08,  1.50it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.52it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.51it/s]Extractor Predicting: 112it [01:12,  1.49it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:13,  1.52it/s]Extractor Predicting: 115it [01:14,  1.51it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:15,  1.65it/s]Extractor Predicting: 118it [01:15,  1.69it/s]Extractor Predicting: 119it [01:16,  1.68it/s]Extractor Predicting: 120it [01:17,  1.67it/s]Extractor Predicting: 121it [01:17,  1.65it/s]Extractor Predicting: 122it [01:18,  1.63it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.64it/s]Extractor Predicting: 125it [01:20,  1.71it/s]Extractor Predicting: 126it [01:20,  1.69it/s]Extractor Predicting: 127it [01:21,  1.70it/s]Extractor Predicting: 128it [01:21,  1.73it/s]Extractor Predicting: 129it [01:22,  1.68it/s]Extractor Predicting: 130it [01:23,  1.66it/s]Extractor Predicting: 131it [01:23,  1.68it/s]Extractor Predicting: 132it [01:24,  1.70it/s]Extractor Predicting: 133it [01:25,  1.48it/s]Extractor Predicting: 134it [01:25,  1.55it/s]Extractor Predicting: 135it [01:26,  1.59it/s]Extractor Predicting: 136it [01:26,  1.65it/s]Extractor Predicting: 137it [01:27,  1.68it/s]Extractor Predicting: 138it [01:27,  1.67it/s]Extractor Predicting: 139it [01:28,  1.64it/s]Extractor Predicting: 140it [01:29,  1.66it/s]Extractor Predicting: 141it [01:29,  1.60it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:31,  1.65it/s]Extractor Predicting: 144it [01:31,  1.62it/s]Extractor Predicting: 145it [01:32,  1.63it/s]Extractor Predicting: 146it [01:32,  1.63it/s]Extractor Predicting: 147it [01:33,  1.64it/s]Extractor Predicting: 148it [01:34,  1.62it/s]Extractor Predicting: 149it [01:34,  1.67it/s]Extractor Predicting: 150it [01:35,  1.66it/s]Extractor Predicting: 151it [01:35,  1.63it/s]Extractor Predicting: 152it [01:36,  1.66it/s]Extractor Predicting: 153it [01:37,  1.70it/s]Extractor Predicting: 154it [01:37,  1.71it/s]Extractor Predicting: 155it [01:38,  1.70it/s]Extractor Predicting: 156it [01:38,  1.68it/s]Extractor Predicting: 157it [01:39,  1.66it/s]Extractor Predicting: 158it [01:40,  1.67it/s]Extractor Predicting: 159it [01:40,  1.63it/s]Extractor Predicting: 160it [01:41,  1.64it/s]Extractor Predicting: 161it [01:41,  1.64it/s]Extractor Predicting: 162it [01:42,  1.62it/s]Extractor Predicting: 163it [01:43,  1.63it/s]Extractor Predicting: 164it [01:43,  1.63it/s]Extractor Predicting: 165it [01:44,  1.64it/s]Extractor Predicting: 166it [01:44,  1.65it/s]Extractor Predicting: 167it [01:45,  1.64it/s]Extractor Predicting: 168it [01:46,  1.61it/s]Extractor Predicting: 169it [01:46,  1.62it/s]Extractor Predicting: 170it [01:47,  1.59it/s]Extractor Predicting: 171it [01:48,  1.58it/s]Extractor Predicting: 172it [01:48,  1.63it/s]Extractor Predicting: 173it [01:49,  1.57it/s]Extractor Predicting: 174it [01:50,  1.56it/s]Extractor Predicting: 175it [01:50,  1.54it/s]Extractor Predicting: 176it [01:51,  1.55it/s]Extractor Predicting: 177it [01:52,  1.52it/s]Extractor Predicting: 178it [01:52,  1.50it/s]Extractor Predicting: 179it [01:53,  1.52it/s]Extractor Predicting: 180it [01:53,  1.56it/s]Extractor Predicting: 181it [01:54,  1.55it/s]Extractor Predicting: 182it [01:55,  1.53it/s]Extractor Predicting: 183it [01:55,  1.53it/s]Extractor Predicting: 184it [01:56,  1.51it/s]Extractor Predicting: 185it [01:57,  1.49it/s]Extractor Predicting: 186it [01:57,  1.49it/s]Extractor Predicting: 187it [01:58,  1.49it/s]Extractor Predicting: 188it [01:59,  1.53it/s]Extractor Predicting: 189it [01:59,  1.53it/s]Extractor Predicting: 190it [02:00,  1.54it/s]Extractor Predicting: 191it [02:01,  1.51it/s]Extractor Predicting: 192it [02:01,  1.51it/s]Extractor Predicting: 193it [02:02,  1.49it/s]Extractor Predicting: 194it [02:03,  1.49it/s]Extractor Predicting: 195it [02:03,  1.48it/s]Extractor Predicting: 196it [02:04,  1.50it/s]Extractor Predicting: 197it [02:05,  1.49it/s]Extractor Predicting: 198it [02:05,  1.50it/s]Extractor Predicting: 199it [02:06,  1.50it/s]Extractor Predicting: 200it [02:07,  1.47it/s]Extractor Predicting: 201it [02:07,  1.49it/s]Extractor Predicting: 202it [02:08,  1.50it/s]Extractor Predicting: 203it [02:09,  1.52it/s]Extractor Predicting: 204it [02:09,  1.54it/s]Extractor Predicting: 205it [02:10,  1.58it/s]Extractor Predicting: 206it [02:11,  1.58it/s]Extractor Predicting: 207it [02:11,  1.56it/s]Extractor Predicting: 208it [02:12,  1.57it/s]Extractor Predicting: 209it [02:13,  1.56it/s]Extractor Predicting: 210it [02:13,  1.57it/s]Extractor Predicting: 211it [02:14,  1.61it/s]Extractor Predicting: 212it [02:14,  1.62it/s]Extractor Predicting: 213it [02:15,  1.64it/s]Extractor Predicting: 214it [02:16,  1.62it/s]Extractor Predicting: 215it [02:16,  1.56it/s]Extractor Predicting: 216it [02:17,  1.58it/s]Extractor Predicting: 217it [02:18,  1.59it/s]Extractor Predicting: 218it [02:18,  1.62it/s]Extractor Predicting: 219it [02:19,  1.64it/s]Extractor Predicting: 220it [02:19,  1.61it/s]Extractor Predicting: 221it [02:20,  1.63it/s]Extractor Predicting: 222it [02:21,  1.66it/s]Extractor Predicting: 223it [02:21,  1.63it/s]Extractor Predicting: 224it [02:22,  1.63it/s]Extractor Predicting: 225it [02:22,  1.62it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:24,  1.57it/s]Extractor Predicting: 228it [02:24,  1.57it/s]Extractor Predicting: 229it [02:25,  1.56it/s]Extractor Predicting: 230it [02:26,  1.56it/s]Extractor Predicting: 231it [02:26,  1.57it/s]Extractor Predicting: 232it [02:27,  1.58it/s]Extractor Predicting: 233it [02:27,  1.65it/s]Extractor Predicting: 234it [02:28,  1.66it/s]Extractor Predicting: 235it [02:29,  1.67it/s]Extractor Predicting: 236it [02:29,  1.71it/s]Extractor Predicting: 237it [02:30,  1.72it/s]Extractor Predicting: 238it [02:30,  1.69it/s]Extractor Predicting: 239it [02:31,  1.69it/s]Extractor Predicting: 240it [02:32,  1.72it/s]Extractor Predicting: 241it [02:32,  1.71it/s]Extractor Predicting: 242it [02:33,  1.72it/s]Extractor Predicting: 243it [02:33,  1.77it/s]Extractor Predicting: 244it [02:34,  1.73it/s]Extractor Predicting: 245it [02:34,  1.80it/s]Extractor Predicting: 246it [02:35,  1.83it/s]Extractor Predicting: 247it [02:35,  1.78it/s]Extractor Predicting: 248it [02:36,  1.51it/s]Extractor Predicting: 249it [02:37,  1.54it/s]Extractor Predicting: 250it [02:38,  1.59it/s]Extractor Predicting: 251it [02:38,  1.65it/s]Extractor Predicting: 252it [02:39,  1.69it/s]Extractor Predicting: 253it [02:39,  1.69it/s]Extractor Predicting: 254it [02:40,  1.67it/s]Extractor Predicting: 255it [02:40,  1.70it/s]Extractor Predicting: 256it [02:41,  1.71it/s]Extractor Predicting: 257it [02:42,  1.74it/s]Extractor Predicting: 258it [02:42,  1.74it/s]Extractor Predicting: 259it [02:43,  1.76it/s]Extractor Predicting: 260it [02:43,  1.74it/s]Extractor Predicting: 261it [02:44,  1.64it/s]Extractor Predicting: 262it [02:45,  1.65it/s]Extractor Predicting: 263it [02:45,  1.60it/s]Extractor Predicting: 264it [02:46,  1.56it/s]Extractor Predicting: 265it [02:47,  1.55it/s]Extractor Predicting: 266it [02:47,  1.55it/s]Extractor Predicting: 267it [02:48,  1.58it/s]Extractor Predicting: 268it [02:48,  1.58it/s]Extractor Predicting: 269it [02:49,  1.54it/s]Extractor Predicting: 270it [02:50,  1.52it/s]Extractor Predicting: 271it [02:51,  1.50it/s]Extractor Predicting: 272it [02:51,  1.52it/s]Extractor Predicting: 273it [02:52,  1.52it/s]Extractor Predicting: 274it [02:53,  1.51it/s]Extractor Predicting: 275it [02:53,  1.51it/s]Extractor Predicting: 276it [02:54,  1.50it/s]Extractor Predicting: 277it [02:55,  1.49it/s]Extractor Predicting: 278it [02:55,  1.49it/s]Extractor Predicting: 279it [02:56,  1.53it/s]Extractor Predicting: 280it [02:57,  1.51it/s]Extractor Predicting: 281it [02:57,  1.50it/s]Extractor Predicting: 282it [02:58,  1.49it/s]Extractor Predicting: 283it [02:59,  1.50it/s]Extractor Predicting: 284it [02:59,  1.49it/s]Extractor Predicting: 285it [03:00,  1.51it/s]Extractor Predicting: 286it [03:01,  1.51it/s]Extractor Predicting: 287it [03:01,  1.48it/s]Extractor Predicting: 288it [03:02,  1.53it/s]Extractor Predicting: 289it [03:02,  1.53it/s]Extractor Predicting: 290it [03:03,  1.56it/s]Extractor Predicting: 291it [03:04,  1.57it/s]Extractor Predicting: 292it [03:04,  1.56it/s]Extractor Predicting: 293it [03:05,  1.57it/s]Extractor Predicting: 294it [03:06,  1.59it/s]Extractor Predicting: 295it [03:06,  1.59it/s]Extractor Predicting: 296it [03:07,  1.58it/s]Extractor Predicting: 297it [03:07,  1.58it/s]Extractor Predicting: 298it [03:08,  1.62it/s]Extractor Predicting: 299it [03:09,  1.60it/s]Extractor Predicting: 300it [03:09,  1.60it/s]Extractor Predicting: 301it [03:10,  1.59it/s]Extractor Predicting: 302it [03:11,  1.56it/s]Extractor Predicting: 303it [03:11,  1.57it/s]Extractor Predicting: 304it [03:12,  1.56it/s]Extractor Predicting: 305it [03:13,  1.55it/s]Extractor Predicting: 306it [03:13,  1.57it/s]Extractor Predicting: 307it [03:14,  1.57it/s]Extractor Predicting: 308it [03:14,  1.56it/s]Extractor Predicting: 309it [03:15,  1.53it/s]Extractor Predicting: 310it [03:16,  1.55it/s]Extractor Predicting: 311it [03:16,  1.55it/s]Extractor Predicting: 312it [03:17,  1.55it/s]Extractor Predicting: 313it [03:18,  1.56it/s]Extractor Predicting: 314it [03:18,  1.55it/s]Extractor Predicting: 315it [03:19,  1.58it/s]Extractor Predicting: 316it [03:20,  1.57it/s]Extractor Predicting: 317it [03:20,  1.58it/s]Extractor Predicting: 318it [03:21,  1.61it/s]Extractor Predicting: 319it [03:21,  1.61it/s]Extractor Predicting: 320it [03:22,  1.59it/s]Extractor Predicting: 321it [03:23,  1.58it/s]Extractor Predicting: 322it [03:23,  1.56it/s]Extractor Predicting: 323it [03:24,  1.59it/s]Extractor Predicting: 324it [03:25,  1.59it/s]Extractor Predicting: 325it [03:25,  1.58it/s]Extractor Predicting: 326it [03:26,  1.60it/s]Extractor Predicting: 327it [03:27,  1.60it/s]Extractor Predicting: 328it [03:27,  1.59it/s]Extractor Predicting: 329it [03:28,  1.59it/s]Extractor Predicting: 330it [03:28,  1.55it/s]Extractor Predicting: 331it [03:29,  1.56it/s]Extractor Predicting: 332it [03:30,  1.59it/s]Extractor Predicting: 333it [03:30,  1.56it/s]Extractor Predicting: 334it [03:31,  1.60it/s]Extractor Predicting: 335it [03:32,  1.54it/s]Extractor Predicting: 336it [03:32,  1.57it/s]Extractor Predicting: 337it [03:33,  1.57it/s]Extractor Predicting: 338it [03:34,  1.58it/s]Extractor Predicting: 339it [03:34,  1.59it/s]Extractor Predicting: 340it [03:35,  1.56it/s]Extractor Predicting: 341it [03:35,  1.59it/s]Extractor Predicting: 342it [03:36,  1.61it/s]Extractor Predicting: 343it [03:37,  1.61it/s]Extractor Predicting: 344it [03:37,  1.64it/s]Extractor Predicting: 345it [03:38,  1.59it/s]Extractor Predicting: 346it [03:39,  1.59it/s]Extractor Predicting: 347it [03:39,  1.60it/s]Extractor Predicting: 348it [03:40,  1.57it/s]Extractor Predicting: 349it [03:40,  1.58it/s]Extractor Predicting: 350it [03:41,  1.55it/s]Extractor Predicting: 351it [03:42,  1.57it/s]Extractor Predicting: 352it [03:42,  1.56it/s]Extractor Predicting: 353it [03:43,  1.57it/s]Extractor Predicting: 354it [03:44,  1.39it/s]Extractor Predicting: 355it [03:45,  1.41it/s]Extractor Predicting: 356it [03:45,  1.46it/s]Extractor Predicting: 357it [03:46,  1.47it/s]Extractor Predicting: 358it [03:47,  1.50it/s]Extractor Predicting: 359it [03:47,  1.52it/s]Extractor Predicting: 360it [03:48,  1.52it/s]Extractor Predicting: 361it [03:48,  1.53it/s]Extractor Predicting: 362it [03:49,  1.57it/s]Extractor Predicting: 363it [03:50,  1.57it/s]Extractor Predicting: 364it [03:50,  1.60it/s]Extractor Predicting: 365it [03:51,  1.56it/s]Extractor Predicting: 366it [03:52,  1.58it/s]Extractor Predicting: 367it [03:52,  1.58it/s]Extractor Predicting: 368it [03:53,  1.59it/s]Extractor Predicting: 369it [03:54,  1.54it/s]Extractor Predicting: 370it [03:54,  1.53it/s]Extractor Predicting: 371it [03:55,  1.54it/s]Extractor Predicting: 372it [03:55,  1.56it/s]Extractor Predicting: 373it [03:56,  1.54it/s]Extractor Predicting: 374it [03:57,  1.58it/s]Extractor Predicting: 375it [03:57,  1.57it/s]Extractor Predicting: 376it [03:58,  1.55it/s]Extractor Predicting: 377it [03:59,  1.61it/s]Extractor Predicting: 378it [03:59,  1.60it/s]Extractor Predicting: 379it [04:00,  1.63it/s]Extractor Predicting: 380it [04:00,  1.60it/s]Extractor Predicting: 381it [04:01,  1.60it/s]Extractor Predicting: 382it [04:02,  1.59it/s]Extractor Predicting: 383it [04:02,  1.56it/s]Extractor Predicting: 384it [04:03,  1.55it/s]Extractor Predicting: 385it [04:04,  1.57it/s]Extractor Predicting: 386it [04:04,  1.55it/s]Extractor Predicting: 387it [04:05,  1.55it/s]Extractor Predicting: 388it [04:06,  1.49it/s]Extractor Predicting: 389it [04:06,  1.51it/s]Extractor Predicting: 390it [04:07,  1.53it/s]Extractor Predicting: 391it [04:08,  1.56it/s]Extractor Predicting: 392it [04:08,  1.57it/s]Extractor Predicting: 393it [04:09,  1.55it/s]Extractor Predicting: 394it [04:10,  1.56it/s]Extractor Predicting: 395it [04:10,  1.56it/s]Extractor Predicting: 396it [04:11,  1.55it/s]Extractor Predicting: 397it [04:11,  1.56it/s]Extractor Predicting: 398it [04:12,  1.53it/s]Extractor Predicting: 399it [04:13,  1.57it/s]Extractor Predicting: 400it [04:13,  1.55it/s]Extractor Predicting: 401it [04:14,  1.56it/s]Extractor Predicting: 402it [04:15,  1.56it/s]Extractor Predicting: 403it [04:15,  1.53it/s]Extractor Predicting: 404it [04:16,  1.56it/s]Extractor Predicting: 405it [04:17,  1.57it/s]Extractor Predicting: 406it [04:17,  1.59it/s]Extractor Predicting: 407it [04:18,  1.58it/s]Extractor Predicting: 408it [04:19,  1.55it/s]Extractor Predicting: 409it [04:19,  1.57it/s]Extractor Predicting: 410it [04:20,  1.58it/s]Extractor Predicting: 411it [04:20,  1.57it/s]Extractor Predicting: 412it [04:21,  1.60it/s]Extractor Predicting: 413it [04:22,  1.59it/s]Extractor Predicting: 414it [04:22,  1.58it/s]Extractor Predicting: 415it [04:23,  1.56it/s]Extractor Predicting: 416it [04:24,  1.60it/s]Extractor Predicting: 417it [04:24,  1.60it/s]Extractor Predicting: 418it [04:25,  1.61it/s]Extractor Predicting: 419it [04:25,  1.62it/s]Extractor Predicting: 420it [04:26,  1.63it/s]Extractor Predicting: 421it [04:27,  1.60it/s]Extractor Predicting: 422it [04:27,  1.61it/s]Extractor Predicting: 423it [04:28,  1.61it/s]Extractor Predicting: 424it [04:28,  1.61it/s]Extractor Predicting: 425it [04:29,  1.59it/s]Extractor Predicting: 426it [04:30,  1.63it/s]Extractor Predicting: 427it [04:30,  1.59it/s]Extractor Predicting: 428it [04:31,  1.55it/s]Extractor Predicting: 429it [04:31,  1.78it/s]Extractor Predicting: 429it [04:31,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:10,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:10,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:10,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:10,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:10,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:31:11,300 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:31:11,301 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:31:11,984 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:31:13,092 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:31:13,092 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:15,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:15,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:15,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:15,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:31:15,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:31:16,789 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:31:16,791 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:31:17,521 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:31:17,719 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:31:17,720 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21248852157943068,
  "recall": 0.11252674576930559,
  "score": 0.14713549945952822,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 5it [00:03,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:31:22,082 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:31:22,083 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:31:22,124 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:31:22,125 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:31:22,137 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:31:30,746 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:31:30,764 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:31:30,826 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:31:30,826 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:31:30,874 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:31:30,900 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.275,
  "recall": 0.05045871559633028,
  "score": 0.08527131782945736,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:31:31,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:31,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:32,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:32,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:33,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:34,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:34,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:35,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:35,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:36,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:36,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:37,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:37,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:38,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:38,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:39,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:39,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:40,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:40,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:41,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:10<03:26, 10.84s/it][WARNING|generation_utils.py:914] 2023-08-29 06:31:42,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:42,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:43,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:43,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:44,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:44,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:45,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:45,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:46,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:46,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:47,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:47,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:48,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:49,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:49,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:50,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:50,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:51,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:51,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:52,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:21<03:14, 10.83s/it][WARNING|generation_utils.py:914] 2023-08-29 06:31:52,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:53,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:54,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:55,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:55,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:56,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:57,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:57,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:58,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:59,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:31:59,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:00,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:01,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:01,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:02,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:02,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:03,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:04,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:04,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:05,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:06,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:35<03:28, 12.24s/it][WARNING|generation_utils.py:914] 2023-08-29 06:32:06,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:07,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:08,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:09,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:09,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:10,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:10,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:11,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:11,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:12,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:13,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:13,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:14,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:14,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:15,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:15,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:16,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:17,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:17,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:47<03:12, 12.05s/it][WARNING|generation_utils.py:914] 2023-08-29 06:32:18,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:19,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:19,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:20,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:20,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:21,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:22,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:22,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:23,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:24,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:24,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:25,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:26,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:26,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:27,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:27,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:28,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:29,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:29,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:30,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:31,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:00<03:09, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-29 06:32:32,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:32,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:33,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:34,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:34,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:35,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:35,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:36,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:37,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:37,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:38,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:38,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:39,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:39,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:40,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:41,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:41,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:42,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:43,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:13<02:54, 12.43s/it][WARNING|generation_utils.py:914] 2023-08-29 06:32:44,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:44,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:45,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:45,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:46,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:46,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:47,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:47,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:48,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:49,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:49,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:50,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:51,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:51,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:52,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:52,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:53,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:53,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:54,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:54,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:55,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:55,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:56,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:26<02:44, 12.64s/it][WARNING|generation_utils.py:914] 2023-08-29 06:32:57,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:57,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:58,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:58,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:59,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:32:59,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:00,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:00,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:01,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:02,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:02,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:03,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:03,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:04,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:04,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:05,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:05,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:06,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:06,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:07,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:07,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:37<02:26, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-29 06:33:08,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:08,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:10,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:10,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:11,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:12,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:12,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:13,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:13,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:14,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:15,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:15,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:16,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:17,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:17,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:18,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:20,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:20,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:21,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:21,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:51<02:20, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-29 06:33:22,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:23,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:23,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:24,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:25,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:25,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:26,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:27,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:28,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:28,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:29,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:29,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:30,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:30,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:31,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:32,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:33,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:33,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:34,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:03<02:07, 12.71s/it][WARNING|generation_utils.py:914] 2023-08-29 06:33:35,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:35,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:36,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:36,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:37,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:38,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:38,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:39,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:40,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:40,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:41,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:41,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:42,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:43,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:43,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:44,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:44,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:45,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:45,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:46,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:47,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:16<01:54, 12.69s/it][WARNING|generation_utils.py:914] 2023-08-29 06:33:47,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:48,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:48,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:49,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:50,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:50,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:51,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:51,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:52,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:52,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:53,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:54,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:54,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:55,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:55,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:56,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:56,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:57,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:58,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:58,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:33:59,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:28<01:40, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-29 06:33:59,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:00,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:01,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:01,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:02,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:03,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:03,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:04,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:04,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:05,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:05,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:06,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:07,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:07,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:08,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:08,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:09,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:10,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:10,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:11,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:40<01:26, 12.38s/it][WARNING|generation_utils.py:914] 2023-08-29 06:34:11,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:12,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:13,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:13,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:14,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:14,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:15,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:15,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:16,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:17,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:17,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:18,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:18,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:19,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:20,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:21,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:21,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:22,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:22,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:52<01:12, 12.11s/it][WARNING|generation_utils.py:914] 2023-08-29 06:34:23,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:23,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:24,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:25,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:26,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:26,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:27,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:27,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:28,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:28,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:29,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:30,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:30,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:31,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:32,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:32,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:33,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:33,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:34,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:35,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:04<01:00, 12.17s/it][WARNING|generation_utils.py:914] 2023-08-29 06:34:35,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:36,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:36,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:37,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:37,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:38,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:39,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:39,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:40,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:40,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:41,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:41,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:42,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:43,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:43,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:44,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:44,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:45,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:46,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:46,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:47,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:16<00:48, 12.09s/it][WARNING|generation_utils.py:914] 2023-08-29 06:34:47,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:48,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:48,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:49,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:50,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:50,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:51,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:52,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:52,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:53,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:54,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:54,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:55,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:55,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:56,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:57,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:57,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:58,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:58,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:34:59,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:28<00:36, 12.12s/it][WARNING|generation_utils.py:914] 2023-08-29 06:34:59,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:00,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:01,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:01,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:02,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:03,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:03,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:04,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:04,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:05,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:06,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:06,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:07,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:08,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:09,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:10,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:10,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:11,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:40<00:24, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-29 06:35:12,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:12,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:13,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:13,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:14,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:14,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:15,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:15,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:16,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:17,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:17,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:18,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:18,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:19,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:19,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:20,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:20,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:21,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:21,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:22,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:22,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:52<00:11, 11.93s/it][WARNING|generation_utils.py:914] 2023-08-29 06:35:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:24,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:24,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:25,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:25,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:26,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:26,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:27,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:28,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:28,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:29,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:29,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:30,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:30,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:31,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:32,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:32,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:33,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:34,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:35:34,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:04<00:00, 11.88s/it]Generating: 100%|██████████| 20/20 [04:04<00:00, 12.20s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:42,700 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:42,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:42,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:42,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:42,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:35:43,152 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:35:43,153 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:35:43,441 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:35:44,572 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:35:44,572 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:47,669 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:47,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:47,702 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:47,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:35:47,703 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:35:48,487 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:35:48,488 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:35:49,091 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:35:49,312 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:35:49,312 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : head of government . Context : On 1 January 2014 , the Prime Minister appointed him as Minister of State for Foreign Affairs and Trade . Head Entity : Prime Minister , Tail Entity : Prime Minister of State for Foreign Affairs and Trade .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 317, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9796875, 'errors': {''}}
['Relation : mother . Context : Later in life , he married his second wife , a young French lawyer who had a career in politics . Head Entity : French lawyer , Tail Entity : Marie .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.975, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.79296875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9151785714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : occupant .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 534, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9765625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 7833
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7933, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.80it/s]Extractor Estimating: 2it [00:01,  1.64it/s]Extractor Estimating: 3it [00:01,  1.72it/s]Extractor Estimating: 4it [00:02,  1.72it/s]Extractor Estimating: 5it [00:02,  1.73it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.74it/s]Extractor Estimating: 9it [00:05,  1.82it/s]Extractor Estimating: 10it [00:05,  1.81it/s]Extractor Estimating: 11it [00:06,  1.85it/s]Extractor Estimating: 12it [00:06,  1.87it/s]Extractor Estimating: 13it [00:07,  1.82it/s]Extractor Estimating: 14it [00:07,  1.82it/s]Extractor Estimating: 15it [00:08,  1.83it/s]Extractor Estimating: 16it [00:09,  1.68it/s]Extractor Estimating: 17it [00:09,  1.73it/s]Extractor Estimating: 18it [00:10,  1.72it/s]Extractor Estimating: 19it [00:10,  1.68it/s]Extractor Estimating: 20it [00:11,  1.66it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:12,  1.73it/s]Extractor Estimating: 23it [00:13,  1.86it/s]Extractor Estimating: 24it [00:13,  1.82it/s]Extractor Estimating: 25it [00:14,  1.86it/s]Extractor Estimating: 26it [00:14,  1.87it/s]Extractor Estimating: 27it [00:15,  1.79it/s]Extractor Estimating: 28it [00:15,  1.76it/s]Extractor Estimating: 29it [00:16,  1.73it/s]Extractor Estimating: 30it [00:17,  1.74it/s]Extractor Estimating: 31it [00:17,  1.74it/s]Extractor Estimating: 32it [00:18,  1.74it/s]Extractor Estimating: 33it [00:18,  1.78it/s]Extractor Estimating: 34it [00:19,  1.76it/s]Extractor Estimating: 35it [00:20,  1.57it/s]Extractor Estimating: 36it [00:20,  1.59it/s]Extractor Estimating: 37it [00:21,  1.63it/s]Extractor Estimating: 38it [00:21,  1.69it/s]Extractor Estimating: 39it [00:22,  1.68it/s]Extractor Estimating: 40it [00:23,  1.68it/s]Extractor Estimating: 41it [00:23,  1.64it/s]Extractor Estimating: 42it [00:24,  1.64it/s]Extractor Estimating: 43it [00:24,  1.69it/s]Extractor Estimating: 44it [00:25,  1.69it/s]Extractor Estimating: 45it [00:25,  1.69it/s]Extractor Estimating: 46it [00:26,  1.71it/s]Extractor Estimating: 47it [00:27,  1.74it/s]Extractor Estimating: 48it [00:27,  1.73it/s]Extractor Estimating: 49it [00:28,  1.73it/s]Extractor Estimating: 50it [00:28,  1.73it/s]Extractor Estimating: 51it [00:29,  1.59it/s]Extractor Estimating: 52it [00:30,  1.65it/s]Extractor Estimating: 53it [00:30,  1.69it/s]Extractor Estimating: 54it [00:31,  1.75it/s]Extractor Estimating: 55it [00:31,  1.80it/s]Extractor Estimating: 56it [00:32,  1.77it/s]Extractor Estimating: 57it [00:32,  1.82it/s]Extractor Estimating: 58it [00:33,  1.75it/s]Extractor Estimating: 59it [00:34,  1.79it/s]Extractor Estimating: 60it [00:34,  1.72it/s]Extractor Estimating: 61it [00:35,  1.77it/s]Extractor Estimating: 62it [00:35,  1.83it/s]Extractor Estimating: 63it [00:36,  1.85it/s]Extractor Estimating: 64it [00:36,  1.75it/s]Extractor Estimating: 65it [00:37,  1.68it/s]Extractor Estimating: 66it [00:38,  1.71it/s]Extractor Estimating: 67it [00:38,  1.64it/s]Extractor Estimating: 68it [00:39,  1.71it/s]Extractor Estimating: 69it [00:39,  1.74it/s]Extractor Estimating: 70it [00:40,  1.72it/s]Extractor Estimating: 71it [00:40,  1.73it/s]Extractor Estimating: 72it [00:41,  1.67it/s]Extractor Estimating: 73it [00:42,  1.68it/s]Extractor Estimating: 74it [00:42,  1.70it/s]Extractor Estimating: 75it [00:43,  1.66it/s]Extractor Estimating: 76it [00:44,  1.59it/s]Extractor Estimating: 77it [00:44,  1.61it/s]Extractor Estimating: 78it [00:45,  1.60it/s]Extractor Estimating: 79it [00:45,  1.63it/s]Extractor Estimating: 80it [00:46,  1.62it/s]Extractor Estimating: 81it [00:47,  1.64it/s]Extractor Estimating: 82it [00:47,  1.64it/s]Extractor Estimating: 83it [00:48,  1.59it/s]Extractor Estimating: 84it [00:48,  1.66it/s]Extractor Estimating: 85it [00:49,  1.68it/s]Extractor Estimating: 86it [00:50,  1.70it/s]Extractor Estimating: 87it [00:50,  1.68it/s]Extractor Estimating: 88it [00:51,  1.67it/s]Extractor Estimating: 89it [00:51,  1.66it/s]Extractor Estimating: 90it [00:52,  1.61it/s]Extractor Estimating: 91it [00:53,  1.53it/s]Extractor Estimating: 92it [00:53,  1.55it/s]Extractor Estimating: 93it [00:54,  1.53it/s]Extractor Estimating: 94it [00:55,  1.54it/s]Extractor Estimating: 95it [00:55,  1.57it/s]Extractor Estimating: 96it [00:56,  1.61it/s]Extractor Estimating: 97it [00:57,  1.63it/s]Extractor Estimating: 98it [00:57,  1.49it/s]Extractor Estimating: 99it [00:58,  1.55it/s]Extractor Estimating: 100it [00:59,  1.57it/s]Extractor Estimating: 101it [00:59,  1.58it/s]Extractor Estimating: 102it [01:00,  1.56it/s]Extractor Estimating: 103it [01:00,  1.65it/s]Extractor Estimating: 104it [01:01,  1.71it/s]Extractor Estimating: 105it [01:01,  1.77it/s]Extractor Estimating: 106it [01:02,  1.86it/s]Extractor Estimating: 107it [01:02,  1.85it/s]Extractor Estimating: 108it [01:03,  1.82it/s]Extractor Estimating: 109it [01:04,  1.75it/s]Extractor Estimating: 110it [01:04,  1.80it/s]Extractor Estimating: 111it [01:05,  1.81it/s]Extractor Estimating: 112it [01:05,  1.81it/s]Extractor Estimating: 113it [01:06,  1.85it/s]Extractor Estimating: 114it [01:06,  1.82it/s]Extractor Estimating: 115it [01:07,  1.88it/s]Extractor Estimating: 116it [01:07,  1.89it/s]Extractor Estimating: 117it [01:08,  1.90it/s]Extractor Estimating: 118it [01:08,  1.97it/s]Extractor Estimating: 119it [01:09,  1.94it/s]Extractor Estimating: 120it [01:09,  1.88it/s]Extractor Estimating: 121it [01:10,  1.82it/s]Extractor Estimating: 122it [01:11,  1.78it/s]Extractor Estimating: 123it [01:11,  1.82it/s]Extractor Estimating: 124it [01:12,  1.82it/s]Extractor Estimating: 125it [01:12,  1.85it/s]Extractor Estimating: 126it [01:13,  1.69it/s]Extractor Estimating: 127it [01:14,  1.67it/s]Extractor Estimating: 128it [01:14,  1.57it/s]Extractor Estimating: 129it [01:15,  1.58it/s]Extractor Estimating: 130it [01:16,  1.57it/s]Extractor Estimating: 131it [01:16,  1.58it/s]Extractor Estimating: 132it [01:17,  1.62it/s]Extractor Estimating: 133it [01:17,  1.61it/s]Extractor Estimating: 134it [01:18,  1.62it/s]Extractor Estimating: 135it [01:19,  1.60it/s]Extractor Estimating: 136it [01:19,  1.60it/s]Extractor Estimating: 137it [01:20,  1.58it/s]Extractor Estimating: 138it [01:21,  1.56it/s]Extractor Estimating: 139it [01:21,  1.55it/s]Extractor Estimating: 140it [01:22,  1.57it/s]Extractor Estimating: 141it [01:22,  1.58it/s]Extractor Estimating: 142it [01:23,  1.65it/s]Extractor Estimating: 143it [01:24,  1.67it/s]Extractor Estimating: 144it [01:24,  1.64it/s]Extractor Estimating: 145it [01:25,  1.68it/s]Extractor Estimating: 146it [01:26,  1.56it/s]Extractor Estimating: 147it [01:26,  1.61it/s]Extractor Estimating: 148it [01:27,  1.62it/s]Extractor Estimating: 149it [01:27,  1.57it/s]Extractor Estimating: 150it [01:28,  1.60it/s]Extractor Estimating: 151it [01:28,  1.77it/s]Extractor Estimating: 152it [01:29,  1.82it/s]Extractor Estimating: 153it [01:29,  1.87it/s]Extractor Estimating: 154it [01:30,  1.92it/s]Extractor Estimating: 155it [01:30,  1.98it/s]Extractor Estimating: 156it [01:31,  2.05it/s]Extractor Estimating: 157it [01:31,  2.12it/s]Extractor Estimating: 158it [01:32,  2.02it/s]Extractor Estimating: 159it [01:32,  1.99it/s]Extractor Estimating: 160it [01:33,  2.02it/s]Extractor Estimating: 161it [01:33,  1.96it/s]Extractor Estimating: 162it [01:34,  2.00it/s]Extractor Estimating: 163it [01:34,  2.04it/s]Extractor Estimating: 164it [01:35,  2.05it/s]Extractor Estimating: 165it [01:35,  2.00it/s]Extractor Estimating: 166it [01:36,  2.06it/s]Extractor Estimating: 167it [01:36,  2.14it/s]Extractor Estimating: 168it [01:37,  2.11it/s]Extractor Estimating: 169it [01:37,  1.99it/s]Extractor Estimating: 170it [01:38,  2.06it/s]Extractor Estimating: 171it [01:38,  2.06it/s]Extractor Estimating: 172it [01:39,  2.14it/s]Extractor Estimating: 173it [01:39,  2.13it/s]Extractor Estimating: 174it [01:40,  2.09it/s]Extractor Estimating: 175it [01:40,  2.07it/s]Extractor Estimating: 176it [01:41,  2.09it/s]Extractor Estimating: 177it [01:41,  2.02it/s]Extractor Estimating: 178it [01:42,  1.99it/s]Extractor Estimating: 179it [01:42,  2.01it/s]Extractor Estimating: 180it [01:43,  1.98it/s]Extractor Estimating: 181it [01:43,  1.92it/s]Extractor Estimating: 182it [01:44,  1.94it/s]Extractor Estimating: 183it [01:44,  1.95it/s]Extractor Estimating: 184it [01:45,  2.02it/s]Extractor Estimating: 185it [01:45,  1.96it/s]Extractor Estimating: 186it [01:46,  1.95it/s]Extractor Estimating: 187it [01:46,  1.99it/s]Extractor Estimating: 188it [01:47,  1.98it/s]Extractor Estimating: 189it [01:47,  2.02it/s]Extractor Estimating: 190it [01:48,  1.72it/s]Extractor Estimating: 191it [01:48,  1.76it/s]Extractor Estimating: 192it [01:49,  1.87it/s]Extractor Estimating: 193it [01:49,  1.93it/s]Extractor Estimating: 194it [01:50,  1.93it/s]Extractor Estimating: 195it [01:50,  1.97it/s]Extractor Estimating: 196it [01:51,  1.93it/s]Extractor Estimating: 197it [01:51,  1.95it/s]Extractor Estimating: 198it [01:52,  1.79it/s]Extractor Estimating: 199it [01:53,  1.78it/s]Extractor Estimating: 200it [01:53,  1.84it/s]Extractor Estimating: 201it [01:54,  1.86it/s]Extractor Estimating: 202it [01:54,  1.78it/s]Extractor Estimating: 203it [01:55,  1.79it/s]Extractor Estimating: 204it [01:56,  1.66it/s]Extractor Estimating: 205it [01:56,  1.63it/s]Extractor Estimating: 206it [01:57,  1.56it/s]Extractor Estimating: 207it [01:58,  1.59it/s]Extractor Estimating: 208it [01:58,  1.58it/s]Extractor Estimating: 209it [01:59,  1.60it/s]Extractor Estimating: 210it [01:59,  1.54it/s]Extractor Estimating: 211it [02:00,  1.60it/s]Extractor Estimating: 212it [02:01,  1.56it/s]Extractor Estimating: 213it [02:01,  1.55it/s]Extractor Estimating: 214it [02:02,  1.57it/s]Extractor Estimating: 215it [02:03,  1.58it/s]Extractor Estimating: 216it [02:03,  1.58it/s]Extractor Estimating: 217it [02:04,  1.57it/s]Extractor Estimating: 218it [02:04,  1.61it/s]Extractor Estimating: 219it [02:05,  1.64it/s]Extractor Estimating: 220it [02:06,  1.61it/s]Extractor Estimating: 221it [02:06,  1.70it/s]Extractor Estimating: 222it [02:07,  1.72it/s]Extractor Estimating: 223it [02:07,  1.65it/s]Extractor Estimating: 224it [02:08,  1.63it/s]Extractor Estimating: 225it [02:09,  1.63it/s]Extractor Estimating: 226it [02:09,  1.70it/s]Extractor Estimating: 227it [02:10,  1.72it/s]Extractor Estimating: 228it [02:10,  1.71it/s]Extractor Estimating: 229it [02:11,  1.73it/s]Extractor Estimating: 230it [02:12,  1.74it/s]Extractor Estimating: 231it [02:12,  1.77it/s]Extractor Estimating: 232it [02:13,  1.74it/s]Extractor Estimating: 233it [02:13,  1.73it/s]Extractor Estimating: 234it [02:14,  1.75it/s]Extractor Estimating: 235it [02:14,  1.77it/s]Extractor Estimating: 236it [02:15,  1.77it/s]Extractor Estimating: 237it [02:15,  1.75it/s]Extractor Estimating: 238it [02:16,  1.72it/s]Extractor Estimating: 239it [02:17,  1.74it/s]Extractor Estimating: 240it [02:17,  1.77it/s]Extractor Estimating: 241it [02:18,  1.78it/s]Extractor Estimating: 242it [02:18,  1.76it/s]Extractor Estimating: 243it [02:19,  1.73it/s]Extractor Estimating: 244it [02:19,  1.80it/s]Extractor Estimating: 245it [02:20,  1.73it/s]Extractor Estimating: 246it [02:21,  1.72it/s]Extractor Estimating: 247it [02:21,  1.74it/s]Extractor Estimating: 248it [02:22,  1.71it/s]Extractor Estimating: 249it [02:22,  1.70it/s]Extractor Estimating: 250it [02:23,  1.72it/s]Extractor Estimating: 251it [02:24,  1.67it/s]Extractor Estimating: 252it [02:24,  1.63it/s]Extractor Estimating: 253it [02:25,  1.62it/s]Extractor Estimating: 254it [02:25,  1.67it/s]Extractor Estimating: 255it [02:26,  1.71it/s]Extractor Estimating: 256it [02:27,  1.73it/s]Extractor Estimating: 257it [02:27,  1.65it/s]Extractor Estimating: 258it [02:28,  1.68it/s]Extractor Estimating: 259it [02:28,  1.69it/s]Extractor Estimating: 260it [02:29,  1.64it/s]Extractor Estimating: 261it [02:30,  1.65it/s]Extractor Estimating: 262it [02:30,  1.60it/s]Extractor Estimating: 263it [02:31,  1.60it/s]Extractor Estimating: 264it [02:32,  1.65it/s]Extractor Estimating: 265it [02:32,  1.63it/s]Extractor Estimating: 266it [02:33,  1.62it/s]Extractor Estimating: 267it [02:33,  1.66it/s]Extractor Estimating: 268it [02:34,  1.69it/s]Extractor Estimating: 269it [02:35,  1.68it/s]Extractor Estimating: 270it [02:35,  1.69it/s]Extractor Estimating: 271it [02:36,  1.70it/s]Extractor Estimating: 272it [02:36,  1.68it/s]Extractor Estimating: 273it [02:37,  1.69it/s]Extractor Estimating: 274it [02:37,  1.66it/s]Extractor Estimating: 275it [02:38,  1.69it/s]Extractor Estimating: 276it [02:39,  1.73it/s]Extractor Estimating: 277it [02:39,  1.74it/s]Extractor Estimating: 278it [02:40,  1.75it/s]Extractor Estimating: 279it [02:40,  1.73it/s]Extractor Estimating: 280it [02:41,  1.74it/s]Extractor Estimating: 281it [02:41,  1.71it/s]Extractor Estimating: 282it [02:42,  1.73it/s]Extractor Estimating: 283it [02:43,  1.76it/s]Extractor Estimating: 284it [02:43,  1.79it/s]Extractor Estimating: 285it [02:44,  1.76it/s]Extractor Estimating: 286it [02:44,  1.76it/s]Extractor Estimating: 287it [02:45,  1.80it/s]Extractor Estimating: 288it [02:45,  1.84it/s]Extractor Estimating: 289it [02:46,  1.77it/s]Extractor Estimating: 290it [02:46,  1.80it/s]Extractor Estimating: 291it [02:47,  1.87it/s]Extractor Estimating: 292it [02:48,  1.87it/s]Extractor Estimating: 293it [02:48,  1.85it/s]Extractor Estimating: 294it [02:49,  1.86it/s]Extractor Estimating: 295it [02:49,  1.78it/s]Extractor Estimating: 296it [02:50,  1.76it/s]Extractor Estimating: 297it [02:50,  1.75it/s]Extractor Estimating: 298it [02:51,  1.79it/s]Extractor Estimating: 299it [02:52,  1.73it/s]Extractor Estimating: 300it [02:52,  1.70it/s]Extractor Estimating: 301it [02:53,  1.72it/s]Extractor Estimating: 302it [02:53,  1.75it/s]Extractor Estimating: 303it [02:54,  1.78it/s]Extractor Estimating: 304it [02:54,  1.83it/s]Extractor Estimating: 305it [02:55,  1.86it/s]Extractor Estimating: 306it [02:56,  1.71it/s]Extractor Estimating: 307it [02:56,  1.78it/s]Extractor Estimating: 308it [02:57,  1.83it/s]Extractor Estimating: 309it [02:57,  1.83it/s]Extractor Estimating: 310it [02:58,  1.85it/s]Extractor Estimating: 311it [02:58,  1.79it/s]Extractor Estimating: 312it [02:59,  1.84it/s]Extractor Estimating: 313it [02:59,  1.81it/s]Extractor Estimating: 314it [03:00,  1.86it/s]Extractor Estimating: 315it [03:00,  1.86it/s]Extractor Estimating: 316it [03:01,  1.82it/s]Extractor Estimating: 317it [03:01,  1.84it/s]Extractor Estimating: 318it [03:02,  1.90it/s]Extractor Estimating: 319it [03:02,  1.91it/s]Extractor Estimating: 320it [03:03,  1.91it/s]Extractor Estimating: 321it [03:04,  1.87it/s]Extractor Estimating: 322it [03:04,  1.86it/s]Extractor Estimating: 323it [03:05,  1.94it/s]Extractor Estimating: 324it [03:05,  1.84it/s]Extractor Estimating: 325it [03:06,  1.81it/s]Extractor Estimating: 326it [03:06,  1.83it/s]Extractor Estimating: 327it [03:07,  1.92it/s]Extractor Estimating: 328it [03:07,  1.93it/s]Extractor Estimating: 329it [03:08,  1.96it/s]Extractor Estimating: 330it [03:08,  1.95it/s]Extractor Estimating: 331it [03:09,  1.95it/s]Extractor Estimating: 332it [03:09,  2.00it/s]Extractor Estimating: 333it [03:10,  2.03it/s]Extractor Estimating: 334it [03:10,  2.04it/s]Extractor Estimating: 335it [03:11,  1.98it/s]Extractor Estimating: 336it [03:11,  1.91it/s]Extractor Estimating: 337it [03:12,  1.90it/s]Extractor Estimating: 338it [03:12,  1.95it/s]Extractor Estimating: 339it [03:13,  1.97it/s]Extractor Estimating: 340it [03:13,  1.91it/s]Extractor Estimating: 341it [03:14,  1.98it/s]Extractor Estimating: 342it [03:14,  2.00it/s]Extractor Estimating: 343it [03:15,  1.93it/s]Extractor Estimating: 344it [03:15,  1.98it/s]Extractor Estimating: 345it [03:16,  2.02it/s]Extractor Estimating: 346it [03:16,  2.05it/s]Extractor Estimating: 347it [03:17,  1.89it/s]Extractor Estimating: 348it [03:17,  1.97it/s]Extractor Estimating: 349it [03:18,  1.99it/s]Extractor Estimating: 350it [03:18,  1.98it/s]Extractor Estimating: 351it [03:19,  1.96it/s]Extractor Estimating: 352it [03:19,  1.86it/s]Extractor Estimating: 353it [03:20,  1.76it/s]Extractor Estimating: 354it [03:21,  1.85it/s]Extractor Estimating: 355it [03:21,  1.88it/s]Extractor Estimating: 356it [03:22,  1.91it/s]Extractor Estimating: 357it [03:22,  1.90it/s]Extractor Estimating: 358it [03:23,  1.89it/s]Extractor Estimating: 359it [03:23,  1.82it/s]Extractor Estimating: 360it [03:24,  1.82it/s]Extractor Estimating: 361it [03:24,  1.79it/s]Extractor Estimating: 362it [03:25,  1.83it/s]Extractor Estimating: 363it [03:25,  1.84it/s]Extractor Estimating: 364it [03:26,  1.83it/s]Extractor Estimating: 365it [03:27,  1.74it/s]Extractor Estimating: 366it [03:27,  1.82it/s]Extractor Estimating: 367it [03:28,  1.76it/s]Extractor Estimating: 368it [03:28,  1.86it/s]Extractor Estimating: 369it [03:29,  1.85it/s]Extractor Estimating: 370it [03:29,  1.86it/s]Extractor Estimating: 371it [03:30,  1.79it/s]Extractor Estimating: 372it [03:30,  1.85it/s]Extractor Estimating: 373it [03:31,  1.74it/s]Extractor Estimating: 374it [03:32,  1.79it/s]Extractor Estimating: 375it [03:32,  1.84it/s]Extractor Estimating: 376it [03:33,  1.83it/s]Extractor Estimating: 377it [03:33,  1.82it/s]Extractor Estimating: 378it [03:34,  1.83it/s]Extractor Estimating: 379it [03:34,  1.89it/s]Extractor Estimating: 380it [03:35,  1.82it/s]Extractor Estimating: 381it [03:35,  1.83it/s]Extractor Estimating: 382it [03:36,  1.91it/s]Extractor Estimating: 383it [03:36,  1.86it/s]Extractor Estimating: 384it [03:37,  1.87it/s]Extractor Estimating: 385it [03:37,  1.85it/s]Extractor Estimating: 386it [03:38,  1.82it/s]Extractor Estimating: 387it [03:39,  1.83it/s]Extractor Estimating: 388it [03:39,  1.83it/s]Extractor Estimating: 389it [03:40,  1.81it/s]Extractor Estimating: 390it [03:40,  1.80it/s]Extractor Estimating: 391it [03:41,  1.85it/s]Extractor Estimating: 392it [03:41,  1.83it/s]Extractor Estimating: 393it [03:42,  1.82it/s]Extractor Estimating: 394it [03:42,  1.79it/s]Extractor Estimating: 395it [03:43,  1.84it/s]Extractor Estimating: 396it [03:44,  1.87it/s]Extractor Estimating: 397it [03:44,  1.82it/s]Extractor Estimating: 398it [03:45,  1.85it/s]Extractor Estimating: 399it [03:45,  1.81it/s]Extractor Estimating: 400it [03:46,  1.78it/s]Extractor Estimating: 401it [03:46,  1.79it/s]Extractor Estimating: 402it [03:47,  1.81it/s]Extractor Estimating: 403it [03:47,  1.79it/s]Extractor Estimating: 404it [03:48,  1.83it/s]Extractor Estimating: 405it [03:48,  1.88it/s]Extractor Estimating: 406it [03:49,  1.85it/s]Extractor Estimating: 407it [03:50,  1.68it/s]Extractor Estimating: 408it [03:50,  1.75it/s]Extractor Estimating: 409it [03:51,  1.71it/s]Extractor Estimating: 410it [03:51,  1.80it/s]Extractor Estimating: 411it [03:52,  1.82it/s]Extractor Estimating: 412it [03:52,  1.82it/s]Extractor Estimating: 413it [03:53,  1.81it/s]Extractor Estimating: 414it [03:54,  1.78it/s]Extractor Estimating: 415it [03:54,  1.75it/s]Extractor Estimating: 416it [03:55,  1.76it/s]Extractor Estimating: 417it [03:55,  1.73it/s]Extractor Estimating: 418it [03:56,  1.74it/s]Extractor Estimating: 419it [03:56,  1.78it/s]Extractor Estimating: 420it [03:57,  1.78it/s]Extractor Estimating: 421it [03:58,  1.80it/s]Extractor Estimating: 422it [03:58,  1.84it/s]Extractor Estimating: 423it [03:59,  1.84it/s]Extractor Estimating: 424it [03:59,  1.88it/s]Extractor Estimating: 425it [04:00,  1.76it/s]Extractor Estimating: 426it [04:00,  1.74it/s]Extractor Estimating: 427it [04:01,  1.66it/s]Extractor Estimating: 428it [04:02,  1.66it/s]Extractor Estimating: 429it [04:02,  1.59it/s]Extractor Estimating: 430it [04:03,  1.61it/s]Extractor Estimating: 431it [04:04,  1.62it/s]Extractor Estimating: 432it [04:04,  1.58it/s]Extractor Estimating: 433it [04:05,  1.61it/s]Extractor Estimating: 434it [04:05,  1.63it/s]Extractor Estimating: 435it [04:06,  1.61it/s]Extractor Estimating: 436it [04:07,  1.64it/s]Extractor Estimating: 437it [04:07,  1.66it/s]Extractor Estimating: 438it [04:08,  1.66it/s]Extractor Estimating: 439it [04:08,  1.61it/s]Extractor Estimating: 440it [04:09,  1.57it/s]Extractor Estimating: 441it [04:10,  1.61it/s]Extractor Estimating: 442it [04:10,  1.60it/s]Extractor Estimating: 443it [04:11,  1.64it/s]Extractor Estimating: 444it [04:12,  1.66it/s]Extractor Estimating: 445it [04:12,  1.65it/s]Extractor Estimating: 446it [04:13,  1.65it/s]Extractor Estimating: 447it [04:13,  1.65it/s]Extractor Estimating: 448it [04:14,  1.67it/s]Extractor Estimating: 449it [04:15,  1.66it/s]Extractor Estimating: 450it [04:15,  1.65it/s]Extractor Estimating: 451it [04:16,  1.68it/s]Extractor Estimating: 452it [04:16,  1.79it/s]Extractor Estimating: 453it [04:17,  1.75it/s]Extractor Estimating: 454it [04:17,  1.72it/s]Extractor Estimating: 455it [04:18,  1.82it/s]Extractor Estimating: 456it [04:18,  1.86it/s]Extractor Estimating: 457it [04:19,  1.90it/s]Extractor Estimating: 458it [04:19,  1.91it/s]Extractor Estimating: 459it [04:20,  1.93it/s]Extractor Estimating: 460it [04:20,  1.92it/s]Extractor Estimating: 461it [04:21,  1.91it/s]Extractor Estimating: 462it [04:22,  1.88it/s]Extractor Estimating: 463it [04:22,  1.88it/s]Extractor Estimating: 464it [04:23,  1.95it/s]Extractor Estimating: 465it [04:23,  2.04it/s]Extractor Estimating: 466it [04:24,  1.87it/s]Extractor Estimating: 467it [04:24,  1.89it/s]Extractor Estimating: 468it [04:25,  1.84it/s]Extractor Estimating: 469it [04:25,  1.87it/s]Extractor Estimating: 470it [04:26,  1.91it/s]Extractor Estimating: 471it [04:26,  1.89it/s]Extractor Estimating: 472it [04:27,  1.91it/s]Extractor Estimating: 473it [04:27,  1.96it/s]Extractor Estimating: 474it [04:28,  1.93it/s]Extractor Estimating: 475it [04:28,  1.93it/s]Extractor Estimating: 476it [04:29,  1.75it/s]Extractor Estimating: 477it [04:30,  1.69it/s]Extractor Estimating: 478it [04:30,  1.62it/s]Extractor Estimating: 479it [04:31,  1.58it/s]Extractor Estimating: 480it [04:32,  1.58it/s]Extractor Estimating: 481it [04:32,  1.64it/s]Extractor Estimating: 482it [04:33,  1.61it/s]Extractor Estimating: 483it [04:33,  1.62it/s]Extractor Estimating: 484it [04:34,  1.60it/s]Extractor Estimating: 485it [04:35,  1.62it/s]Extractor Estimating: 486it [04:35,  1.59it/s]Extractor Estimating: 487it [04:36,  1.61it/s]Extractor Estimating: 488it [04:37,  1.62it/s]Extractor Estimating: 489it [04:37,  1.60it/s]Extractor Estimating: 490it [04:38,  1.59it/s]Extractor Estimating: 491it [04:38,  1.59it/s]Extractor Estimating: 492it [04:39,  1.58it/s]Extractor Estimating: 493it [04:40,  1.58it/s]Extractor Estimating: 494it [04:40,  1.53it/s]Extractor Estimating: 495it [04:41,  1.53it/s]Extractor Estimating: 496it [04:42,  1.55it/s]Extractor Estimating: 497it [04:42,  1.50it/s]Extractor Estimating: 498it [04:43,  1.51it/s]Extractor Estimating: 499it [04:44,  1.54it/s]Extractor Estimating: 500it [04:44,  1.73it/s]Extractor Estimating: 500it [04:44,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:50,084 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:50,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:50,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:50,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:50,100 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:40:50,843 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:40:50,844 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:40:51,437 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:40:52,536 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:40:52,537 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:55,525 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:55,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:55,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:55,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:40:55,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:40:56,332 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:40:56,333 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:40:56,932 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:40:57,129 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:40:57,130 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:31:06,113 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:31:06,596 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9993 mean pseudo reward: 0.9445733827202357
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 16493
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16593, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16593, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.954, loss:545.1335
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.975, loss:562.2142
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.957, loss:537.0064
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.978, loss:519.9972
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.977, loss:483.4400
>> valid entity prec:0.5571, rec:0.5485, f1:0.5528
>> valid relation prec:0.1050, rec:0.0359, f1:0.0535
>> valid relation with NER prec:0.1050, rec:0.0359, f1:0.0535
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.256, loss:509.5392
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.980, loss:529.6359
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.979, loss:520.5345
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.952, loss:469.3106
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.955, loss:499.8956
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5450, rec:0.5633, f1:0.5540
>> valid relation prec:0.1611, rec:0.0689, f1:0.0965
>> valid relation with NER prec:0.1611, rec:0.0689, f1:0.0965
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.266, loss:510.8546
g_step 1200, step 366, avg_time 0.953, loss:522.6356
g_step 1300, step 49, avg_time 0.984, loss:513.0746
g_step 1400, step 149, avg_time 0.968, loss:464.6836
g_step 1500, step 249, avg_time 0.958, loss:488.0458
>> valid entity prec:0.5396, rec:0.5895, f1:0.5634
>> valid relation prec:0.1122, rec:0.0396, f1:0.0586
>> valid relation with NER prec:0.1122, rec:0.0396, f1:0.0586
new max entity f1 on valid!
g_step 1600, step 349, avg_time 2.255, loss:486.1726
g_step 1700, step 32, avg_time 0.974, loss:504.5956
g_step 1800, step 132, avg_time 0.984, loss:458.6335
g_step 1900, step 232, avg_time 0.967, loss:454.9672
g_step 2000, step 332, avg_time 0.954, loss:458.0312
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5427, rec:0.5324, f1:0.5375
>> valid relation prec:0.1236, rec:0.0451, f1:0.0661
>> valid relation with NER prec:0.1236, rec:0.0451, f1:0.0661
g_step 2100, step 15, avg_time 2.256, loss:483.7859
g_step 2200, step 115, avg_time 0.963, loss:407.6958
g_step 2300, step 215, avg_time 1.000, loss:454.9048
g_step 2400, step 315, avg_time 0.986, loss:432.8438
g_step 2500, step 415, avg_time 0.983, loss:451.3550
>> valid entity prec:0.5531, rec:0.4915, f1:0.5205
>> valid relation prec:0.0934, rec:0.0316, f1:0.0472
>> valid relation with NER prec:0.0934, rec:0.0316, f1:0.0472
g_step 2600, step 98, avg_time 2.258, loss:399.9246
g_step 2700, step 198, avg_time 0.960, loss:414.8350
g_step 2800, step 298, avg_time 0.953, loss:439.0683
g_step 2900, step 398, avg_time 0.965, loss:428.9009
g_step 3000, step 81, avg_time 0.972, loss:426.3372
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5408, rec:0.5785, f1:0.5590
>> valid relation prec:0.1245, rec:0.0491, f1:0.0704
>> valid relation with NER prec:0.1245, rec:0.0491, f1:0.0704
g_step 3100, step 181, avg_time 2.241, loss:382.5309
g_step 3200, step 281, avg_time 0.951, loss:413.2009
g_step 3300, step 381, avg_time 0.945, loss:410.7127
g_step 3400, step 64, avg_time 0.949, loss:378.4050
g_step 3500, step 164, avg_time 0.961, loss:378.6326
>> valid entity prec:0.5447, rec:0.5559, f1:0.5502
>> valid relation prec:0.0867, rec:0.0313, f1:0.0460
>> valid relation with NER prec:0.0867, rec:0.0313, f1:0.0460
g_step 3600, step 264, avg_time 2.237, loss:385.2256
g_step 3700, step 364, avg_time 0.972, loss:404.9489
g_step 3800, step 47, avg_time 0.971, loss:379.1076
g_step 3900, step 147, avg_time 0.966, loss:381.6225
g_step 4000, step 247, avg_time 0.973, loss:382.9582
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5398, rec:0.5227, f1:0.5311
>> valid relation prec:0.0820, rec:0.0327, f1:0.0468
>> valid relation with NER prec:0.0820, rec:0.0327, f1:0.0468
g_step 4100, step 347, avg_time 2.276, loss:381.4060
g_step 4200, step 30, avg_time 1.007, loss:376.9855
g_step 4300, step 130, avg_time 0.994, loss:338.9673
g_step 4400, step 230, avg_time 0.971, loss:359.8732
g_step 4500, step 330, avg_time 0.998, loss:356.3436
>> valid entity prec:0.5590, rec:0.5482, f1:0.5536
>> valid relation prec:0.1076, rec:0.0451, f1:0.0635
>> valid relation with NER prec:0.1076, rec:0.0451, f1:0.0635
g_step 4600, step 13, avg_time 2.261, loss:373.7027
g_step 4700, step 113, avg_time 0.994, loss:343.3535
g_step 4800, step 213, avg_time 0.983, loss:362.3168
g_step 4900, step 313, avg_time 0.995, loss:357.6428
g_step 5000, step 413, avg_time 0.988, loss:378.6379
learning rate was adjusted to 0.0008
>> valid entity prec:0.5443, rec:0.5181, f1:0.5309
>> valid relation prec:0.0718, rec:0.0281, f1:0.0404
>> valid relation with NER prec:0.0718, rec:0.0281, f1:0.0404
g_step 5100, step 96, avg_time 2.264, loss:322.1817
g_step 5200, step 196, avg_time 0.999, loss:325.8726
g_step 5300, step 296, avg_time 0.985, loss:363.9320
g_step 5400, step 396, avg_time 0.992, loss:343.2017
g_step 5500, step 79, avg_time 0.976, loss:326.8144
>> valid entity prec:0.5467, rec:0.5297, f1:0.5380
>> valid relation prec:0.1030, rec:0.0388, f1:0.0563
>> valid relation with NER prec:0.1030, rec:0.0388, f1:0.0563
g_step 5600, step 179, avg_time 2.253, loss:308.7191
g_step 5700, step 279, avg_time 0.988, loss:347.9597
g_step 5800, step 379, avg_time 0.990, loss:347.1988
g_step 5900, step 62, avg_time 0.982, loss:309.2593
g_step 6000, step 162, avg_time 0.991, loss:315.1723
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5571, rec:0.5291, f1:0.5427
>> valid relation prec:0.0789, rec:0.0356, f1:0.0491
>> valid relation with NER prec:0.0789, rec:0.0356, f1:0.0491
g_step 6100, step 262, avg_time 2.270, loss:303.9612
g_step 6200, step 362, avg_time 0.994, loss:316.8715
g_step 6300, step 45, avg_time 0.984, loss:300.6535
g_step 6400, step 145, avg_time 0.975, loss:291.3575
g_step 6500, step 245, avg_time 0.973, loss:317.5618
>> valid entity prec:0.5632, rec:0.4946, f1:0.5267
>> valid relation prec:0.1077, rec:0.0416, f1:0.0601
>> valid relation with NER prec:0.1077, rec:0.0416, f1:0.0601
g_step 6600, step 345, avg_time 2.287, loss:313.1054
g_step 6700, step 28, avg_time 0.968, loss:313.1410
g_step 6800, step 128, avg_time 0.981, loss:296.7092
g_step 6900, step 228, avg_time 0.983, loss:305.3778
g_step 7000, step 328, avg_time 0.990, loss:305.6507
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5663, rec:0.4920, f1:0.5266
>> valid relation prec:0.1101, rec:0.0471, f1:0.0660
>> valid relation with NER prec:0.1101, rec:0.0471, f1:0.0660
g_step 7100, step 11, avg_time 2.284, loss:305.0960
g_step 7200, step 111, avg_time 0.978, loss:250.7757
g_step 7300, step 211, avg_time 0.982, loss:290.6173
g_step 7400, step 311, avg_time 0.969, loss:297.2711
g_step 7500, step 411, avg_time 0.974, loss:286.0182
>> valid entity prec:0.5431, rec:0.5064, f1:0.5241
>> valid relation prec:0.1020, rec:0.0462, f1:0.0636
>> valid relation with NER prec:0.1020, rec:0.0462, f1:0.0636
g_step 7600, step 94, avg_time 2.280, loss:269.6379
g_step 7700, step 194, avg_time 0.985, loss:279.2679
g_step 7800, step 294, avg_time 0.987, loss:275.8049
g_step 7900, step 394, avg_time 0.999, loss:295.3188
g_step 8000, step 77, avg_time 0.986, loss:264.6330
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5616, rec:0.4616, f1:0.5068
>> valid relation prec:0.0892, rec:0.0345, f1:0.0497
>> valid relation with NER prec:0.0892, rec:0.0345, f1:0.0497
g_step 8100, step 177, avg_time 2.266, loss:263.2476
g_step 8200, step 277, avg_time 0.982, loss:288.3431
g_step 8300, step 377, avg_time 0.988, loss:294.6712
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:31:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:31:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-31-06_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:31:08 - WARNING - datasets.builder -   Using custom data configuration default-a5e664756deff01c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a5e664756deff01c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:31:11,076 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:31:11,077 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:31:11,077 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:31:11,078 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:31:11,193 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,256 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,256 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,256 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,256 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,257 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:31:11,257 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:31:11,694 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:31:14,841 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:31:14,897 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a5e664756deff01c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:04,  2.06ba/s] 20%|██        | 2/10 [00:00<00:02,  3.16ba/s] 30%|███       | 3/10 [00:00<00:01,  3.81ba/s] 40%|████      | 4/10 [00:01<00:01,  4.23ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.50ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.70ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.83ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.90ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.96ba/s]100%|██████████| 10/10 [00:02<00:00,  5.00ba/s]100%|██████████| 10/10 [00:02<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.43ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.82ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.40ba/s]100%|██████████| 4/4 [00:01<00:00,  4.50ba/s]100%|██████████| 4/4 [00:01<00:00,  3.77ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.45ba/s] 30%|███       | 3/10 [00:00<00:01,  6.87ba/s] 50%|█████     | 5/10 [00:00<00:00,  8.57ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.40ba/s] 90%|█████████ | 9/10 [00:01<00:00,  7.83ba/s]100%|██████████| 10/10 [00:01<00:00,  7.92ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.06ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.07ba/s]100%|██████████| 4/4 [00:00<00:00, 10.15ba/s]
[INFO|trainer.py:414] 2023-08-29 09:31:22,019 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:31:22,075 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:31:22,076 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 09:31:22,076 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:31:22,076 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:31:22,076 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:31:22,076 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:31:22,076 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:55,  3.31it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:47,  3.41it/s]  1%|          | 4/780 [00:01<03:46,  3.43it/s]  1%|          | 5/780 [00:01<03:45,  3.43it/s]  1%|          | 6/780 [00:01<03:46,  3.42it/s]  1%|          | 7/780 [00:02<03:46,  3.42it/s]  1%|          | 8/780 [00:02<03:46,  3.41it/s]  1%|          | 9/780 [00:02<03:48,  3.37it/s]  1%|▏         | 10/780 [00:02<03:47,  3.38it/s]  1%|▏         | 11/780 [00:03<03:46,  3.39it/s]  2%|▏         | 12/780 [00:03<03:46,  3.39it/s]  2%|▏         | 13/780 [00:03<03:45,  3.39it/s]  2%|▏         | 14/780 [00:04<03:45,  3.40it/s]  2%|▏         | 15/780 [00:04<03:45,  3.40it/s]  2%|▏         | 16/780 [00:04<03:44,  3.40it/s]  2%|▏         | 17/780 [00:04<03:44,  3.40it/s]  2%|▏         | 18/780 [00:05<03:43,  3.40it/s]  2%|▏         | 19/780 [00:05<03:43,  3.40it/s]  3%|▎         | 20/780 [00:05<03:49,  3.31it/s]  3%|▎         | 21/780 [00:06<03:47,  3.33it/s]  3%|▎         | 22/780 [00:06<03:46,  3.35it/s]  3%|▎         | 23/780 [00:06<03:44,  3.37it/s]  3%|▎         | 24/780 [00:07<03:43,  3.38it/s]  3%|▎         | 25/780 [00:07<03:43,  3.38it/s]  3%|▎         | 26/780 [00:07<03:42,  3.39it/s]  3%|▎         | 27/780 [00:07<03:41,  3.40it/s]  4%|▎         | 28/780 [00:08<03:41,  3.40it/s]  4%|▎         | 29/780 [00:08<03:41,  3.40it/s]  4%|▍         | 30/780 [00:08<03:40,  3.40it/s]  4%|▍         | 31/780 [00:09<03:49,  3.27it/s]  4%|▍         | 32/780 [00:09<03:46,  3.31it/s]  4%|▍         | 33/780 [00:09<03:43,  3.34it/s]  4%|▍         | 34/780 [00:10<03:42,  3.36it/s]  4%|▍         | 35/780 [00:10<03:41,  3.37it/s]  5%|▍         | 36/780 [00:10<03:40,  3.38it/s]  5%|▍         | 37/780 [00:10<03:39,  3.39it/s]  5%|▍         | 38/780 [00:11<03:38,  3.39it/s]  5%|▌         | 39/780 [00:11<03:38,  3.39it/s]  5%|▌         | 40/780 [00:11<03:38,  3.39it/s]  5%|▌         | 41/780 [00:12<03:37,  3.40it/s]  5%|▌         | 42/780 [00:12<03:45,  3.28it/s]  6%|▌         | 43/780 [00:12<03:42,  3.31it/s]  6%|▌         | 44/780 [00:13<03:40,  3.34it/s]  6%|▌         | 45/780 [00:13<03:39,  3.35it/s]  6%|▌         | 46/780 [00:13<03:38,  3.37it/s]  6%|▌         | 47/780 [00:13<03:37,  3.38it/s]  6%|▌         | 48/780 [00:14<03:36,  3.38it/s]  6%|▋         | 49/780 [00:14<03:36,  3.38it/s]  6%|▋         | 50/780 [00:14<03:35,  3.39it/s]  7%|▋         | 51/780 [00:15<03:35,  3.39it/s]  7%|▋         | 52/780 [00:15<03:34,  3.39it/s]  7%|▋         | 53/780 [00:15<03:34,  3.39it/s]  7%|▋         | 54/780 [00:15<03:34,  3.39it/s]  7%|▋         | 55/780 [00:16<03:33,  3.39it/s]  7%|▋         | 56/780 [00:16<03:33,  3.39it/s]  7%|▋         | 57/780 [00:16<03:33,  3.39it/s]  7%|▋         | 58/780 [00:17<03:32,  3.40it/s]  8%|▊         | 59/780 [00:17<03:32,  3.40it/s]  8%|▊         | 60/780 [00:17<03:32,  3.39it/s]  8%|▊         | 61/780 [00:18<03:36,  3.31it/s]  8%|▊         | 62/780 [00:18<03:35,  3.34it/s]  8%|▊         | 63/780 [00:18<03:34,  3.35it/s]  8%|▊         | 64/780 [00:18<03:33,  3.36it/s]  8%|▊         | 65/780 [00:19<03:32,  3.37it/s]  8%|▊         | 66/780 [00:19<03:31,  3.37it/s]  9%|▊         | 67/780 [00:19<03:30,  3.38it/s]  9%|▊         | 68/780 [00:20<03:30,  3.38it/s]  9%|▉         | 69/780 [00:20<03:30,  3.38it/s]  9%|▉         | 70/780 [00:20<03:29,  3.39it/s]  9%|▉         | 71/780 [00:21<03:29,  3.39it/s]  9%|▉         | 72/780 [00:21<03:32,  3.34it/s]  9%|▉         | 73/780 [00:21<03:30,  3.35it/s]  9%|▉         | 74/780 [00:21<03:29,  3.37it/s] 10%|▉         | 75/780 [00:22<03:28,  3.37it/s] 10%|▉         | 76/780 [00:22<03:28,  3.38it/s] 10%|▉         | 77/780 [00:22<03:27,  3.38it/s] 10%|█         | 78/780 [00:23<03:27,  3.39it/s] 10%|█         | 79/780 [00:23<03:26,  3.39it/s] 10%|█         | 80/780 [00:23<03:26,  3.39it/s] 10%|█         | 81/780 [00:23<03:26,  3.39it/s] 11%|█         | 82/780 [00:24<03:25,  3.39it/s] 11%|█         | 83/780 [00:24<03:29,  3.33it/s] 11%|█         | 84/780 [00:24<03:27,  3.35it/s] 11%|█         | 85/780 [00:25<03:26,  3.36it/s] 11%|█         | 86/780 [00:25<03:25,  3.37it/s] 11%|█         | 87/780 [00:25<03:25,  3.38it/s] 11%|█▏        | 88/780 [00:26<03:24,  3.38it/s] 11%|█▏        | 89/780 [00:26<03:24,  3.39it/s] 12%|█▏        | 90/780 [00:26<03:23,  3.39it/s] 12%|█▏        | 91/780 [00:26<03:23,  3.39it/s] 12%|█▏        | 92/780 [00:27<03:23,  3.39it/s] 12%|█▏        | 93/780 [00:27<03:22,  3.39it/s] 12%|█▏        | 94/780 [00:27<03:24,  3.35it/s] 12%|█▏        | 95/780 [00:28<03:23,  3.36it/s] 12%|█▏        | 96/780 [00:28<03:22,  3.37it/s] 12%|█▏        | 97/780 [00:28<03:22,  3.38it/s] 13%|█▎        | 98/780 [00:29<03:21,  3.38it/s] 13%|█▎        | 99/780 [00:29<03:21,  3.39it/s] 13%|█▎        | 100/780 [00:29<03:20,  3.39it/s] 13%|█▎        | 101/780 [00:29<03:20,  3.39it/s] 13%|█▎        | 102/780 [00:30<03:20,  3.39it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.39it/s] 13%|█▎        | 104/780 [00:30<03:19,  3.39it/s] 13%|█▎        | 105/780 [00:31<03:24,  3.30it/s] 14%|█▎        | 106/780 [00:31<03:22,  3.32it/s] 14%|█▎        | 107/780 [00:31<03:21,  3.34it/s] 14%|█▍        | 108/780 [00:32<03:20,  3.36it/s] 14%|█▍        | 109/780 [00:32<03:19,  3.37it/s] 14%|█▍        | 110/780 [00:32<03:18,  3.37it/s] 14%|█▍        | 111/780 [00:32<03:17,  3.38it/s] 14%|█▍        | 112/780 [00:33<03:17,  3.38it/s] 14%|█▍        | 113/780 [00:33<03:16,  3.39it/s] 15%|█▍        | 114/780 [00:33<03:16,  3.39it/s] 15%|█▍        | 115/780 [00:34<03:16,  3.39it/s] 15%|█▍        | 116/780 [00:34<03:20,  3.32it/s] 15%|█▌        | 117/780 [00:34<03:18,  3.34it/s] 15%|█▌        | 118/780 [00:34<03:17,  3.35it/s] 15%|█▌        | 119/780 [00:35<03:16,  3.36it/s] 15%|█▌        | 120/780 [00:35<03:15,  3.37it/s] 16%|█▌        | 121/780 [00:35<03:15,  3.38it/s] 16%|█▌        | 122/780 [00:36<03:14,  3.38it/s] 16%|█▌        | 123/780 [00:36<03:14,  3.38it/s] 16%|█▌        | 124/780 [00:36<03:13,  3.38it/s] 16%|█▌        | 125/780 [00:37<03:13,  3.39it/s] 16%|█▌        | 126/780 [00:37<03:13,  3.39it/s] 16%|█▋        | 127/780 [00:37<03:17,  3.30it/s] 16%|█▋        | 128/780 [00:37<03:16,  3.33it/s] 17%|█▋        | 129/780 [00:38<03:14,  3.34it/s] 17%|█▋        | 130/780 [00:38<03:13,  3.36it/s] 17%|█▋        | 131/780 [00:38<03:12,  3.37it/s] 17%|█▋        | 132/780 [00:39<03:12,  3.37it/s] 17%|█▋        | 133/780 [00:39<03:11,  3.38it/s] 17%|█▋        | 134/780 [00:39<03:10,  3.38it/s] 17%|█▋        | 135/780 [00:40<03:10,  3.38it/s] 17%|█▋        | 136/780 [00:40<03:10,  3.38it/s] 18%|█▊        | 137/780 [00:40<03:10,  3.38it/s] 18%|█▊        | 138/780 [00:40<03:14,  3.30it/s] 18%|█▊        | 139/780 [00:41<03:12,  3.33it/s] 18%|█▊        | 140/780 [00:41<03:10,  3.35it/s] 18%|█▊        | 141/780 [00:41<03:10,  3.36it/s] 18%|█▊        | 142/780 [00:42<03:09,  3.36it/s] 18%|█▊        | 143/780 [00:42<03:08,  3.37it/s] 18%|█▊        | 144/780 [00:42<03:08,  3.38it/s] 19%|█▊        | 145/780 [00:42<03:07,  3.38it/s] 19%|█▊        | 146/780 [00:43<03:07,  3.38it/s] 19%|█▉        | 147/780 [00:43<03:07,  3.38it/s] 19%|█▉        | 148/780 [00:43<03:06,  3.38it/s] 19%|█▉        | 149/780 [00:44<03:14,  3.24it/s] 19%|█▉        | 150/780 [00:44<03:31,  2.97it/s] 19%|█▉        | 151/780 [00:44<03:23,  3.09it/s] 19%|█▉        | 152/780 [00:45<03:18,  3.17it/s] 20%|█▉        | 153/780 [00:45<03:14,  3.23it/s] 20%|█▉        | 154/780 [00:45<03:11,  3.28it/s] 20%|█▉        | 155/780 [00:46<03:08,  3.31it/s] 20%|██        | 156/780 [00:46<03:07,  3.34it/s][INFO|trainer.py:2140] 2023-08-29 09:32:08,514 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:32:08,514 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:32:08,514 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.62it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.29it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.45it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.08it/s][A
  7%|▋         | 32/436 [00:00<00:08, 44.91it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.81it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.68it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.66it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.62it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.51it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.39it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.34it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.42it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.41it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.46it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 41.25it/s][A
 21%|██        | 92/436 [00:02<00:08, 42.26it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.04it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.39it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.75it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 43.98it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.07it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.16it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 43.97it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.03it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.14it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.42it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.53it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.46it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.44it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.28it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.16it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.25it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.40it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.62it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.61it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.53it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.44it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.29it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.28it/s][A
 51%|█████     | 222/436 [00:05<00:05, 40.92it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 42.13it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 42.99it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.63it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.04it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.14it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.12it/s][A
 60%|██████    | 262/436 [00:05<00:03, 43.84it/s][A
 61%|██████    | 267/436 [00:06<00:03, 43.90it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.05it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.32it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.58it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.64it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.74it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.65it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.41it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.14it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.00it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.10it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.45it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.60it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.73it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.72it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.63it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.27it/s][A
 81%|████████  | 352/436 [00:08<00:01, 44.10it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 40.08it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 41.50it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 42.44it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.21it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 43.83it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.16it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.14it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.05it/s][A
 91%|█████████ | 397/436 [00:09<00:00, 43.75it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 43.82it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.00it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.31it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.52it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.62it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.80it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.64it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.64it/s][A 20%|██        | 156/780 [00:56<03:07,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:32:19,034 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 09:32:19,505 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:32:25,493 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:32:25,578 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:32:25,609 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:10<1:15:51,  7.31s/it] 20%|██        | 158/780 [01:10<53:57,  5.20s/it]   20%|██        | 159/780 [01:10<38:36,  3.73s/it] 21%|██        | 160/780 [01:10<27:52,  2.70s/it] 21%|██        | 161/780 [01:11<20:23,  1.98s/it] 21%|██        | 162/780 [01:11<15:08,  1.47s/it] 21%|██        | 163/780 [01:11<11:28,  1.12s/it] 21%|██        | 164/780 [01:12<08:55,  1.15it/s] 21%|██        | 165/780 [01:12<07:07,  1.44it/s] 21%|██▏       | 166/780 [01:12<05:55,  1.73it/s] 21%|██▏       | 167/780 [01:12<05:01,  2.03it/s] 22%|██▏       | 168/780 [01:13<04:24,  2.32it/s] 22%|██▏       | 169/780 [01:13<04:02,  2.52it/s] 22%|██▏       | 170/780 [01:13<03:42,  2.74it/s] 22%|██▏       | 171/780 [01:14<03:28,  2.92it/s] 22%|██▏       | 172/780 [01:14<03:18,  3.06it/s] 22%|██▏       | 173/780 [01:14<03:11,  3.17it/s] 22%|██▏       | 174/780 [01:15<03:06,  3.25it/s] 22%|██▏       | 175/780 [01:15<03:03,  3.30it/s] 23%|██▎       | 176/780 [01:15<03:00,  3.35it/s] 23%|██▎       | 177/780 [01:15<02:58,  3.38it/s] 23%|██▎       | 178/780 [01:16<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:16<02:56,  3.41it/s] 23%|██▎       | 180/780 [01:16<02:55,  3.42it/s] 23%|██▎       | 181/780 [01:17<02:54,  3.43it/s] 23%|██▎       | 182/780 [01:17<02:54,  3.43it/s] 23%|██▎       | 183/780 [01:17<02:53,  3.44it/s] 24%|██▎       | 184/780 [01:17<02:53,  3.44it/s] 24%|██▎       | 185/780 [01:18<02:52,  3.44it/s] 24%|██▍       | 186/780 [01:18<02:52,  3.44it/s] 24%|██▍       | 187/780 [01:18<02:52,  3.44it/s] 24%|██▍       | 188/780 [01:19<02:52,  3.44it/s] 24%|██▍       | 189/780 [01:19<02:55,  3.37it/s] 24%|██▍       | 190/780 [01:19<02:54,  3.39it/s] 24%|██▍       | 191/780 [01:19<02:53,  3.40it/s] 25%|██▍       | 192/780 [01:20<02:52,  3.42it/s] 25%|██▍       | 193/780 [01:20<02:51,  3.43it/s] 25%|██▍       | 194/780 [01:20<02:50,  3.43it/s] 25%|██▌       | 195/780 [01:21<02:50,  3.44it/s] 25%|██▌       | 196/780 [01:21<02:49,  3.44it/s] 25%|██▌       | 197/780 [01:21<02:49,  3.44it/s] 25%|██▌       | 198/780 [01:22<02:49,  3.44it/s] 26%|██▌       | 199/780 [01:22<02:48,  3.44it/s] 26%|██▌       | 200/780 [01:22<02:50,  3.40it/s] 26%|██▌       | 201/780 [01:22<02:49,  3.41it/s] 26%|██▌       | 202/780 [01:23<02:48,  3.42it/s] 26%|██▌       | 203/780 [01:23<02:48,  3.43it/s] 26%|██▌       | 204/780 [01:23<02:47,  3.43it/s] 26%|██▋       | 205/780 [01:24<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:24<02:47,  3.44it/s] 27%|██▋       | 207/780 [01:24<02:46,  3.44it/s] 27%|██▋       | 208/780 [01:24<02:46,  3.44it/s] 27%|██▋       | 209/780 [01:25<02:46,  3.44it/s] 27%|██▋       | 210/780 [01:25<02:45,  3.44it/s] 27%|██▋       | 211/780 [01:25<02:49,  3.36it/s] 27%|██▋       | 212/780 [01:26<02:47,  3.39it/s] 27%|██▋       | 213/780 [01:26<02:46,  3.40it/s] 27%|██▋       | 214/780 [01:26<02:45,  3.41it/s] 28%|██▊       | 215/780 [01:26<02:45,  3.42it/s] 28%|██▊       | 216/780 [01:27<02:44,  3.42it/s] 28%|██▊       | 217/780 [01:27<02:44,  3.43it/s] 28%|██▊       | 218/780 [01:27<02:43,  3.43it/s] 28%|██▊       | 219/780 [01:28<02:43,  3.43it/s] 28%|██▊       | 220/780 [01:28<02:43,  3.43it/s] 28%|██▊       | 221/780 [01:28<02:42,  3.44it/s] 28%|██▊       | 222/780 [01:29<02:46,  3.35it/s] 29%|██▊       | 223/780 [01:29<02:44,  3.38it/s] 29%|██▊       | 224/780 [01:29<02:43,  3.40it/s] 29%|██▉       | 225/780 [01:29<02:42,  3.41it/s] 29%|██▉       | 226/780 [01:30<02:41,  3.42it/s] 29%|██▉       | 227/780 [01:30<02:41,  3.43it/s] 29%|██▉       | 228/780 [01:30<02:40,  3.43it/s] 29%|██▉       | 229/780 [01:31<02:40,  3.42it/s] 29%|██▉       | 230/780 [01:31<02:40,  3.43it/s] 30%|██▉       | 231/780 [01:31<02:39,  3.43it/s] 30%|██▉       | 232/780 [01:31<02:39,  3.43it/s] 30%|██▉       | 233/780 [01:32<02:45,  3.30it/s] 30%|███       | 234/780 [01:32<02:43,  3.34it/s] 30%|███       | 235/780 [01:32<02:41,  3.37it/s] 30%|███       | 236/780 [01:33<02:40,  3.39it/s] 30%|███       | 237/780 [01:33<02:39,  3.41it/s] 31%|███       | 238/780 [01:33<02:38,  3.42it/s] 31%|███       | 239/780 [01:34<02:38,  3.42it/s] 31%|███       | 240/780 [01:34<02:37,  3.43it/s] 31%|███       | 241/780 [01:34<02:36,  3.43it/s] 31%|███       | 242/780 [01:34<02:36,  3.43it/s] 31%|███       | 243/780 [01:35<02:36,  3.44it/s] 31%|███▏      | 244/780 [01:35<02:41,  3.32it/s] 31%|███▏      | 245/780 [01:35<02:39,  3.36it/s] 32%|███▏      | 246/780 [01:36<02:37,  3.38it/s] 32%|███▏      | 247/780 [01:36<02:36,  3.40it/s] 32%|███▏      | 248/780 [01:36<02:36,  3.41it/s] 32%|███▏      | 249/780 [01:36<02:35,  3.42it/s] 32%|███▏      | 250/780 [01:37<02:34,  3.43it/s] 32%|███▏      | 251/780 [01:37<02:34,  3.43it/s] 32%|███▏      | 252/780 [01:37<02:33,  3.43it/s] 32%|███▏      | 253/780 [01:38<02:33,  3.43it/s] 33%|███▎      | 254/780 [01:38<02:33,  3.44it/s] 33%|███▎      | 255/780 [01:38<02:34,  3.39it/s] 33%|███▎      | 256/780 [01:39<02:33,  3.41it/s] 33%|███▎      | 257/780 [01:39<02:32,  3.42it/s] 33%|███▎      | 258/780 [01:39<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:39<02:31,  3.43it/s] 33%|███▎      | 260/780 [01:40<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:40<02:31,  3.43it/s] 34%|███▎      | 262/780 [01:40<02:30,  3.43it/s] 34%|███▎      | 263/780 [01:41<02:30,  3.44it/s] 34%|███▍      | 264/780 [01:41<02:30,  3.44it/s] 34%|███▍      | 265/780 [01:41<02:29,  3.44it/s] 34%|███▍      | 266/780 [01:41<02:33,  3.35it/s] 34%|███▍      | 267/780 [01:42<02:31,  3.38it/s] 34%|███▍      | 268/780 [01:42<02:31,  3.38it/s] 34%|███▍      | 269/780 [01:42<02:30,  3.40it/s] 35%|███▍      | 270/780 [01:43<02:29,  3.41it/s] 35%|███▍      | 271/780 [01:43<02:28,  3.42it/s] 35%|███▍      | 272/780 [01:43<02:28,  3.43it/s] 35%|███▌      | 273/780 [01:43<02:27,  3.43it/s] 35%|███▌      | 274/780 [01:44<02:37,  3.21it/s] 35%|███▌      | 275/780 [01:44<02:54,  2.90it/s] 35%|███▌      | 276/780 [01:45<02:48,  3.00it/s] 36%|███▌      | 277/780 [01:45<02:41,  3.12it/s] 36%|███▌      | 278/780 [01:45<02:36,  3.21it/s] 36%|███▌      | 279/780 [01:45<02:33,  3.27it/s] 36%|███▌      | 280/780 [01:46<02:30,  3.32it/s] 36%|███▌      | 281/780 [01:46<02:28,  3.35it/s] 36%|███▌      | 282/780 [01:46<02:27,  3.38it/s] 36%|███▋      | 283/780 [01:47<02:26,  3.39it/s] 36%|███▋      | 284/780 [01:47<02:25,  3.41it/s] 37%|███▋      | 285/780 [01:47<02:25,  3.41it/s] 37%|███▋      | 286/780 [01:47<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:48<02:23,  3.43it/s] 37%|███▋      | 288/780 [01:48<02:23,  3.43it/s] 37%|███▋      | 289/780 [01:48<02:23,  3.43it/s] 37%|███▋      | 290/780 [01:49<02:22,  3.43it/s] 37%|███▋      | 291/780 [01:49<02:22,  3.43it/s] 37%|███▋      | 292/780 [01:49<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:50<02:28,  3.29it/s] 38%|███▊      | 294/780 [01:50<02:25,  3.33it/s] 38%|███▊      | 295/780 [01:50<02:24,  3.36it/s] 38%|███▊      | 296/780 [01:50<02:23,  3.38it/s] 38%|███▊      | 297/780 [01:51<02:22,  3.40it/s] 38%|███▊      | 298/780 [01:51<02:21,  3.41it/s] 38%|███▊      | 299/780 [01:51<02:20,  3.42it/s] 38%|███▊      | 300/780 [01:52<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:52<02:19,  3.43it/s] 39%|███▊      | 302/780 [01:52<02:19,  3.43it/s] 39%|███▉      | 303/780 [01:52<02:19,  3.43it/s] 39%|███▉      | 304/780 [01:53<02:25,  3.26it/s] 39%|███▉      | 305/780 [01:53<02:23,  3.31it/s] 39%|███▉      | 306/780 [01:53<02:21,  3.35it/s] 39%|███▉      | 307/780 [01:54<02:20,  3.37it/s] 39%|███▉      | 308/780 [01:54<02:19,  3.39it/s] 40%|███▉      | 309/780 [01:54<02:18,  3.40it/s] 40%|███▉      | 310/780 [01:55<02:17,  3.41it/s] 40%|███▉      | 311/780 [01:55<02:17,  3.42it/s] 40%|████      | 312/780 [01:55<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:33:17,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:33:17,775 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:33:17,775 >>   Batch size = 8
{'eval_loss': 1.231946587562561, 'eval_runtime': 9.9275, 'eval_samples_per_second': 350.744, 'eval_steps_per_second': 43.919, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.43it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.91it/s][A
  4%|▍         | 17/436 [00:00<00:10, 38.85it/s][A
  5%|▌         | 22/436 [00:00<00:10, 40.80it/s][A
  6%|▌         | 27/436 [00:00<00:09, 42.25it/s][A
  7%|▋         | 32/436 [00:00<00:09, 43.07it/s][A
  8%|▊         | 37/436 [00:00<00:09, 43.61it/s][A
 10%|▉         | 42/436 [00:00<00:08, 43.99it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.33it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.81it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.98it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.30it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.44it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.54it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.64it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.63it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.54it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.19it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.04it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.37it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.57it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.68it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.54it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.36it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.08it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 41.39it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 42.41it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.12it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.71it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.08it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.34it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.39it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.25it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 43.78it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.94it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.07it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.21it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.43it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.60it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.64it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.59it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.26it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.13it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.05it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.22it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.37it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.50it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.68it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.64it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.51it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.22it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 43.92it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.03it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.23it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.39it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.49it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.43it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.35it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.35it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.23it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.21it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.19it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.25it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.46it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.62it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.54it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.50it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.21it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.06it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.14it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.15it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.27it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.49it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.62it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.57it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.45it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.34it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.26it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 42.98it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 43.41it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 43.81it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 43.81it/s][A 40%|████      | 312/780 [02:05<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:33:27,814 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 09:33:27,927 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:33:30,939 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:33:31,093 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:33:31,147 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:15<47:31,  6.11s/it] 40%|████      | 314/780 [02:15<33:55,  4.37s/it] 40%|████      | 315/780 [02:15<24:23,  3.15s/it] 41%|████      | 316/780 [02:16<17:42,  2.29s/it] 41%|████      | 317/780 [02:16<13:03,  1.69s/it] 41%|████      | 318/780 [02:16<09:47,  1.27s/it] 41%|████      | 319/780 [02:17<07:31,  1.02it/s] 41%|████      | 320/780 [02:17<05:56,  1.29it/s] 41%|████      | 321/780 [02:17<04:49,  1.59it/s] 41%|████▏     | 322/780 [02:18<04:02,  1.89it/s] 41%|████▏     | 323/780 [02:18<03:29,  2.18it/s] 42%|████▏     | 324/780 [02:18<03:06,  2.44it/s] 42%|████▏     | 325/780 [02:18<02:50,  2.67it/s] 42%|████▏     | 326/780 [02:19<02:39,  2.85it/s] 42%|████▏     | 327/780 [02:19<02:31,  2.99it/s] 42%|████▏     | 328/780 [02:19<02:25,  3.10it/s] 42%|████▏     | 329/780 [02:20<02:21,  3.19it/s] 42%|████▏     | 330/780 [02:20<02:18,  3.25it/s] 42%|████▏     | 331/780 [02:20<02:16,  3.29it/s] 43%|████▎     | 332/780 [02:20<02:18,  3.23it/s] 43%|████▎     | 333/780 [02:21<02:16,  3.28it/s] 43%|████▎     | 334/780 [02:21<02:14,  3.31it/s] 43%|████▎     | 335/780 [02:21<02:13,  3.34it/s] 43%|████▎     | 336/780 [02:22<02:12,  3.36it/s] 43%|████▎     | 337/780 [02:22<02:11,  3.37it/s] 43%|████▎     | 338/780 [02:22<02:10,  3.38it/s] 43%|████▎     | 339/780 [02:23<02:10,  3.38it/s] 44%|████▎     | 340/780 [02:23<02:09,  3.39it/s] 44%|████▎     | 341/780 [02:23<02:09,  3.39it/s] 44%|████▍     | 342/780 [02:23<02:09,  3.39it/s] 44%|████▍     | 343/780 [02:24<02:11,  3.33it/s] 44%|████▍     | 344/780 [02:24<02:10,  3.35it/s] 44%|████▍     | 345/780 [02:24<02:09,  3.36it/s] 44%|████▍     | 346/780 [02:25<02:08,  3.37it/s] 44%|████▍     | 347/780 [02:25<02:08,  3.38it/s] 45%|████▍     | 348/780 [02:25<02:07,  3.38it/s] 45%|████▍     | 349/780 [02:26<02:07,  3.38it/s] 45%|████▍     | 350/780 [02:26<02:07,  3.38it/s] 45%|████▌     | 351/780 [02:26<02:06,  3.39it/s] 45%|████▌     | 352/780 [02:26<02:06,  3.39it/s] 45%|████▌     | 353/780 [02:27<02:05,  3.39it/s] 45%|████▌     | 354/780 [02:27<02:07,  3.33it/s] 46%|████▌     | 355/780 [02:27<02:06,  3.35it/s] 46%|████▌     | 356/780 [02:28<02:05,  3.37it/s] 46%|████▌     | 357/780 [02:28<02:05,  3.37it/s] 46%|████▌     | 358/780 [02:28<02:04,  3.38it/s] 46%|████▌     | 359/780 [02:28<02:04,  3.38it/s] 46%|████▌     | 360/780 [02:29<02:04,  3.39it/s] 46%|████▋     | 361/780 [02:29<02:03,  3.39it/s] 46%|████▋     | 362/780 [02:29<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:30<02:02,  3.39it/s] 47%|████▋     | 364/780 [02:30<02:02,  3.39it/s] 47%|████▋     | 365/780 [02:30<02:04,  3.34it/s] 47%|████▋     | 366/780 [02:31<02:03,  3.36it/s] 47%|████▋     | 367/780 [02:31<02:02,  3.37it/s] 47%|████▋     | 368/780 [02:31<02:02,  3.37it/s] 47%|████▋     | 369/780 [02:31<02:01,  3.38it/s] 47%|████▋     | 370/780 [02:32<02:01,  3.39it/s] 48%|████▊     | 371/780 [02:32<02:00,  3.39it/s] 48%|████▊     | 372/780 [02:32<02:00,  3.39it/s] 48%|████▊     | 373/780 [02:33<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:33<01:59,  3.39it/s] 48%|████▊     | 375/780 [02:33<01:59,  3.39it/s] 48%|████▊     | 376/780 [02:34<02:01,  3.32it/s] 48%|████▊     | 377/780 [02:34<02:00,  3.34it/s] 48%|████▊     | 378/780 [02:34<01:59,  3.36it/s] 49%|████▊     | 379/780 [02:34<01:59,  3.37it/s] 49%|████▊     | 380/780 [02:35<01:58,  3.37it/s] 49%|████▉     | 381/780 [02:35<01:58,  3.38it/s] 49%|████▉     | 382/780 [02:35<01:57,  3.39it/s] 49%|████▉     | 383/780 [02:36<01:57,  3.38it/s] 49%|████▉     | 384/780 [02:36<01:57,  3.38it/s] 49%|████▉     | 385/780 [02:36<01:56,  3.38it/s] 49%|████▉     | 386/780 [02:36<01:56,  3.39it/s] 50%|████▉     | 387/780 [02:37<01:58,  3.33it/s] 50%|████▉     | 388/780 [02:37<01:57,  3.35it/s] 50%|████▉     | 389/780 [02:37<01:56,  3.36it/s] 50%|█████     | 390/780 [02:38<01:55,  3.37it/s] 50%|█████     | 391/780 [02:38<01:55,  3.37it/s] 50%|█████     | 392/780 [02:38<01:54,  3.38it/s] 50%|█████     | 393/780 [02:39<01:54,  3.38it/s] 51%|█████     | 394/780 [02:39<01:54,  3.39it/s] 51%|█████     | 395/780 [02:39<01:53,  3.38it/s] 51%|█████     | 396/780 [02:39<01:53,  3.38it/s] 51%|█████     | 397/780 [02:40<01:53,  3.38it/s] 51%|█████     | 398/780 [02:40<01:56,  3.29it/s] 51%|█████     | 399/780 [02:40<01:54,  3.32it/s] 51%|█████▏    | 400/780 [02:41<01:53,  3.35it/s] 51%|█████▏    | 401/780 [02:41<01:52,  3.36it/s] 52%|█████▏    | 402/780 [02:41<01:52,  3.37it/s] 52%|█████▏    | 403/780 [02:42<01:51,  3.38it/s] 52%|█████▏    | 404/780 [02:42<01:51,  3.38it/s] 52%|█████▏    | 405/780 [02:42<01:50,  3.38it/s] 52%|█████▏    | 406/780 [02:42<01:50,  3.38it/s] 52%|█████▏    | 407/780 [02:43<01:50,  3.39it/s] 52%|█████▏    | 408/780 [02:43<01:49,  3.39it/s] 52%|█████▏    | 409/780 [02:43<01:50,  3.35it/s] 53%|█████▎    | 410/780 [02:44<01:49,  3.36it/s] 53%|█████▎    | 411/780 [02:44<01:57,  3.15it/s] 53%|█████▎    | 412/780 [02:44<02:07,  2.89it/s] 53%|█████▎    | 413/780 [02:45<02:01,  3.02it/s] 53%|█████▎    | 414/780 [02:45<01:57,  3.12it/s] 53%|█████▎    | 415/780 [02:45<01:54,  3.20it/s] 53%|█████▎    | 416/780 [02:46<01:51,  3.26it/s] 53%|█████▎    | 417/780 [02:46<01:50,  3.29it/s] 54%|█████▎    | 418/780 [02:46<01:48,  3.32it/s] 54%|█████▎    | 419/780 [02:46<01:49,  3.28it/s] 54%|█████▍    | 420/780 [02:47<01:48,  3.31it/s] 54%|█████▍    | 421/780 [02:47<01:47,  3.33it/s] 54%|█████▍    | 422/780 [02:47<01:46,  3.35it/s] 54%|█████▍    | 423/780 [02:48<01:46,  3.36it/s] 54%|█████▍    | 424/780 [02:48<01:45,  3.37it/s] 54%|█████▍    | 425/780 [02:48<01:45,  3.37it/s] 55%|█████▍    | 426/780 [02:49<01:44,  3.38it/s] 55%|█████▍    | 427/780 [02:49<01:44,  3.38it/s] 55%|█████▍    | 428/780 [02:49<01:43,  3.39it/s] 55%|█████▌    | 429/780 [02:49<01:43,  3.39it/s] 55%|█████▌    | 430/780 [02:50<01:43,  3.39it/s] 55%|█████▌    | 431/780 [02:50<01:43,  3.39it/s] 55%|█████▌    | 432/780 [02:50<01:42,  3.39it/s] 56%|█████▌    | 433/780 [02:51<01:42,  3.39it/s] 56%|█████▌    | 434/780 [02:51<01:42,  3.39it/s] 56%|█████▌    | 435/780 [02:51<01:45,  3.26it/s] 56%|█████▌    | 436/780 [02:52<01:44,  3.30it/s] 56%|█████▌    | 437/780 [02:52<01:43,  3.33it/s] 56%|█████▌    | 438/780 [02:52<01:42,  3.35it/s] 56%|█████▋    | 439/780 [02:52<01:41,  3.35it/s] 56%|█████▋    | 440/780 [02:53<01:41,  3.37it/s] 57%|█████▋    | 441/780 [02:53<01:40,  3.37it/s] 57%|█████▋    | 442/780 [02:53<01:40,  3.37it/s] 57%|█████▋    | 443/780 [02:54<01:39,  3.38it/s] 57%|█████▋    | 444/780 [02:54<01:39,  3.38it/s] 57%|█████▋    | 445/780 [02:54<01:38,  3.38it/s] 57%|█████▋    | 446/780 [02:55<01:44,  3.18it/s] 57%|█████▋    | 447/780 [02:55<01:42,  3.24it/s] 57%|█████▋    | 448/780 [02:55<01:41,  3.28it/s] 58%|█████▊    | 449/780 [02:55<01:39,  3.32it/s] 58%|█████▊    | 450/780 [02:56<01:38,  3.34it/s] 58%|█████▊    | 451/780 [02:56<01:38,  3.35it/s] 58%|█████▊    | 452/780 [02:56<01:37,  3.36it/s] 58%|█████▊    | 453/780 [02:57<01:37,  3.37it/s] 58%|█████▊    | 454/780 [02:57<01:36,  3.38it/s] 58%|█████▊    | 455/780 [02:57<01:36,  3.38it/s] 58%|█████▊    | 456/780 [02:58<01:43,  3.14it/s] 59%|█████▊    | 457/780 [02:58<01:40,  3.21it/s] 59%|█████▊    | 458/780 [02:58<01:38,  3.26it/s] 59%|█████▉    | 459/780 [02:58<01:37,  3.30it/s] 59%|█████▉    | 460/780 [02:59<01:35,  3.34it/s] 59%|█████▉    | 461/780 [02:59<01:34,  3.37it/s] 59%|█████▉    | 462/780 [02:59<01:33,  3.39it/s] 59%|█████▉    | 463/780 [03:00<01:33,  3.40it/s] 59%|█████▉    | 464/780 [03:00<01:32,  3.41it/s] 60%|█████▉    | 465/780 [03:00<01:32,  3.42it/s] 60%|█████▉    | 466/780 [03:01<01:33,  3.36it/s] 60%|█████▉    | 467/780 [03:01<01:32,  3.38it/s] 60%|██████    | 468/780 [03:01<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 09:34:23,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:34:23,701 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:34:23,701 >>   Batch size = 8
{'eval_loss': 1.2551560401916504, 'eval_runtime': 9.9295, 'eval_samples_per_second': 350.674, 'eval_steps_per_second': 43.91, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.62it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.09it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.99it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.87it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.78it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.56it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.49it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.53it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.43it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.26it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.24it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.25it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.31it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 44.35it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.24it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.43it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 43.52it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 43.84it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.02it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.12it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.20it/s][A
 29%|██▉       | 127/436 [00:02<00:07, 44.07it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.15it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.34it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.34it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.47it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.35it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.45it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.33it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.11it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.37it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.35it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.42it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.43it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.24it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.26it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.25it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.32it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 43.32it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 43.71it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 43.99it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.13it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.24it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.07it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.19it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.22it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.15it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.23it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.40it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.48it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.50it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.49it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.28it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.29it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.23it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.21it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.15it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.29it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.52it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.42it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.33it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.25it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.22it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.25it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.49it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.99it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.26it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.35it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.36it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.23it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.19it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.19it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.13it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.24it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.24it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.50it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.56it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.49it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.49it/s][A 60%|██████    | 468/780 [03:11<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:34:33,837 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 09:34:34,017 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:34:36,965 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:34:37,096 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:34:37,149 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:21<31:50,  6.14s/it] 60%|██████    | 470/780 [03:21<22:40,  4.39s/it] 60%|██████    | 471/780 [03:21<16:16,  3.16s/it] 61%|██████    | 472/780 [03:22<11:51,  2.31s/it] 61%|██████    | 473/780 [03:22<08:43,  1.71s/it] 61%|██████    | 474/780 [03:22<06:32,  1.28s/it] 61%|██████    | 475/780 [03:23<05:00,  1.01it/s] 61%|██████    | 476/780 [03:23<03:56,  1.28it/s] 61%|██████    | 477/780 [03:23<03:11,  1.58it/s] 61%|██████▏   | 478/780 [03:24<02:40,  1.88it/s] 61%|██████▏   | 479/780 [03:24<02:18,  2.17it/s] 62%|██████▏   | 480/780 [03:24<02:03,  2.44it/s] 62%|██████▏   | 481/780 [03:24<01:52,  2.66it/s] 62%|██████▏   | 482/780 [03:25<01:44,  2.85it/s] 62%|██████▏   | 483/780 [03:25<01:40,  2.94it/s] 62%|██████▏   | 484/780 [03:25<01:36,  3.07it/s] 62%|██████▏   | 485/780 [03:26<01:33,  3.16it/s] 62%|██████▏   | 486/780 [03:26<01:31,  3.23it/s] 62%|██████▏   | 487/780 [03:26<01:29,  3.28it/s] 63%|██████▎   | 488/780 [03:27<01:28,  3.31it/s] 63%|██████▎   | 489/780 [03:27<01:27,  3.34it/s] 63%|██████▎   | 490/780 [03:27<01:26,  3.36it/s] 63%|██████▎   | 491/780 [03:27<01:25,  3.37it/s] 63%|██████▎   | 492/780 [03:28<01:25,  3.38it/s] 63%|██████▎   | 493/780 [03:28<01:25,  3.37it/s] 63%|██████▎   | 494/780 [03:28<01:25,  3.33it/s] 63%|██████▎   | 495/780 [03:29<01:25,  3.35it/s] 64%|██████▎   | 496/780 [03:29<01:24,  3.37it/s] 64%|██████▎   | 497/780 [03:29<01:24,  3.37it/s] 64%|██████▍   | 498/780 [03:29<01:23,  3.37it/s] 64%|██████▍   | 499/780 [03:30<01:23,  3.38it/s] 64%|██████▍   | 500/780 [03:30<01:22,  3.39it/s]                                                  64%|██████▍   | 500/780 [03:30<01:22,  3.39it/s] 64%|██████▍   | 501/780 [03:30<01:22,  3.39it/s] 64%|██████▍   | 502/780 [03:31<01:22,  3.39it/s] 64%|██████▍   | 503/780 [03:31<01:21,  3.39it/s] 65%|██████▍   | 504/780 [03:31<01:21,  3.39it/s] 65%|██████▍   | 505/780 [03:32<01:24,  3.25it/s] 65%|██████▍   | 506/780 [03:32<01:23,  3.30it/s] 65%|██████▌   | 507/780 [03:32<01:21,  3.34it/s] 65%|██████▌   | 508/780 [03:32<01:20,  3.37it/s] 65%|██████▌   | 509/780 [03:33<01:20,  3.39it/s] 65%|██████▌   | 510/780 [03:33<01:19,  3.40it/s] 66%|██████▌   | 511/780 [03:33<01:18,  3.41it/s] 66%|██████▌   | 512/780 [03:34<01:18,  3.42it/s] 66%|██████▌   | 513/780 [03:34<01:17,  3.43it/s] 66%|██████▌   | 514/780 [03:34<01:17,  3.43it/s] 66%|██████▌   | 515/780 [03:34<01:17,  3.43it/s] 66%|██████▌   | 516/780 [03:35<01:19,  3.34it/s] 66%|██████▋   | 517/780 [03:35<01:17,  3.37it/s] 66%|██████▋   | 518/780 [03:35<01:17,  3.39it/s] 67%|██████▋   | 519/780 [03:36<01:16,  3.40it/s] 67%|██████▋   | 520/780 [03:36<01:16,  3.41it/s] 67%|██████▋   | 521/780 [03:36<01:15,  3.42it/s] 67%|██████▋   | 522/780 [03:37<01:15,  3.43it/s] 67%|██████▋   | 523/780 [03:37<01:14,  3.43it/s] 67%|██████▋   | 524/780 [03:37<01:14,  3.43it/s] 67%|██████▋   | 525/780 [03:37<01:14,  3.43it/s] 67%|██████▋   | 526/780 [03:38<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:38<01:15,  3.34it/s] 68%|██████▊   | 528/780 [03:38<01:14,  3.37it/s] 68%|██████▊   | 529/780 [03:39<01:14,  3.39it/s] 68%|██████▊   | 530/780 [03:39<01:13,  3.40it/s] 68%|██████▊   | 531/780 [03:39<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:39<01:12,  3.42it/s] 68%|██████▊   | 533/780 [03:40<01:11,  3.43it/s] 68%|██████▊   | 534/780 [03:40<01:11,  3.43it/s] 69%|██████▊   | 535/780 [03:40<01:11,  3.44it/s] 69%|██████▊   | 536/780 [03:41<01:11,  3.43it/s] 69%|██████▉   | 537/780 [03:41<01:10,  3.44it/s] 69%|██████▉   | 538/780 [03:41<01:13,  3.31it/s] 69%|██████▉   | 539/780 [03:42<01:11,  3.35it/s] 69%|██████▉   | 540/780 [03:42<01:11,  3.37it/s] 69%|██████▉   | 541/780 [03:42<01:10,  3.40it/s] 69%|██████▉   | 542/780 [03:42<01:09,  3.41it/s] 70%|██████▉   | 543/780 [03:43<01:09,  3.42it/s] 70%|██████▉   | 544/780 [03:43<01:08,  3.42it/s] 70%|██████▉   | 545/780 [03:43<01:08,  3.42it/s] 70%|███████   | 546/780 [03:44<01:08,  3.43it/s] 70%|███████   | 547/780 [03:44<01:07,  3.43it/s] 70%|███████   | 548/780 [03:44<01:11,  3.27it/s] 70%|███████   | 549/780 [03:45<01:19,  2.89it/s] 71%|███████   | 550/780 [03:45<01:15,  3.04it/s] 71%|███████   | 551/780 [03:45<01:12,  3.14it/s] 71%|███████   | 552/780 [03:46<01:10,  3.23it/s] 71%|███████   | 553/780 [03:46<01:09,  3.29it/s] 71%|███████   | 554/780 [03:46<01:07,  3.33it/s] 71%|███████   | 555/780 [03:46<01:06,  3.36it/s] 71%|███████▏  | 556/780 [03:47<01:06,  3.38it/s] 71%|███████▏  | 557/780 [03:47<01:05,  3.40it/s] 72%|███████▏  | 558/780 [03:47<01:05,  3.41it/s] 72%|███████▏  | 559/780 [03:48<01:07,  3.30it/s] 72%|███████▏  | 560/780 [03:48<01:05,  3.34it/s] 72%|███████▏  | 561/780 [03:48<01:05,  3.36it/s] 72%|███████▏  | 562/780 [03:48<01:04,  3.38it/s] 72%|███████▏  | 563/780 [03:49<01:03,  3.40it/s] 72%|███████▏  | 564/780 [03:49<01:03,  3.41it/s] 72%|███████▏  | 565/780 [03:49<01:02,  3.42it/s] 73%|███████▎  | 566/780 [03:50<01:02,  3.42it/s] 73%|███████▎  | 567/780 [03:50<01:02,  3.43it/s] 73%|███████▎  | 568/780 [03:50<01:01,  3.43it/s] 73%|███████▎  | 569/780 [03:51<01:01,  3.43it/s] 73%|███████▎  | 570/780 [03:51<01:01,  3.44it/s] 73%|███████▎  | 571/780 [03:51<01:00,  3.44it/s] 73%|███████▎  | 572/780 [03:51<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:52<01:00,  3.43it/s] 74%|███████▎  | 574/780 [03:52<01:00,  3.43it/s] 74%|███████▎  | 575/780 [03:52<00:59,  3.43it/s] 74%|███████▍  | 576/780 [03:53<01:01,  3.31it/s] 74%|███████▍  | 577/780 [03:53<01:00,  3.34it/s] 74%|███████▍  | 578/780 [03:53<00:59,  3.37it/s] 74%|███████▍  | 579/780 [03:53<00:59,  3.39it/s] 74%|███████▍  | 580/780 [03:54<00:58,  3.41it/s] 74%|███████▍  | 581/780 [03:54<00:58,  3.42it/s] 75%|███████▍  | 582/780 [03:54<00:57,  3.42it/s] 75%|███████▍  | 583/780 [03:55<00:57,  3.43it/s] 75%|███████▍  | 584/780 [03:55<00:57,  3.43it/s] 75%|███████▌  | 585/780 [03:55<00:56,  3.44it/s] 75%|███████▌  | 586/780 [03:56<00:56,  3.43it/s] 75%|███████▌  | 587/780 [03:56<00:58,  3.30it/s] 75%|███████▌  | 588/780 [03:56<00:57,  3.34it/s] 76%|███████▌  | 589/780 [03:56<00:56,  3.37it/s] 76%|███████▌  | 590/780 [03:57<00:56,  3.39it/s] 76%|███████▌  | 591/780 [03:57<00:55,  3.40it/s] 76%|███████▌  | 592/780 [03:57<00:55,  3.41it/s] 76%|███████▌  | 593/780 [03:58<00:54,  3.42it/s] 76%|███████▌  | 594/780 [03:58<00:54,  3.42it/s] 76%|███████▋  | 595/780 [03:58<00:53,  3.43it/s] 76%|███████▋  | 596/780 [03:58<00:53,  3.43it/s] 77%|███████▋  | 597/780 [03:59<00:53,  3.44it/s] 77%|███████▋  | 598/780 [03:59<00:58,  3.13it/s] 77%|███████▋  | 599/780 [03:59<00:56,  3.22it/s] 77%|███████▋  | 600/780 [04:00<00:54,  3.28it/s] 77%|███████▋  | 601/780 [04:00<00:53,  3.32it/s] 77%|███████▋  | 602/780 [04:00<00:53,  3.36it/s] 77%|███████▋  | 603/780 [04:01<00:52,  3.38it/s] 77%|███████▋  | 604/780 [04:01<00:51,  3.40it/s] 78%|███████▊  | 605/780 [04:01<00:51,  3.41it/s] 78%|███████▊  | 606/780 [04:01<00:50,  3.42it/s] 78%|███████▊  | 607/780 [04:02<00:50,  3.43it/s] 78%|███████▊  | 608/780 [04:02<00:53,  3.21it/s] 78%|███████▊  | 609/780 [04:02<00:52,  3.27it/s] 78%|███████▊  | 610/780 [04:03<00:51,  3.32it/s] 78%|███████▊  | 611/780 [04:03<00:50,  3.35it/s] 78%|███████▊  | 612/780 [04:03<00:49,  3.37it/s] 79%|███████▊  | 613/780 [04:04<00:49,  3.39it/s] 79%|███████▊  | 614/780 [04:04<00:48,  3.40it/s] 79%|███████▉  | 615/780 [04:04<00:48,  3.42it/s] 79%|███████▉  | 616/780 [04:04<00:47,  3.42it/s] 79%|███████▉  | 617/780 [04:05<00:47,  3.42it/s] 79%|███████▉  | 618/780 [04:05<00:47,  3.43it/s] 79%|███████▉  | 619/780 [04:05<00:49,  3.28it/s] 79%|███████▉  | 620/780 [04:06<00:48,  3.32it/s] 80%|███████▉  | 621/780 [04:06<00:47,  3.35it/s] 80%|███████▉  | 622/780 [04:06<00:46,  3.38it/s] 80%|███████▉  | 623/780 [04:07<00:46,  3.39it/s] 80%|████████  | 624/780 [04:07<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 09:35:29,435 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:35:29,436 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:35:29,436 >>   Batch size = 8
{'eval_loss': 1.2642340660095215, 'eval_runtime': 9.8782, 'eval_samples_per_second': 352.493, 'eval_steps_per_second': 44.138, 'epoch': 3.0}
{'loss': 0.3716, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 54.66it/s][A
  3%|▎         | 12/436 [00:00<00:08, 47.61it/s][A
  4%|▍         | 17/436 [00:00<00:09, 46.08it/s][A
  5%|▌         | 22/436 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/436 [00:00<00:09, 45.08it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.75it/s][A
  8%|▊         | 37/436 [00:00<00:08, 44.54it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.48it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.61it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.65it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 42.89it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 43.39it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.58it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 43.70it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 43.88it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.95it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 44.12it/s][A
 21%|██        | 92/436 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.19it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.34it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 44.41it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.26it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.15it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.25it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.36it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 44.42it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 44.31it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 44.44it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 44.36it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.24it/s][A
 41%|████      | 177/436 [00:03<00:05, 44.37it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.34it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 42.81it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 43.45it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 43.76it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.90it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.03it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.11it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.13it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.22it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.07it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.23it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.36it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.38it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.45it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.29it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.20it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.15it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.21it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 44.32it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.39it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.37it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.44it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.33it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.35it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.26it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.31it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.15it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 42.98it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 43.68it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 42.68it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 43.30it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 41.98it/s][A
 81%|████████  | 352/436 [00:07<00:01, 42.90it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 43.32it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 43.60it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 43.54it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 43.75it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.05it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.28it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.22it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.26it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.33it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.45it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.30it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.15it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.41it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.38it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.39it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 44.39it/s][A 80%|████████  | 624/780 [04:17<00:45,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:35:39,579 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 09:35:39,788 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:35:42,432 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:35:42,510 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:35:42,548 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:26<15:44,  6.10s/it] 80%|████████  | 626/780 [04:27<11:12,  4.36s/it] 80%|████████  | 627/780 [04:27<08:00,  3.14s/it] 81%|████████  | 628/780 [04:27<05:47,  2.29s/it] 81%|████████  | 629/780 [04:28<04:15,  1.69s/it] 81%|████████  | 630/780 [04:28<03:10,  1.27s/it] 81%|████████  | 631/780 [04:28<02:25,  1.02it/s] 81%|████████  | 632/780 [04:29<01:54,  1.29it/s] 81%|████████  | 633/780 [04:29<01:32,  1.59it/s] 81%|████████▏ | 634/780 [04:29<01:17,  1.89it/s] 81%|████████▏ | 635/780 [04:29<01:06,  2.18it/s] 82%|████████▏ | 636/780 [04:30<00:58,  2.44it/s] 82%|████████▏ | 637/780 [04:30<00:54,  2.61it/s] 82%|████████▏ | 638/780 [04:30<00:50,  2.80it/s] 82%|████████▏ | 639/780 [04:31<00:47,  2.95it/s] 82%|████████▏ | 640/780 [04:31<00:45,  3.08it/s] 82%|████████▏ | 641/780 [04:31<00:43,  3.17it/s] 82%|████████▏ | 642/780 [04:32<00:42,  3.23it/s] 82%|████████▏ | 643/780 [04:32<00:41,  3.27it/s] 83%|████████▎ | 644/780 [04:32<00:41,  3.31it/s] 83%|████████▎ | 645/780 [04:32<00:40,  3.33it/s] 83%|████████▎ | 646/780 [04:33<00:40,  3.34it/s] 83%|████████▎ | 647/780 [04:33<00:39,  3.36it/s] 83%|████████▎ | 648/780 [04:33<00:40,  3.28it/s] 83%|████████▎ | 649/780 [04:34<00:39,  3.31it/s] 83%|████████▎ | 650/780 [04:34<00:39,  3.33it/s] 83%|████████▎ | 651/780 [04:34<00:38,  3.35it/s] 84%|████████▎ | 652/780 [04:34<00:38,  3.36it/s] 84%|████████▎ | 653/780 [04:35<00:37,  3.37it/s] 84%|████████▍ | 654/780 [04:35<00:37,  3.38it/s] 84%|████████▍ | 655/780 [04:35<00:36,  3.38it/s] 84%|████████▍ | 656/780 [04:36<00:36,  3.38it/s] 84%|████████▍ | 657/780 [04:36<00:36,  3.38it/s] 84%|████████▍ | 658/780 [04:36<00:36,  3.39it/s] 84%|████████▍ | 659/780 [04:37<00:36,  3.30it/s] 85%|████████▍ | 660/780 [04:37<00:36,  3.33it/s] 85%|████████▍ | 661/780 [04:37<00:35,  3.35it/s] 85%|████████▍ | 662/780 [04:37<00:35,  3.36it/s] 85%|████████▌ | 663/780 [04:38<00:34,  3.37it/s] 85%|████████▌ | 664/780 [04:38<00:34,  3.38it/s] 85%|████████▌ | 665/780 [04:38<00:33,  3.38it/s] 85%|████████▌ | 666/780 [04:39<00:33,  3.39it/s] 86%|████████▌ | 667/780 [04:39<00:33,  3.39it/s] 86%|████████▌ | 668/780 [04:39<00:33,  3.39it/s] 86%|████████▌ | 669/780 [04:40<00:32,  3.39it/s] 86%|████████▌ | 670/780 [04:40<00:33,  3.28it/s] 86%|████████▌ | 671/780 [04:40<00:32,  3.31it/s] 86%|████████▌ | 672/780 [04:40<00:32,  3.33it/s] 86%|████████▋ | 673/780 [04:41<00:31,  3.35it/s] 86%|████████▋ | 674/780 [04:41<00:31,  3.36it/s] 87%|████████▋ | 675/780 [04:41<00:31,  3.37it/s] 87%|████████▋ | 676/780 [04:42<00:30,  3.38it/s] 87%|████████▋ | 677/780 [04:42<00:30,  3.38it/s] 87%|████████▋ | 678/780 [04:42<00:30,  3.38it/s] 87%|████████▋ | 679/780 [04:43<00:29,  3.39it/s] 87%|████████▋ | 680/780 [04:43<00:29,  3.39it/s] 87%|████████▋ | 681/780 [04:43<00:30,  3.28it/s] 87%|████████▋ | 682/780 [04:43<00:29,  3.31it/s] 88%|████████▊ | 683/780 [04:44<00:29,  3.33it/s] 88%|████████▊ | 684/780 [04:44<00:29,  3.26it/s] 88%|████████▊ | 685/780 [04:44<00:30,  3.14it/s] 88%|████████▊ | 686/780 [04:45<00:33,  2.78it/s] 88%|████████▊ | 687/780 [04:45<00:31,  2.94it/s] 88%|████████▊ | 688/780 [04:45<00:30,  3.06it/s] 88%|████████▊ | 689/780 [04:46<00:28,  3.15it/s] 88%|████████▊ | 690/780 [04:46<00:27,  3.22it/s] 89%|████████▊ | 691/780 [04:46<00:27,  3.21it/s] 89%|████████▊ | 692/780 [04:47<00:26,  3.27it/s] 89%|████████▉ | 693/780 [04:47<00:26,  3.30it/s] 89%|████████▉ | 694/780 [04:47<00:25,  3.33it/s] 89%|████████▉ | 695/780 [04:48<00:25,  3.35it/s] 89%|████████▉ | 696/780 [04:48<00:25,  3.36it/s] 89%|████████▉ | 697/780 [04:48<00:24,  3.37it/s] 89%|████████▉ | 698/780 [04:48<00:24,  3.37it/s] 90%|████████▉ | 699/780 [04:49<00:23,  3.38it/s] 90%|████████▉ | 700/780 [04:49<00:23,  3.38it/s] 90%|████████▉ | 701/780 [04:49<00:23,  3.39it/s] 90%|█████████ | 702/780 [04:50<00:23,  3.34it/s] 90%|█████████ | 703/780 [04:50<00:22,  3.35it/s] 90%|█████████ | 704/780 [04:50<00:22,  3.36it/s] 90%|█████████ | 705/780 [04:50<00:22,  3.37it/s] 91%|█████████ | 706/780 [04:51<00:21,  3.37it/s] 91%|█████████ | 707/780 [04:51<00:21,  3.37it/s] 91%|█████████ | 708/780 [04:51<00:21,  3.38it/s] 91%|█████████ | 709/780 [04:52<00:20,  3.38it/s] 91%|█████████ | 710/780 [04:52<00:20,  3.38it/s] 91%|█████████ | 711/780 [04:52<00:20,  3.39it/s] 91%|█████████▏| 712/780 [04:53<00:20,  3.39it/s] 91%|█████████▏| 713/780 [04:53<00:19,  3.39it/s] 92%|█████████▏| 714/780 [04:53<00:19,  3.39it/s] 92%|█████████▏| 715/780 [04:53<00:19,  3.39it/s] 92%|█████████▏| 716/780 [04:54<00:18,  3.39it/s] 92%|█████████▏| 717/780 [04:54<00:19,  3.31it/s] 92%|█████████▏| 718/780 [04:54<00:18,  3.33it/s] 92%|█████████▏| 719/780 [04:55<00:18,  3.35it/s] 92%|█████████▏| 720/780 [04:55<00:17,  3.36it/s] 92%|█████████▏| 721/780 [04:55<00:17,  3.37it/s] 93%|█████████▎| 722/780 [04:56<00:17,  3.38it/s] 93%|█████████▎| 723/780 [04:56<00:16,  3.38it/s] 93%|█████████▎| 724/780 [04:56<00:16,  3.39it/s] 93%|█████████▎| 725/780 [04:56<00:16,  3.39it/s] 93%|█████████▎| 726/780 [04:57<00:15,  3.39it/s] 93%|█████████▎| 727/780 [04:57<00:15,  3.39it/s] 93%|█████████▎| 728/780 [04:57<00:15,  3.29it/s] 93%|█████████▎| 729/780 [04:58<00:15,  3.31it/s] 94%|█████████▎| 730/780 [04:58<00:14,  3.34it/s] 94%|█████████▎| 731/780 [04:58<00:14,  3.35it/s] 94%|█████████▍| 732/780 [04:59<00:14,  3.36it/s] 94%|█████████▍| 733/780 [04:59<00:13,  3.37it/s] 94%|█████████▍| 734/780 [04:59<00:13,  3.38it/s] 94%|█████████▍| 735/780 [04:59<00:13,  3.38it/s] 94%|█████████▍| 736/780 [05:00<00:13,  3.38it/s] 94%|█████████▍| 737/780 [05:00<00:12,  3.38it/s] 95%|█████████▍| 738/780 [05:00<00:12,  3.38it/s] 95%|█████████▍| 739/780 [05:01<00:12,  3.20it/s] 95%|█████████▍| 740/780 [05:01<00:12,  3.25it/s] 95%|█████████▌| 741/780 [05:01<00:11,  3.29it/s] 95%|█████████▌| 742/780 [05:02<00:11,  3.32it/s] 95%|█████████▌| 743/780 [05:02<00:11,  3.34it/s] 95%|█████████▌| 744/780 [05:02<00:10,  3.36it/s] 96%|█████████▌| 745/780 [05:02<00:10,  3.36it/s] 96%|█████████▌| 746/780 [05:03<00:10,  3.37it/s] 96%|█████████▌| 747/780 [05:03<00:09,  3.38it/s] 96%|█████████▌| 748/780 [05:03<00:09,  3.38it/s] 96%|█████████▌| 749/780 [05:04<00:09,  3.30it/s] 96%|█████████▌| 750/780 [05:04<00:09,  3.32it/s] 96%|█████████▋| 751/780 [05:04<00:08,  3.34it/s] 96%|█████████▋| 752/780 [05:04<00:08,  3.36it/s] 97%|█████████▋| 753/780 [05:05<00:08,  3.37it/s] 97%|█████████▋| 754/780 [05:05<00:07,  3.37it/s] 97%|█████████▋| 755/780 [05:05<00:07,  3.38it/s] 97%|█████████▋| 756/780 [05:06<00:07,  3.38it/s] 97%|█████████▋| 757/780 [05:06<00:06,  3.39it/s] 97%|█████████▋| 758/780 [05:06<00:06,  3.39it/s] 97%|█████████▋| 759/780 [05:07<00:06,  3.39it/s] 97%|█████████▋| 760/780 [05:07<00:06,  3.22it/s] 98%|█████████▊| 761/780 [05:07<00:05,  3.27it/s] 98%|█████████▊| 762/780 [05:08<00:05,  3.31it/s] 98%|█████████▊| 763/780 [05:08<00:05,  3.34it/s] 98%|█████████▊| 764/780 [05:08<00:04,  3.37it/s] 98%|█████████▊| 765/780 [05:08<00:04,  3.39it/s] 98%|█████████▊| 766/780 [05:09<00:04,  3.40it/s] 98%|█████████▊| 767/780 [05:09<00:03,  3.41it/s] 98%|█████████▊| 768/780 [05:09<00:03,  3.42it/s] 99%|█████████▊| 769/780 [05:10<00:03,  3.43it/s] 99%|█████████▊| 770/780 [05:10<00:02,  3.43it/s] 99%|█████████▉| 771/780 [05:10<00:02,  3.30it/s] 99%|█████████▉| 772/780 [05:10<00:02,  3.34it/s] 99%|█████████▉| 773/780 [05:11<00:02,  3.37it/s] 99%|█████████▉| 774/780 [05:11<00:01,  3.39it/s] 99%|█████████▉| 775/780 [05:11<00:01,  3.40it/s] 99%|█████████▉| 776/780 [05:12<00:01,  3.41it/s]100%|█████████▉| 777/780 [05:12<00:00,  3.42it/s]100%|█████████▉| 778/780 [05:12<00:00,  3.43it/s]100%|█████████▉| 779/780 [05:12<00:00,  3.43it/s]100%|██████████| 780/780 [05:13<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:36:35,359 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:36:35,359 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:36:35,359 >>   Batch size = 8
{'eval_loss': 1.2755297422409058, 'eval_runtime': 9.9169, 'eval_samples_per_second': 351.118, 'eval_steps_per_second': 43.965, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.93it/s][A
  3%|▎         | 12/436 [00:00<00:09, 44.55it/s][A
  4%|▍         | 17/436 [00:00<00:09, 44.53it/s][A
  5%|▌         | 22/436 [00:00<00:09, 44.22it/s][A
  6%|▌         | 27/436 [00:00<00:09, 44.30it/s][A
  7%|▋         | 32/436 [00:00<00:09, 44.22it/s][A
  8%|▊         | 37/436 [00:00<00:09, 44.32it/s][A
 10%|▉         | 42/436 [00:00<00:08, 44.27it/s][A
 11%|█         | 47/436 [00:01<00:08, 44.27it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 44.45it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 44.39it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.27it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 42.75it/s][A
 19%|█▉        | 82/436 [00:01<00:08, 43.35it/s][A
 20%|█▉        | 87/436 [00:01<00:08, 43.40it/s][A
 21%|██        | 92/436 [00:02<00:07, 43.83it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 44.13it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 44.01it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 44.26it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 44.21it/s][A
 27%|██▋       | 117/436 [00:02<00:07, 43.99it/s][A
 28%|██▊       | 122/436 [00:02<00:07, 44.07it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 44.23it/s][A
 30%|███       | 132/436 [00:02<00:06, 44.26it/s][A
 31%|███▏      | 137/436 [00:03<00:06, 44.49it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 44.51it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 43.49it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 43.77it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 43.71it/s][A
 37%|███▋      | 162/436 [00:03<00:06, 43.64it/s][A
 38%|███▊      | 167/436 [00:03<00:06, 43.90it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 44.11it/s][A
 41%|████      | 177/436 [00:04<00:05, 44.30it/s][A
 42%|████▏     | 182/436 [00:04<00:05, 44.30it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 44.26it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 44.40it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 44.42it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.27it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.28it/s][A
 51%|█████     | 222/436 [00:05<00:04, 44.45it/s][A
 52%|█████▏    | 227/436 [00:05<00:04, 44.48it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 44.44it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 44.35it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 44.26it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 44.22it/s][A
 59%|█████▉    | 257/436 [00:05<00:04, 44.29it/s][A
 60%|██████    | 262/436 [00:05<00:03, 44.31it/s][A
 61%|██████    | 267/436 [00:06<00:03, 44.39it/s][A
 62%|██████▏   | 272/436 [00:06<00:03, 44.44it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 44.41it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 43.96it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 44.05it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 44.14it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 44.06it/s][A
 69%|██████▉   | 302/436 [00:06<00:03, 44.11it/s][A
 70%|███████   | 307/436 [00:06<00:02, 44.28it/s][A
 72%|███████▏  | 312/436 [00:07<00:02, 44.35it/s][A
 73%|███████▎  | 317/436 [00:07<00:02, 44.41it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 44.32it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 44.24it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 44.24it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 44.40it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 44.41it/s][A
 80%|███████▉  | 347/436 [00:07<00:02, 44.35it/s][A
 81%|████████  | 352/436 [00:07<00:01, 44.39it/s][A
 82%|████████▏ | 357/436 [00:08<00:01, 44.53it/s][A
 83%|████████▎ | 362/436 [00:08<00:01, 44.43it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 44.33it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 44.21it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 44.13it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 44.38it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 44.33it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 44.37it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 44.36it/s][A
 92%|█████████▏| 402/436 [00:09<00:00, 44.48it/s][A
 93%|█████████▎| 407/436 [00:09<00:00, 44.42it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 44.35it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 44.32it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 44.42it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 44.30it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:23<00:00,  3.43it/s]
100%|██████████| 436/436 [00:09<00:00, 44.30it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:36:45,375 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 09:36:45,530 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:36:48,373 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:36:48,440 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:36:48,482 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:36:54,030 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:36:54,047 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156 (score: 1.231946587562561).
                                                 100%|██████████| 780/780 [05:39<00:00,  3.43it/s]100%|██████████| 780/780 [05:39<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-29 09:37:01,981 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 09:37:02,084 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:37:04,605 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:37:04,705 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:37:04,755 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:37:05,139 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,156 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,157 >>   train_loss               =     0.3649
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,157 >>   train_runtime            = 0:05:39.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,157 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,157 >>   train_samples_per_second =    147.112
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:05,157 >>   train_steps_per_second   =      2.295
{'eval_loss': 1.2856682538986206, 'eval_runtime': 9.8644, 'eval_samples_per_second': 352.988, 'eval_steps_per_second': 44.199, 'epoch': 5.0}
{'train_runtime': 339.878, 'train_samples_per_second': 147.112, 'train_steps_per_second': 2.295, 'train_loss': 0.3648733677008213, 'epoch': 5.0}
08/29/2023 09:37:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:37:05,362 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:37:05,363 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 09:37:05,363 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 55.39it/s]  3%|▎         | 12/436 [00:00<00:08, 48.56it/s]  4%|▍         | 17/436 [00:00<00:08, 47.14it/s]  5%|▌         | 22/436 [00:00<00:08, 46.33it/s]  6%|▌         | 27/436 [00:00<00:08, 45.93it/s]  7%|▋         | 32/436 [00:00<00:08, 45.79it/s]  8%|▊         | 37/436 [00:00<00:08, 45.57it/s] 10%|▉         | 42/436 [00:00<00:08, 45.07it/s] 11%|█         | 47/436 [00:01<00:08, 44.35it/s] 12%|█▏        | 52/436 [00:01<00:08, 44.08it/s] 13%|█▎        | 57/436 [00:01<00:09, 41.70it/s] 14%|█▍        | 62/436 [00:01<00:08, 42.80it/s] 15%|█▌        | 67/436 [00:01<00:08, 43.46it/s] 17%|█▋        | 72/436 [00:01<00:09, 40.39it/s] 18%|█▊        | 77/436 [00:01<00:08, 41.83it/s] 19%|█▉        | 82/436 [00:01<00:08, 42.81it/s] 20%|█▉        | 87/436 [00:02<00:10, 32.78it/s] 21%|██        | 92/436 [00:02<00:09, 35.82it/s] 22%|██▏       | 97/436 [00:02<00:08, 38.07it/s] 23%|██▎       | 102/436 [00:02<00:08, 39.99it/s] 25%|██▍       | 107/436 [00:02<00:07, 41.35it/s] 26%|██▌       | 112/436 [00:02<00:07, 42.46it/s] 27%|██▋       | 117/436 [00:02<00:07, 43.25it/s] 28%|██▊       | 122/436 [00:02<00:07, 42.51it/s] 29%|██▉       | 127/436 [00:02<00:07, 42.63it/s] 30%|███       | 132/436 [00:03<00:07, 42.80it/s] 31%|███▏      | 137/436 [00:03<00:06, 43.30it/s] 33%|███▎      | 142/436 [00:03<00:06, 43.81it/s] 34%|███▎      | 147/436 [00:03<00:06, 44.17it/s] 35%|███▍      | 152/436 [00:03<00:06, 44.32it/s] 36%|███▌      | 157/436 [00:03<00:06, 44.61it/s] 37%|███▋      | 162/436 [00:03<00:06, 44.53it/s] 38%|███▊      | 167/436 [00:03<00:06, 44.27it/s] 39%|███▉      | 172/436 [00:04<00:05, 44.07it/s] 41%|████      | 177/436 [00:04<00:05, 43.96it/s] 42%|████▏     | 182/436 [00:04<00:05, 44.02it/s] 43%|████▎     | 187/436 [00:04<00:05, 44.15it/s] 44%|████▍     | 192/436 [00:04<00:05, 44.37it/s] 45%|████▌     | 197/436 [00:04<00:05, 44.60it/s] 46%|████▋     | 202/436 [00:04<00:05, 44.67it/s] 47%|████▋     | 207/436 [00:04<00:05, 44.53it/s] 49%|████▊     | 212/436 [00:04<00:05, 44.31it/s] 50%|████▉     | 217/436 [00:05<00:04, 44.12it/s] 51%|█████     | 222/436 [00:05<00:04, 44.13it/s] 52%|█████▏    | 227/436 [00:05<00:04, 44.20it/s] 53%|█████▎    | 232/436 [00:05<00:04, 44.28it/s] 54%|█████▍    | 237/436 [00:05<00:04, 44.49it/s] 56%|█████▌    | 242/436 [00:05<00:04, 44.71it/s] 57%|█████▋    | 247/436 [00:05<00:04, 44.55it/s] 58%|█████▊    | 252/436 [00:05<00:04, 44.47it/s] 59%|█████▉    | 257/436 [00:05<00:04, 41.86it/s] 60%|██████    | 262/436 [00:06<00:04, 42.56it/s] 61%|██████    | 267/436 [00:06<00:03, 42.93it/s] 62%|██████▏   | 272/436 [00:06<00:03, 43.16it/s] 64%|██████▎   | 277/436 [00:06<00:03, 43.60it/s] 65%|██████▍   | 282/436 [00:06<00:03, 43.93it/s] 66%|██████▌   | 287/436 [00:06<00:03, 44.24it/s] 67%|██████▋   | 292/436 [00:06<00:03, 44.33it/s] 68%|██████▊   | 297/436 [00:06<00:03, 44.21it/s] 69%|██████▉   | 302/436 [00:06<00:03, 44.12it/s] 70%|███████   | 307/436 [00:07<00:02, 44.23it/s] 72%|███████▏  | 312/436 [00:07<00:02, 44.11it/s] 73%|███████▎  | 317/436 [00:07<00:02, 44.20it/s] 74%|███████▍  | 322/436 [00:07<00:02, 44.31it/s] 75%|███████▌  | 327/436 [00:07<00:02, 44.41it/s] 76%|███████▌  | 332/436 [00:07<00:02, 44.42it/s] 77%|███████▋  | 337/436 [00:07<00:02, 44.37it/s] 78%|███████▊  | 342/436 [00:07<00:02, 44.36it/s] 80%|███████▉  | 347/436 [00:07<00:02, 44.26it/s] 81%|████████  | 352/436 [00:08<00:01, 44.33it/s] 82%|████████▏ | 357/436 [00:08<00:01, 44.26it/s] 83%|████████▎ | 362/436 [00:08<00:01, 44.14it/s] 84%|████████▍ | 367/436 [00:08<00:01, 44.42it/s] 85%|████████▌ | 372/436 [00:08<00:01, 44.47it/s] 86%|████████▋ | 377/436 [00:08<00:01, 44.38it/s] 88%|████████▊ | 382/436 [00:08<00:01, 44.34it/s] 89%|████████▉ | 387/436 [00:08<00:01, 44.26it/s] 90%|████████▉ | 392/436 [00:09<00:01, 42.78it/s] 91%|█████████ | 397/436 [00:09<00:00, 43.27it/s] 92%|█████████▏| 402/436 [00:09<00:00, 43.58it/s] 93%|█████████▎| 407/436 [00:09<00:00, 43.80it/s] 94%|█████████▍| 412/436 [00:09<00:00, 44.07it/s] 96%|█████████▌| 417/436 [00:09<00:00, 44.22it/s] 97%|█████████▋| 422/436 [00:09<00:00, 44.37it/s] 98%|█████████▊| 427/436 [00:09<00:00, 44.28it/s] 99%|█████████▉| 432/436 [00:09<00:00, 44.05it/s]100%|██████████| 436/436 [00:10<00:00, 43.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:37:15,385 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   eval_loss               =     1.2319
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   eval_runtime            = 0:00:10.02
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   eval_samples_per_second =    347.416
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   eval_steps_per_second   =     43.502
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:37:15,385 >>   perplexity              =     3.4279
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:28,831 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:28,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:28,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:28,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:28,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:37:29,571 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:37:29,572 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:37:30,158 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:37:31,245 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:37:31,245 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:34,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:34,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:34,247 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:34,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:37:34,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:37:35,074 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:37:35,076 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:37:35,685 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:37:35,885 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:37:35,885 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:26,  1.45it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:32,  1.45it/s]Extractor Predicting: 51it [00:33,  1.46it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:35,  1.46it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:36,  1.46it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.48it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:41,  1.46it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:42,  1.51it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.52it/s]Extractor Predicting: 70it [00:46,  1.57it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:48,  1.57it/s]Extractor Predicting: 74it [00:48,  1.59it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.51it/s]Extractor Predicting: 79it [00:52,  1.51it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:56,  1.50it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.48it/s]Extractor Predicting: 90it [00:59,  1.47it/s]Extractor Predicting: 91it [01:00,  1.46it/s]Extractor Predicting: 92it [01:00,  1.45it/s]Extractor Predicting: 93it [01:01,  1.49it/s]Extractor Predicting: 94it [01:02,  1.47it/s]Extractor Predicting: 95it [01:02,  1.51it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:04,  1.50it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.46it/s]Extractor Predicting: 100it [01:06,  1.44it/s]Extractor Predicting: 101it [01:06,  1.48it/s]Extractor Predicting: 102it [01:07,  1.45it/s]Extractor Predicting: 103it [01:08,  1.45it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.35it/s]Extractor Predicting: 108it [01:11,  1.39it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.48it/s]Extractor Predicting: 111it [01:13,  1.53it/s]Extractor Predicting: 112it [01:14,  1.53it/s]Extractor Predicting: 113it [01:15,  1.51it/s]Extractor Predicting: 114it [01:15,  1.49it/s]Extractor Predicting: 115it [01:16,  1.47it/s]Extractor Predicting: 116it [01:17,  1.48it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:18,  1.47it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:19,  1.52it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:21,  1.46it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.49it/s]Extractor Predicting: 126it [01:23,  1.50it/s]Extractor Predicting: 127it [01:24,  1.50it/s]Extractor Predicting: 128it [01:25,  1.47it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:27,  1.48it/s]Extractor Predicting: 132it [01:27,  1.51it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:29,  1.42it/s]Extractor Predicting: 135it [01:30,  1.44it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:32,  1.47it/s]Extractor Predicting: 139it [01:32,  1.46it/s]Extractor Predicting: 140it [01:33,  1.44it/s]Extractor Predicting: 141it [01:34,  1.45it/s]Extractor Predicting: 142it [01:34,  1.49it/s]Extractor Predicting: 143it [01:35,  1.51it/s]Extractor Predicting: 143it [01:35,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:28,884 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:28,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:28,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:28,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:28,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:39:29,968 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:39:29,969 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:39:30,592 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:39:31,668 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:39:31,668 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:34,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:34,658 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:34,658 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:34,658 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:39:34,658 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:39:35,418 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:39:35,419 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:39:36,056 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:39:36,263 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:39:36,264 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2591623036649215,
  "recall": 0.08529580700746697,
  "score": 0.12834917891097666,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.66it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:18,  1.67it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:20,  1.51it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:22,  1.48it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.48it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.49it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.50it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:32,  1.51it/s]Extractor Predicting: 52it [00:33,  1.56it/s]Extractor Predicting: 53it [00:34,  1.51it/s]Extractor Predicting: 54it [00:34,  1.52it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:36,  1.57it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.51it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.58it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.53it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.52it/s]Extractor Predicting: 69it [00:44,  1.55it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:49,  1.59it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:50,  1.59it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:52,  1.56it/s]Extractor Predicting: 82it [00:52,  1.58it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:57,  1.61it/s]Extractor Predicting: 90it [00:57,  1.59it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:59,  1.55it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.54it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.52it/s]Extractor Predicting: 99it [01:04,  1.36it/s]Extractor Predicting: 100it [01:04,  1.38it/s]Extractor Predicting: 101it [01:05,  1.41it/s]Extractor Predicting: 102it [01:06,  1.46it/s]Extractor Predicting: 103it [01:06,  1.48it/s]Extractor Predicting: 104it [01:07,  1.49it/s]Extractor Predicting: 105it [01:08,  1.47it/s]Extractor Predicting: 106it [01:08,  1.48it/s]Extractor Predicting: 107it [01:09,  1.48it/s]Extractor Predicting: 108it [01:10,  1.50it/s]Extractor Predicting: 109it [01:10,  1.50it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:12,  1.49it/s]Extractor Predicting: 112it [01:12,  1.46it/s]Extractor Predicting: 113it [01:13,  1.48it/s]Extractor Predicting: 114it [01:14,  1.49it/s]Extractor Predicting: 115it [01:14,  1.47it/s]Extractor Predicting: 116it [01:15,  1.53it/s]Extractor Predicting: 117it [01:16,  1.62it/s]Extractor Predicting: 118it [01:16,  1.65it/s]Extractor Predicting: 119it [01:17,  1.64it/s]Extractor Predicting: 120it [01:17,  1.62it/s]Extractor Predicting: 121it [01:18,  1.61it/s]Extractor Predicting: 122it [01:19,  1.59it/s]Extractor Predicting: 123it [01:19,  1.58it/s]Extractor Predicting: 124it [01:20,  1.62it/s]Extractor Predicting: 125it [01:20,  1.66it/s]Extractor Predicting: 126it [01:21,  1.65it/s]Extractor Predicting: 127it [01:22,  1.66it/s]Extractor Predicting: 128it [01:22,  1.69it/s]Extractor Predicting: 129it [01:23,  1.66it/s]Extractor Predicting: 130it [01:23,  1.66it/s]Extractor Predicting: 131it [01:24,  1.65it/s]Extractor Predicting: 132it [01:25,  1.66it/s]Extractor Predicting: 133it [01:25,  1.66it/s]Extractor Predicting: 134it [01:26,  1.68it/s]Extractor Predicting: 135it [01:26,  1.70it/s]Extractor Predicting: 136it [01:27,  1.73it/s]Extractor Predicting: 137it [01:27,  1.74it/s]Extractor Predicting: 138it [01:28,  1.69it/s]Extractor Predicting: 139it [01:29,  1.66it/s]Extractor Predicting: 140it [01:29,  1.67it/s]Extractor Predicting: 141it [01:30,  1.64it/s]Extractor Predicting: 142it [01:31,  1.64it/s]Extractor Predicting: 143it [01:31,  1.66it/s]Extractor Predicting: 144it [01:32,  1.63it/s]Extractor Predicting: 145it [01:32,  1.64it/s]Extractor Predicting: 146it [01:33,  1.65it/s]Extractor Predicting: 147it [01:34,  1.65it/s]Extractor Predicting: 148it [01:34,  1.61it/s]Extractor Predicting: 149it [01:35,  1.67it/s]Extractor Predicting: 150it [01:35,  1.65it/s]Extractor Predicting: 151it [01:36,  1.63it/s]Extractor Predicting: 152it [01:37,  1.66it/s]Extractor Predicting: 153it [01:37,  1.69it/s]Extractor Predicting: 154it [01:38,  1.70it/s]Extractor Predicting: 155it [01:38,  1.70it/s]Extractor Predicting: 156it [01:39,  1.68it/s]Extractor Predicting: 157it [01:40,  1.67it/s]Extractor Predicting: 158it [01:40,  1.68it/s]Extractor Predicting: 159it [01:41,  1.61it/s]Extractor Predicting: 160it [01:41,  1.63it/s]Extractor Predicting: 161it [01:42,  1.64it/s]Extractor Predicting: 162it [01:43,  1.64it/s]Extractor Predicting: 163it [01:43,  1.64it/s]Extractor Predicting: 164it [01:44,  1.61it/s]Extractor Predicting: 165it [01:45,  1.64it/s]Extractor Predicting: 166it [01:45,  1.64it/s]Extractor Predicting: 167it [01:46,  1.65it/s]Extractor Predicting: 168it [01:46,  1.62it/s]Extractor Predicting: 169it [01:47,  1.60it/s]Extractor Predicting: 170it [01:48,  1.57it/s]Extractor Predicting: 171it [01:48,  1.56it/s]Extractor Predicting: 172it [01:49,  1.63it/s]Extractor Predicting: 173it [01:50,  1.58it/s]Extractor Predicting: 174it [01:50,  1.52it/s]Extractor Predicting: 175it [01:51,  1.51it/s]Extractor Predicting: 176it [01:52,  1.53it/s]Extractor Predicting: 177it [01:52,  1.53it/s]Extractor Predicting: 178it [01:53,  1.51it/s]Extractor Predicting: 179it [01:54,  1.52it/s]Extractor Predicting: 180it [01:54,  1.55it/s]Extractor Predicting: 181it [01:55,  1.55it/s]Extractor Predicting: 182it [01:55,  1.53it/s]Extractor Predicting: 183it [01:56,  1.53it/s]Extractor Predicting: 184it [01:57,  1.51it/s]Extractor Predicting: 185it [01:57,  1.51it/s]Extractor Predicting: 186it [01:58,  1.51it/s]Extractor Predicting: 187it [01:59,  1.49it/s]Extractor Predicting: 188it [01:59,  1.53it/s]Extractor Predicting: 189it [02:00,  1.54it/s]Extractor Predicting: 190it [02:01,  1.55it/s]Extractor Predicting: 191it [02:01,  1.51it/s]Extractor Predicting: 192it [02:02,  1.51it/s]Extractor Predicting: 193it [02:03,  1.49it/s]Extractor Predicting: 194it [02:03,  1.49it/s]Extractor Predicting: 195it [02:04,  1.49it/s]Extractor Predicting: 196it [02:05,  1.51it/s]Extractor Predicting: 197it [02:05,  1.49it/s]Extractor Predicting: 198it [02:06,  1.49it/s]Extractor Predicting: 199it [02:07,  1.49it/s]Extractor Predicting: 200it [02:07,  1.49it/s]Extractor Predicting: 201it [02:08,  1.50it/s]Extractor Predicting: 202it [02:09,  1.49it/s]Extractor Predicting: 203it [02:09,  1.52it/s]Extractor Predicting: 204it [02:10,  1.54it/s]Extractor Predicting: 205it [02:11,  1.59it/s]Extractor Predicting: 206it [02:11,  1.58it/s]Extractor Predicting: 207it [02:12,  1.37it/s]Extractor Predicting: 208it [02:13,  1.42it/s]Extractor Predicting: 209it [02:14,  1.45it/s]Extractor Predicting: 210it [02:14,  1.50it/s]Extractor Predicting: 211it [02:15,  1.55it/s]Extractor Predicting: 212it [02:15,  1.54it/s]Extractor Predicting: 213it [02:16,  1.58it/s]Extractor Predicting: 214it [02:17,  1.57it/s]Extractor Predicting: 215it [02:17,  1.53it/s]Extractor Predicting: 216it [02:18,  1.55it/s]Extractor Predicting: 217it [02:19,  1.54it/s]Extractor Predicting: 218it [02:19,  1.58it/s]Extractor Predicting: 219it [02:20,  1.60it/s]Extractor Predicting: 220it [02:21,  1.58it/s]Extractor Predicting: 221it [02:21,  1.60it/s]Extractor Predicting: 222it [02:22,  1.61it/s]Extractor Predicting: 223it [02:22,  1.59it/s]Extractor Predicting: 224it [02:23,  1.60it/s]Extractor Predicting: 225it [02:24,  1.60it/s]Extractor Predicting: 226it [02:24,  1.55it/s]Extractor Predicting: 227it [02:25,  1.53it/s]Extractor Predicting: 228it [02:26,  1.54it/s]Extractor Predicting: 229it [02:26,  1.53it/s]Extractor Predicting: 230it [02:27,  1.53it/s]Extractor Predicting: 231it [02:28,  1.54it/s]Extractor Predicting: 232it [02:28,  1.57it/s]Extractor Predicting: 233it [02:29,  1.63it/s]Extractor Predicting: 234it [02:29,  1.63it/s]Extractor Predicting: 235it [02:30,  1.64it/s]Extractor Predicting: 236it [02:31,  1.69it/s]Extractor Predicting: 237it [02:31,  1.69it/s]Extractor Predicting: 238it [02:32,  1.70it/s]Extractor Predicting: 239it [02:32,  1.69it/s]Extractor Predicting: 240it [02:33,  1.69it/s]Extractor Predicting: 241it [02:33,  1.68it/s]Extractor Predicting: 242it [02:34,  1.69it/s]Extractor Predicting: 243it [02:35,  1.74it/s]Extractor Predicting: 244it [02:35,  1.72it/s]Extractor Predicting: 245it [02:36,  1.78it/s]Extractor Predicting: 246it [02:36,  1.80it/s]Extractor Predicting: 247it [02:37,  1.74it/s]Extractor Predicting: 248it [02:37,  1.70it/s]Extractor Predicting: 249it [02:38,  1.71it/s]Extractor Predicting: 250it [02:39,  1.71it/s]Extractor Predicting: 251it [02:39,  1.74it/s]Extractor Predicting: 252it [02:40,  1.74it/s]Extractor Predicting: 253it [02:40,  1.73it/s]Extractor Predicting: 254it [02:41,  1.71it/s]Extractor Predicting: 255it [02:42,  1.74it/s]Extractor Predicting: 256it [02:42,  1.74it/s]Extractor Predicting: 257it [02:43,  1.77it/s]Extractor Predicting: 258it [02:43,  1.74it/s]Extractor Predicting: 259it [02:44,  1.77it/s]Extractor Predicting: 260it [02:44,  1.74it/s]Extractor Predicting: 261it [02:45,  1.65it/s]Extractor Predicting: 262it [02:46,  1.65it/s]Extractor Predicting: 263it [02:46,  1.59it/s]Extractor Predicting: 264it [02:47,  1.55it/s]Extractor Predicting: 265it [02:48,  1.55it/s]Extractor Predicting: 266it [02:48,  1.56it/s]Extractor Predicting: 267it [02:49,  1.59it/s]Extractor Predicting: 268it [02:50,  1.57it/s]Extractor Predicting: 269it [02:50,  1.54it/s]Extractor Predicting: 270it [02:51,  1.52it/s]Extractor Predicting: 271it [02:52,  1.51it/s]Extractor Predicting: 272it [02:52,  1.54it/s]Extractor Predicting: 273it [02:53,  1.52it/s]Extractor Predicting: 274it [02:54,  1.52it/s]Extractor Predicting: 275it [02:54,  1.52it/s]Extractor Predicting: 276it [02:55,  1.53it/s]Extractor Predicting: 277it [02:56,  1.51it/s]Extractor Predicting: 278it [02:56,  1.51it/s]Extractor Predicting: 279it [02:57,  1.54it/s]Extractor Predicting: 280it [02:57,  1.52it/s]Extractor Predicting: 281it [02:58,  1.51it/s]Extractor Predicting: 282it [02:59,  1.51it/s]Extractor Predicting: 283it [02:59,  1.52it/s]Extractor Predicting: 284it [03:00,  1.50it/s]Extractor Predicting: 285it [03:01,  1.51it/s]Extractor Predicting: 286it [03:01,  1.51it/s]Extractor Predicting: 287it [03:02,  1.50it/s]Extractor Predicting: 288it [03:03,  1.55it/s]Extractor Predicting: 289it [03:03,  1.54it/s]Extractor Predicting: 290it [03:04,  1.57it/s]Extractor Predicting: 291it [03:05,  1.58it/s]Extractor Predicting: 292it [03:05,  1.58it/s]Extractor Predicting: 293it [03:06,  1.58it/s]Extractor Predicting: 294it [03:07,  1.58it/s]Extractor Predicting: 295it [03:07,  1.59it/s]Extractor Predicting: 296it [03:08,  1.58it/s]Extractor Predicting: 297it [03:08,  1.59it/s]Extractor Predicting: 298it [03:09,  1.63it/s]Extractor Predicting: 299it [03:10,  1.58it/s]Extractor Predicting: 300it [03:10,  1.59it/s]Extractor Predicting: 301it [03:11,  1.58it/s]Extractor Predicting: 302it [03:12,  1.56it/s]Extractor Predicting: 303it [03:12,  1.57it/s]Extractor Predicting: 304it [03:13,  1.55it/s]Extractor Predicting: 305it [03:14,  1.54it/s]Extractor Predicting: 306it [03:14,  1.56it/s]Extractor Predicting: 307it [03:15,  1.57it/s]Extractor Predicting: 308it [03:15,  1.56it/s]Extractor Predicting: 309it [03:16,  1.52it/s]Extractor Predicting: 310it [03:17,  1.54it/s]Extractor Predicting: 311it [03:17,  1.55it/s]Extractor Predicting: 312it [03:18,  1.56it/s]Extractor Predicting: 313it [03:19,  1.56it/s]Extractor Predicting: 314it [03:19,  1.55it/s]Extractor Predicting: 315it [03:20,  1.57it/s]Extractor Predicting: 316it [03:21,  1.56it/s]Extractor Predicting: 317it [03:21,  1.58it/s]Extractor Predicting: 318it [03:22,  1.61it/s]Extractor Predicting: 319it [03:22,  1.60it/s]Extractor Predicting: 320it [03:23,  1.58it/s]Extractor Predicting: 321it [03:24,  1.38it/s]Extractor Predicting: 322it [03:25,  1.43it/s]Extractor Predicting: 323it [03:25,  1.48it/s]Extractor Predicting: 324it [03:26,  1.50it/s]Extractor Predicting: 325it [03:27,  1.51it/s]Extractor Predicting: 326it [03:27,  1.54it/s]Extractor Predicting: 327it [03:28,  1.55it/s]Extractor Predicting: 328it [03:28,  1.54it/s]Extractor Predicting: 329it [03:29,  1.54it/s]Extractor Predicting: 330it [03:30,  1.54it/s]Extractor Predicting: 331it [03:30,  1.53it/s]Extractor Predicting: 332it [03:31,  1.55it/s]Extractor Predicting: 333it [03:32,  1.53it/s]Extractor Predicting: 334it [03:32,  1.57it/s]Extractor Predicting: 335it [03:33,  1.55it/s]Extractor Predicting: 336it [03:34,  1.57it/s]Extractor Predicting: 337it [03:34,  1.53it/s]Extractor Predicting: 338it [03:35,  1.55it/s]Extractor Predicting: 339it [03:36,  1.56it/s]Extractor Predicting: 340it [03:36,  1.54it/s]Extractor Predicting: 341it [03:37,  1.56it/s]Extractor Predicting: 342it [03:38,  1.57it/s]Extractor Predicting: 343it [03:38,  1.57it/s]Extractor Predicting: 344it [03:39,  1.60it/s]Extractor Predicting: 345it [03:39,  1.57it/s]Extractor Predicting: 346it [03:40,  1.57it/s]Extractor Predicting: 347it [03:41,  1.57it/s]Extractor Predicting: 348it [03:41,  1.53it/s]Extractor Predicting: 349it [03:42,  1.55it/s]Extractor Predicting: 350it [03:43,  1.54it/s]Extractor Predicting: 351it [03:43,  1.55it/s]Extractor Predicting: 352it [03:44,  1.51it/s]Extractor Predicting: 353it [03:45,  1.52it/s]Extractor Predicting: 354it [03:45,  1.54it/s]Extractor Predicting: 355it [03:46,  1.53it/s]Extractor Predicting: 356it [03:47,  1.55it/s]Extractor Predicting: 357it [03:47,  1.51it/s]Extractor Predicting: 358it [03:48,  1.54it/s]Extractor Predicting: 359it [03:49,  1.55it/s]Extractor Predicting: 360it [03:49,  1.55it/s]Extractor Predicting: 361it [03:50,  1.55it/s]Extractor Predicting: 362it [03:50,  1.57it/s]Extractor Predicting: 363it [03:51,  1.57it/s]Extractor Predicting: 364it [03:52,  1.60it/s]Extractor Predicting: 365it [03:52,  1.59it/s]Extractor Predicting: 366it [03:53,  1.60it/s]Extractor Predicting: 367it [03:54,  1.56it/s]Extractor Predicting: 368it [03:54,  1.57it/s]Extractor Predicting: 369it [03:55,  1.54it/s]Extractor Predicting: 370it [03:56,  1.54it/s]Extractor Predicting: 371it [03:56,  1.55it/s]Extractor Predicting: 372it [03:57,  1.55it/s]Extractor Predicting: 373it [03:57,  1.55it/s]Extractor Predicting: 374it [03:58,  1.58it/s]Extractor Predicting: 375it [03:59,  1.57it/s]Extractor Predicting: 376it [03:59,  1.55it/s]Extractor Predicting: 377it [04:00,  1.61it/s]Extractor Predicting: 378it [04:01,  1.61it/s]Extractor Predicting: 379it [04:01,  1.64it/s]Extractor Predicting: 380it [04:02,  1.59it/s]Extractor Predicting: 381it [04:02,  1.59it/s]Extractor Predicting: 382it [04:03,  1.59it/s]Extractor Predicting: 383it [04:04,  1.57it/s]Extractor Predicting: 384it [04:04,  1.55it/s]Extractor Predicting: 385it [04:05,  1.56it/s]Extractor Predicting: 386it [04:06,  1.54it/s]Extractor Predicting: 387it [04:06,  1.54it/s]Extractor Predicting: 388it [04:07,  1.49it/s]Extractor Predicting: 389it [04:08,  1.51it/s]Extractor Predicting: 390it [04:08,  1.52it/s]Extractor Predicting: 391it [04:09,  1.54it/s]Extractor Predicting: 392it [04:10,  1.56it/s]Extractor Predicting: 393it [04:10,  1.55it/s]Extractor Predicting: 394it [04:11,  1.56it/s]Extractor Predicting: 395it [04:12,  1.56it/s]Extractor Predicting: 396it [04:12,  1.54it/s]Extractor Predicting: 397it [04:13,  1.56it/s]Extractor Predicting: 398it [04:14,  1.54it/s]Extractor Predicting: 399it [04:14,  1.57it/s]Extractor Predicting: 400it [04:15,  1.55it/s]Extractor Predicting: 401it [04:15,  1.56it/s]Extractor Predicting: 402it [04:16,  1.56it/s]Extractor Predicting: 403it [04:17,  1.54it/s]Extractor Predicting: 404it [04:17,  1.57it/s]Extractor Predicting: 405it [04:18,  1.56it/s]Extractor Predicting: 406it [04:19,  1.59it/s]Extractor Predicting: 407it [04:19,  1.58it/s]Extractor Predicting: 408it [04:20,  1.57it/s]Extractor Predicting: 409it [04:20,  1.59it/s]Extractor Predicting: 410it [04:21,  1.58it/s]Extractor Predicting: 411it [04:22,  1.57it/s]Extractor Predicting: 412it [04:22,  1.60it/s]Extractor Predicting: 413it [04:23,  1.60it/s]Extractor Predicting: 414it [04:24,  1.59it/s]Extractor Predicting: 415it [04:24,  1.55it/s]Extractor Predicting: 416it [04:25,  1.59it/s]Extractor Predicting: 417it [04:26,  1.59it/s]Extractor Predicting: 418it [04:26,  1.62it/s]Extractor Predicting: 419it [04:27,  1.62it/s]Extractor Predicting: 420it [04:27,  1.62it/s]Extractor Predicting: 421it [04:28,  1.59it/s]Extractor Predicting: 422it [04:29,  1.59it/s]Extractor Predicting: 423it [04:29,  1.60it/s]Extractor Predicting: 424it [04:30,  1.40it/s]Extractor Predicting: 425it [04:31,  1.43it/s]Extractor Predicting: 426it [04:31,  1.50it/s]Extractor Predicting: 427it [04:32,  1.51it/s]Extractor Predicting: 428it [04:33,  1.48it/s]Extractor Predicting: 429it [04:33,  1.70it/s]Extractor Predicting: 429it [04:33,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:22,266 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:22,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:22,280 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:22,280 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:22,280 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:44:23,022 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:44:23,023 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:44:23,625 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:44:24,742 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:44:24,742 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:27,830 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:27,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:27,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:27,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:44:27,870 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:44:28,789 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:44:28,790 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:44:29,449 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:44:29,733 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:44:29,733 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.19394261424017004,
  "recall": 0.10649679050768333,
  "score": 0.13749372174786542,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 5it [00:03,  1.53it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1951219512195122,
  "recall": 0.03669724770642202,
  "score": 0.06177606177606178,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
