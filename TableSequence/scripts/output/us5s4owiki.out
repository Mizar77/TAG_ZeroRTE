Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_5_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/wiki/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:32, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:34<02:16, 17.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:02<02:36, 22.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:19<02:01, 20.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:36<01:35, 19.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:52<01:12, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:08<00:51, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:22<00:32, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:36<00:15, 15.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:52<00:00, 15.52s/it]Generating: 100%|██████████| 10/10 [02:52<00:00, 17.21s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.80859375, 'errors': {'', "('William', 'given name', '', 'He was born in the province of Oise , his father being William , Duke of Normandy , 1st Earl of Normandy , and his mother being Mary .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 133, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 287, 'raw': 416}
{'target': 600, 'success': 310, 'raw': 448}
{'target': 600, 'success': 333, 'raw': 480}
{'target': 600, 'success': 351, 'raw': 512}
{'target': 600, 'success': 377, 'raw': 544}
{'target': 600, 'success': 403, 'raw': 576}
{'target': 600, 'success': 424, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 468, 'raw': 672}
{'target': 600, 'success': 486, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 532, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 576, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Brazil', 'languages spoken, written or signed', '', 'Many languages are spoken in Brazil , and there are various communities and organizations advocating for a stronger free and open society .')", "('Estonia', 'languages spoken, written or signed', '', 'The language of Estonia is Estonian , the official language of the Republic of the Union ( DSO ) ) .')"}}
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n']
['Relation : lowest point . Context : Later in the year ( 1143 ) , Puyi and his allies made a conquest over the eastern part of the province of Sinti , which was occupied by the Franks and Normans . Head Entity : Sinti , Tail Entity : southern part .\n', 'Relation : lowest point . Context : After the death of Emperor Xuanzong his elder son Liu Bang was the first emperor to die at the age of only two . Head Entity : emperor , Tail Entity : highest point .\n']
{'target': 600, 'success': 12, 'raw': 32}
{'target': 600, 'success': 23, 'raw': 64}
{'target': 600, 'success': 32, 'raw': 96}
{'target': 600, 'success': 44, 'raw': 128}
{'target': 600, 'success': 55, 'raw': 160}
{'target': 600, 'success': 70, 'raw': 192}
{'target': 600, 'success': 86, 'raw': 224}
{'target': 600, 'success': 102, 'raw': 256}
{'target': 600, 'success': 115, 'raw': 288}
{'target': 600, 'success': 125, 'raw': 320}
{'target': 600, 'success': 141, 'raw': 352}
{'target': 600, 'success': 158, 'raw': 384}
{'target': 600, 'success': 176, 'raw': 416}
{'target': 600, 'success': 192, 'raw': 448}
{'target': 600, 'success': 206, 'raw': 480}
{'target': 600, 'success': 221, 'raw': 512}
{'target': 600, 'success': 233, 'raw': 544}
{'target': 600, 'success': 249, 'raw': 576}
{'target': 600, 'success': 265, 'raw': 608}
{'target': 600, 'success': 273, 'raw': 640}
{'target': 600, 'success': 288, 'raw': 672}
{'target': 600, 'success': 308, 'raw': 704}
{'target': 600, 'success': 317, 'raw': 736}
{'target': 600, 'success': 332, 'raw': 768}
{'target': 600, 'success': 342, 'raw': 800}
{'target': 600, 'success': 353, 'raw': 832}
{'target': 600, 'success': 369, 'raw': 864}
{'target': 600, 'success': 385, 'raw': 896}
{'target': 600, 'success': 394, 'raw': 928}
{'target': 600, 'success': 409, 'raw': 960}
{'target': 600, 'success': 423, 'raw': 992}
{'target': 600, 'success': 432, 'raw': 1024}
{'target': 600, 'success': 446, 'raw': 1056}
{'target': 600, 'success': 460, 'raw': 1088}
{'target': 600, 'success': 473, 'raw': 1120}
{'target': 600, 'success': 486, 'raw': 1152}
{'target': 600, 'success': 498, 'raw': 1184}
{'target': 600, 'success': 512, 'raw': 1216}
{'target': 600, 'success': 522, 'raw': 1248}
{'target': 600, 'success': 533, 'raw': 1280}
{'target': 600, 'success': 550, 'raw': 1312}
{'target': 600, 'success': 560, 'raw': 1344}
{'target': 600, 'success': 572, 'raw': 1376}
{'target': 600, 'success': 584, 'raw': 1408}
{'target': 600, 'success': 600, 'raw': 1440}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.4166666666666667, 'errors': {'', "('second', 'lowest point', '', 'He is currently playing with fellow former New Zealand prop Tom Paine in the Championship side Auckland Roosters , and he moved up the rankings to second .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('New York Giants', 'lowest point', '', 'In the second season , a team led by George Foreman and David Southee were allowed to make another bowl game against the New York Giants .')", "('2004 FIFA World Cup', 'lowest point', '', 'The next day , he made a triumphant recovery and made his international debut at the 2004 FIFA World Cup against Ecuador .')", "('Manchester', 'lowest point', '', 'The area was home to the city of Manchester ( now Manchester City ) , founded in 1859 by the English Civil War hero Sir Walter Scott .')", "('63', 'lowest point', '', 'On the night of 9 November 2014 , the Southeastern Conference led by Jaylen Brown was led by Duke Blue Devils in a 63 66 victory over the Southeast Division rival St. Charles .')", "('lowest point', 'lowest point', '', 'For comparison , in 1998 , it was the lowest point per square mile since 1960 , when it was 1 . 10 .')"}}
['Relation : mother . Context : Later in Life , he came to love the beauty and natural beauty of the forests at the end of the third trimester , when she suffered death . Head Entity : forests at the end of the third trimester , Tail Entity : Mother .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('king', 'mother', '', 'The first male and last female monarch of the Kingdom of Mexico , the king of the Andes ( d.')"}}
['Relation : mouth of the watercourse . Context : Later in the year ( 1143 ) , Puyi and his allies made atone for the fall of the last King of the Puebla , Puzco . Head Entity : Puyi , Tail Entity : Puducese .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 200, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 311, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 357, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 440, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 488, 'raw': 704}
{'target': 600, 'success': 509, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.7013888888888888, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n']
['Relation : genre . Context : The song was nominated for the Grammy Award for Best Rap Singles and Best Pop Albums in 1985 , while the singles from 1993 and 1994 were also nominated for the Grammy Award for Best Pop Albums . Head Entity : 1997 , Tail Entity : Rap Singles .\n', 'Relation : genre . Context : Eileen McClelland ( born June 24 , 1953 ) is an American radio talk show host who has appeared as a regular on both the Howard Stern Show and the Morning Mix . Head Entity : Howard Stern Show , Tail Entity : television talk show .\n', 'Relation : genre . Context : This film explores the social , social structure of the New York city of Times Square , which is populated and industrialized by wealthy Manhattanites . Head Entity : Times Square , Tail Entity : New York City .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 493, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 570, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.76625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('programming languages', 'is a list of', '', 'It can be used primarily in conjunction with other programming languages such as .')", "('The albums', 'is a list of', '', 'The albums songs , songs played and discography of the albums members , the artist , are listed in alphabetical order .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8111979166666666, 'errors': {'', 'too many values to unpack (expected 2)', "('Pluto', 'located on astronomical body', '', 'In the past , Pluto has been considered to be a binary system inhabited by one giant being , as well as being the first to be named after Pluto .')", "('Its orbit', 'located on astronomical body', '', 'Its orbit shows an eccentricity of 0 . 18 and an inclination of 8 degrees from the plane of the ecliptic .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('it', 'manufacturer', '', 'It is known in Japan for its high quality and performance components , along with its long range of performance airsoft s.')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12731
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12831, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.40s/it]Extractor Estimating: 2it [00:18,  8.27s/it]Extractor Estimating: 3it [00:19,  4.82s/it]Extractor Estimating: 4it [00:19,  3.18s/it]Extractor Estimating: 5it [00:20,  2.26s/it]Extractor Estimating: 6it [00:20,  1.70s/it]Extractor Estimating: 7it [00:21,  1.35s/it]Extractor Estimating: 8it [00:22,  1.13s/it]Extractor Estimating: 9it [00:22,  1.04it/s]Extractor Estimating: 10it [00:23,  1.17it/s]Extractor Estimating: 11it [00:24,  1.28it/s]Extractor Estimating: 12it [00:24,  1.40it/s]Extractor Estimating: 13it [00:25,  1.45it/s]Extractor Estimating: 14it [00:25,  1.47it/s]Extractor Estimating: 15it [00:26,  1.52it/s]Extractor Estimating: 16it [00:27,  1.51it/s]Extractor Estimating: 17it [00:28,  1.20it/s]Extractor Estimating: 18it [00:29,  1.29it/s]Extractor Estimating: 19it [00:29,  1.35it/s]Extractor Estimating: 20it [00:30,  1.44it/s]Extractor Estimating: 21it [00:30,  1.50it/s]Extractor Estimating: 22it [00:31,  1.54it/s]Extractor Estimating: 23it [00:32,  1.49it/s]Extractor Estimating: 24it [00:33,  1.43it/s]Extractor Estimating: 25it [00:33,  1.54it/s]Extractor Estimating: 26it [00:34,  1.63it/s]Extractor Estimating: 27it [00:34,  1.65it/s]Extractor Estimating: 28it [00:35,  1.68it/s]Extractor Estimating: 29it [00:35,  1.72it/s]Extractor Estimating: 30it [00:36,  1.74it/s]Extractor Estimating: 31it [00:36,  1.73it/s]Extractor Estimating: 32it [00:37,  1.73it/s]Extractor Estimating: 33it [00:38,  1.75it/s]Extractor Estimating: 34it [00:38,  1.77it/s]Extractor Estimating: 35it [00:39,  1.72it/s]Extractor Estimating: 36it [00:39,  1.72it/s]Extractor Estimating: 37it [00:40,  1.70it/s]Extractor Estimating: 38it [00:41,  1.73it/s]Extractor Estimating: 39it [00:41,  1.78it/s]Extractor Estimating: 40it [00:42,  1.73it/s]Extractor Estimating: 41it [00:42,  1.69it/s]Extractor Estimating: 42it [00:43,  1.72it/s]Extractor Estimating: 43it [00:44,  1.27it/s]Extractor Estimating: 44it [00:45,  1.39it/s]Extractor Estimating: 45it [00:45,  1.51it/s]Extractor Estimating: 46it [00:46,  1.55it/s]Extractor Estimating: 47it [00:46,  1.62it/s]Extractor Estimating: 48it [00:47,  1.69it/s]Extractor Estimating: 49it [00:47,  1.72it/s]Extractor Estimating: 50it [00:48,  1.75it/s]Extractor Estimating: 51it [00:49,  1.69it/s]Extractor Estimating: 52it [00:49,  1.65it/s]Extractor Estimating: 53it [00:50,  1.63it/s]Extractor Estimating: 54it [00:51,  1.61it/s]Extractor Estimating: 55it [00:51,  1.58it/s]Extractor Estimating: 56it [00:52,  1.56it/s]Extractor Estimating: 57it [00:53,  1.52it/s]Extractor Estimating: 58it [00:53,  1.54it/s]Extractor Estimating: 59it [00:54,  1.59it/s]Extractor Estimating: 60it [00:54,  1.58it/s]Extractor Estimating: 61it [00:55,  1.55it/s]Extractor Estimating: 62it [00:56,  1.57it/s]Extractor Estimating: 63it [00:56,  1.53it/s]Extractor Estimating: 64it [00:57,  1.55it/s]Extractor Estimating: 65it [00:58,  1.57it/s]Extractor Estimating: 66it [00:58,  1.54it/s]Extractor Estimating: 67it [00:59,  1.55it/s]Extractor Estimating: 68it [01:00,  1.55it/s]Extractor Estimating: 69it [01:00,  1.55it/s]Extractor Estimating: 70it [01:01,  1.56it/s]Extractor Estimating: 71it [01:02,  1.55it/s]Extractor Estimating: 72it [01:02,  1.56it/s]Extractor Estimating: 73it [01:03,  1.52it/s]Extractor Estimating: 74it [01:03,  1.54it/s]Extractor Estimating: 75it [01:04,  1.55it/s]Extractor Estimating: 76it [01:05,  1.49it/s]Extractor Estimating: 77it [01:05,  1.52it/s]Extractor Estimating: 78it [01:06,  1.50it/s]Extractor Estimating: 79it [01:07,  1.47it/s]Extractor Estimating: 80it [01:07,  1.50it/s]Extractor Estimating: 81it [01:08,  1.48it/s]Extractor Estimating: 82it [01:09,  1.52it/s]Extractor Estimating: 83it [01:10,  1.48it/s]Extractor Estimating: 84it [01:10,  1.48it/s]Extractor Estimating: 85it [01:11,  1.46it/s]Extractor Estimating: 86it [01:12,  1.15it/s]Extractor Estimating: 87it [01:13,  1.20it/s]Extractor Estimating: 88it [01:14,  1.27it/s]Extractor Estimating: 89it [01:15,  1.23it/s]Extractor Estimating: 90it [01:15,  1.29it/s]Extractor Estimating: 91it [01:16,  1.35it/s]Extractor Estimating: 92it [01:16,  1.41it/s]Extractor Estimating: 93it [01:17,  1.44it/s]Extractor Estimating: 94it [01:18,  1.46it/s]Extractor Estimating: 95it [01:19,  1.45it/s]Extractor Estimating: 96it [01:19,  1.46it/s]Extractor Estimating: 97it [01:20,  1.53it/s]Extractor Estimating: 98it [01:21,  1.47it/s]Extractor Estimating: 99it [01:21,  1.47it/s]Extractor Estimating: 100it [01:22,  1.47it/s]Extractor Estimating: 101it [01:23,  1.47it/s]Extractor Estimating: 102it [01:23,  1.52it/s]Extractor Estimating: 103it [01:24,  1.55it/s]Extractor Estimating: 104it [01:24,  1.62it/s]Extractor Estimating: 105it [01:25,  1.60it/s]Extractor Estimating: 106it [01:26,  1.66it/s]Extractor Estimating: 107it [01:26,  1.68it/s]Extractor Estimating: 108it [01:27,  1.68it/s]Extractor Estimating: 109it [01:27,  1.69it/s]Extractor Estimating: 110it [01:28,  1.71it/s]Extractor Estimating: 111it [01:28,  1.67it/s]Extractor Estimating: 112it [01:29,  1.63it/s]Extractor Estimating: 113it [01:30,  1.58it/s]Extractor Estimating: 114it [01:30,  1.65it/s]Extractor Estimating: 115it [01:31,  1.64it/s]Extractor Estimating: 116it [01:32,  1.68it/s]Extractor Estimating: 117it [01:32,  1.63it/s]Extractor Estimating: 118it [01:33,  1.62it/s]Extractor Estimating: 119it [01:34,  1.55it/s]Extractor Estimating: 120it [01:34,  1.59it/s]Extractor Estimating: 121it [01:35,  1.56it/s]Extractor Estimating: 122it [01:35,  1.55it/s]Extractor Estimating: 123it [01:36,  1.56it/s]Extractor Estimating: 124it [01:37,  1.57it/s]Extractor Estimating: 125it [01:37,  1.62it/s]Extractor Estimating: 126it [01:38,  1.63it/s]Extractor Estimating: 127it [01:39,  1.59it/s]Extractor Estimating: 128it [01:39,  1.61it/s]Extractor Estimating: 129it [01:40,  1.62it/s]Extractor Estimating: 130it [01:40,  1.62it/s]Extractor Estimating: 131it [01:41,  1.65it/s]Extractor Estimating: 132it [01:42,  1.64it/s]Extractor Estimating: 133it [01:42,  1.65it/s]Extractor Estimating: 134it [01:43,  1.66it/s]Extractor Estimating: 135it [01:43,  1.61it/s]Extractor Estimating: 136it [01:44,  1.62it/s]Extractor Estimating: 137it [01:45,  1.57it/s]Extractor Estimating: 138it [01:45,  1.59it/s]Extractor Estimating: 139it [01:46,  1.61it/s]Extractor Estimating: 140it [01:47,  1.55it/s]Extractor Estimating: 141it [01:47,  1.56it/s]Extractor Estimating: 142it [01:48,  1.56it/s]Extractor Estimating: 143it [01:49,  1.55it/s]Extractor Estimating: 144it [01:49,  1.52it/s]Extractor Estimating: 145it [01:50,  1.57it/s]Extractor Estimating: 146it [01:50,  1.58it/s]Extractor Estimating: 147it [01:51,  1.61it/s]Extractor Estimating: 148it [01:52,  1.60it/s]Extractor Estimating: 149it [01:52,  1.55it/s]Extractor Estimating: 150it [01:53,  1.64it/s]Extractor Estimating: 151it [01:53,  1.66it/s]Extractor Estimating: 152it [01:54,  1.66it/s]Extractor Estimating: 153it [01:55,  1.70it/s]Extractor Estimating: 154it [01:55,  1.64it/s]Extractor Estimating: 155it [01:56,  1.67it/s]Extractor Estimating: 156it [01:56,  1.65it/s]Extractor Estimating: 157it [01:57,  1.46it/s]Extractor Estimating: 158it [01:58,  1.50it/s]Extractor Estimating: 159it [01:59,  1.57it/s]Extractor Estimating: 160it [01:59,  1.57it/s]Extractor Estimating: 161it [02:00,  1.63it/s]Extractor Estimating: 162it [02:00,  1.62it/s]Extractor Estimating: 163it [02:01,  1.64it/s]Extractor Estimating: 164it [02:02,  1.61it/s]Extractor Estimating: 165it [02:02,  1.62it/s]Extractor Estimating: 166it [02:03,  1.66it/s]Extractor Estimating: 167it [02:03,  1.71it/s]Extractor Estimating: 168it [02:04,  1.69it/s]Extractor Estimating: 169it [02:05,  1.66it/s]Extractor Estimating: 170it [02:05,  1.66it/s]Extractor Estimating: 171it [02:06,  1.66it/s]Extractor Estimating: 172it [02:06,  1.74it/s]Extractor Estimating: 173it [02:07,  1.65it/s]Extractor Estimating: 174it [02:08,  1.62it/s]Extractor Estimating: 175it [02:08,  1.67it/s]Extractor Estimating: 176it [02:09,  1.72it/s]Extractor Estimating: 177it [02:09,  1.66it/s]Extractor Estimating: 178it [02:10,  1.69it/s]Extractor Estimating: 179it [02:10,  1.74it/s]Extractor Estimating: 180it [02:11,  1.72it/s]Extractor Estimating: 181it [02:12,  1.73it/s]Extractor Estimating: 182it [02:12,  1.71it/s]Extractor Estimating: 183it [02:13,  1.77it/s]Extractor Estimating: 184it [02:13,  1.72it/s]Extractor Estimating: 185it [02:14,  1.71it/s]Extractor Estimating: 186it [02:15,  1.62it/s]Extractor Estimating: 187it [02:15,  1.63it/s]Extractor Estimating: 188it [02:16,  1.68it/s]Extractor Estimating: 189it [02:16,  1.65it/s]Extractor Estimating: 190it [02:17,  1.65it/s]Extractor Estimating: 191it [02:18,  1.67it/s]Extractor Estimating: 192it [02:18,  1.68it/s]Extractor Estimating: 193it [02:19,  1.78it/s]Extractor Estimating: 194it [02:19,  1.72it/s]Extractor Estimating: 195it [02:20,  1.65it/s]Extractor Estimating: 196it [02:21,  1.67it/s]Extractor Estimating: 197it [02:21,  1.70it/s]Extractor Estimating: 198it [02:22,  1.69it/s]Extractor Estimating: 199it [02:22,  1.67it/s]Extractor Estimating: 200it [02:23,  1.65it/s]Extractor Estimating: 201it [02:24,  1.66it/s]Extractor Estimating: 202it [02:24,  1.67it/s]Extractor Estimating: 203it [02:25,  1.68it/s]Extractor Estimating: 204it [02:25,  1.69it/s]Extractor Estimating: 205it [02:26,  1.62it/s]Extractor Estimating: 206it [02:27,  1.69it/s]Extractor Estimating: 207it [02:27,  1.65it/s]Extractor Estimating: 208it [02:28,  1.73it/s]Extractor Estimating: 209it [02:28,  1.69it/s]Extractor Estimating: 210it [02:29,  1.68it/s]Extractor Estimating: 211it [02:30,  1.58it/s]Extractor Estimating: 212it [02:30,  1.61it/s]Extractor Estimating: 213it [02:31,  1.65it/s]Extractor Estimating: 214it [02:31,  1.64it/s]Extractor Estimating: 215it [02:32,  1.64it/s]Extractor Estimating: 216it [02:33,  1.70it/s]Extractor Estimating: 217it [02:33,  1.69it/s]Extractor Estimating: 218it [02:34,  1.67it/s]Extractor Estimating: 219it [02:34,  1.68it/s]Extractor Estimating: 220it [02:35,  1.65it/s]Extractor Estimating: 221it [02:36,  1.71it/s]Extractor Estimating: 222it [02:36,  1.70it/s]Extractor Estimating: 223it [02:37,  1.71it/s]Extractor Estimating: 224it [02:37,  1.69it/s]Extractor Estimating: 225it [02:38,  1.66it/s]Extractor Estimating: 226it [02:39,  1.62it/s]Extractor Estimating: 227it [02:39,  1.61it/s]Extractor Estimating: 228it [02:40,  1.59it/s]Extractor Estimating: 229it [02:40,  1.59it/s]Extractor Estimating: 230it [02:41,  1.61it/s]Extractor Estimating: 231it [02:42,  1.62it/s]Extractor Estimating: 232it [02:42,  1.62it/s]Extractor Estimating: 233it [02:43,  1.54it/s]Extractor Estimating: 234it [02:44,  1.55it/s]Extractor Estimating: 235it [02:44,  1.59it/s]Extractor Estimating: 236it [02:45,  1.59it/s]Extractor Estimating: 237it [02:46,  1.54it/s]Extractor Estimating: 238it [02:46,  1.59it/s]Extractor Estimating: 239it [02:47,  1.58it/s]Extractor Estimating: 240it [02:48,  1.47it/s]Extractor Estimating: 241it [02:48,  1.50it/s]Extractor Estimating: 242it [02:49,  1.51it/s]Extractor Estimating: 243it [02:49,  1.58it/s]Extractor Estimating: 244it [02:50,  1.54it/s]Extractor Estimating: 245it [02:51,  1.53it/s]Extractor Estimating: 246it [02:51,  1.55it/s]Extractor Estimating: 247it [02:52,  1.57it/s]Extractor Estimating: 248it [02:53,  1.59it/s]Extractor Estimating: 249it [02:53,  1.57it/s]Extractor Estimating: 250it [02:54,  1.54it/s]Extractor Estimating: 250it [02:54,  1.43it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 1019 mean pseudo reward: 0.9321196511141194
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 15096
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15196, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15196, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 14, avg_time 1.246, loss:321.8822
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 28, avg_time 0.982, loss:271.6480
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 42, avg_time 0.984, loss:238.4784
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 13, avg_time 0.974, loss:205.8066
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 27, avg_time 0.968, loss:183.3345
>> valid entity prec:0.5046, rec:0.4304, f1:0.4645
>> valid relation prec:0.1806, rec:0.0694, f1:0.1003
>> valid relation with NER prec:0.1806, rec:0.0694, f1:0.1003
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 41, avg_time 2.318, loss:170.6659
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 12, avg_time 0.971, loss:154.6380
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 26, avg_time 0.974, loss:147.7685
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:17:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:17:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-17-37_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:17:38 - WARNING - datasets.builder -   Using custom data configuration default-a1e44cc9e85080f6
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a1e44cc9e85080f6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:17:40,366 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:17:40,367 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:17:40,368 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:17:40,369 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:17:40,452 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,498 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,498 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,499 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,499 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,499 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:17:40,499 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:17:40,792 >> loading weights file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:17:44,010 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:17:44,011 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a1e44cc9e85080f6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 08:17:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14cfac1589e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.60ba/s]100%|██████████| 2/2 [00:00<00:00,  3.15ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.31ba/s] 40%|████      | 2/5 [00:00<00:00,  3.93ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.12ba/s]100%|██████████| 5/5 [00:01<00:00,  4.96ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  5.75ba/s]100%|██████████| 2/2 [00:00<00:00, 11.11ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.70ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.08ba/s]100%|██████████| 5/5 [00:00<00:00, 10.49ba/s]100%|██████████| 5/5 [00:00<00:00,  8.83ba/s]
[INFO|trainer.py:414] 2023-08-29 08:17:47,409 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:17:47,473 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:17:47,474 >>   Num examples = 1019
[INFO|trainer.py:1149] 2023-08-29 08:17:47,474 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:17:47,474 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:17:47,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:17:47,474 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:17:47,474 >>   Total optimization steps = 80
  0%|          | 0/80 [00:00<?, ?it/s]  1%|▏         | 1/80 [00:00<00:23,  3.30it/s]  2%|▎         | 2/80 [00:00<00:23,  3.38it/s]  4%|▍         | 3/80 [00:00<00:22,  3.41it/s]  5%|▌         | 4/80 [00:01<00:22,  3.41it/s]  6%|▋         | 5/80 [00:01<00:21,  3.42it/s]  8%|▊         | 6/80 [00:01<00:21,  3.43it/s]  9%|▉         | 7/80 [00:02<00:21,  3.43it/s] 10%|█         | 8/80 [00:02<00:20,  3.43it/s] 11%|█▏        | 9/80 [00:02<00:20,  3.43it/s] 12%|█▎        | 10/80 [00:02<00:20,  3.43it/s] 14%|█▍        | 11/80 [00:03<00:20,  3.41it/s] 15%|█▌        | 12/80 [00:03<00:20,  3.29it/s] 16%|█▋        | 13/80 [00:03<00:20,  3.32it/s] 18%|█▊        | 14/80 [00:04<00:19,  3.34it/s] 19%|█▉        | 15/80 [00:04<00:19,  3.35it/s] 20%|██        | 16/80 [00:04<00:18,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:17:52,187 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:17:52,187 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:17:52,188 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.33it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.83it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.17it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.33it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.01it/s][A
  6%|▋         | 32/505 [00:00<00:11, 42.96it/s][A
  7%|▋         | 37/505 [00:00<00:10, 43.56it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.81it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.04it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.27it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.22it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.24it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.97it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.15it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.22it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.36it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.31it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.33it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.35it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.23it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.21it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.04it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.11it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.34it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.46it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.43it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.48it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.31it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.19it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.14it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.11it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.00it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.47it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.91it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.04it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.13it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.03it/s][A
 40%|████      | 202/505 [00:04<00:06, 43.98it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.89it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.02it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.26it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.41it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.40it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.37it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.25it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.16it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.03it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.99it/s][A
 51%|█████     | 257/505 [00:05<00:05, 41.98it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 42.72it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.33it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.76it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.08it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.15it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.14it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.99it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.77it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.77it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.01it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.16it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.42it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.47it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.40it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.36it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.05it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.85it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.94it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.15it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.30it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.51it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.48it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.46it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.26it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.05it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.84it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.01it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.54it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.91it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.23it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.20it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.07it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.87it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 43.64it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.88it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.19it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.38it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.50it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.35it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.24it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.06it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.80it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.81it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 41.99it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 42.88it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.45it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.79it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.00it/s][A                                               
                                                 [A 20%|██        | 16/80 [00:16<00:18,  3.43it/s]
100%|██████████| 505/505 [00:11<00:00, 44.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:18:03,719 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16
[INFO|configuration_utils.py:351] 2023-08-29 08:18:03,912 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:18:06,318 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:18:06,386 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:18:06,428 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16/special_tokens_map.json
 21%|██▏       | 17/80 [00:26<07:14,  6.90s/it] 22%|██▎       | 18/80 [00:27<05:05,  4.93s/it] 24%|██▍       | 19/80 [00:27<03:35,  3.54s/it] 25%|██▌       | 20/80 [00:27<02:33,  2.57s/it] 26%|██▋       | 21/80 [00:28<01:51,  1.88s/it] 28%|██▊       | 22/80 [00:28<01:21,  1.41s/it] 29%|██▉       | 23/80 [00:28<01:01,  1.07s/it] 30%|███       | 24/80 [00:29<00:47,  1.19it/s] 31%|███▏      | 25/80 [00:29<00:37,  1.48it/s] 32%|███▎      | 26/80 [00:29<00:30,  1.78it/s] 34%|███▍      | 27/80 [00:29<00:25,  2.08it/s] 35%|███▌      | 28/80 [00:30<00:22,  2.29it/s] 36%|███▋      | 29/80 [00:30<00:20,  2.54it/s] 38%|███▊      | 30/80 [00:30<00:18,  2.74it/s] 39%|███▉      | 31/80 [00:31<00:16,  2.91it/s] 40%|████      | 32/80 [00:31<00:15,  3.10it/s][INFO|trainer.py:2140] 2023-08-29 08:18:18,952 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:18:18,952 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:18:18,952 >>   Batch size = 8
{'eval_loss': 1.0996289253234863, 'eval_runtime': 11.4811, 'eval_samples_per_second': 351.795, 'eval_steps_per_second': 43.985, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.55it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.00it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.18it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.44it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.04it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.73it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.42it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.37it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.37it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.47it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.56it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.27it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.28it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.27it/s][A
 15%|█▌        | 77/505 [00:01<00:10, 41.01it/s][A
 16%|█▌        | 82/505 [00:01<00:10, 42.08it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 42.76it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.38it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 43.83it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.93it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.95it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 43.95it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.78it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.86it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.84it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.09it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.30it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.33it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.38it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.26it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.11it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.98it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.98it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.99it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 44.19it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.34it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.35it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.35it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.25it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.07it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.04it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.75it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.93it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.12it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.31it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.23it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.16it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 43.99it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.85it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.81it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.94it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.09it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.28it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.34it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.50it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.40it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.12it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.04it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.95it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.98it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.07it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.26it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.42it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.41it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.06it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.94it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.96it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 42.99it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.52it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.88it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.11it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.06it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.96it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.96it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.86it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.85it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.11it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.15it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.31it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.39it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.23it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.16it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.11it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.95it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.14it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.23it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.32it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.26it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.20it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.11it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.94it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.95it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.18it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.64it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.97it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.03it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.14it/s][A                                               
                                                 [A 40%|████      | 32/80 [00:42<00:15,  3.10it/s]
100%|██████████| 505/505 [00:11<00:00, 44.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:18:30,451 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32
[INFO|configuration_utils.py:351] 2023-08-29 08:18:30,593 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:18:33,760 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:18:33,898 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:18:33,963 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32/special_tokens_map.json
 41%|████▏     | 33/80 [00:53<05:20,  6.82s/it] 42%|████▎     | 34/80 [00:53<03:43,  4.87s/it] 44%|████▍     | 35/80 [00:54<02:37,  3.50s/it] 45%|████▌     | 36/80 [00:54<01:51,  2.54s/it] 46%|████▋     | 37/80 [00:54<01:20,  1.86s/it] 48%|████▊     | 38/80 [00:54<00:58,  1.39s/it] 49%|████▉     | 39/80 [00:55<00:43,  1.06s/it] 50%|█████     | 40/80 [00:55<00:33,  1.20it/s] 51%|█████▏    | 41/80 [00:55<00:26,  1.49it/s] 52%|█████▎    | 42/80 [00:56<00:21,  1.79it/s] 54%|█████▍    | 43/80 [00:56<00:17,  2.08it/s] 55%|█████▌    | 44/80 [00:56<00:15,  2.36it/s] 56%|█████▋    | 45/80 [00:57<00:13,  2.57it/s] 57%|█████▊    | 46/80 [00:57<00:12,  2.78it/s] 59%|█████▉    | 47/80 [00:57<00:11,  2.95it/s] 60%|██████    | 48/80 [00:57<00:10,  3.14it/s][INFO|trainer.py:2140] 2023-08-29 08:18:45,355 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:18:45,355 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:18:45,355 >>   Batch size = 8
{'eval_loss': 1.0805670022964478, 'eval_runtime': 11.4753, 'eval_samples_per_second': 351.972, 'eval_steps_per_second': 44.007, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.44it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.08it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.40it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.44it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.05it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.73it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.54it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.37it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.29it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.43it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.39it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.39it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.35it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.30it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.29it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.18it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.35it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.60it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 43.89it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.09it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.14it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.19it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.16it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.09it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.06it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.11it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.35it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.37it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.31it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.30it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.30it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.22it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.10it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.07it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.17it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.30it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.40it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.34it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.27it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.27it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.22it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.18it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 42.58it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 43.20it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 43.59it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.92it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 43.94it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.02it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.02it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.99it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.78it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.97it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.03it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.28it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.43it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.34it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.29it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.18it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.09it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.98it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.17it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.39it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.48it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.27it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.26it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.11it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.05it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.04it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.70it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.03it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.25it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.21it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.12it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.02it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.90it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.84it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.08it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.22it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.36it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.31it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.28it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.13it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.06it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.97it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.93it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.20it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.28it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.21it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.31it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.22it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.18it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.11it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.99it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.66it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.96it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 43.99it/s][A                                               
                                                 [A 60%|██████    | 48/80 [01:09<00:10,  3.14it/s]
100%|██████████| 505/505 [00:11<00:00, 43.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:18:56,862 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-29 08:18:57,017 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:18:59,612 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:18:59,878 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:18:59,997 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48/special_tokens_map.json
 61%|██████▏   | 49/80 [01:20<03:38,  7.03s/it] 62%|██████▎   | 50/80 [01:20<02:30,  5.03s/it] 64%|██████▍   | 51/80 [01:21<01:44,  3.61s/it] 65%|██████▌   | 52/80 [01:21<01:13,  2.61s/it] 66%|██████▋   | 53/80 [01:21<00:51,  1.92s/it] 68%|██████▊   | 54/80 [01:22<00:37,  1.43s/it] 69%|██████▉   | 55/80 [01:22<00:27,  1.09s/it] 70%|███████   | 56/80 [01:22<00:20,  1.17it/s] 71%|███████▏  | 57/80 [01:22<00:15,  1.46it/s] 72%|███████▎  | 58/80 [01:23<00:12,  1.76it/s] 74%|███████▍  | 59/80 [01:23<00:10,  2.06it/s] 75%|███████▌  | 60/80 [01:23<00:08,  2.33it/s] 76%|███████▋  | 61/80 [01:24<00:07,  2.50it/s] 78%|███████▊  | 62/80 [01:24<00:06,  2.71it/s] 79%|███████▉  | 63/80 [01:24<00:05,  2.88it/s] 80%|████████  | 64/80 [01:25<00:05,  3.07it/s][INFO|trainer.py:2140] 2023-08-29 08:19:12,554 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:19:12,554 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:19:12,554 >>   Batch size = 8
{'eval_loss': 1.0800036191940308, 'eval_runtime': 11.4489, 'eval_samples_per_second': 352.783, 'eval_steps_per_second': 44.109, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.86it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.20it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.38it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.42it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.17it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.70it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.51it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.35it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.37it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.46it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.53it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.45it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.30it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.22it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.13it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.18it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 42.40it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 43.15it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 43.67it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.87it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.07it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.02it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.04it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.83it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.91it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.36it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.41it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.48it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.35it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.27it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.02it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.92it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.97it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.11it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.36it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.38it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.47it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.36it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.15it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.89it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 43.85it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 41.52it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 42.44it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 43.14it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.64it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.01it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.09it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.95it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.88it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.64it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.52it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.73it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.03it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.13it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.36it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.55it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.40it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.20it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.81it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.81it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.88it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.15it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.28it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.44it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.47it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.47it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.26it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 43.87it/s][A
 71%|███████   | 357/505 [00:08<00:03, 41.97it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 42.80it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 43.37it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 43.72it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.99it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.16it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.96it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 43.63it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.75it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.89it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.15it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.36it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.50it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.50it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.34it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.97it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 43.77it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.88it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.98it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.11it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.28it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.45it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.50it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.30it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.05it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.00it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.63it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.79it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.02it/s][A                                               
                                                 [A 80%|████████  | 64/80 [01:36<00:05,  3.07it/s]
100%|██████████| 505/505 [00:11<00:00, 44.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:19:24,081 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64
[INFO|configuration_utils.py:351] 2023-08-29 08:19:24,190 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:19:26,812 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:19:26,923 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:19:26,983 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64/special_tokens_map.json
 81%|████████▏ | 65/80 [01:46<01:41,  6.74s/it] 82%|████████▎ | 66/80 [01:47<01:07,  4.82s/it] 84%|████████▍ | 67/80 [01:47<00:45,  3.47s/it] 85%|████████▌ | 68/80 [01:47<00:30,  2.51s/it] 86%|████████▋ | 69/80 [01:48<00:20,  1.88s/it] 88%|████████▊ | 70/80 [01:48<00:14,  1.41s/it] 89%|████████▉ | 71/80 [01:48<00:09,  1.07s/it] 90%|█████████ | 72/80 [01:49<00:06,  1.19it/s] 91%|█████████▏| 73/80 [01:49<00:04,  1.45it/s] 92%|█████████▎| 74/80 [01:49<00:03,  1.75it/s] 94%|█████████▍| 75/80 [01:49<00:02,  2.04it/s] 95%|█████████▌| 76/80 [01:50<00:01,  2.03it/s] 96%|█████████▋| 77/80 [01:50<00:01,  2.30it/s] 98%|█████████▊| 78/80 [01:51<00:00,  2.51it/s] 99%|█████████▉| 79/80 [01:51<00:00,  2.72it/s]100%|██████████| 80/80 [01:51<00:00,  2.94it/s][INFO|trainer.py:2140] 2023-08-29 08:19:39,121 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:19:39,121 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:19:39,121 >>   Batch size = 8
{'eval_loss': 1.0784956216812134, 'eval_runtime': 11.4813, 'eval_samples_per_second': 351.789, 'eval_steps_per_second': 43.985, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.46it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.37it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.68it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.67it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.17it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.52it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.34it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.33it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.35it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.53it/s][A
 11%|█▏        | 57/505 [00:01<00:12, 37.01it/s][A
 12%|█▏        | 62/505 [00:01<00:11, 39.76it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 41.22it/s][A
 14%|█▍        | 72/505 [00:01<00:10, 42.13it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.00it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 43.56it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 43.85it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.21it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 42.37it/s][A
 20%|██        | 102/505 [00:02<00:09, 42.53it/s][A
 21%|██        | 107/505 [00:02<00:09, 42.89it/s][A
 22%|██▏       | 112/505 [00:02<00:09, 43.34it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.71it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.08it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.26it/s][A
 26%|██▌       | 132/505 [00:03<00:08, 44.41it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.07it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 43.78it/s][A
 30%|███       | 152/505 [00:03<00:08, 43.70it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.97it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.13it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.29it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.49it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 44.56it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.41it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.08it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 43.89it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.92it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.04it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.21it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.30it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.45it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.58it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.46it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 43.29it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 43.42it/s][A
 48%|████▊     | 242/505 [00:05<00:06, 43.54it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.79it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.93it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.15it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.36it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.38it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.22it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 43.95it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.07it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.03it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.12it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.09it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.29it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.45it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.44it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.36it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.15it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.15it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.12it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.14it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.13it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.37it/s][A
 70%|██████▉   | 352/505 [00:08<00:03, 44.38it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.42it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.37it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 42.40it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 42.95it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.30it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.65it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.74it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.02it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.19it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.95it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.82it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.89it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.00it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.12it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.13it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.19it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.26it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.25it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.17it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.11it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.06it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.20it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.31it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.30it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.26it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.22it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.19it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.08it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 43.04it/s][A                                               
                                                 [A100%|██████████| 80/80 [02:03<00:00,  2.94it/s]
100%|██████████| 505/505 [00:11<00:00, 43.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:19:50,710 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-29 08:19:50,923 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:19:53,630 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:19:53,773 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:19:53,822 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:20:01,045 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:20:01,068 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80 (score: 1.0778758525848389).
                                               100%|██████████| 80/80 [02:21<00:00,  2.94it/s]100%|██████████| 80/80 [02:21<00:00,  1.77s/it]
[INFO|trainer.py:1894] 2023-08-29 08:20:09,168 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 08:20:09,249 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:20:11,857 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:20:12,108 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:20:12,214 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:20:12,868 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   train_loss               =      0.585
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   train_runtime            = 0:02:21.65
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   train_samples            =       1019
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   train_samples_per_second =     35.968
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:12,869 >>   train_steps_per_second   =      0.565
{'eval_loss': 1.0778758525848389, 'eval_runtime': 11.5273, 'eval_samples_per_second': 350.386, 'eval_steps_per_second': 43.809, 'epoch': 5.0}
{'train_runtime': 141.655, 'train_samples_per_second': 35.968, 'train_steps_per_second': 0.565, 'train_loss': 0.5850375175476075, 'epoch': 5.0}
08/29/2023 08:20:13 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:20:13,132 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:20:13,132 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 08:20:13,132 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.17it/s]  2%|▏         | 12/505 [00:00<00:10, 48.83it/s]  3%|▎         | 17/505 [00:00<00:10, 47.19it/s]  4%|▍         | 22/505 [00:00<00:10, 46.48it/s]  5%|▌         | 27/505 [00:00<00:10, 45.92it/s]  6%|▋         | 32/505 [00:00<00:10, 45.57it/s]  7%|▋         | 37/505 [00:00<00:10, 45.45it/s]  8%|▊         | 42/505 [00:00<00:10, 44.87it/s]  9%|▉         | 47/505 [00:01<00:10, 44.39it/s] 10%|█         | 52/505 [00:01<00:10, 44.14it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.43it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.66it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.80it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.86it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.77it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.44it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.18it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.04it/s] 20%|██        | 102/505 [00:02<00:09, 41.34it/s] 21%|██        | 107/505 [00:02<00:09, 42.43it/s] 22%|██▏       | 112/505 [00:02<00:09, 43.13it/s] 23%|██▎       | 117/505 [00:02<00:08, 43.78it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.17it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.43it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.20it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.04it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.82it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.93it/s] 30%|███       | 152/505 [00:03<00:07, 44.13it/s] 31%|███       | 157/505 [00:03<00:07, 44.30it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.57it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.72it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.71it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.60it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.19it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.11it/s] 38%|███▊      | 192/505 [00:04<00:07, 43.96it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.18it/s] 40%|████      | 202/505 [00:04<00:06, 44.44it/s] 41%|████      | 207/505 [00:04<00:06, 44.53it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.70it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.70it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.52it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.36it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.08it/s] 47%|████▋     | 237/505 [00:05<00:06, 41.58it/s] 48%|████▊     | 242/505 [00:05<00:06, 42.54it/s] 49%|████▉     | 247/505 [00:05<00:05, 43.27it/s] 50%|████▉     | 252/505 [00:05<00:05, 43.67it/s] 51%|█████     | 257/505 [00:05<00:05, 44.12it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.35it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.40it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.28it/s] 55%|█████▍    | 277/505 [00:06<00:05, 43.95it/s] 56%|█████▌    | 282/505 [00:06<00:05, 43.84it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.05it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.24it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.43it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.58it/s] 61%|██████    | 307/505 [00:06<00:04, 44.66it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.59it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.38it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.14it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.00it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.13it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.32it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.52it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.57it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.54it/s] 71%|███████   | 357/505 [00:08<00:03, 44.57it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.34it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.16it/s] 74%|███████▎  | 372/505 [00:08<00:03, 43.82it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.01it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.28it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.46it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.48it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.43it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.45it/s] 81%|████████  | 407/505 [00:09<00:02, 44.23it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.10it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.12it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.01it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.25it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.47it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.46it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.42it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.30it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.25it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.22it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.16it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.35it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.44it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.50it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.57it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.40it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.11it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.17it/s]100%|██████████| 505/505 [00:11<00:00, 44.26it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:20:24,562 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   eval_loss               =     1.0779
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   eval_runtime            = 0:00:11.42
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   eval_samples_per_second =    353.388
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   eval_steps_per_second   =     44.184
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:20:24,562 >>   perplexity              =     2.9384
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:33,582 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:33,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:33,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:33,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:33,599 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:20:34,036 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:20:34,037 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:20:34,327 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:20:35,439 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:20:35,439 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:37,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:37,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:37,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:37,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:20:37,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:20:38,370 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:20:38,371 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:20:38,682 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:20:38,910 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:20:38,910 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-16
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-80
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-32
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-64
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-48
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.45it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.62it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.63it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.61it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:34,  1.61it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:36,  1.56it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:38,  1.55it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 64it [00:41,  1.56it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.51it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:46,  1.47it/s]Extractor Predicting: 73it [00:47,  1.47it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:48,  1.46it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:50,  1.49it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:59,  1.57it/s]Extractor Predicting: 92it [00:59,  1.59it/s]Extractor Predicting: 93it [01:00,  1.55it/s]Extractor Predicting: 94it [01:01,  1.53it/s]Extractor Predicting: 95it [01:01,  1.52it/s]Extractor Predicting: 96it [01:02,  1.50it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:03,  1.51it/s]Extractor Predicting: 99it [01:04,  1.51it/s]Extractor Predicting: 100it [01:05,  1.51it/s]Extractor Predicting: 101it [01:05,  1.47it/s]Extractor Predicting: 102it [01:06,  1.48it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:09,  1.54it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:10,  1.57it/s]Extractor Predicting: 109it [01:10,  1.57it/s]Extractor Predicting: 110it [01:11,  1.55it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:12,  1.54it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:14,  1.52it/s]Extractor Predicting: 115it [01:14,  1.56it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:16,  1.50it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:18,  1.54it/s]Extractor Predicting: 121it [01:18,  1.52it/s]Extractor Predicting: 122it [01:19,  1.40it/s]Extractor Predicting: 123it [01:20,  1.46it/s]Extractor Predicting: 124it [01:20,  1.47it/s]Extractor Predicting: 125it [01:21,  1.38it/s]Extractor Predicting: 126it [01:22,  1.44it/s]Extractor Predicting: 127it [01:23,  1.41it/s]Extractor Predicting: 128it [01:23,  1.40it/s]Extractor Predicting: 129it [01:24,  1.41it/s]Extractor Predicting: 130it [01:25,  1.42it/s]Extractor Predicting: 131it [01:25,  1.38it/s]Extractor Predicting: 132it [01:26,  1.39it/s]Extractor Predicting: 133it [01:27,  1.41it/s]Extractor Predicting: 134it [01:28,  1.45it/s]Extractor Predicting: 135it [01:28,  1.46it/s]Extractor Predicting: 136it [01:29,  1.46it/s]Extractor Predicting: 137it [01:30,  1.49it/s]Extractor Predicting: 138it [01:30,  1.44it/s]Extractor Predicting: 139it [01:31,  1.43it/s]Extractor Predicting: 140it [01:32,  1.42it/s]Extractor Predicting: 141it [01:32,  1.43it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:34,  1.44it/s]Extractor Predicting: 144it [01:34,  1.46it/s]Extractor Predicting: 145it [01:35,  1.44it/s]Extractor Predicting: 146it [01:36,  1.43it/s]Extractor Predicting: 147it [01:37,  1.44it/s]Extractor Predicting: 148it [01:37,  1.41it/s]Extractor Predicting: 149it [01:38,  1.47it/s]Extractor Predicting: 149it [01:38,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:29,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:29,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:29,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:29,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:29,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:22:29,708 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:22:29,709 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:22:30,281 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:22:31,303 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:22:31,303 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:34,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:34,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:34,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:34,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:22:34,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:22:34,914 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:22:34,915 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:22:35,518 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:22:35,684 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:22:35,684 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.23869346733668342,
  "recall": 0.07056202030205497,
  "score": 0.10892413529524173,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:08,  1.77it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:09,  1.79it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:13,  1.76it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:16,  1.78it/s]Extractor Predicting: 29it [00:16,  1.77it/s]Extractor Predicting: 30it [00:17,  1.77it/s]Extractor Predicting: 31it [00:17,  1.79it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:19,  1.73it/s]Extractor Predicting: 34it [00:19,  1.71it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:20,  1.74it/s]Extractor Predicting: 37it [00:21,  1.71it/s]Extractor Predicting: 38it [00:22,  1.68it/s]Extractor Predicting: 39it [00:22,  1.70it/s]Extractor Predicting: 40it [00:23,  1.68it/s]Extractor Predicting: 41it [00:23,  1.66it/s]Extractor Predicting: 42it [00:24,  1.69it/s]Extractor Predicting: 43it [00:25,  1.69it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:26,  1.55it/s]Extractor Predicting: 46it [00:27,  1.53it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:28,  1.59it/s]Extractor Predicting: 49it [00:28,  1.63it/s]Extractor Predicting: 50it [00:29,  1.65it/s]Extractor Predicting: 51it [00:29,  1.69it/s]Extractor Predicting: 52it [00:30,  1.66it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:31,  1.66it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:33,  1.60it/s]Extractor Predicting: 57it [00:33,  1.61it/s]Extractor Predicting: 58it [00:34,  1.63it/s]Extractor Predicting: 59it [00:34,  1.61it/s]Extractor Predicting: 60it [00:35,  1.60it/s]Extractor Predicting: 61it [00:36,  1.61it/s]Extractor Predicting: 62it [00:36,  1.65it/s]Extractor Predicting: 63it [00:37,  1.63it/s]Extractor Predicting: 64it [00:37,  1.66it/s]Extractor Predicting: 65it [00:38,  1.64it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:39,  1.65it/s]Extractor Predicting: 68it [00:40,  1.65it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:41,  1.59it/s]Extractor Predicting: 71it [00:42,  1.59it/s]Extractor Predicting: 72it [00:42,  1.62it/s]Extractor Predicting: 73it [00:43,  1.61it/s]Extractor Predicting: 74it [00:44,  1.62it/s]Extractor Predicting: 75it [00:44,  1.52it/s]Extractor Predicting: 76it [00:45,  1.54it/s]Extractor Predicting: 77it [00:46,  1.55it/s]Extractor Predicting: 78it [00:46,  1.52it/s]Extractor Predicting: 79it [00:47,  1.57it/s]Extractor Predicting: 80it [00:47,  1.62it/s]Extractor Predicting: 81it [00:48,  1.60it/s]Extractor Predicting: 82it [00:49,  1.58it/s]Extractor Predicting: 83it [00:49,  1.58it/s]Extractor Predicting: 84it [00:50,  1.61it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:51,  1.59it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:52,  1.64it/s]Extractor Predicting: 89it [00:53,  1.68it/s]Extractor Predicting: 90it [00:54,  1.68it/s]Extractor Predicting: 91it [00:54,  1.67it/s]Extractor Predicting: 92it [00:55,  1.67it/s]Extractor Predicting: 93it [00:55,  1.68it/s]Extractor Predicting: 94it [00:56,  1.69it/s]Extractor Predicting: 95it [00:57,  1.67it/s]Extractor Predicting: 96it [00:57,  1.71it/s]Extractor Predicting: 97it [00:58,  1.73it/s]Extractor Predicting: 98it [00:58,  1.70it/s]Extractor Predicting: 99it [00:59,  1.69it/s]Extractor Predicting: 100it [01:00,  1.72it/s]Extractor Predicting: 101it [01:00,  1.71it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:01,  1.64it/s]Extractor Predicting: 104it [01:02,  1.68it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:03,  1.65it/s]Extractor Predicting: 107it [01:04,  1.64it/s]Extractor Predicting: 108it [01:04,  1.61it/s]Extractor Predicting: 109it [01:05,  1.61it/s]Extractor Predicting: 110it [01:06,  1.64it/s]Extractor Predicting: 111it [01:06,  1.63it/s]Extractor Predicting: 112it [01:07,  1.62it/s]Extractor Predicting: 113it [01:07,  1.66it/s]Extractor Predicting: 114it [01:08,  1.65it/s]Extractor Predicting: 115it [01:09,  1.66it/s]Extractor Predicting: 116it [01:09,  1.69it/s]Extractor Predicting: 117it [01:10,  1.70it/s]Extractor Predicting: 118it [01:10,  1.73it/s]Extractor Predicting: 119it [01:11,  1.75it/s]Extractor Predicting: 120it [01:12,  1.75it/s]Extractor Predicting: 121it [01:12,  1.74it/s]Extractor Predicting: 122it [01:13,  1.73it/s]Extractor Predicting: 123it [01:13,  1.70it/s]Extractor Predicting: 124it [01:14,  1.70it/s]Extractor Predicting: 125it [01:14,  1.70it/s]Extractor Predicting: 126it [01:15,  1.68it/s]Extractor Predicting: 127it [01:16,  1.65it/s]Extractor Predicting: 128it [01:16,  1.63it/s]Extractor Predicting: 129it [01:17,  1.60it/s]Extractor Predicting: 130it [01:18,  1.58it/s]Extractor Predicting: 131it [01:18,  1.56it/s]Extractor Predicting: 132it [01:19,  1.59it/s]Extractor Predicting: 133it [01:20,  1.60it/s]Extractor Predicting: 134it [01:20,  1.63it/s]Extractor Predicting: 135it [01:21,  1.57it/s]Extractor Predicting: 136it [01:21,  1.57it/s]Extractor Predicting: 137it [01:22,  1.56it/s]Extractor Predicting: 138it [01:23,  1.58it/s]Extractor Predicting: 139it [01:23,  1.56it/s]Extractor Predicting: 140it [01:24,  1.54it/s]Extractor Predicting: 141it [01:25,  1.55it/s]Extractor Predicting: 142it [01:25,  1.56it/s]Extractor Predicting: 143it [01:26,  1.58it/s]Extractor Predicting: 144it [01:27,  1.55it/s]Extractor Predicting: 145it [01:27,  1.55it/s]Extractor Predicting: 146it [01:28,  1.55it/s]Extractor Predicting: 147it [01:28,  1.57it/s]Extractor Predicting: 148it [01:29,  1.58it/s]Extractor Predicting: 149it [01:30,  1.61it/s]Extractor Predicting: 150it [01:30,  1.60it/s]Extractor Predicting: 151it [01:31,  1.59it/s]Extractor Predicting: 152it [01:32,  1.58it/s]Extractor Predicting: 153it [01:32,  1.56it/s]Extractor Predicting: 154it [01:33,  1.56it/s]Extractor Predicting: 155it [01:34,  1.58it/s]Extractor Predicting: 156it [01:34,  1.63it/s]Extractor Predicting: 157it [01:35,  1.69it/s]Extractor Predicting: 158it [01:35,  1.54it/s]Extractor Predicting: 159it [01:36,  1.58it/s]Extractor Predicting: 160it [01:37,  1.61it/s]Extractor Predicting: 161it [01:37,  1.67it/s]Extractor Predicting: 162it [01:38,  1.64it/s]Extractor Predicting: 163it [01:38,  1.59it/s]Extractor Predicting: 164it [01:39,  1.63it/s]Extractor Predicting: 165it [01:40,  1.69it/s]Extractor Predicting: 166it [01:40,  1.70it/s]Extractor Predicting: 167it [01:41,  1.76it/s]Extractor Predicting: 168it [01:41,  1.77it/s]Extractor Predicting: 169it [01:42,  1.78it/s]Extractor Predicting: 170it [01:42,  1.74it/s]Extractor Predicting: 171it [01:43,  1.67it/s]Extractor Predicting: 172it [01:44,  1.64it/s]Extractor Predicting: 173it [01:44,  1.71it/s]Extractor Predicting: 173it [01:44,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:31,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:31,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:31,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:31,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:31,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:24:32,275 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:24:32,276 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:24:32,895 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:24:33,953 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:24:33,953 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:37,010 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:37,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:37,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:37,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:24:37,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:24:37,737 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:24:37,738 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:24:38,338 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:24:38,512 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:24:38,512 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21431892143189216,
  "recall": 0.11119150988904969,
  "score": 0.14641892964903924,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.79it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.72it/s]Extractor Predicting: 17it [00:10,  1.81it/s]Extractor Predicting: 18it [00:11,  1.84it/s]Extractor Predicting: 19it [00:11,  1.85it/s]Extractor Predicting: 20it [00:12,  1.88it/s]Extractor Predicting: 21it [00:12,  1.92it/s]Extractor Predicting: 22it [00:13,  1.88it/s]Extractor Predicting: 23it [00:13,  1.88it/s]Extractor Predicting: 24it [00:14,  1.84it/s]Extractor Predicting: 25it [00:15,  1.85it/s]Extractor Predicting: 26it [00:15,  1.89it/s]Extractor Predicting: 27it [00:16,  1.85it/s]Extractor Predicting: 28it [00:16,  1.88it/s]Extractor Predicting: 29it [00:17,  1.89it/s]Extractor Predicting: 30it [00:17,  1.91it/s]Extractor Predicting: 31it [00:18,  1.94it/s]Extractor Predicting: 32it [00:18,  1.93it/s]Extractor Predicting: 33it [00:19,  1.94it/s]Extractor Predicting: 34it [00:19,  1.93it/s]Extractor Predicting: 35it [00:20,  1.94it/s]Extractor Predicting: 36it [00:20,  1.89it/s]Extractor Predicting: 37it [00:21,  1.85it/s]Extractor Predicting: 38it [00:21,  1.89it/s]Extractor Predicting: 39it [00:22,  1.89it/s]Extractor Predicting: 40it [00:22,  1.90it/s]Extractor Predicting: 41it [00:23,  1.89it/s]Extractor Predicting: 42it [00:23,  1.93it/s]Extractor Predicting: 43it [00:24,  1.89it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.52it/s]Extractor Predicting: 46it [00:26,  1.49it/s]Extractor Predicting: 47it [00:27,  1.49it/s]Extractor Predicting: 48it [00:28,  1.48it/s]Extractor Predicting: 49it [00:28,  1.47it/s]Extractor Predicting: 50it [00:29,  1.47it/s]Extractor Predicting: 51it [00:30,  1.48it/s]Extractor Predicting: 52it [00:30,  1.47it/s]Extractor Predicting: 53it [00:31,  1.47it/s]Extractor Predicting: 54it [00:32,  1.45it/s]Extractor Predicting: 55it [00:32,  1.48it/s]Extractor Predicting: 56it [00:33,  1.46it/s]Extractor Predicting: 57it [00:34,  1.44it/s]Extractor Predicting: 58it [00:34,  1.45it/s]Extractor Predicting: 59it [00:35,  1.74it/s]Extractor Predicting: 59it [00:35,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:25:15,363 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:15,364 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:25:15,402 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:15,403 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:25:15,419 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:25:30,588 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:25:30,631 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:25:30,871 >> loading configuration file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:25:30,871 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:25:31,010 >> Didn't find file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:25:31,091 >> loading file outputs/wrapper/wiki/unseen_5_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7004008016032064,
  "recall": 0.2259941804073715,
  "score": 0.3417257394280127,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:25:31,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:32,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:32,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:33,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:33,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:34,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:35,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:35,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:36,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:36,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:37,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:38,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:38,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:39,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:39,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:40,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:41,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:41,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:42,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:42,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:43,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:52, 12.54s/it][WARNING|generation_utils.py:914] 2023-08-29 08:25:44,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:44,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:45,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:45,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:46,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:47,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:47,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:48,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:48,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:49,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:49,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:50,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:51,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:51,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:52,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:52,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:53,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:53,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:54,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:55,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:56,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:56,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:57,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:57,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:49, 13.63s/it][WARNING|generation_utils.py:914] 2023-08-29 08:25:58,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:59,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:25:59,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:00,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:00,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:01,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:01,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:02,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:02,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:03,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:04,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:04,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:05,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:06,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:06,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:07,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:08,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:09,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:10,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:10,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:11,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:12,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:12,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:13,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:42<01:41, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:13,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:14,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:15,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:15,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:16,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:16,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:17,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:18,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:18,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:19,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:20,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:20,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:21,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:22,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:22,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:23,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:24,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:25,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:27,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:56<01:25, 14.23s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:27,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:28,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:28,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:29,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:30,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:30,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:31,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:32,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:32,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:33,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:33,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:34,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:34,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:35,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:36,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:36,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:37,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:38,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:38,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:39,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:39,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:40,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:09<01:09, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:41,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:41,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:42,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:42,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:43,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:44,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:44,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:45,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:45,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:46,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:47,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:47,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:48,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:49,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:49,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:50,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:50,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:51,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:52,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:52,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:53,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:54,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:54,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:23<00:55, 14.00s/it][WARNING|generation_utils.py:914] 2023-08-29 08:26:55,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:55,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:56,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:57,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:57,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:58,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:58,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:59,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:26:59,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:00,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:01,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:01,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:03,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:03,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:04,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:04,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:05,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:06,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:07,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:07,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:37<00:41, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:08,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:09,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:09,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:10,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:10,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:11,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:11,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:12,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:13,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:13,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:14,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:14,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:15,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:15,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:16,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:16,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:17,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:17,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:18,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:18,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:19,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:20,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:48<00:26, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:20,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:21,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:21,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:22,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:22,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:23,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:24,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:24,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:25,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:25,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:26,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:26,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:27,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:27,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:28,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:28,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:29,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:30,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:30,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:31,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:00<00:12, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-29 08:27:31,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:32,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:33,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:33,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:34,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:34,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:35,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:36,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:36,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:37,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:37,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:38,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:39,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:39,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:40,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:40,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:41,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:42,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:42,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:43,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:43,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:27:44,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:13<00:00, 12.78s/it]Generating: 100%|██████████| 10/10 [02:13<00:00, 13.35s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:51,971 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:51,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:51,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:51,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:51,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:27:52,733 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:27:52,734 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:27:53,317 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:27:54,409 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:27:54,410 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:57,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:57,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:57,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:57,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:27:57,315 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:27:57,962 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:27:57,964 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:27:58,543 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:27:58,716 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:27:58,716 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : given name .', 'success_rate': 0.9122023809523809, 'errors': {'', "('Victoria Cross', 'given name', '', 'In the spring , 2010 , he was awarded the Victoria Cross for his actions in Afghanistan , where he was killed by a Taliban sniper .')", 'not enough values to unpack (expected 2, got 1)', "('Lieutenant Governor', 'given name', '', 'He served as the Lieutenant Governor of North Carolina from March 1891 until March 1942 and from 1941 to February 1944 .')", "('St. Louis Rams', 'given name', '', 'He played for the St. Louis Rams in the National Football League from 1948 to 1959 .')", "('Dagenham', 'given name', '', 'In 1977 he was elected as an MP of the Lower House for the South London North East constituency of Dagenham and the constituency has grown since .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 536, 'raw': 704}
{'target': 600, 'success': 558, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 547, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 596, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9077380952380952, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8536931818181818, 'errors': {'', "('sister', 'is a list of', '', 'It is the sister of .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8650568181818182, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8792613636363636, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 9532
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9632, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.70it/s]Extractor Estimating: 2it [00:01,  1.61it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.64it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.62it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:06,  1.59it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.65it/s]Extractor Estimating: 15it [00:09,  1.64it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:11,  1.62it/s]Extractor Estimating: 19it [00:11,  1.67it/s]Extractor Estimating: 20it [00:12,  1.71it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.65it/s]Extractor Estimating: 24it [00:14,  1.63it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:15,  1.73it/s]Extractor Estimating: 27it [00:16,  1.74it/s]Extractor Estimating: 28it [00:16,  1.73it/s]Extractor Estimating: 29it [00:17,  1.77it/s]Extractor Estimating: 30it [00:18,  1.76it/s]Extractor Estimating: 31it [00:18,  1.68it/s]Extractor Estimating: 32it [00:19,  1.71it/s]Extractor Estimating: 33it [00:19,  1.72it/s]Extractor Estimating: 34it [00:20,  1.75it/s]Extractor Estimating: 35it [00:20,  1.73it/s]Extractor Estimating: 36it [00:21,  1.65it/s]Extractor Estimating: 37it [00:22,  1.74it/s]Extractor Estimating: 38it [00:22,  1.79it/s]Extractor Estimating: 39it [00:23,  1.79it/s]Extractor Estimating: 40it [00:23,  1.78it/s]Extractor Estimating: 41it [00:24,  1.81it/s]Extractor Estimating: 42it [00:25,  1.68it/s]Extractor Estimating: 43it [00:25,  1.67it/s]Extractor Estimating: 44it [00:26,  1.67it/s]Extractor Estimating: 45it [00:26,  1.72it/s]Extractor Estimating: 46it [00:27,  1.73it/s]Extractor Estimating: 47it [00:27,  1.81it/s]Extractor Estimating: 48it [00:28,  1.84it/s]Extractor Estimating: 49it [00:28,  1.77it/s]Extractor Estimating: 50it [00:29,  1.75it/s]Extractor Estimating: 51it [00:30,  1.69it/s]Extractor Estimating: 52it [00:30,  1.69it/s]Extractor Estimating: 53it [00:31,  1.66it/s]Extractor Estimating: 54it [00:32,  1.65it/s]Extractor Estimating: 55it [00:32,  1.68it/s]Extractor Estimating: 56it [00:33,  1.67it/s]Extractor Estimating: 57it [00:33,  1.64it/s]Extractor Estimating: 58it [00:34,  1.62it/s]Extractor Estimating: 59it [00:35,  1.62it/s]Extractor Estimating: 60it [00:35,  1.64it/s]Extractor Estimating: 61it [00:36,  1.61it/s]Extractor Estimating: 62it [00:36,  1.64it/s]Extractor Estimating: 63it [00:37,  1.60it/s]Extractor Estimating: 64it [00:38,  1.62it/s]Extractor Estimating: 65it [00:38,  1.62it/s]Extractor Estimating: 66it [00:39,  1.61it/s]Extractor Estimating: 67it [00:40,  1.61it/s]Extractor Estimating: 68it [00:40,  1.61it/s]Extractor Estimating: 69it [00:41,  1.60it/s]Extractor Estimating: 70it [00:41,  1.62it/s]Extractor Estimating: 71it [00:42,  1.61it/s]Extractor Estimating: 72it [00:43,  1.64it/s]Extractor Estimating: 73it [00:43,  1.61it/s]Extractor Estimating: 74it [00:44,  1.65it/s]Extractor Estimating: 75it [00:44,  1.65it/s]Extractor Estimating: 76it [00:45,  1.63it/s]Extractor Estimating: 77it [00:46,  1.62it/s]Extractor Estimating: 78it [00:46,  1.56it/s]Extractor Estimating: 79it [00:47,  1.55it/s]Extractor Estimating: 80it [00:48,  1.58it/s]Extractor Estimating: 81it [00:48,  1.46it/s]Extractor Estimating: 82it [00:49,  1.48it/s]Extractor Estimating: 83it [00:50,  1.53it/s]Extractor Estimating: 84it [00:50,  1.56it/s]Extractor Estimating: 85it [00:51,  1.55it/s]Extractor Estimating: 86it [00:52,  1.51it/s]Extractor Estimating: 87it [00:52,  1.57it/s]Extractor Estimating: 88it [00:53,  1.59it/s]Extractor Estimating: 89it [00:53,  1.61it/s]Extractor Estimating: 90it [00:54,  1.60it/s]Extractor Estimating: 91it [00:55,  1.56it/s]Extractor Estimating: 92it [00:55,  1.54it/s]Extractor Estimating: 93it [00:56,  1.53it/s]Extractor Estimating: 94it [00:57,  1.50it/s]Extractor Estimating: 95it [00:57,  1.51it/s]Extractor Estimating: 96it [00:58,  1.52it/s]Extractor Estimating: 97it [00:59,  1.53it/s]Extractor Estimating: 98it [00:59,  1.53it/s]Extractor Estimating: 99it [01:00,  1.53it/s]Extractor Estimating: 100it [01:01,  1.56it/s]Extractor Estimating: 101it [01:01,  1.57it/s]Extractor Estimating: 102it [01:02,  1.59it/s]Extractor Estimating: 103it [01:02,  1.62it/s]Extractor Estimating: 104it [01:03,  1.66it/s]Extractor Estimating: 105it [01:04,  1.67it/s]Extractor Estimating: 106it [01:04,  1.66it/s]Extractor Estimating: 107it [01:05,  1.63it/s]Extractor Estimating: 108it [01:05,  1.67it/s]Extractor Estimating: 109it [01:06,  1.71it/s]Extractor Estimating: 110it [01:07,  1.74it/s]Extractor Estimating: 111it [01:07,  1.78it/s]Extractor Estimating: 112it [01:08,  1.79it/s]Extractor Estimating: 113it [01:08,  1.84it/s]Extractor Estimating: 114it [01:09,  1.76it/s]Extractor Estimating: 115it [01:09,  1.72it/s]Extractor Estimating: 116it [01:10,  1.72it/s]Extractor Estimating: 117it [01:11,  1.72it/s]Extractor Estimating: 118it [01:11,  1.75it/s]Extractor Estimating: 119it [01:12,  1.71it/s]Extractor Estimating: 120it [01:12,  1.72it/s]Extractor Estimating: 121it [01:13,  1.74it/s]Extractor Estimating: 122it [01:13,  1.77it/s]Extractor Estimating: 123it [01:14,  1.83it/s]Extractor Estimating: 124it [01:14,  1.79it/s]Extractor Estimating: 125it [01:15,  1.75it/s]Extractor Estimating: 126it [01:16,  1.75it/s]Extractor Estimating: 127it [01:16,  1.73it/s]Extractor Estimating: 128it [01:17,  1.69it/s]Extractor Estimating: 129it [01:17,  1.70it/s]Extractor Estimating: 130it [01:18,  1.68it/s]Extractor Estimating: 131it [01:19,  1.61it/s]Extractor Estimating: 132it [01:19,  1.63it/s]Extractor Estimating: 133it [01:20,  1.61it/s]Extractor Estimating: 134it [01:21,  1.63it/s]Extractor Estimating: 135it [01:21,  1.64it/s]Extractor Estimating: 136it [01:22,  1.63it/s]Extractor Estimating: 137it [01:22,  1.63it/s]Extractor Estimating: 138it [01:23,  1.64it/s]Extractor Estimating: 139it [01:24,  1.67it/s]Extractor Estimating: 140it [01:24,  1.67it/s]Extractor Estimating: 141it [01:25,  1.66it/s]Extractor Estimating: 142it [01:25,  1.69it/s]Extractor Estimating: 143it [01:26,  1.53it/s]Extractor Estimating: 144it [01:27,  1.60it/s]Extractor Estimating: 145it [01:27,  1.56it/s]Extractor Estimating: 146it [01:28,  1.57it/s]Extractor Estimating: 147it [01:29,  1.62it/s]Extractor Estimating: 148it [01:29,  1.64it/s]Extractor Estimating: 149it [01:30,  1.59it/s]Extractor Estimating: 150it [01:30,  1.62it/s]Extractor Estimating: 151it [01:31,  1.68it/s]Extractor Estimating: 152it [01:32,  1.71it/s]Extractor Estimating: 153it [01:32,  1.73it/s]Extractor Estimating: 154it [01:33,  1.74it/s]Extractor Estimating: 155it [01:33,  1.78it/s]Extractor Estimating: 156it [01:34,  1.76it/s]Extractor Estimating: 157it [01:34,  1.80it/s]Extractor Estimating: 158it [01:35,  1.79it/s]Extractor Estimating: 159it [01:35,  1.81it/s]Extractor Estimating: 160it [01:36,  1.85it/s]Extractor Estimating: 161it [01:36,  1.88it/s]Extractor Estimating: 162it [01:37,  1.80it/s]Extractor Estimating: 163it [01:38,  1.80it/s]Extractor Estimating: 164it [01:38,  1.80it/s]Extractor Estimating: 165it [01:39,  1.80it/s]Extractor Estimating: 166it [01:39,  1.81it/s]Extractor Estimating: 167it [01:40,  1.75it/s]Extractor Estimating: 168it [01:40,  1.77it/s]Extractor Estimating: 169it [01:41,  1.77it/s]Extractor Estimating: 170it [01:42,  1.82it/s]Extractor Estimating: 171it [01:42,  1.86it/s]Extractor Estimating: 172it [01:43,  1.80it/s]Extractor Estimating: 173it [01:43,  1.80it/s]Extractor Estimating: 174it [01:44,  1.77it/s]Extractor Estimating: 175it [01:44,  1.76it/s]Extractor Estimating: 176it [01:45,  1.78it/s]Extractor Estimating: 177it [01:46,  1.76it/s]Extractor Estimating: 178it [01:46,  1.82it/s]Extractor Estimating: 179it [01:47,  1.82it/s]Extractor Estimating: 180it [01:47,  1.82it/s]Extractor Estimating: 181it [01:48,  1.79it/s]Extractor Estimating: 182it [01:48,  1.83it/s]Extractor Estimating: 183it [01:49,  1.83it/s]Extractor Estimating: 184it [01:49,  1.81it/s]Extractor Estimating: 185it [01:50,  1.83it/s]Extractor Estimating: 186it [01:50,  1.82it/s]Extractor Estimating: 187it [01:51,  1.82it/s]Extractor Estimating: 188it [01:51,  1.83it/s]Extractor Estimating: 189it [01:52,  1.82it/s]Extractor Estimating: 190it [01:53,  1.81it/s]Extractor Estimating: 191it [01:53,  1.80it/s]Extractor Estimating: 192it [01:54,  1.83it/s]Extractor Estimating: 193it [01:54,  1.80it/s]Extractor Estimating: 194it [01:55,  1.81it/s]Extractor Estimating: 195it [01:55,  1.83it/s]Extractor Estimating: 196it [01:56,  1.79it/s]Extractor Estimating: 197it [01:56,  1.85it/s]Extractor Estimating: 198it [01:57,  1.79it/s]Extractor Estimating: 199it [01:58,  1.79it/s]Extractor Estimating: 200it [01:58,  1.85it/s]Extractor Estimating: 201it [01:59,  1.89it/s]Extractor Estimating: 202it [01:59,  1.83it/s]Extractor Estimating: 203it [02:00,  1.80it/s]Extractor Estimating: 204it [02:00,  1.87it/s]Extractor Estimating: 205it [02:01,  1.88it/s]Extractor Estimating: 206it [02:01,  1.89it/s]Extractor Estimating: 207it [02:02,  1.88it/s]Extractor Estimating: 208it [02:02,  1.88it/s]Extractor Estimating: 209it [02:03,  1.89it/s]Extractor Estimating: 210it [02:03,  1.85it/s]Extractor Estimating: 211it [02:04,  1.84it/s]Extractor Estimating: 212it [02:05,  1.83it/s]Extractor Estimating: 213it [02:05,  1.81it/s]Extractor Estimating: 214it [02:06,  1.79it/s]Extractor Estimating: 215it [02:06,  1.79it/s]Extractor Estimating: 216it [02:07,  1.79it/s]Extractor Estimating: 217it [02:07,  1.75it/s]Extractor Estimating: 218it [02:08,  1.77it/s]Extractor Estimating: 219it [02:08,  1.80it/s]Extractor Estimating: 220it [02:09,  1.76it/s]Extractor Estimating: 221it [02:10,  1.81it/s]Extractor Estimating: 222it [02:10,  1.85it/s]Extractor Estimating: 223it [02:11,  1.89it/s]Extractor Estimating: 224it [02:11,  1.83it/s]Extractor Estimating: 225it [02:12,  1.77it/s]Extractor Estimating: 226it [02:13,  1.58it/s]Extractor Estimating: 227it [02:13,  1.60it/s]Extractor Estimating: 228it [02:14,  1.62it/s]Extractor Estimating: 229it [02:14,  1.64it/s]Extractor Estimating: 230it [02:15,  1.60it/s]Extractor Estimating: 231it [02:16,  1.57it/s]Extractor Estimating: 232it [02:16,  1.64it/s]Extractor Estimating: 233it [02:17,  1.68it/s]Extractor Estimating: 234it [02:17,  1.73it/s]Extractor Estimating: 235it [02:18,  1.71it/s]Extractor Estimating: 236it [02:19,  1.66it/s]Extractor Estimating: 237it [02:19,  1.65it/s]Extractor Estimating: 238it [02:20,  1.68it/s]Extractor Estimating: 239it [02:20,  1.65it/s]Extractor Estimating: 240it [02:21,  1.68it/s]Extractor Estimating: 241it [02:22,  1.65it/s]Extractor Estimating: 242it [02:22,  1.65it/s]Extractor Estimating: 243it [02:23,  1.68it/s]Extractor Estimating: 244it [02:23,  1.63it/s]Extractor Estimating: 245it [02:24,  1.65it/s]Extractor Estimating: 246it [02:25,  1.66it/s]Extractor Estimating: 247it [02:25,  1.57it/s]Extractor Estimating: 248it [02:26,  1.59it/s]Extractor Estimating: 249it [02:27,  1.61it/s]Extractor Estimating: 250it [02:27,  1.56it/s]Extractor Estimating: 250it [02:27,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,039 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:45,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:30:45,802 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:30:45,803 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:30:46,413 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:30:47,560 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:30:47,561 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:50,543 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:30:50,574 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:30:51,390 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:30:51,392 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:30:52,022 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:30:52,245 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:30:52,245 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:04:27,971 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:04:28,148 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 2000 mean pseudo reward: 0.973725985982835
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 15633
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15733, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15733, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 16, avg_time 0.960, loss:251.8345
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 32, avg_time 0.965, loss:228.6031
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 48, avg_time 0.961, loss:219.9288
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 64, avg_time 0.957, loss:203.7750
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.956, loss:182.4725
>> valid entity prec:0.5175, rec:0.4106, f1:0.4579
>> valid relation prec:0.2004, rec:0.0823, f1:0.1167
>> valid relation with NER prec:0.2004, rec:0.0823, f1:0.1167
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 12, avg_time 2.302, loss:178.6459
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 28, avg_time 0.966, loss:169.7391
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 44, avg_time 0.953, loss:170.0699
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.950, loss:168.4424
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 76, avg_time 0.958, loss:171.4986
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5038, rec:0.4912, f1:0.4974
>> valid relation prec:0.1630, rec:0.0942, f1:0.1194
>> valid relation with NER prec:0.1630, rec:0.0942, f1:0.1194
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 8, avg_time 2.298, loss:149.0709
g_step 1200, step 24, avg_time 0.956, loss:140.9060
g_step 1300, step 40, avg_time 0.955, loss:142.0159
g_step 1400, step 56, avg_time 0.966, loss:130.0969
g_step 1500, step 72, avg_time 0.957, loss:128.5721
>> valid entity prec:0.4760, rec:0.3410, f1:0.3974
>> valid relation prec:0.1894, rec:0.0684, f1:0.1005
>> valid relation with NER prec:0.1894, rec:0.0684, f1:0.1005
g_step 1600, step 4, avg_time 2.294, loss:120.9410
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:04:28 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:04:28 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-04-27_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:04:29 - WARNING - datasets.builder -   Using custom data configuration default-52a52045383c7e8a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-52a52045383c7e8a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:04:31,105 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:31,133 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:04:31,134 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:31,135 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:04:31,250 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:31,289 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:04:31,693 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:04:34,988 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:04:34,989 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-52a52045383c7e8a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  2.57ba/s]100%|██████████| 2/2 [00:00<00:00,  3.62ba/s]100%|██████████| 2/2 [00:00<00:00,  3.41ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.94ba/s] 40%|████      | 2/5 [00:00<00:00,  3.49ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.89ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.50ba/s]100%|██████████| 5/5 [00:01<00:00,  4.33ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  5.78ba/s]100%|██████████| 2/2 [00:00<00:00,  7.39ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.48ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.35ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.74ba/s]100%|██████████| 5/5 [00:00<00:00, 10.23ba/s]
[INFO|trainer.py:414] 2023-08-29 09:04:38,573 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:04:38,675 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:04:38,675 >>   Num examples = 2000
[INFO|trainer.py:1149] 2023-08-29 09:04:38,675 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:04:38,675 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:04:38,675 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:04:38,675 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:04:38,675 >>   Total optimization steps = 155
  0%|          | 0/155 [00:00<?, ?it/s]  1%|          | 1/155 [00:00<00:46,  3.32it/s]  1%|▏         | 2/155 [00:00<00:45,  3.40it/s]  2%|▏         | 3/155 [00:00<00:44,  3.42it/s]  3%|▎         | 4/155 [00:01<00:43,  3.43it/s]  3%|▎         | 5/155 [00:01<00:43,  3.44it/s]  4%|▍         | 6/155 [00:01<00:43,  3.44it/s]  5%|▍         | 7/155 [00:02<00:43,  3.43it/s]  5%|▌         | 8/155 [00:02<00:43,  3.42it/s]  6%|▌         | 9/155 [00:02<00:44,  3.27it/s]  6%|▋         | 10/155 [00:02<00:43,  3.31it/s]  7%|▋         | 11/155 [00:03<00:43,  3.34it/s]  8%|▊         | 12/155 [00:03<00:42,  3.36it/s]  8%|▊         | 13/155 [00:03<00:42,  3.37it/s]  9%|▉         | 14/155 [00:04<00:41,  3.38it/s] 10%|▉         | 15/155 [00:04<00:41,  3.38it/s] 10%|█         | 16/155 [00:04<00:41,  3.39it/s] 11%|█         | 17/155 [00:05<00:40,  3.39it/s] 12%|█▏        | 18/155 [00:05<00:40,  3.40it/s] 12%|█▏        | 19/155 [00:05<00:40,  3.40it/s] 13%|█▎        | 20/155 [00:05<00:39,  3.40it/s] 14%|█▎        | 21/155 [00:06<00:39,  3.40it/s] 14%|█▍        | 22/155 [00:06<00:39,  3.40it/s] 15%|█▍        | 23/155 [00:06<00:38,  3.40it/s] 15%|█▌        | 24/155 [00:07<00:38,  3.40it/s] 16%|█▌        | 25/155 [00:07<00:38,  3.40it/s] 17%|█▋        | 26/155 [00:07<00:37,  3.40it/s] 17%|█▋        | 27/155 [00:07<00:38,  3.34it/s] 18%|█▊        | 28/155 [00:08<00:37,  3.36it/s] 19%|█▊        | 29/155 [00:08<00:37,  3.37it/s] 19%|█▉        | 30/155 [00:08<00:37,  3.38it/s] 20%|██        | 31/155 [00:09<00:36,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 09:04:47,878 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:04:47,879 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:04:47,879 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 54.81it/s][A
  2%|▏         | 12/505 [00:00<00:10, 47.69it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.28it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.56it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.06it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.78it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.83it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.66it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.66it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.67it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.54it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.50it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.52it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.33it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.44it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.45it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.55it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.69it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.55it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.51it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.42it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.36it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.47it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.39it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.44it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.56it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.50it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.49it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.42it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.39it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.23it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.26it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.24it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.37it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.41it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.51it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.46it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.40it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.33it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.36it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.36it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.39it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.39it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.52it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.44it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.44it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.46it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.37it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.32it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.44it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.40it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.48it/s][A
 53%|█████▎    | 267/505 [00:05<00:05, 44.45it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.47it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.46it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.39it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.27it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.37it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.39it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.46it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.51it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.36it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.52it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.54it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.48it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.45it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.28it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.37it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.56it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.40it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.45it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.48it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.43it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.50it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.37it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 42.80it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.51it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.78it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.97it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 43.98it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.20it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.29it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.23it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.07it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.21it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.37it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.48it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.42it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.46it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.38it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.45it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.43it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.31it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.31it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.50it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.41it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.49it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.41it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.42it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.47it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.47it/s][A 20%|██        | 31/155 [00:20<00:36,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:04:59,333 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31
[INFO|configuration_utils.py:351] 2023-08-29 09:04:59,486 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:05:02,036 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:05:02,099 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:05:02,131 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31/special_tokens_map.json
 21%|██        | 32/155 [00:29<12:39,  6.18s/it] 21%|██▏       | 33/155 [00:29<08:59,  4.42s/it] 22%|██▏       | 34/155 [00:29<06:24,  3.18s/it] 23%|██▎       | 35/155 [00:29<04:37,  2.32s/it] 23%|██▎       | 36/155 [00:30<03:23,  1.71s/it] 24%|██▍       | 37/155 [00:30<02:31,  1.28s/it] 25%|██▍       | 38/155 [00:30<01:55,  1.01it/s] 25%|██▌       | 39/155 [00:31<01:30,  1.28it/s] 26%|██▌       | 40/155 [00:31<01:12,  1.58it/s] 26%|██▋       | 41/155 [00:31<01:00,  1.88it/s] 27%|██▋       | 42/155 [00:32<00:52,  2.17it/s] 28%|██▊       | 43/155 [00:32<00:46,  2.43it/s] 28%|██▊       | 44/155 [00:32<00:42,  2.62it/s] 29%|██▉       | 45/155 [00:32<00:39,  2.81it/s] 30%|██▉       | 46/155 [00:33<00:36,  2.96it/s] 30%|███       | 47/155 [00:33<00:35,  3.08it/s] 31%|███       | 48/155 [00:33<00:33,  3.17it/s] 32%|███▏      | 49/155 [00:34<00:32,  3.24it/s] 32%|███▏      | 50/155 [00:34<00:31,  3.28it/s] 33%|███▎      | 51/155 [00:34<00:31,  3.32it/s] 34%|███▎      | 52/155 [00:34<00:30,  3.34it/s] 34%|███▍      | 53/155 [00:35<00:30,  3.36it/s] 35%|███▍      | 54/155 [00:35<00:29,  3.37it/s] 35%|███▌      | 55/155 [00:35<00:30,  3.24it/s] 36%|███▌      | 56/155 [00:36<00:30,  3.28it/s] 37%|███▋      | 57/155 [00:36<00:29,  3.31it/s] 37%|███▋      | 58/155 [00:36<00:29,  3.34it/s] 38%|███▊      | 59/155 [00:37<00:28,  3.36it/s] 39%|███▊      | 60/155 [00:37<00:28,  3.37it/s] 39%|███▉      | 61/155 [00:37<00:27,  3.38it/s] 40%|████      | 62/155 [00:37<00:27,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 09:05:16,698 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:05:16,698 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:05:16,698 >>   Batch size = 8
{'eval_loss': 1.1080795526504517, 'eval_runtime': 11.4145, 'eval_samples_per_second': 353.849, 'eval_steps_per_second': 44.242, 'epoch': 0.98}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:09, 55.33it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.13it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.38it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.49it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.96it/s][A
  6%|▋         | 32/505 [00:00<00:11, 42.46it/s][A
  7%|▋         | 37/505 [00:00<00:10, 43.19it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.48it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.91it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.13it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.36it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.45it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.33it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.07it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.17it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.26it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.31it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.49it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.58it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.74it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.55it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.39it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.11it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.22it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.36it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.34it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.48it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.56it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.52it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.60it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.37it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.20it/s][A
 33%|███▎      | 167/505 [00:03<00:08, 41.73it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 42.60it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 43.30it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 43.68it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.12it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.29it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.27it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.24it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.94it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.91it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.07it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.30it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.51it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.57it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.58it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.64it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.42it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.10it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.08it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.12it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.46it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.52it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.49it/s][A
 56%|█████▌    | 282/505 [00:06<00:04, 44.68it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.60it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.38it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.13it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 41.78it/s][A
 61%|██████    | 307/505 [00:06<00:04, 42.64it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.36it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.74it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.11it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.14it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.09it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.91it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.88it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.05it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.38it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.48it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.62it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.62it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.50it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.38it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.16it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.08it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.21it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.23it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.48it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.55it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.60it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.53it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.33it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.21it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.66it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.98it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 42.02it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 42.88it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 43.47it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.90it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.98it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.02it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.95it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.01it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.91it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 41.80it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 26.37it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 30.44it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 30.44it/s][A 40%|████      | 62/155 [00:49<00:27,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:05:28,663 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62
[INFO|configuration_utils.py:351] 2023-08-29 09:05:28,828 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:05:31,706 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:05:31,803 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:05:31,854 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62/special_tokens_map.json
 41%|████      | 63/155 [01:00<10:27,  6.82s/it] 41%|████▏     | 64/155 [01:00<07:23,  4.87s/it] 42%|████▏     | 65/155 [01:00<05:14,  3.50s/it] 43%|████▎     | 66/155 [01:00<03:45,  2.54s/it] 43%|████▎     | 67/155 [01:01<02:43,  1.86s/it] 44%|████▍     | 68/155 [01:01<02:00,  1.39s/it] 45%|████▍     | 69/155 [01:01<01:31,  1.06s/it] 45%|████▌     | 70/155 [01:02<01:10,  1.21it/s] 46%|████▌     | 71/155 [01:02<00:56,  1.50it/s] 46%|████▋     | 72/155 [01:02<00:45,  1.80it/s] 47%|████▋     | 73/155 [01:02<00:38,  2.11it/s] 48%|████▊     | 74/155 [01:03<00:33,  2.38it/s] 48%|████▊     | 75/155 [01:03<00:31,  2.57it/s] 49%|████▉     | 76/155 [01:03<00:28,  2.78it/s] 50%|████▉     | 77/155 [01:04<00:26,  2.95it/s] 50%|█████     | 78/155 [01:04<00:24,  3.09it/s] 51%|█████     | 79/155 [01:04<00:23,  3.19it/s] 52%|█████▏    | 80/155 [01:05<00:23,  3.26it/s] 52%|█████▏    | 81/155 [01:05<00:22,  3.32it/s] 53%|█████▎    | 82/155 [01:05<00:21,  3.35it/s] 54%|█████▎    | 83/155 [01:05<00:21,  3.38it/s] 54%|█████▍    | 84/155 [01:06<00:20,  3.40it/s] 55%|█████▍    | 85/155 [01:06<00:20,  3.41it/s] 55%|█████▌    | 86/155 [01:06<00:20,  3.31it/s] 56%|█████▌    | 87/155 [01:07<00:20,  3.35it/s] 57%|█████▋    | 88/155 [01:07<00:19,  3.37it/s] 57%|█████▋    | 89/155 [01:07<00:19,  3.40it/s] 58%|█████▊    | 90/155 [01:07<00:19,  3.41it/s] 59%|█████▊    | 91/155 [01:08<00:18,  3.42it/s] 59%|█████▉    | 92/155 [01:08<00:18,  3.43it/s] 60%|██████    | 93/155 [01:08<00:18,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:05:47,543 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:05:47,543 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:05:47,543 >>   Batch size = 8
{'eval_loss': 1.1221996545791626, 'eval_runtime': 11.7417, 'eval_samples_per_second': 343.986, 'eval_steps_per_second': 43.009, 'epoch': 1.98}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.91it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.57it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.51it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.77it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.08it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.57it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.50it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.35it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.47it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.55it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.67it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.73it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.56it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.41it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.34it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.24it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.18it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.26it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.42it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.63it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.61it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.53it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.45it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.36it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.18it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.21it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.33it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.47it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.61it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.54it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.49it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.41it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.31it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.58it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.76it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.07it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.31it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.46it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.58it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.43it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.28it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.21it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.15it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.24it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.29it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.51it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.60it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.69it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.47it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.29it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.31it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.30it/s][A
 53%|█████▎    | 267/505 [00:05<00:05, 44.23it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.31it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.50it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.59it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.63it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.39it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.19it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 41.77it/s][A
 61%|██████    | 307/505 [00:06<00:04, 42.58it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.10it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 43.59it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 43.84it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.18it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.27it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.13it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.98it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.09it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.23it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.42it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.49it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.52it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.48it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.38it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.18it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.15it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.16it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.28it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.40it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.54it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.58it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.53it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.43it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.25it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.18it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.15it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.65it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.95it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.21it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.37it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.41it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.24it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.20it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.00it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.06it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.29it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.41it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.53it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.57it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.57it/s][A 60%|██████    | 93/155 [01:20<00:18,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:05:59,115 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93
[INFO|configuration_utils.py:351] 2023-08-29 09:05:59,273 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:06:02,803 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:06:02,913 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:06:02,972 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93/special_tokens_map.json
 61%|██████    | 94/155 [01:31<07:09,  7.03s/it] 61%|██████▏   | 95/155 [01:31<05:00,  5.02s/it] 62%|██████▏   | 96/155 [01:32<03:32,  3.60s/it] 63%|██████▎   | 97/155 [01:32<02:31,  2.61s/it] 63%|██████▎   | 98/155 [01:32<01:49,  1.91s/it] 64%|██████▍   | 99/155 [01:33<01:19,  1.43s/it] 65%|██████▍   | 100/155 [01:33<00:59,  1.09s/it] 65%|██████▌   | 101/155 [01:33<00:45,  1.18it/s] 66%|██████▌   | 102/155 [01:33<00:36,  1.47it/s] 66%|██████▋   | 103/155 [01:34<00:29,  1.77it/s] 67%|██████▋   | 104/155 [01:34<00:24,  2.08it/s] 68%|██████▊   | 105/155 [01:34<00:21,  2.36it/s] 68%|██████▊   | 106/155 [01:35<00:19,  2.56it/s] 69%|██████▉   | 107/155 [01:35<00:17,  2.77it/s] 70%|██████▉   | 108/155 [01:35<00:15,  2.95it/s] 70%|███████   | 109/155 [01:36<00:14,  3.08it/s] 71%|███████   | 110/155 [01:36<00:14,  3.18it/s] 72%|███████▏  | 111/155 [01:36<00:13,  3.25it/s] 72%|███████▏  | 112/155 [01:36<00:12,  3.31it/s] 73%|███████▎  | 113/155 [01:37<00:12,  3.35it/s] 74%|███████▎  | 114/155 [01:37<00:12,  3.38it/s] 74%|███████▍  | 115/155 [01:37<00:11,  3.40it/s] 75%|███████▍  | 116/155 [01:38<00:11,  3.41it/s] 75%|███████▌  | 117/155 [01:38<00:11,  3.27it/s] 76%|███████▌  | 118/155 [01:38<00:11,  3.32it/s] 77%|███████▋  | 119/155 [01:38<00:10,  3.35it/s] 77%|███████▋  | 120/155 [01:39<00:10,  3.38it/s] 78%|███████▊  | 121/155 [01:39<00:10,  3.40it/s] 79%|███████▊  | 122/155 [01:39<00:09,  3.41it/s] 79%|███████▉  | 123/155 [01:40<00:09,  3.42it/s] 80%|████████  | 124/155 [01:40<00:09,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:06:19,124 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:06:19,124 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:06:19,124 >>   Batch size = 8
{'eval_loss': 1.1415292024612427, 'eval_runtime': 11.4458, 'eval_samples_per_second': 352.879, 'eval_steps_per_second': 44.121, 'epoch': 2.98}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.91it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.52it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.66it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.82it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.20it/s][A
  6%|▋         | 32/505 [00:00<00:11, 40.93it/s][A
  7%|▋         | 37/505 [00:00<00:11, 42.07it/s][A
  8%|▊         | 42/505 [00:00<00:10, 42.85it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.45it/s][A
 10%|█         | 52/505 [00:01<00:10, 43.91it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.26it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.28it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.23it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 43.95it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.98it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.16it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.25it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.43it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.65it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.76it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.67it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.38it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.23it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.21it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.25it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.41it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.47it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.73it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.73it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.63it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.42it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.26it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 42.61it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 43.32it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 43.65it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.35it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.47it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.54it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.26it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.00it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.04it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.22it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.35it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.57it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.58it/s][A
 47%|████▋     | 237/505 [00:05<00:05, 44.75it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.67it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.41it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.16it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.08it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.29it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.42it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.57it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.69it/s][A
 56%|█████▌    | 282/505 [00:06<00:04, 44.80it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.63it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.39it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.10it/s][A
 60%|█████▉    | 302/505 [00:06<00:05, 38.92it/s][A
 61%|██████    | 307/505 [00:06<00:04, 40.55it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 41.73it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 42.49it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 43.34it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 43.81it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.22it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.09it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 43.30it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.42it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 43.68it/s][A
 71%|███████   | 357/505 [00:08<00:03, 43.95it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.21it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.45it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.68it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.64it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.36it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 42.88it/s][A
 78%|███████▊  | 392/505 [00:09<00:04, 27.50it/s][A
 79%|███████▊  | 397/505 [00:09<00:03, 31.32it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 34.41it/s][A
 81%|████████  | 407/505 [00:09<00:02, 37.06it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 39.15it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 40.70it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 41.92it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 41.77it/s][A
 86%|████████▌ | 432/505 [00:10<00:01, 42.28it/s][A
 87%|████████▋ | 437/505 [00:10<00:01, 42.56it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 42.87it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.36it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.76it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.18it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.39it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.39it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.51it/s][A
 94%|█████████▍| 477/505 [00:11<00:00, 44.29it/s][A
 95%|█████████▌| 482/505 [00:11<00:00, 44.04it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.26it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.10it/s][A 80%|████████  | 124/155 [01:52<00:09,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:06:30,988 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-29 09:06:31,115 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:06:34,046 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:06:34,160 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:06:34,208 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124/special_tokens_map.json
 81%|████████  | 125/155 [02:02<03:25,  6.85s/it] 81%|████████▏ | 126/155 [02:02<02:21,  4.89s/it] 82%|████████▏ | 127/155 [02:03<01:38,  3.51s/it] 83%|████████▎ | 128/155 [02:03<01:08,  2.55s/it] 83%|████████▎ | 129/155 [02:03<00:48,  1.87s/it] 84%|████████▍ | 130/155 [02:04<00:34,  1.40s/it] 85%|████████▍ | 131/155 [02:04<00:25,  1.07s/it] 85%|████████▌ | 132/155 [02:04<00:19,  1.20it/s] 86%|████████▌ | 133/155 [02:04<00:14,  1.49it/s] 86%|████████▋ | 134/155 [02:05<00:11,  1.79it/s] 87%|████████▋ | 135/155 [02:05<00:09,  2.08it/s] 88%|████████▊ | 136/155 [02:05<00:08,  2.36it/s] 88%|████████▊ | 137/155 [02:06<00:07,  2.54it/s] 89%|████████▉ | 138/155 [02:06<00:06,  2.75it/s] 90%|████████▉ | 139/155 [02:06<00:05,  2.92it/s] 90%|█████████ | 140/155 [02:07<00:04,  3.05it/s] 91%|█████████ | 141/155 [02:07<00:04,  3.14it/s] 92%|█████████▏| 142/155 [02:07<00:04,  3.21it/s] 92%|█████████▏| 143/155 [02:07<00:03,  3.27it/s] 93%|█████████▎| 144/155 [02:08<00:03,  3.30it/s] 94%|█████████▎| 145/155 [02:08<00:03,  3.33it/s] 94%|█████████▍| 146/155 [02:08<00:02,  3.35it/s] 95%|█████████▍| 147/155 [02:09<00:02,  3.37it/s] 95%|█████████▌| 148/155 [02:09<00:02,  3.27it/s] 96%|█████████▌| 149/155 [02:09<00:01,  3.31it/s] 97%|█████████▋| 150/155 [02:09<00:01,  3.33it/s] 97%|█████████▋| 151/155 [02:10<00:01,  3.35it/s] 98%|█████████▊| 152/155 [02:10<00:00,  3.36it/s] 99%|█████████▊| 153/155 [02:10<00:00,  3.37it/s] 99%|█████████▉| 154/155 [02:11<00:00,  3.38it/s]100%|██████████| 155/155 [02:11<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 09:06:50,154 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:06:50,155 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:06:50,155 >>   Batch size = 8
{'eval_loss': 1.148814082145691, 'eval_runtime': 11.7361, 'eval_samples_per_second': 344.151, 'eval_steps_per_second': 43.029, 'epoch': 3.98}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.09it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.86it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.08it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.26it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.32it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.62it/s][A
  7%|▋         | 37/505 [00:00<00:10, 42.96it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.29it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.75it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.16it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.40it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.59it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.54it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.27it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.07it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.06it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.25it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.35it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.59it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.68it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.77it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.49it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.37it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.08it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.13it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.14it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.36it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.55it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.64it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.65it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.57it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.35it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.29it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 42.35it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 43.12it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 43.46it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 43.92it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.12it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.12it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.34it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.22it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 43.94it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.08it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.30it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.44it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.63it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.64it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.63it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.47it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.32it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.12it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.16it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.27it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.45it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.59it/s][A
 56%|█████▌    | 282/505 [00:06<00:04, 44.76it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.65it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.48it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.25it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.16it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.40it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.75it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.15it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.15it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.36it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.27it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.09it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 43.97it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.08it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.25it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.39it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.64it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.63it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.48it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.38it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.23it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.08it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.12it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.25it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.40it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.58it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.64it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.53it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.42it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.31it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.17it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 42.84it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.43it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 43.78it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.15it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.22it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.28it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.19it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.10it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.89it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.01it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.29it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.37it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 41.29it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 41.29it/s][A100%|██████████| 155/155 [02:22<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:07:01,702 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155
[INFO|configuration_utils.py:351] 2023-08-29 09:07:01,833 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:07:04,586 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:07:04,700 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:07:04,771 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:07:10,487 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:07:10,509 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31 (score: 1.1080795526504517).
                                                 100%|██████████| 155/155 [02:40<00:00,  3.38it/s]100%|██████████| 155/155 [02:40<00:00,  1.03s/it]
[INFO|trainer.py:1894] 2023-08-29 09:07:19,080 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 09:07:19,285 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:07:24,559 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:07:24,836 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:07:24,980 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:07:25,679 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   epoch                    =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   train_loss               =     0.4351
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   train_runtime            = 0:02:40.31
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   train_samples            =       2000
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   train_samples_per_second =     62.376
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:25,680 >>   train_steps_per_second   =      0.967
{'eval_loss': 1.154759168624878, 'eval_runtime': 11.4486, 'eval_samples_per_second': 352.793, 'eval_steps_per_second': 44.11, 'epoch': 4.98}
{'train_runtime': 160.3188, 'train_samples_per_second': 62.376, 'train_steps_per_second': 0.967, 'train_loss': 0.435126224640877, 'epoch': 4.98}
08/29/2023 09:07:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:07:26,122 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:07:26,122 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 09:07:26,122 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.05it/s]  2%|▏         | 12/505 [00:00<00:10, 48.77it/s]  3%|▎         | 17/505 [00:00<00:10, 47.12it/s]  4%|▍         | 22/505 [00:00<00:10, 46.44it/s]  5%|▌         | 27/505 [00:00<00:10, 45.95it/s]  6%|▋         | 32/505 [00:00<00:11, 42.61it/s]  7%|▋         | 37/505 [00:00<00:10, 43.60it/s]  8%|▊         | 42/505 [00:00<00:10, 43.73it/s]  9%|▉         | 47/505 [00:01<00:10, 43.62it/s] 10%|█         | 52/505 [00:01<00:10, 43.81it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.03it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.32it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.46it/s] 14%|█▍        | 72/505 [00:01<00:10, 42.56it/s] 15%|█▌        | 77/505 [00:01<00:09, 43.19it/s] 16%|█▌        | 82/505 [00:02<00:21, 19.37it/s] 17%|█▋        | 87/505 [00:02<00:17, 23.54it/s] 18%|█▊        | 92/505 [00:02<00:15, 27.44it/s] 19%|█▉        | 97/505 [00:02<00:13, 31.09it/s] 20%|██        | 102/505 [00:02<00:11, 34.34it/s] 21%|██        | 107/505 [00:02<00:10, 36.95it/s] 22%|██▏       | 112/505 [00:02<00:10, 39.09it/s] 23%|██▎       | 117/505 [00:03<00:09, 40.65it/s] 24%|██▍       | 122/505 [00:03<00:09, 41.61it/s] 25%|██▌       | 127/505 [00:03<00:08, 42.02it/s] 26%|██▌       | 132/505 [00:03<00:08, 42.44it/s] 27%|██▋       | 137/505 [00:03<00:08, 42.96it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.53it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.01it/s] 30%|███       | 152/505 [00:03<00:07, 44.25it/s] 31%|███       | 157/505 [00:03<00:07, 44.50it/s] 32%|███▏      | 162/505 [00:04<00:07, 44.62it/s] 33%|███▎      | 167/505 [00:04<00:07, 44.46it/s] 34%|███▍      | 172/505 [00:04<00:07, 44.10it/s] 35%|███▌      | 177/505 [00:04<00:07, 43.97it/s] 36%|███▌      | 182/505 [00:04<00:07, 43.98it/s] 37%|███▋      | 187/505 [00:04<00:07, 43.05it/s] 38%|███▊      | 192/505 [00:04<00:07, 43.63it/s] 39%|███▉      | 197/505 [00:04<00:07, 43.89it/s] 40%|████      | 202/505 [00:05<00:06, 44.25it/s] 41%|████      | 207/505 [00:05<00:06, 44.49it/s] 42%|████▏     | 212/505 [00:05<00:06, 44.52it/s] 43%|████▎     | 217/505 [00:05<00:06, 44.24it/s] 44%|████▍     | 222/505 [00:05<00:06, 44.12it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.01it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.14it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.21it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.48it/s] 49%|████▉     | 247/505 [00:06<00:05, 44.59it/s] 50%|████▉     | 252/505 [00:06<00:05, 44.70it/s] 51%|█████     | 257/505 [00:06<00:05, 44.67it/s] 52%|█████▏    | 262/505 [00:06<00:05, 44.35it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.24it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.15it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.22it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.29it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.50it/s] 58%|█████▊    | 292/505 [00:07<00:04, 44.59it/s] 59%|█████▉    | 297/505 [00:07<00:04, 44.60it/s] 60%|█████▉    | 302/505 [00:07<00:04, 44.59it/s] 61%|██████    | 307/505 [00:07<00:04, 44.35it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.21it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.20it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.18it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.32it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.45it/s] 67%|██████▋   | 337/505 [00:08<00:03, 44.60it/s] 68%|██████▊   | 342/505 [00:08<00:03, 44.56it/s] 69%|██████▊   | 347/505 [00:08<00:03, 44.50it/s] 70%|██████▉   | 352/505 [00:08<00:03, 44.34it/s] 71%|███████   | 357/505 [00:08<00:03, 44.26it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.19it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.32it/s] 74%|███████▎  | 372/505 [00:08<00:02, 44.34it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.50it/s] 76%|███████▌  | 382/505 [00:09<00:02, 44.58it/s] 77%|███████▋  | 387/505 [00:09<00:02, 44.58it/s] 78%|███████▊  | 392/505 [00:09<00:02, 44.36it/s] 79%|███████▊  | 397/505 [00:09<00:02, 44.33it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.20it/s] 81%|████████  | 407/505 [00:09<00:02, 44.24it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.33it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.30it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.50it/s] 85%|████████▍ | 427/505 [00:10<00:01, 44.66it/s] 86%|████████▌ | 432/505 [00:10<00:01, 44.65it/s] 87%|████████▋ | 437/505 [00:10<00:01, 44.60it/s] 88%|████████▊ | 442/505 [00:10<00:01, 44.43it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.40it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.47it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.47it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.56it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.61it/s] 93%|█████████▎| 472/505 [00:11<00:00, 44.76it/s] 94%|█████████▍| 477/505 [00:11<00:00, 44.76it/s] 95%|█████████▌| 482/505 [00:11<00:00, 44.58it/s] 96%|█████████▋| 487/505 [00:11<00:00, 44.46it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.47it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.49it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.55it/s]100%|██████████| 505/505 [00:11<00:00, 42.60it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:07:37,995 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   epoch                   =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   eval_loss               =     1.1081
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   eval_runtime            = 0:00:11.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   eval_samples_per_second =      340.2
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   eval_steps_per_second   =     42.536
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:07:37,995 >>   perplexity              =     3.0285
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:48,353 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:48,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:48,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:48,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:48,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:07:49,140 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:07:49,141 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:07:49,734 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:07:50,885 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:07:50,885 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:53,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:53,973 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:53,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:53,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:07:53,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:07:54,767 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:07:54,768 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:07:55,407 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:07:55,642 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:07:55,642 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-31
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-155
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-93
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/checkpoint-62
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.54it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.60it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.58it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.59it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:32,  1.55it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.61it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.60it/s]Extractor Predicting: 57it [00:37,  1.59it/s]Extractor Predicting: 58it [00:37,  1.55it/s]Extractor Predicting: 59it [00:38,  1.56it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:39,  1.60it/s]Extractor Predicting: 62it [00:40,  1.58it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.54it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:44,  1.48it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:46,  1.46it/s]Extractor Predicting: 73it [00:47,  1.46it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:49,  1.45it/s]Extractor Predicting: 76it [00:49,  1.48it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:51,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.51it/s]Extractor Predicting: 82it [00:53,  1.54it/s]Extractor Predicting: 83it [00:54,  1.50it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.50it/s]Extractor Predicting: 86it [00:56,  1.52it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:58,  1.53it/s]Extractor Predicting: 90it [00:58,  1.54it/s]Extractor Predicting: 91it [00:59,  1.55it/s]Extractor Predicting: 92it [01:00,  1.54it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:01,  1.53it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:02,  1.50it/s]Extractor Predicting: 97it [01:03,  1.48it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:04,  1.49it/s]Extractor Predicting: 100it [01:05,  1.37it/s]Extractor Predicting: 101it [01:06,  1.38it/s]Extractor Predicting: 102it [01:07,  1.41it/s]Extractor Predicting: 103it [01:07,  1.43it/s]Extractor Predicting: 104it [01:08,  1.48it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.53it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:11,  1.55it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.55it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:14,  1.51it/s]Extractor Predicting: 115it [01:15,  1.56it/s]Extractor Predicting: 116it [01:16,  1.55it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:17,  1.47it/s]Extractor Predicting: 119it [01:18,  1.51it/s]Extractor Predicting: 120it [01:18,  1.52it/s]Extractor Predicting: 121it [01:19,  1.50it/s]Extractor Predicting: 122it [01:20,  1.52it/s]Extractor Predicting: 123it [01:20,  1.50it/s]Extractor Predicting: 124it [01:21,  1.51it/s]Extractor Predicting: 125it [01:22,  1.41it/s]Extractor Predicting: 126it [01:22,  1.46it/s]Extractor Predicting: 127it [01:23,  1.42it/s]Extractor Predicting: 128it [01:24,  1.39it/s]Extractor Predicting: 129it [01:25,  1.40it/s]Extractor Predicting: 130it [01:25,  1.41it/s]Extractor Predicting: 131it [01:26,  1.39it/s]Extractor Predicting: 132it [01:27,  1.39it/s]Extractor Predicting: 133it [01:28,  1.37it/s]Extractor Predicting: 134it [01:28,  1.42it/s]Extractor Predicting: 135it [01:29,  1.44it/s]Extractor Predicting: 136it [01:30,  1.45it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:31,  1.41it/s]Extractor Predicting: 139it [01:32,  1.42it/s]Extractor Predicting: 140it [01:32,  1.42it/s]Extractor Predicting: 141it [01:33,  1.43it/s]Extractor Predicting: 142it [01:34,  1.44it/s]Extractor Predicting: 143it [01:34,  1.43it/s]Extractor Predicting: 144it [01:35,  1.44it/s]Extractor Predicting: 145it [01:36,  1.43it/s]Extractor Predicting: 146it [01:37,  1.44it/s]Extractor Predicting: 147it [01:37,  1.45it/s]Extractor Predicting: 148it [01:38,  1.40it/s]Extractor Predicting: 149it [01:39,  1.46it/s]Extractor Predicting: 149it [01:39,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:46,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:46,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:46,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:46,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:46,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:09:46,772 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:09:46,773 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:47,080 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:48,157 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:48,158 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:49,953 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:49,986 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:49,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:49,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:49,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:09:50,323 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:09:50,324 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:50,642 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:50,795 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:50,795 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1964930376482723,
  "recall": 0.09433027977222085,
  "score": 0.12746738039478087,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:08,  1.78it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.71it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:13,  1.77it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:16,  1.77it/s]Extractor Predicting: 29it [00:16,  1.62it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:18,  1.73it/s]Extractor Predicting: 33it [00:19,  1.70it/s]Extractor Predicting: 34it [00:19,  1.66it/s]Extractor Predicting: 35it [00:20,  1.65it/s]Extractor Predicting: 36it [00:21,  1.71it/s]Extractor Predicting: 37it [00:21,  1.69it/s]Extractor Predicting: 38it [00:22,  1.68it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:23,  1.66it/s]Extractor Predicting: 41it [00:24,  1.65it/s]Extractor Predicting: 42it [00:24,  1.68it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:26,  1.56it/s]Extractor Predicting: 46it [00:27,  1.53it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:28,  1.59it/s]Extractor Predicting: 49it [00:29,  1.63it/s]Extractor Predicting: 50it [00:29,  1.65it/s]Extractor Predicting: 51it [00:30,  1.67it/s]Extractor Predicting: 52it [00:30,  1.65it/s]Extractor Predicting: 53it [00:31,  1.66it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:33,  1.58it/s]Extractor Predicting: 57it [00:33,  1.59it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:36,  1.59it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:37,  1.61it/s]Extractor Predicting: 64it [00:38,  1.65it/s]Extractor Predicting: 65it [00:38,  1.68it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:40,  1.65it/s]Extractor Predicting: 69it [00:41,  1.62it/s]Extractor Predicting: 70it [00:41,  1.61it/s]Extractor Predicting: 71it [00:42,  1.60it/s]Extractor Predicting: 72it [00:43,  1.62it/s]Extractor Predicting: 73it [00:43,  1.61it/s]Extractor Predicting: 74it [00:44,  1.62it/s]Extractor Predicting: 75it [00:44,  1.64it/s]Extractor Predicting: 76it [00:45,  1.62it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:46,  1.58it/s]Extractor Predicting: 79it [00:47,  1.59it/s]Extractor Predicting: 80it [00:48,  1.64it/s]Extractor Predicting: 81it [00:48,  1.61it/s]Extractor Predicting: 82it [00:49,  1.59it/s]Extractor Predicting: 83it [00:50,  1.59it/s]Extractor Predicting: 84it [00:50,  1.60it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:51,  1.58it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:53,  1.64it/s]Extractor Predicting: 89it [00:53,  1.65it/s]Extractor Predicting: 90it [00:54,  1.66it/s]Extractor Predicting: 91it [00:54,  1.65it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:56,  1.69it/s]Extractor Predicting: 94it [00:56,  1.70it/s]Extractor Predicting: 95it [00:57,  1.66it/s]Extractor Predicting: 96it [00:57,  1.69it/s]Extractor Predicting: 97it [00:58,  1.72it/s]Extractor Predicting: 98it [00:59,  1.70it/s]Extractor Predicting: 99it [00:59,  1.69it/s]Extractor Predicting: 100it [01:00,  1.71it/s]Extractor Predicting: 101it [01:00,  1.69it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:02,  1.64it/s]Extractor Predicting: 104it [01:02,  1.68it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:03,  1.64it/s]Extractor Predicting: 107it [01:04,  1.66it/s]Extractor Predicting: 108it [01:05,  1.63it/s]Extractor Predicting: 109it [01:05,  1.62it/s]Extractor Predicting: 110it [01:06,  1.64it/s]Extractor Predicting: 111it [01:06,  1.62it/s]Extractor Predicting: 112it [01:07,  1.63it/s]Extractor Predicting: 113it [01:08,  1.51it/s]Extractor Predicting: 114it [01:08,  1.55it/s]Extractor Predicting: 115it [01:09,  1.58it/s]Extractor Predicting: 116it [01:10,  1.62it/s]Extractor Predicting: 117it [01:10,  1.64it/s]Extractor Predicting: 118it [01:11,  1.70it/s]Extractor Predicting: 119it [01:11,  1.72it/s]Extractor Predicting: 120it [01:12,  1.72it/s]Extractor Predicting: 121it [01:12,  1.72it/s]Extractor Predicting: 122it [01:13,  1.70it/s]Extractor Predicting: 123it [01:14,  1.68it/s]Extractor Predicting: 124it [01:14,  1.70it/s]Extractor Predicting: 125it [01:15,  1.70it/s]Extractor Predicting: 126it [01:15,  1.67it/s]Extractor Predicting: 127it [01:16,  1.65it/s]Extractor Predicting: 128it [01:17,  1.62it/s]Extractor Predicting: 129it [01:17,  1.59it/s]Extractor Predicting: 130it [01:18,  1.58it/s]Extractor Predicting: 131it [01:19,  1.55it/s]Extractor Predicting: 132it [01:19,  1.58it/s]Extractor Predicting: 133it [01:20,  1.59it/s]Extractor Predicting: 134it [01:21,  1.62it/s]Extractor Predicting: 135it [01:21,  1.56it/s]Extractor Predicting: 136it [01:22,  1.56it/s]Extractor Predicting: 137it [01:22,  1.55it/s]Extractor Predicting: 138it [01:23,  1.57it/s]Extractor Predicting: 139it [01:24,  1.56it/s]Extractor Predicting: 140it [01:24,  1.54it/s]Extractor Predicting: 141it [01:25,  1.55it/s]Extractor Predicting: 142it [01:26,  1.56it/s]Extractor Predicting: 143it [01:26,  1.58it/s]Extractor Predicting: 144it [01:27,  1.56it/s]Extractor Predicting: 145it [01:28,  1.55it/s]Extractor Predicting: 146it [01:28,  1.56it/s]Extractor Predicting: 147it [01:29,  1.57it/s]Extractor Predicting: 148it [01:30,  1.58it/s]Extractor Predicting: 149it [01:30,  1.58it/s]Extractor Predicting: 150it [01:31,  1.57it/s]Extractor Predicting: 151it [01:31,  1.56it/s]Extractor Predicting: 152it [01:32,  1.57it/s]Extractor Predicting: 153it [01:33,  1.56it/s]Extractor Predicting: 154it [01:33,  1.52it/s]Extractor Predicting: 155it [01:34,  1.55it/s]Extractor Predicting: 156it [01:35,  1.60it/s]Extractor Predicting: 157it [01:35,  1.66it/s]Extractor Predicting: 158it [01:36,  1.67it/s]Extractor Predicting: 159it [01:36,  1.65it/s]Extractor Predicting: 160it [01:37,  1.67it/s]Extractor Predicting: 161it [01:38,  1.71it/s]Extractor Predicting: 162it [01:38,  1.67it/s]Extractor Predicting: 163it [01:39,  1.66it/s]Extractor Predicting: 164it [01:39,  1.66it/s]Extractor Predicting: 165it [01:40,  1.71it/s]Extractor Predicting: 166it [01:40,  1.71it/s]Extractor Predicting: 167it [01:41,  1.76it/s]Extractor Predicting: 168it [01:42,  1.78it/s]Extractor Predicting: 169it [01:42,  1.81it/s]Extractor Predicting: 170it [01:43,  1.74it/s]Extractor Predicting: 171it [01:43,  1.67it/s]Extractor Predicting: 172it [01:44,  1.64it/s]Extractor Predicting: 173it [01:45,  1.70it/s]Extractor Predicting: 173it [01:45,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,503 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,526 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:45,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:11:46,133 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:11:46,134 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:46,707 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:47,846 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:47,846 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:11:49,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:11:50,354 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:11:50,356 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:11:51,068 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:11:51,230 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:11:51,231 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20739219712525667,
  "recall": 0.12180414857694163,
  "score": 0.15347211669958974,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.79it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:05,  1.45it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.72it/s]Extractor Predicting: 17it [00:10,  1.81it/s]Extractor Predicting: 18it [00:11,  1.84it/s]Extractor Predicting: 19it [00:11,  1.86it/s]Extractor Predicting: 20it [00:12,  1.88it/s]Extractor Predicting: 21it [00:12,  1.92it/s]Extractor Predicting: 22it [00:13,  1.88it/s]Extractor Predicting: 23it [00:13,  1.88it/s]Extractor Predicting: 24it [00:14,  1.86it/s]Extractor Predicting: 25it [00:15,  1.87it/s]Extractor Predicting: 26it [00:15,  1.91it/s]Extractor Predicting: 27it [00:16,  1.86it/s]Extractor Predicting: 28it [00:16,  1.88it/s]Extractor Predicting: 29it [00:17,  1.89it/s]Extractor Predicting: 30it [00:17,  1.93it/s]Extractor Predicting: 31it [00:18,  1.95it/s]Extractor Predicting: 32it [00:18,  1.93it/s]Extractor Predicting: 33it [00:19,  1.94it/s]Extractor Predicting: 34it [00:19,  1.91it/s]Extractor Predicting: 35it [00:20,  1.91it/s]Extractor Predicting: 36it [00:20,  1.87it/s]Extractor Predicting: 37it [00:21,  1.83it/s]Extractor Predicting: 38it [00:21,  1.89it/s]Extractor Predicting: 39it [00:22,  1.89it/s]Extractor Predicting: 40it [00:22,  1.87it/s]Extractor Predicting: 41it [00:23,  1.87it/s]Extractor Predicting: 42it [00:24,  1.92it/s]Extractor Predicting: 43it [00:24,  1.88it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:25,  1.64it/s]Extractor Predicting: 46it [00:26,  1.55it/s]Extractor Predicting: 47it [00:27,  1.53it/s]Extractor Predicting: 48it [00:28,  1.51it/s]Extractor Predicting: 49it [00:28,  1.51it/s]Extractor Predicting: 50it [00:29,  1.50it/s]Extractor Predicting: 51it [00:30,  1.48it/s]Extractor Predicting: 52it [00:30,  1.47it/s]Extractor Predicting: 53it [00:31,  1.47it/s]Extractor Predicting: 54it [00:32,  1.46it/s]Extractor Predicting: 55it [00:32,  1.48it/s]Extractor Predicting: 56it [00:33,  1.43it/s]Extractor Predicting: 57it [00:34,  1.44it/s]Extractor Predicting: 58it [00:34,  1.45it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 59it [00:35,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-29 09:12:29,156 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:12:29,203 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:12:29,302 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:12:29,303 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 09:12:29,364 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:12:40,275 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 09:12:40,275 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 09:12:40,538 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:12:40,539 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:12:40,610 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,661 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,662 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,662 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,662 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,662 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:12:40,662 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6713221601489758,
  "recall": 0.23310701584222437,
  "score": 0.34605231581473483,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 09:12:40,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:42,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:43,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:44,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:44,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:45,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:45,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:46,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:47,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:47,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:48,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:49,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:49,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:50,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:50,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:51,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:51,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:52,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:49, 12.17s/it][WARNING|generation_utils.py:914] 2023-08-29 09:12:53,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:53,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:54,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:54,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:55,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:55,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:56,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:57,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:57,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:58,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:59,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:12:59,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:00,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:00,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:01,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:01,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:02,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:02,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:03,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:03,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:04,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:04,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:05,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:06,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:25<01:43, 12.91s/it][WARNING|generation_utils.py:914] 2023-08-29 09:13:06,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:07,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:07,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:08,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:08,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:09,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:10,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:10,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:11,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:12,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:12,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:13,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:14,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:15,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:15,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:16,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:16,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:17,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:17,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:18,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:19,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:19,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:20,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:20,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:40<01:36, 13.75s/it][WARNING|generation_utils.py:914] 2023-08-29 09:13:21,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:22,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:22,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:23,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:24,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:24,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:25,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:25,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:26,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:27,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:27,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:28,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:29,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:29,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:30,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:31,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:32,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:32,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:33,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:34,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:53<01:22, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-29 09:13:34,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:35,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:36,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:36,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:37,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:37,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:38,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:38,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:39,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:39,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:40,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:41,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:41,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:42,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:42,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:43,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:43,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:44,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:44,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:45,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:45,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:05<01:04, 12.92s/it][WARNING|generation_utils.py:914] 2023-08-29 09:13:46,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:47,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:47,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:48,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:48,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:49,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:50,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:50,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:51,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:52,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:54,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:55,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:56,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:57,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:57,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:58,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:58,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:59,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:13:59,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:19<00:53, 13.33s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:00,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:01,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:01,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:02,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:02,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:03,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:04,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:05,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:05,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:06,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:06,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:07,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:08,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:08,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:09,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:10,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:10,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:11,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:12,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:13,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:13,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:33<00:40, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:14,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:14,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:15,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:15,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:16,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:16,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:17,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:17,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:18,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:18,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:19,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:19,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:20,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:20,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:21,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:21,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:22,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:22,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:23,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:23,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:24,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:44<00:25, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:25,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:25,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:26,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:26,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:27,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:27,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:28,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:28,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:29,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:29,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:30,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:30,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:31,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:32,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:32,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:33,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:33,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:34,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:34,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:35,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:54<00:12, 12.07s/it][WARNING|generation_utils.py:914] 2023-08-29 09:14:35,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:36,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:37,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:37,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:38,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:38,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:39,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:40,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:40,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:41,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:41,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:42,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:42,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:43,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:44,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:44,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:45,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:45,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:46,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:47,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:14:47,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:07<00:00, 12.12s/it]Generating: 100%|██████████| 10/10 [02:07<00:00, 12.72s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:55,543 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:55,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:55,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:55,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:55,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:14:56,343 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:14:56,344 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:14:57,008 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:14:58,154 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:14:58,154 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:01,140 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:01,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:01,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:01,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:01,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:15:01,930 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:15:01,931 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:15:02,564 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:15:02,813 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:15:02,813 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : given name .', 'success_rate': 0.9734375, 'errors': {'', "('James R. Cramer', 'given name', '', 'James R. Cramer ( April 18 , 1903 ) is a Republican politician from Texas who served in the Texas House of Representatives between 1980 and 1986 and from 1987 to 1991 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.7916666666666666, 'errors': {''}}
['Relation : lowest point . Context : Later in the year , the band members decided to cancel their tour of other cities at the end of 2010 , citing financial reasons . Head Entity : Los Angeles , Tail Entity : Los Angeles .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.75875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9255952380952381, 'errors': {''}}
['Relation : mouth of the watercourse . Context : Later in the year , the watercourse was widened and the lake was opened to serve the surrounding river Thames , flowing through Somerset . Head Entity : Somerset , Tail Entity : River Thames .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8735795454545454, 'errors': {'', "('FC Hüttenbach', 'genre', '', 'After that he started his career at the club , where he played in the Bundesliga and in 2007 , he played a total of one season for Nantes club FC Hüttenbach as a forward .')", 'not enough values to unpack (expected 2, got 1)', "('The People Show', 'genre', '', 'The following year , he appeared on the BBC s The People Show as a character who played the role of a newspaper reporter .')", "('The Big Bang Theory', 'genre', '', 'After working on the soundtrack for the 2005 film , The Big Bang Theory , he made his film debut in The Big Bang Theory episode entitled # 2 .')", "('A Baby Is Born', 'genre', '', 'Her next release , the single A Baby Is Born , was released on August 14 , 2011 by Jai Courtney .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.8778409090909091, 'errors': {'', "('languages', 'is a list of', '', 'It is available in several languages and under several subnets .')", "('language', 'is a list of', '', 'It is an index to the languages of all the languages in existence .')", 'not enough values to unpack (expected 2, got 1)', "('( )', 'is a list of', '', '( ) is a list of .')", "('Group of 20', 'is a list of', '', 'It is a member of the Group of 20 .')", "('Google', 'is a list of', '', 'The list is created when a user first signs in with Google .')", "('languages', 'is a list of', '', 'It has been used in many languages for the past thirty years and is the largest such list of languages .')", "('languages', 'is a list of', '', 'It is the most comprehensive , in alphabetical order , of the list of known languages .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9421875, 'errors': {''}}
['Relation : member of . Context : Later in the year , the band members , including bassist Andy Warhol , formed a new lineup , together with singer Michael Jackson . Head Entity : Michael Jackson , Tail Entity : jazz .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 8286
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8386, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.49it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:03,  1.52it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.56it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:09,  1.58it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.57it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.58it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:13,  1.65it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:15,  1.63it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:16,  1.69it/s]Extractor Estimating: 28it [00:17,  1.74it/s]Extractor Estimating: 29it [00:18,  1.76it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:19,  1.75it/s]Extractor Estimating: 32it [00:19,  1.79it/s]Extractor Estimating: 33it [00:20,  1.74it/s]Extractor Estimating: 34it [00:20,  1.73it/s]Extractor Estimating: 35it [00:21,  1.82it/s]Extractor Estimating: 36it [00:22,  1.67it/s]Extractor Estimating: 37it [00:22,  1.72it/s]Extractor Estimating: 38it [00:23,  1.72it/s]Extractor Estimating: 39it [00:23,  1.76it/s]Extractor Estimating: 40it [00:24,  1.80it/s]Extractor Estimating: 41it [00:24,  1.85it/s]Extractor Estimating: 42it [00:25,  1.84it/s]Extractor Estimating: 43it [00:25,  1.82it/s]Extractor Estimating: 44it [00:26,  1.71it/s]Extractor Estimating: 45it [00:27,  1.74it/s]Extractor Estimating: 46it [00:27,  1.77it/s]Extractor Estimating: 47it [00:28,  1.76it/s]Extractor Estimating: 48it [00:28,  1.73it/s]Extractor Estimating: 49it [00:29,  1.75it/s]Extractor Estimating: 50it [00:29,  1.77it/s]Extractor Estimating: 51it [00:30,  1.74it/s]Extractor Estimating: 52it [00:31,  1.72it/s]Extractor Estimating: 53it [00:31,  1.66it/s]Extractor Estimating: 54it [00:32,  1.65it/s]Extractor Estimating: 55it [00:33,  1.60it/s]Extractor Estimating: 56it [00:33,  1.62it/s]Extractor Estimating: 57it [00:34,  1.61it/s]Extractor Estimating: 58it [00:34,  1.67it/s]Extractor Estimating: 59it [00:35,  1.69it/s]Extractor Estimating: 60it [00:36,  1.69it/s]Extractor Estimating: 61it [00:36,  1.64it/s]Extractor Estimating: 62it [00:37,  1.62it/s]Extractor Estimating: 63it [00:37,  1.60it/s]Extractor Estimating: 64it [00:38,  1.59it/s]Extractor Estimating: 65it [00:39,  1.59it/s]Extractor Estimating: 66it [00:39,  1.64it/s]Extractor Estimating: 67it [00:40,  1.66it/s]Extractor Estimating: 68it [00:40,  1.72it/s]Extractor Estimating: 69it [00:41,  1.73it/s]Extractor Estimating: 70it [00:42,  1.73it/s]Extractor Estimating: 71it [00:42,  1.74it/s]Extractor Estimating: 72it [00:43,  1.65it/s]Extractor Estimating: 73it [00:43,  1.69it/s]Extractor Estimating: 74it [00:44,  1.67it/s]Extractor Estimating: 75it [00:45,  1.64it/s]Extractor Estimating: 76it [00:45,  1.58it/s]Extractor Estimating: 77it [00:46,  1.56it/s]Extractor Estimating: 78it [00:47,  1.59it/s]Extractor Estimating: 79it [00:47,  1.60it/s]Extractor Estimating: 80it [00:48,  1.59it/s]Extractor Estimating: 81it [00:48,  1.62it/s]Extractor Estimating: 82it [00:49,  1.64it/s]Extractor Estimating: 83it [00:50,  1.70it/s]Extractor Estimating: 84it [00:50,  1.67it/s]Extractor Estimating: 85it [00:51,  1.65it/s]Extractor Estimating: 86it [00:51,  1.61it/s]Extractor Estimating: 87it [00:52,  1.66it/s]Extractor Estimating: 88it [00:53,  1.61it/s]Extractor Estimating: 89it [00:53,  1.58it/s]Extractor Estimating: 90it [00:54,  1.60it/s]Extractor Estimating: 91it [00:55,  1.62it/s]Extractor Estimating: 92it [00:55,  1.65it/s]Extractor Estimating: 93it [00:56,  1.64it/s]Extractor Estimating: 94it [00:56,  1.62it/s]Extractor Estimating: 95it [00:57,  1.64it/s]Extractor Estimating: 96it [00:58,  1.58it/s]Extractor Estimating: 97it [00:58,  1.58it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [01:00,  1.60it/s]Extractor Estimating: 100it [01:00,  1.60it/s]Extractor Estimating: 101it [01:01,  1.64it/s]Extractor Estimating: 102it [01:01,  1.69it/s]Extractor Estimating: 103it [01:02,  1.68it/s]Extractor Estimating: 104it [01:02,  1.77it/s]Extractor Estimating: 105it [01:03,  1.80it/s]Extractor Estimating: 106it [01:03,  1.77it/s]Extractor Estimating: 107it [01:04,  1.74it/s]Extractor Estimating: 108it [01:05,  1.77it/s]Extractor Estimating: 109it [01:05,  1.77it/s]Extractor Estimating: 110it [01:06,  1.81it/s]Extractor Estimating: 111it [01:06,  1.84it/s]Extractor Estimating: 112it [01:07,  1.86it/s]Extractor Estimating: 113it [01:07,  1.86it/s]Extractor Estimating: 114it [01:08,  1.88it/s]Extractor Estimating: 115it [01:09,  1.70it/s]Extractor Estimating: 116it [01:09,  1.75it/s]Extractor Estimating: 117it [01:10,  1.77it/s]Extractor Estimating: 118it [01:10,  1.75it/s]Extractor Estimating: 119it [01:11,  1.77it/s]Extractor Estimating: 120it [01:11,  1.76it/s]Extractor Estimating: 121it [01:12,  1.86it/s]Extractor Estimating: 122it [01:12,  1.83it/s]Extractor Estimating: 123it [01:13,  1.82it/s]Extractor Estimating: 124it [01:13,  1.85it/s]Extractor Estimating: 125it [01:14,  1.82it/s]Extractor Estimating: 126it [01:15,  1.76it/s]Extractor Estimating: 127it [01:15,  1.69it/s]Extractor Estimating: 128it [01:16,  1.69it/s]Extractor Estimating: 129it [01:16,  1.68it/s]Extractor Estimating: 130it [01:17,  1.62it/s]Extractor Estimating: 131it [01:18,  1.57it/s]Extractor Estimating: 132it [01:18,  1.56it/s]Extractor Estimating: 133it [01:19,  1.61it/s]Extractor Estimating: 134it [01:20,  1.57it/s]Extractor Estimating: 135it [01:20,  1.60it/s]Extractor Estimating: 136it [01:21,  1.62it/s]Extractor Estimating: 137it [01:21,  1.64it/s]Extractor Estimating: 138it [01:22,  1.61it/s]Extractor Estimating: 139it [01:23,  1.56it/s]Extractor Estimating: 140it [01:23,  1.54it/s]Extractor Estimating: 141it [01:24,  1.53it/s]Extractor Estimating: 142it [01:25,  1.52it/s]Extractor Estimating: 143it [01:25,  1.51it/s]Extractor Estimating: 144it [01:26,  1.55it/s]Extractor Estimating: 145it [01:27,  1.57it/s]Extractor Estimating: 146it [01:27,  1.56it/s]Extractor Estimating: 147it [01:28,  1.56it/s]Extractor Estimating: 148it [01:29,  1.57it/s]Extractor Estimating: 149it [01:29,  1.63it/s]Extractor Estimating: 150it [01:30,  1.52it/s]Extractor Estimating: 151it [01:30,  1.63it/s]Extractor Estimating: 152it [01:31,  1.62it/s]Extractor Estimating: 153it [01:32,  1.70it/s]Extractor Estimating: 154it [01:32,  1.74it/s]Extractor Estimating: 155it [01:33,  1.78it/s]Extractor Estimating: 156it [01:33,  1.77it/s]Extractor Estimating: 157it [01:34,  1.81it/s]Extractor Estimating: 158it [01:34,  1.81it/s]Extractor Estimating: 159it [01:35,  1.85it/s]Extractor Estimating: 160it [01:35,  1.84it/s]Extractor Estimating: 161it [01:36,  1.87it/s]Extractor Estimating: 162it [01:36,  1.88it/s]Extractor Estimating: 163it [01:37,  1.91it/s]Extractor Estimating: 164it [01:37,  1.93it/s]Extractor Estimating: 165it [01:38,  1.89it/s]Extractor Estimating: 166it [01:39,  1.90it/s]Extractor Estimating: 167it [01:39,  1.86it/s]Extractor Estimating: 168it [01:40,  1.74it/s]Extractor Estimating: 169it [01:40,  1.78it/s]Extractor Estimating: 170it [01:41,  1.79it/s]Extractor Estimating: 171it [01:41,  1.78it/s]Extractor Estimating: 172it [01:42,  1.81it/s]Extractor Estimating: 173it [01:42,  1.83it/s]Extractor Estimating: 174it [01:43,  1.79it/s]Extractor Estimating: 175it [01:44,  1.82it/s]Extractor Estimating: 176it [01:44,  1.83it/s]Extractor Estimating: 177it [01:45,  1.89it/s]Extractor Estimating: 178it [01:45,  1.93it/s]Extractor Estimating: 179it [01:46,  1.92it/s]Extractor Estimating: 180it [01:46,  1.96it/s]Extractor Estimating: 181it [01:47,  2.00it/s]Extractor Estimating: 182it [01:47,  1.99it/s]Extractor Estimating: 183it [01:48,  2.05it/s]Extractor Estimating: 184it [01:48,  2.04it/s]Extractor Estimating: 185it [01:49,  2.03it/s]Extractor Estimating: 186it [01:49,  1.95it/s]Extractor Estimating: 187it [01:50,  1.86it/s]Extractor Estimating: 188it [01:50,  1.79it/s]Extractor Estimating: 189it [01:51,  1.80it/s]Extractor Estimating: 190it [01:51,  1.88it/s]Extractor Estimating: 191it [01:52,  1.93it/s]Extractor Estimating: 192it [01:52,  1.93it/s]Extractor Estimating: 193it [01:53,  1.89it/s]Extractor Estimating: 194it [01:53,  1.87it/s]Extractor Estimating: 195it [01:54,  1.87it/s]Extractor Estimating: 196it [01:54,  1.94it/s]Extractor Estimating: 197it [01:55,  1.94it/s]Extractor Estimating: 198it [01:56,  1.90it/s]Extractor Estimating: 199it [01:56,  1.98it/s]Extractor Estimating: 200it [01:56,  1.95it/s]Extractor Estimating: 201it [01:57,  1.92it/s]Extractor Estimating: 202it [01:58,  1.86it/s]Extractor Estimating: 203it [01:58,  1.69it/s]Extractor Estimating: 204it [01:59,  1.71it/s]Extractor Estimating: 205it [01:59,  1.77it/s]Extractor Estimating: 206it [02:00,  1.88it/s]Extractor Estimating: 207it [02:00,  1.90it/s]Extractor Estimating: 208it [02:01,  1.86it/s]Extractor Estimating: 209it [02:01,  1.88it/s]Extractor Estimating: 210it [02:02,  1.86it/s]Extractor Estimating: 211it [02:03,  1.84it/s]Extractor Estimating: 212it [02:03,  1.81it/s]Extractor Estimating: 213it [02:04,  1.83it/s]Extractor Estimating: 214it [02:04,  1.78it/s]Extractor Estimating: 215it [02:05,  1.80it/s]Extractor Estimating: 216it [02:05,  1.84it/s]Extractor Estimating: 217it [02:06,  1.81it/s]Extractor Estimating: 218it [02:06,  1.92it/s]Extractor Estimating: 219it [02:07,  1.91it/s]Extractor Estimating: 220it [02:07,  1.87it/s]Extractor Estimating: 221it [02:08,  1.85it/s]Extractor Estimating: 222it [02:09,  1.84it/s]Extractor Estimating: 223it [02:09,  1.80it/s]Extractor Estimating: 224it [02:10,  1.80it/s]Extractor Estimating: 225it [02:10,  1.77it/s]Extractor Estimating: 226it [02:11,  1.70it/s]Extractor Estimating: 227it [02:12,  1.63it/s]Extractor Estimating: 228it [02:12,  1.64it/s]Extractor Estimating: 229it [02:13,  1.63it/s]Extractor Estimating: 230it [02:13,  1.66it/s]Extractor Estimating: 231it [02:14,  1.67it/s]Extractor Estimating: 232it [02:15,  1.67it/s]Extractor Estimating: 233it [02:15,  1.70it/s]Extractor Estimating: 234it [02:16,  1.70it/s]Extractor Estimating: 235it [02:16,  1.73it/s]Extractor Estimating: 236it [02:17,  1.69it/s]Extractor Estimating: 237it [02:18,  1.68it/s]Extractor Estimating: 238it [02:18,  1.67it/s]Extractor Estimating: 239it [02:19,  1.66it/s]Extractor Estimating: 240it [02:19,  1.61it/s]Extractor Estimating: 241it [02:20,  1.63it/s]Extractor Estimating: 242it [02:21,  1.66it/s]Extractor Estimating: 243it [02:21,  1.67it/s]Extractor Estimating: 244it [02:22,  1.69it/s]Extractor Estimating: 245it [02:22,  1.66it/s]Extractor Estimating: 246it [02:23,  1.66it/s]Extractor Estimating: 247it [02:24,  1.71it/s]Extractor Estimating: 248it [02:24,  1.64it/s]Extractor Estimating: 249it [02:25,  1.64it/s]Extractor Estimating: 249it [02:25,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:48,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:48,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:48,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:48,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:48,371 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:17:49,151 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:17:49,152 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:17:49,774 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:17:50,923 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:17:50,923 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:53,902 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:53,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:53,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:53,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:17:53,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:17:54,683 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:17:54,684 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:17:55,320 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:17:55,533 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:17:55,533 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:09:16,256 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:09:16,273 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 2978 mean pseudo reward: 0.9806805429869518
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 16081
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16181, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16181, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.967, loss:217.6381
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 0.967, loss:194.2563
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.954, loss:183.6638
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 0.972, loss:168.4907
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 0.965, loss:169.0629
>> valid entity prec:0.4840, rec:0.5395, f1:0.5103
>> valid relation prec:0.1620, rec:0.1220, f1:0.1392
>> valid relation with NER prec:0.1620, rec:0.1220, f1:0.1392
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.968, loss:159.6425
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 0.954, loss:159.4149
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.955, loss:151.5887
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 0.955, loss:161.8425
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 0.962, loss:157.9338
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5214, rec:0.4186, f1:0.4644
>> valid relation prec:0.1892, rec:0.0977, f1:0.1288
>> valid relation with NER prec:0.1892, rec:0.0977, f1:0.1288
g_step 1100, step 100, avg_time 0.971, loss:156.0717
g_step 1200, step 75, avg_time 0.946, loss:141.0839
g_step 1300, step 50, avg_time 0.965, loss:137.4103
g_step 1400, step 25, avg_time 0.952, loss:147.9176
g_step 1500, step 125, avg_time 0.959, loss:136.2834
>> valid entity prec:0.5222, rec:0.4753, f1:0.4977
>> valid relation prec:0.1909, rec:0.0977, f1:0.1292
>> valid relation with NER prec:0.1909, rec:0.0977, f1:0.1292
g_step 1600, step 100, avg_time 0.969, loss:125.3241
g_step 1700, step 75, avg_time 0.967, loss:121.2034
g_step 1800, step 50, avg_time 0.957, loss:111.8176
g_step 1900, step 25, avg_time 0.955, loss:126.7604
g_step 2000, step 125, avg_time 0.955, loss:113.1641
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5046, rec:0.4486, f1:0.4750
>> valid relation prec:0.1838, rec:0.0992, f1:0.1288
>> valid relation with NER prec:0.1838, rec:0.0992, f1:0.1288
g_step 2100, step 100, avg_time 0.973, loss:104.3459
g_step 2200, step 75, avg_time 0.952, loss:105.5749
g_step 2300, step 50, avg_time 0.959, loss:100.3330
g_step 2400, step 25, avg_time 0.954, loss:101.0574
g_step 2500, step 125, avg_time 0.966, loss:99.0510
>> valid entity prec:0.5233, rec:0.4567, f1:0.4877
>> valid relation prec:0.1834, rec:0.1264, f1:0.1497
>> valid relation with NER prec:0.1834, rec:0.1264, f1:0.1497
new max relation f1 on valid!
new max relation f1 with NER on valid!
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:09:16 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:09:16 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-09-16_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:09:17 - WARNING - datasets.builder -   Using custom data configuration default-484d90d3481b0321
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-484d90d3481b0321/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:09:18,867 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:09:18,868 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:09:18,869 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:09:18,870 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:09:18,948 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:09:18,992 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:09:19,314 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:09:22,625 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:09:22,646 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-484d90d3481b0321/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.77ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.79ba/s]100%|██████████| 3/3 [00:00<00:00,  4.31ba/s]100%|██████████| 3/3 [00:00<00:00,  4.00ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.26ba/s] 40%|████      | 2/5 [00:00<00:00,  3.91ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.15ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.28ba/s]100%|██████████| 5/5 [00:00<00:00,  5.07ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  5.75ba/s]100%|██████████| 3/3 [00:00<00:00,  8.62ba/s]100%|██████████| 3/3 [00:00<00:00,  8.21ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.70ba/s]100%|██████████| 5/5 [00:00<00:00, 12.22ba/s]100%|██████████| 5/5 [00:00<00:00, 10.83ba/s]
[INFO|trainer.py:414] 2023-08-29 10:09:26,113 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:09:26,141 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:09:26,141 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-29 10:09:26,141 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:09:26,141 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:09:26,141 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:09:26,141 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:09:26,141 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:10,  3.32it/s]  1%|          | 2/235 [00:00<01:08,  3.41it/s]  1%|▏         | 3/235 [00:00<01:07,  3.43it/s]  2%|▏         | 4/235 [00:01<01:11,  3.24it/s]  2%|▏         | 5/235 [00:01<01:09,  3.31it/s]  3%|▎         | 6/235 [00:01<01:08,  3.36it/s]  3%|▎         | 7/235 [00:02<01:07,  3.39it/s]  3%|▎         | 8/235 [00:02<01:07,  3.36it/s]  4%|▍         | 9/235 [00:02<01:06,  3.39it/s]  4%|▍         | 10/235 [00:02<01:06,  3.40it/s]  5%|▍         | 11/235 [00:03<01:05,  3.42it/s]  5%|▌         | 12/235 [00:03<01:05,  3.42it/s]  6%|▌         | 13/235 [00:03<01:04,  3.43it/s]  6%|▌         | 14/235 [00:04<01:04,  3.44it/s]  6%|▋         | 15/235 [00:04<01:03,  3.44it/s]  7%|▋         | 16/235 [00:04<01:03,  3.44it/s]  7%|▋         | 17/235 [00:04<01:03,  3.44it/s]  8%|▊         | 18/235 [00:05<01:03,  3.44it/s]  8%|▊         | 19/235 [00:05<01:04,  3.36it/s]  9%|▊         | 20/235 [00:05<01:03,  3.38it/s]  9%|▉         | 21/235 [00:06<01:02,  3.40it/s]  9%|▉         | 22/235 [00:06<01:02,  3.42it/s] 10%|▉         | 23/235 [00:06<01:01,  3.42it/s] 10%|█         | 24/235 [00:07<01:01,  3.43it/s] 11%|█         | 25/235 [00:07<01:01,  3.43it/s] 11%|█         | 26/235 [00:07<01:00,  3.44it/s] 11%|█▏        | 27/235 [00:07<01:00,  3.44it/s] 12%|█▏        | 28/235 [00:08<01:00,  3.44it/s] 12%|█▏        | 29/235 [00:08<00:59,  3.44it/s] 13%|█▎        | 30/235 [00:08<01:00,  3.40it/s] 13%|█▎        | 31/235 [00:09<00:59,  3.41it/s] 14%|█▎        | 32/235 [00:09<00:59,  3.42it/s] 14%|█▍        | 33/235 [00:09<00:58,  3.43it/s] 14%|█▍        | 34/235 [00:09<00:58,  3.44it/s] 15%|█▍        | 35/235 [00:10<00:58,  3.44it/s] 15%|█▌        | 36/235 [00:10<00:57,  3.44it/s] 16%|█▌        | 37/235 [00:10<00:57,  3.44it/s] 16%|█▌        | 38/235 [00:11<00:57,  3.44it/s] 17%|█▋        | 39/235 [00:11<00:56,  3.44it/s] 17%|█▋        | 40/235 [00:11<00:56,  3.45it/s] 17%|█▋        | 41/235 [00:12<00:57,  3.36it/s] 18%|█▊        | 42/235 [00:12<00:57,  3.39it/s] 18%|█▊        | 43/235 [00:12<00:56,  3.40it/s] 19%|█▊        | 44/235 [00:12<00:55,  3.41it/s] 19%|█▉        | 45/235 [00:13<00:55,  3.42it/s] 20%|█▉        | 46/235 [00:13<00:55,  3.43it/s] 20%|██        | 47/235 [00:13<00:52,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 10:09:39,879 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:09:39,879 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:09:39,879 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.53it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.63it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.08it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.21it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.47it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.92it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.70it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.42it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.51it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.24it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.56it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.76it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.68it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.47it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.32it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.27it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.22it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.25it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.48it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.68it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.83it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.67it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.46it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.39it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.28it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.27it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.20it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.49it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.69it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.74it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.69it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.46it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.40it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.32it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.26it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.24it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 43.09it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 43.74it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.16it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.31it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.32it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.24it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.24it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.22it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.16it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.30it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.48it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.57it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.71it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.58it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.47it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.44it/s][A
 53%|█████▎    | 267/505 [00:05<00:05, 44.28it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.27it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.41it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.50it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.59it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.73it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.59it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.50it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.33it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.25it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.29it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 42.22it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 42.95it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.58it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.91it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.12it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.18it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.06it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.08it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.06it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.17it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.38it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.54it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.61it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.64it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.49it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.31it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.24it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.34it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.39it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.68it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.68it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.67it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.59it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.46it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.41it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.40it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 41.96it/s][A
 91%|█████████▏| 462/505 [00:10<00:01, 42.89it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.54it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.02it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.37it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.48it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.49it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.45it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.05it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.12it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.12it/s][A 20%|██        | 47/235 [00:25<00:52,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:09:51,369 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-29 10:09:51,618 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:09:55,452 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:09:55,660 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:09:55,717 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:36<21:27,  6.89s/it] 21%|██        | 49/235 [00:36<15:14,  4.92s/it] 21%|██▏       | 50/235 [00:36<10:52,  3.53s/it] 22%|██▏       | 51/235 [00:36<07:50,  2.56s/it] 22%|██▏       | 52/235 [00:37<05:43,  1.88s/it] 23%|██▎       | 53/235 [00:37<04:15,  1.40s/it] 23%|██▎       | 54/235 [00:37<03:13,  1.07s/it] 23%|██▎       | 55/235 [00:38<02:30,  1.19it/s] 24%|██▍       | 56/235 [00:38<02:00,  1.48it/s] 24%|██▍       | 57/235 [00:38<01:39,  1.78it/s] 25%|██▍       | 58/235 [00:38<01:25,  2.08it/s] 25%|██▌       | 59/235 [00:39<01:14,  2.35it/s] 26%|██▌       | 60/235 [00:39<01:08,  2.55it/s] 26%|██▌       | 61/235 [00:39<01:03,  2.76it/s] 26%|██▋       | 62/235 [00:40<00:59,  2.92it/s] 27%|██▋       | 63/235 [00:40<00:56,  3.05it/s] 27%|██▋       | 64/235 [00:40<00:54,  3.14it/s] 28%|██▊       | 65/235 [00:41<00:52,  3.22it/s] 28%|██▊       | 66/235 [00:41<00:51,  3.27it/s] 29%|██▊       | 67/235 [00:41<00:50,  3.30it/s] 29%|██▉       | 68/235 [00:41<00:50,  3.33it/s] 29%|██▉       | 69/235 [00:42<00:49,  3.35it/s] 30%|██▉       | 70/235 [00:42<00:49,  3.36it/s] 30%|███       | 71/235 [00:42<00:50,  3.23it/s] 31%|███       | 72/235 [00:43<00:49,  3.28it/s] 31%|███       | 73/235 [00:43<00:48,  3.31it/s] 31%|███▏      | 74/235 [00:43<00:48,  3.34it/s] 32%|███▏      | 75/235 [00:44<00:47,  3.35it/s] 32%|███▏      | 76/235 [00:44<00:47,  3.36it/s] 33%|███▎      | 77/235 [00:44<00:46,  3.37it/s] 33%|███▎      | 78/235 [00:44<00:46,  3.38it/s] 34%|███▎      | 79/235 [00:45<00:46,  3.38it/s] 34%|███▍      | 80/235 [00:45<00:45,  3.38it/s] 34%|███▍      | 81/235 [00:45<00:45,  3.39it/s] 35%|███▍      | 82/235 [00:46<00:46,  3.30it/s] 35%|███▌      | 83/235 [00:46<00:45,  3.33it/s] 36%|███▌      | 84/235 [00:46<00:45,  3.34it/s] 36%|███▌      | 85/235 [00:47<00:44,  3.36it/s] 37%|███▋      | 86/235 [00:47<00:44,  3.37it/s] 37%|███▋      | 87/235 [00:47<00:43,  3.38it/s] 37%|███▋      | 88/235 [00:47<00:43,  3.38it/s] 38%|███▊      | 89/235 [00:48<00:43,  3.38it/s] 38%|███▊      | 90/235 [00:48<00:42,  3.39it/s] 39%|███▊      | 91/235 [00:48<00:42,  3.39it/s] 39%|███▉      | 92/235 [00:49<00:42,  3.39it/s] 40%|███▉      | 93/235 [00:49<00:42,  3.32it/s] 40%|████      | 94/235 [00:49<00:40,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 10:10:15,833 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:10:15,833 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:10:15,833 >>   Batch size = 8
{'eval_loss': 1.1386537551879883, 'eval_runtime': 11.4031, 'eval_samples_per_second': 354.201, 'eval_steps_per_second': 44.286, 'epoch': 1.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.75it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.65it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.07it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.14it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.50it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.96it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.78it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.59it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.69it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.82it/s][A
 11%|█▏        | 57/505 [00:01<00:09, 44.84it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.81it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.71it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.59it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.41it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.36it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.50it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.53it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.58it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.77it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.75it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.66it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 43.51it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.78it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.87it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.12it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.30it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.40it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.64it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.51it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.44it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.44it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.43it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.47it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.49it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.51it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.54it/s][A
 38%|███▊      | 192/505 [00:04<00:06, 44.72it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.65it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.40it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.36it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.43it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.42it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.57it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.59it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.62it/s][A
 47%|████▋     | 237/505 [00:05<00:05, 44.67it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.61it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.47it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 42.79it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.42it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.76it/s][A
 53%|█████▎    | 267/505 [00:05<00:05, 43.95it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.07it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.19it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.31it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.32it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.02it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.15it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.40it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.47it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.52it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.49it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.50it/s][A
 65%|██████▍   | 327/505 [00:07<00:03, 44.51it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.41it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.28it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.34it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.45it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.55it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.60it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.62it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.63it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.43it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.43it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.35it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.25it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.69it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.03it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.21it/s][A
 81%|████████  | 407/505 [00:09<00:02, 42.95it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.44it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.82it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 43.93it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.85it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.15it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.30it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.50it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.38it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.45it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.47it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.56it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.40it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.31it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.39it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.54it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.56it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.51it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.49it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.58it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.58it/s][A 40%|████      | 94/235 [01:01<00:40,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:10:27,661 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 10:10:27,777 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:10:30,711 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:10:30,818 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:10:30,870 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:11<15:39,  6.71s/it] 41%|████      | 96/235 [01:11<11:05,  4.79s/it] 41%|████▏     | 97/235 [01:11<07:54,  3.44s/it] 42%|████▏     | 98/235 [01:12<05:42,  2.50s/it] 42%|████▏     | 99/235 [01:12<04:09,  1.84s/it] 43%|████▎     | 100/235 [01:12<03:05,  1.37s/it] 43%|████▎     | 101/235 [01:13<02:20,  1.05s/it] 43%|████▎     | 102/235 [01:13<01:49,  1.21it/s] 44%|████▍     | 103/235 [01:13<01:27,  1.50it/s] 44%|████▍     | 104/235 [01:14<01:12,  1.81it/s] 45%|████▍     | 105/235 [01:14<01:01,  2.10it/s] 45%|████▌     | 106/235 [01:14<00:54,  2.37it/s] 46%|████▌     | 107/235 [01:14<00:50,  2.55it/s] 46%|████▌     | 108/235 [01:15<00:46,  2.76it/s] 46%|████▋     | 109/235 [01:15<00:43,  2.92it/s] 47%|████▋     | 110/235 [01:15<00:41,  3.05it/s] 47%|████▋     | 111/235 [01:16<00:39,  3.15it/s] 48%|████▊     | 112/235 [01:16<00:38,  3.22it/s] 48%|████▊     | 113/235 [01:16<00:37,  3.27it/s] 49%|████▊     | 114/235 [01:17<00:36,  3.30it/s] 49%|████▉     | 115/235 [01:17<00:36,  3.33it/s] 49%|████▉     | 116/235 [01:17<00:35,  3.35it/s] 50%|████▉     | 117/235 [01:17<00:35,  3.36it/s] 50%|█████     | 118/235 [01:18<00:35,  3.29it/s] 51%|█████     | 119/235 [01:18<00:34,  3.32it/s] 51%|█████     | 120/235 [01:18<00:34,  3.34it/s] 51%|█████▏    | 121/235 [01:19<00:33,  3.36it/s] 52%|█████▏    | 122/235 [01:19<00:33,  3.39it/s] 52%|█████▏    | 123/235 [01:19<00:32,  3.40it/s] 53%|█████▎    | 124/235 [01:19<00:32,  3.41it/s] 53%|█████▎    | 125/235 [01:20<00:32,  3.42it/s] 54%|█████▎    | 126/235 [01:20<00:31,  3.43it/s] 54%|█████▍    | 127/235 [01:20<00:31,  3.44it/s] 54%|█████▍    | 128/235 [01:21<00:31,  3.43it/s] 55%|█████▍    | 129/235 [01:21<00:32,  3.29it/s] 55%|█████▌    | 130/235 [01:21<00:31,  3.33it/s] 56%|█████▌    | 131/235 [01:22<00:30,  3.36it/s] 56%|█████▌    | 132/235 [01:22<00:30,  3.39it/s] 57%|█████▋    | 133/235 [01:22<00:29,  3.41it/s] 57%|█████▋    | 134/235 [01:22<00:29,  3.42it/s] 57%|█████▋    | 135/235 [01:23<00:29,  3.42it/s] 58%|█████▊    | 136/235 [01:23<00:28,  3.43it/s] 58%|█████▊    | 137/235 [01:23<00:28,  3.43it/s] 59%|█████▊    | 138/235 [01:24<00:28,  3.44it/s] 59%|█████▉    | 139/235 [01:24<00:27,  3.44it/s] 60%|█████▉    | 140/235 [01:24<00:29,  3.26it/s] 60%|██████    | 141/235 [01:24<00:27,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 10:10:51,124 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:10:51,124 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:10:51,124 >>   Batch size = 8
{'eval_loss': 1.1537182331085205, 'eval_runtime': 11.634, 'eval_samples_per_second': 347.171, 'eval_steps_per_second': 43.407, 'epoch': 2.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.19it/s][A
  2%|▏         | 12/505 [00:00<00:10, 49.05it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.94it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.83it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.39it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.82it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.49it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.43it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.47it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.65it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.65it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.70it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.71it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.61it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.46it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.27it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.19it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.44it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.58it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.59it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.57it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.66it/s][A
 23%|██▎       | 117/505 [00:02<00:09, 39.29it/s][A
 24%|██▍       | 122/505 [00:02<00:09, 40.89it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 42.05it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 42.90it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.35it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 43.87it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.24it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.21it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.93it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.81it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.95it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.09it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 44.36it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.59it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.68it/s][A
 38%|███▊      | 192/505 [00:04<00:06, 44.76it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.55it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.06it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.99it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.08it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.25it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.41it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.61it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.73it/s][A
 47%|████▋     | 237/505 [00:05<00:05, 44.76it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.55it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.24it/s][A
 50%|████▉     | 252/505 [00:05<00:06, 41.13it/s][A
 51%|█████     | 257/505 [00:05<00:05, 42.24it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 42.98it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 43.55it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.00it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.20it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.37it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.13it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.76it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.89it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.14it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.37it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.59it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.69it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.78it/s][A
 65%|██████▍   | 327/505 [00:07<00:03, 44.70it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.35it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.99it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.07it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.26it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.46it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.64it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.73it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.84it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.72it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.46it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 42.50it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.24it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.86it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.25it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.44it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.59it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.48it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.22it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 43.90it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.19it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.36it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.57it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.78it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.79it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.68it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.41it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.19it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.15it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.36it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.40it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.60it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.68it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.73it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.73it/s][A 60%|██████    | 141/235 [01:36<00:27,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:11:02,681 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-29 10:11:02,841 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:11:06,686 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:11:06,939 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:11:07,077 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [01:51<12:45,  8.23s/it] 61%|██████    | 143/235 [01:52<08:59,  5.87s/it] 61%|██████▏   | 144/235 [01:52<06:21,  4.19s/it] 62%|██████▏   | 145/235 [01:52<04:32,  3.02s/it] 62%|██████▏   | 146/235 [01:52<03:16,  2.21s/it] 63%|██████▎   | 147/235 [01:53<02:23,  1.63s/it] 63%|██████▎   | 148/235 [01:53<01:47,  1.23s/it] 63%|██████▎   | 149/235 [01:53<01:21,  1.05it/s] 64%|██████▍   | 150/235 [01:54<01:03,  1.33it/s] 64%|██████▍   | 151/235 [01:54<00:51,  1.63it/s] 65%|██████▍   | 152/235 [01:54<00:42,  1.93it/s] 65%|██████▌   | 153/235 [01:55<00:36,  2.23it/s] 66%|██████▌   | 154/235 [01:55<00:33,  2.42it/s] 66%|██████▌   | 155/235 [01:55<00:30,  2.66it/s] 66%|██████▋   | 156/235 [01:55<00:27,  2.85it/s] 67%|██████▋   | 157/235 [01:56<00:25,  3.01it/s] 67%|██████▋   | 158/235 [01:56<00:24,  3.13it/s] 68%|██████▊   | 159/235 [01:56<00:23,  3.21it/s] 68%|██████▊   | 160/235 [01:57<00:22,  3.28it/s] 69%|██████▊   | 161/235 [01:57<00:22,  3.33it/s] 69%|██████▉   | 162/235 [01:57<00:21,  3.36it/s] 69%|██████▉   | 163/235 [01:57<00:21,  3.39it/s] 70%|██████▉   | 164/235 [01:58<00:20,  3.40it/s] 70%|███████   | 165/235 [01:58<00:21,  3.30it/s] 71%|███████   | 166/235 [01:58<00:21,  3.23it/s] 71%|███████   | 167/235 [01:59<00:20,  3.29it/s] 71%|███████▏  | 168/235 [01:59<00:20,  3.34it/s] 72%|███████▏  | 169/235 [01:59<00:19,  3.37it/s] 72%|███████▏  | 170/235 [02:00<00:19,  3.39it/s] 73%|███████▎  | 171/235 [02:00<00:18,  3.40it/s] 73%|███████▎  | 172/235 [02:00<00:18,  3.42it/s] 74%|███████▎  | 173/235 [02:00<00:18,  3.42it/s] 74%|███████▍  | 174/235 [02:01<00:23,  2.54it/s] 74%|███████▍  | 175/235 [02:01<00:22,  2.70it/s] 75%|███████▍  | 176/235 [02:02<00:20,  2.89it/s] 75%|███████▌  | 177/235 [02:02<00:19,  3.04it/s] 76%|███████▌  | 178/235 [02:02<00:18,  3.15it/s] 76%|███████▌  | 179/235 [02:03<00:17,  3.23it/s] 77%|███████▋  | 180/235 [02:03<00:16,  3.29it/s] 77%|███████▋  | 181/235 [02:03<00:16,  3.34it/s] 77%|███████▋  | 182/235 [02:03<00:15,  3.37it/s] 78%|███████▊  | 183/235 [02:04<00:15,  3.39it/s] 78%|███████▊  | 184/235 [02:04<00:14,  3.41it/s] 79%|███████▊  | 185/235 [02:04<00:14,  3.42it/s] 79%|███████▉  | 186/235 [02:05<00:14,  3.33it/s] 80%|███████▉  | 187/235 [02:05<00:14,  3.36it/s] 80%|████████  | 188/235 [02:05<00:13,  3.50it/s][INFO|trainer.py:2140] 2023-08-29 10:11:31,786 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:11:31,786 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:11:31,786 >>   Batch size = 8
{'eval_loss': 1.1775867938995361, 'eval_runtime': 11.4399, 'eval_samples_per_second': 353.064, 'eval_steps_per_second': 44.144, 'epoch': 3.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.59it/s][A
  2%|▏         | 12/505 [00:00<00:10, 49.17it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.32it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.20it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.54it/s][A
  6%|▋         | 32/505 [00:00<00:10, 45.06it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.81it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.64it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.79it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.85it/s][A
 11%|█▏        | 57/505 [00:01<00:09, 44.97it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.99it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.88it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.70it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.47it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.44it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.47it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.49it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.60it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.74it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.86it/s][A
 22%|██▏       | 112/505 [00:02<00:09, 41.68it/s][A
 23%|██▎       | 117/505 [00:02<00:09, 42.67it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 43.18it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 43.49it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.69it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.06it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.32it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.53it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.34it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.41it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.61it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.53it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.46it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.44it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.57it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.67it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.65it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.38it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.53it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.69it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.60it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.54it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.48it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.57it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.67it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.64it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.59it/s][A
 49%|████▉     | 247/505 [00:05<00:06, 42.49it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.27it/s][A
 51%|█████     | 257/505 [00:05<00:05, 43.71it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 43.91it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.07it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.29it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.45it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.43it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.27it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.31it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.47it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.60it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.59it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.66it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.69it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.69it/s][A
 65%|██████▍   | 327/505 [00:07<00:03, 44.57it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.37it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.30it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.48it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.60it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.58it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.68it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.65it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.56it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.54it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.50it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 42.14it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.01it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 43.60it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 43.95it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.17it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.27it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.29it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.39it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.09it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.19it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.41it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.57it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.66it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.58it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.62it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.69it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.44it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.27it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.34it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.41it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.60it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.67it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.59it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.69it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.63it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.63it/s][A 80%|████████  | 188/235 [02:17<00:13,  3.50it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:11:43,568 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 10:11:43,805 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:11:47,575 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:11:47,995 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:11:48,235 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [02:36<07:09,  9.34s/it] 81%|████████  | 190/235 [02:36<04:58,  6.63s/it] 81%|████████▏ | 191/235 [02:36<03:27,  4.73s/it] 82%|████████▏ | 192/235 [02:36<02:26,  3.40s/it] 82%|████████▏ | 193/235 [02:37<01:43,  2.47s/it] 83%|████████▎ | 194/235 [02:37<01:14,  1.82s/it] 83%|████████▎ | 195/235 [02:37<00:54,  1.36s/it] 83%|████████▎ | 196/235 [02:38<00:40,  1.04s/it] 84%|████████▍ | 197/235 [02:38<00:31,  1.23it/s] 84%|████████▍ | 198/235 [02:38<00:24,  1.52it/s] 85%|████████▍ | 199/235 [02:39<00:20,  1.76it/s] 85%|████████▌ | 200/235 [02:39<00:17,  2.05it/s] 86%|████████▌ | 201/235 [02:39<00:14,  2.33it/s] 86%|████████▌ | 202/235 [02:40<00:12,  2.57it/s] 86%|████████▋ | 203/235 [02:40<00:11,  2.77it/s] 87%|████████▋ | 204/235 [02:40<00:10,  2.93it/s] 87%|████████▋ | 205/235 [02:40<00:09,  3.05it/s] 88%|████████▊ | 206/235 [02:41<00:09,  3.15it/s] 88%|████████▊ | 207/235 [02:41<00:08,  3.22it/s] 89%|████████▊ | 208/235 [02:41<00:08,  3.27it/s] 89%|████████▉ | 209/235 [02:42<00:08,  3.12it/s] 89%|████████▉ | 210/235 [02:42<00:07,  3.20it/s] 90%|████████▉ | 211/235 [02:42<00:07,  3.26it/s] 90%|█████████ | 212/235 [02:43<00:06,  3.29it/s] 91%|█████████ | 213/235 [02:43<00:06,  3.32it/s] 91%|█████████ | 214/235 [02:43<00:06,  3.34it/s] 91%|█████████▏| 215/235 [02:43<00:05,  3.36it/s] 92%|█████████▏| 216/235 [02:44<00:05,  3.37it/s] 92%|█████████▏| 217/235 [02:44<00:05,  3.37it/s] 93%|█████████▎| 218/235 [02:44<00:05,  3.38it/s] 93%|█████████▎| 219/235 [02:45<00:04,  3.27it/s] 94%|█████████▎| 220/235 [02:45<00:04,  3.30it/s] 94%|█████████▍| 221/235 [02:45<00:04,  3.33it/s] 94%|█████████▍| 222/235 [02:45<00:03,  3.35it/s] 95%|█████████▍| 223/235 [02:46<00:03,  3.36it/s] 95%|█████████▌| 224/235 [02:46<00:03,  3.37it/s] 96%|█████████▌| 225/235 [02:46<00:02,  3.38it/s] 96%|█████████▌| 226/235 [02:47<00:02,  3.38it/s] 97%|█████████▋| 227/235 [02:47<00:02,  3.38it/s] 97%|█████████▋| 228/235 [02:47<00:02,  3.38it/s] 97%|█████████▋| 229/235 [02:48<00:01,  3.38it/s] 98%|█████████▊| 230/235 [02:48<00:01,  2.99it/s] 98%|█████████▊| 231/235 [02:48<00:01,  3.10it/s] 99%|█████████▊| 232/235 [02:49<00:00,  3.18it/s] 99%|█████████▉| 233/235 [02:49<00:00,  3.24it/s]100%|█████████▉| 234/235 [02:49<00:00,  3.28it/s]100%|██████████| 235/235 [02:49<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 10:12:16,076 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:12:16,076 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:12:16,076 >>   Batch size = 8
{'eval_loss': 1.1889268159866333, 'eval_runtime': 11.3876, 'eval_samples_per_second': 354.683, 'eval_steps_per_second': 44.346, 'epoch': 4.0}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.01it/s][A
  2%|▏         | 12/505 [00:00<00:10, 49.01it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.11it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.23it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.62it/s][A
  6%|▋         | 32/505 [00:00<00:10, 45.09it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.72it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.49it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.48it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.64it/s][A
 11%|█▏        | 57/505 [00:01<00:11, 38.83it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 40.56it/s][A
 13%|█▎        | 67/505 [00:01<00:10, 41.84it/s][A
 14%|█▍        | 72/505 [00:01<00:10, 42.80it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 43.50it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.01it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.27it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.38it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.12it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.82it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.00it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.20it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.57it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.77it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.89it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.88it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.67it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.39it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.09it/s][A
 30%|███       | 152/505 [00:03<00:08, 44.09it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.29it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.46it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.64it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.74it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.86it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.82it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.41it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 41.19it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 42.32it/s][A
 40%|████      | 202/505 [00:04<00:07, 42.97it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.67it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.08it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.30it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.47it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.30it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.08it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.01it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.19it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.40it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.65it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.77it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.84it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.90it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.44it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.24it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 43.93it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.17it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.41it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.65it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.78it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.87it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.72it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.46it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.33it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 42.70it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.35it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.76it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.20it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.47it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.59it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.30it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.27it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.07it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 44.03it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.33it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.53it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.63it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.81it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 41.75it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 42.98it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.20it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.53it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.71it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.06it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.35it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.64it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.52it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 44.40it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.49it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.34it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.24it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.61it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.02it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.30it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.52it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.51it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.48it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.47it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 42.62it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 38.06it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 38.06it/s][A100%|██████████| 235/235 [03:01<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:12:27,803 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-29 10:12:28,005 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:12:30,982 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:12:31,143 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:12:31,220 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:12:37,501 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:12:37,522 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47 (score: 1.1386537551879883).
                                                 100%|██████████| 235/235 [03:18<00:00,  3.43it/s]100%|██████████| 235/235 [03:18<00:00,  1.18it/s]
[INFO|trainer.py:1894] 2023-08-29 10:12:45,188 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 10:12:45,281 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:12:48,117 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:12:48,255 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:12:48,325 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:12:48,836 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,836 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,836 >>   train_loss               =     0.3809
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,836 >>   train_runtime            = 0:03:18.92
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,837 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,837 >>   train_samples_per_second =     75.405
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:12:48,837 >>   train_steps_per_second   =      1.181
{'eval_loss': 1.1943836212158203, 'eval_runtime': 11.6271, 'eval_samples_per_second': 347.378, 'eval_steps_per_second': 43.433, 'epoch': 5.0}
{'train_runtime': 198.9261, 'train_samples_per_second': 75.405, 'train_steps_per_second': 1.181, 'train_loss': 0.3809469994078291, 'epoch': 5.0}
08/29/2023 10:12:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:12:49,018 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:12:49,018 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 10:12:49,018 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:08, 56.66it/s]  2%|▏         | 12/505 [00:00<00:10, 49.17it/s]  3%|▎         | 17/505 [00:00<00:10, 47.55it/s]  4%|▍         | 22/505 [00:00<00:10, 46.69it/s]  5%|▌         | 27/505 [00:00<00:10, 46.10it/s]  6%|▋         | 32/505 [00:00<00:10, 45.79it/s]  7%|▋         | 37/505 [00:00<00:10, 45.53it/s]  8%|▊         | 42/505 [00:00<00:10, 45.07it/s]  9%|▉         | 47/505 [00:01<00:10, 44.50it/s] 10%|█         | 52/505 [00:01<00:10, 44.21it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.25it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.42it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.62it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.83it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.98it/s] 16%|█▌        | 82/505 [00:01<00:09, 44.93it/s] 17%|█▋        | 87/505 [00:01<00:09, 44.63it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.17it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.13it/s] 20%|██        | 102/505 [00:02<00:09, 40.79it/s] 21%|██        | 107/505 [00:02<00:09, 42.01it/s] 22%|██▏       | 112/505 [00:02<00:09, 42.86it/s] 23%|██▎       | 117/505 [00:02<00:08, 43.51it/s] 24%|██▍       | 122/505 [00:02<00:08, 43.96it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.28it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.35it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.29it/s] 28%|██▊       | 142/505 [00:03<00:08, 43.98it/s] 29%|██▉       | 147/505 [00:03<00:08, 43.82it/s] 30%|███       | 152/505 [00:03<00:07, 44.14it/s] 31%|███       | 157/505 [00:03<00:07, 44.31it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.51it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.73it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.83it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.74it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.43it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.23it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.06it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.19it/s] 40%|████      | 202/505 [00:04<00:06, 44.29it/s] 41%|████      | 207/505 [00:04<00:06, 44.49it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.65it/s] 43%|████▎     | 217/505 [00:04<00:06, 44.74it/s] 44%|████▍     | 222/505 [00:04<00:06, 44.72it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.46it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.20it/s] 47%|████▋     | 237/505 [00:05<00:06, 42.25it/s] 48%|████▊     | 242/505 [00:05<00:06, 42.90it/s] 49%|████▉     | 247/505 [00:05<00:05, 43.51it/s] 50%|████▉     | 252/505 [00:05<00:05, 43.93it/s] 51%|█████     | 257/505 [00:05<00:05, 44.23it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.50it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.44it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.35it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.07it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.04it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.22it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.35it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.48it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.65it/s] 61%|██████    | 307/505 [00:06<00:04, 44.73it/s] 62%|██████▏   | 312/505 [00:07<00:04, 44.63it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.38it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.18it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.25it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.28it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.37it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.53it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.60it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.69it/s] 71%|███████   | 357/505 [00:08<00:03, 44.61it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.34it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.18it/s] 74%|███████▎  | 372/505 [00:08<00:03, 43.56it/s] 75%|███████▍  | 377/505 [00:08<00:02, 43.85it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.08it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.28it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.48it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.45it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.42it/s] 81%|████████  | 407/505 [00:09<00:02, 44.28it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.10it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.14it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.40it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.48it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.56it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.61it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.43it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.48it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.31it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.07it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.15it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.34it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.51it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.67it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.57it/s] 96%|█████████▋| 487/505 [00:10<00:00, 44.47it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.36it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.37it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.29it/s]100%|██████████| 505/505 [00:11<00:00, 44.31it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:13:00,433 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   eval_loss               =     1.1387
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   eval_runtime            = 0:00:11.41
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   eval_samples_per_second =    353.831
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   eval_steps_per_second   =      44.24
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:13:00,433 >>   perplexity              =     3.1226
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:10,027 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:10,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:10,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:10,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:10,068 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:13:10,905 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:13:10,906 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:13:11,525 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:13:12,652 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:13:12,653 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:15,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:15,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:15,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:15,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:13:15,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:13:16,438 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:13:16,439 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:13:17,059 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:13:17,300 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:13:17,300 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-235
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-141
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-94
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/checkpoint-47
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.48it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.51it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:24,  1.61it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:26,  1.58it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.45it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:29,  1.52it/s]Extractor Predicting: 46it [00:30,  1.49it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.45it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.58it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:37,  1.45it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.33it/s]Extractor Predicting: 62it [00:40,  1.39it/s]Extractor Predicting: 63it [00:41,  1.41it/s]Extractor Predicting: 64it [00:42,  1.45it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:44,  1.48it/s]Extractor Predicting: 69it [00:45,  1.46it/s]Extractor Predicting: 70it [00:46,  1.50it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.47it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:49,  1.44it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.48it/s]Extractor Predicting: 82it [00:54,  1.53it/s]Extractor Predicting: 83it [00:54,  1.49it/s]Extractor Predicting: 84it [00:55,  1.51it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:58,  1.55it/s]Extractor Predicting: 89it [00:58,  1.52it/s]Extractor Predicting: 90it [00:59,  1.54it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.56it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:03,  1.50it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:04,  1.48it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.47it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:09,  1.51it/s]Extractor Predicting: 106it [01:10,  1.54it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:12,  1.56it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.41it/s]Extractor Predicting: 113it [01:14,  1.43it/s]Extractor Predicting: 114it [01:15,  1.42it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:16,  1.49it/s]Extractor Predicting: 117it [01:17,  1.50it/s]Extractor Predicting: 118it [01:18,  1.46it/s]Extractor Predicting: 119it [01:18,  1.49it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:20,  1.50it/s]Extractor Predicting: 123it [01:21,  1.53it/s]Extractor Predicting: 124it [01:22,  1.51it/s]Extractor Predicting: 125it [01:23,  1.40it/s]Extractor Predicting: 126it [01:23,  1.45it/s]Extractor Predicting: 127it [01:24,  1.42it/s]Extractor Predicting: 128it [01:25,  1.41it/s]Extractor Predicting: 129it [01:25,  1.41it/s]Extractor Predicting: 130it [01:26,  1.41it/s]Extractor Predicting: 131it [01:27,  1.39it/s]Extractor Predicting: 132it [01:28,  1.39it/s]Extractor Predicting: 133it [01:28,  1.40it/s]Extractor Predicting: 134it [01:29,  1.44it/s]Extractor Predicting: 135it [01:30,  1.45it/s]Extractor Predicting: 136it [01:30,  1.45it/s]Extractor Predicting: 137it [01:31,  1.48it/s]Extractor Predicting: 138it [01:32,  1.43it/s]Extractor Predicting: 139it [01:32,  1.42it/s]Extractor Predicting: 140it [01:33,  1.42it/s]Extractor Predicting: 141it [01:34,  1.43it/s]Extractor Predicting: 142it [01:34,  1.43it/s]Extractor Predicting: 143it [01:35,  1.43it/s]Extractor Predicting: 144it [01:36,  1.42it/s]Extractor Predicting: 145it [01:37,  1.41it/s]Extractor Predicting: 146it [01:37,  1.43it/s]Extractor Predicting: 147it [01:38,  1.44it/s]Extractor Predicting: 148it [01:39,  1.40it/s]Extractor Predicting: 149it [01:39,  1.38it/s]Extractor Predicting: 149it [01:39,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:08,915 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:08,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:08,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:08,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:08,936 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:15:09,573 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:15:09,574 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:15:10,044 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:15:11,236 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:15:11,236 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:13,216 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:13,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:13,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:13,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:15:13,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:15:13,838 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:15:13,839 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:15:14,180 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:15:14,430 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:15:14,430 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19121647881849982,
  "recall": 0.12181232978460015,
  "score": 0.14882032667876588,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.77it/s]Extractor Predicting: 14it [00:08,  1.76it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:09,  1.78it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.71it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:13,  1.76it/s]Extractor Predicting: 24it [00:14,  1.72it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:16,  1.78it/s]Extractor Predicting: 29it [00:16,  1.77it/s]Extractor Predicting: 30it [00:17,  1.75it/s]Extractor Predicting: 31it [00:18,  1.78it/s]Extractor Predicting: 32it [00:18,  1.77it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:19,  1.70it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:21,  1.72it/s]Extractor Predicting: 37it [00:21,  1.69it/s]Extractor Predicting: 38it [00:22,  1.68it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:23,  1.67it/s]Extractor Predicting: 41it [00:24,  1.65it/s]Extractor Predicting: 42it [00:24,  1.67it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:25,  1.70it/s]Extractor Predicting: 45it [00:26,  1.56it/s]Extractor Predicting: 46it [00:27,  1.54it/s]Extractor Predicting: 47it [00:27,  1.57it/s]Extractor Predicting: 48it [00:28,  1.48it/s]Extractor Predicting: 49it [00:29,  1.54it/s]Extractor Predicting: 50it [00:29,  1.56it/s]Extractor Predicting: 51it [00:30,  1.62it/s]Extractor Predicting: 52it [00:30,  1.60it/s]Extractor Predicting: 53it [00:31,  1.62it/s]Extractor Predicting: 54it [00:32,  1.64it/s]Extractor Predicting: 55it [00:32,  1.60it/s]Extractor Predicting: 56it [00:33,  1.56it/s]Extractor Predicting: 57it [00:34,  1.58it/s]Extractor Predicting: 58it [00:34,  1.61it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:36,  1.61it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.65it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:39,  1.67it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:40,  1.65it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:42,  1.55it/s]Extractor Predicting: 71it [00:42,  1.56it/s]Extractor Predicting: 72it [00:43,  1.59it/s]Extractor Predicting: 73it [00:44,  1.59it/s]Extractor Predicting: 74it [00:44,  1.61it/s]Extractor Predicting: 75it [00:45,  1.61it/s]Extractor Predicting: 76it [00:45,  1.60it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:47,  1.57it/s]Extractor Predicting: 79it [00:47,  1.60it/s]Extractor Predicting: 80it [00:48,  1.63it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:49,  1.58it/s]Extractor Predicting: 83it [00:50,  1.58it/s]Extractor Predicting: 84it [00:50,  1.61it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:52,  1.60it/s]Extractor Predicting: 88it [00:53,  1.62it/s]Extractor Predicting: 89it [00:53,  1.67it/s]Extractor Predicting: 90it [00:54,  1.64it/s]Extractor Predicting: 91it [00:55,  1.64it/s]Extractor Predicting: 92it [00:55,  1.64it/s]Extractor Predicting: 93it [00:56,  1.68it/s]Extractor Predicting: 94it [00:56,  1.69it/s]Extractor Predicting: 95it [00:57,  1.67it/s]Extractor Predicting: 96it [00:58,  1.71it/s]Extractor Predicting: 97it [00:58,  1.73it/s]Extractor Predicting: 98it [00:59,  1.69it/s]Extractor Predicting: 99it [00:59,  1.68it/s]Extractor Predicting: 100it [01:00,  1.70it/s]Extractor Predicting: 101it [01:01,  1.69it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:02,  1.64it/s]Extractor Predicting: 104it [01:02,  1.68it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:04,  1.67it/s]Extractor Predicting: 108it [01:05,  1.59it/s]Extractor Predicting: 109it [01:06,  1.59it/s]Extractor Predicting: 110it [01:06,  1.62it/s]Extractor Predicting: 111it [01:07,  1.63it/s]Extractor Predicting: 112it [01:07,  1.63it/s]Extractor Predicting: 113it [01:08,  1.64it/s]Extractor Predicting: 114it [01:09,  1.63it/s]Extractor Predicting: 115it [01:09,  1.65it/s]Extractor Predicting: 116it [01:10,  1.68it/s]Extractor Predicting: 117it [01:10,  1.69it/s]Extractor Predicting: 118it [01:11,  1.74it/s]Extractor Predicting: 119it [01:11,  1.70it/s]Extractor Predicting: 120it [01:12,  1.71it/s]Extractor Predicting: 121it [01:13,  1.71it/s]Extractor Predicting: 122it [01:13,  1.71it/s]Extractor Predicting: 123it [01:14,  1.69it/s]Extractor Predicting: 124it [01:14,  1.71it/s]Extractor Predicting: 125it [01:15,  1.69it/s]Extractor Predicting: 126it [01:16,  1.67it/s]Extractor Predicting: 127it [01:16,  1.64it/s]Extractor Predicting: 128it [01:17,  1.62it/s]Extractor Predicting: 129it [01:18,  1.60it/s]Extractor Predicting: 130it [01:18,  1.56it/s]Extractor Predicting: 131it [01:19,  1.41it/s]Extractor Predicting: 132it [01:20,  1.46it/s]Extractor Predicting: 133it [01:20,  1.50it/s]Extractor Predicting: 134it [01:21,  1.56it/s]Extractor Predicting: 135it [01:22,  1.51it/s]Extractor Predicting: 136it [01:22,  1.51it/s]Extractor Predicting: 137it [01:23,  1.52it/s]Extractor Predicting: 138it [01:24,  1.54it/s]Extractor Predicting: 139it [01:24,  1.54it/s]Extractor Predicting: 140it [01:25,  1.51it/s]Extractor Predicting: 141it [01:26,  1.52it/s]Extractor Predicting: 142it [01:26,  1.53it/s]Extractor Predicting: 143it [01:27,  1.56it/s]Extractor Predicting: 144it [01:27,  1.54it/s]Extractor Predicting: 145it [01:28,  1.53it/s]Extractor Predicting: 146it [01:29,  1.53it/s]Extractor Predicting: 147it [01:29,  1.55it/s]Extractor Predicting: 148it [01:30,  1.56it/s]Extractor Predicting: 149it [01:31,  1.59it/s]Extractor Predicting: 150it [01:31,  1.57it/s]Extractor Predicting: 151it [01:32,  1.57it/s]Extractor Predicting: 152it [01:33,  1.57it/s]Extractor Predicting: 153it [01:33,  1.55it/s]Extractor Predicting: 154it [01:34,  1.54it/s]Extractor Predicting: 155it [01:35,  1.56it/s]Extractor Predicting: 156it [01:35,  1.61it/s]Extractor Predicting: 157it [01:36,  1.66it/s]Extractor Predicting: 158it [01:36,  1.66it/s]Extractor Predicting: 159it [01:37,  1.67it/s]Extractor Predicting: 160it [01:37,  1.68it/s]Extractor Predicting: 161it [01:38,  1.72it/s]Extractor Predicting: 162it [01:39,  1.68it/s]Extractor Predicting: 163it [01:39,  1.62it/s]Extractor Predicting: 164it [01:40,  1.65it/s]Extractor Predicting: 165it [01:40,  1.69it/s]Extractor Predicting: 166it [01:41,  1.70it/s]Extractor Predicting: 167it [01:42,  1.76it/s]Extractor Predicting: 168it [01:42,  1.78it/s]Extractor Predicting: 169it [01:43,  1.77it/s]Extractor Predicting: 170it [01:43,  1.73it/s]Extractor Predicting: 171it [01:44,  1.66it/s]Extractor Predicting: 172it [01:45,  1.63it/s]Extractor Predicting: 173it [01:45,  1.67it/s]Extractor Predicting: 173it [01:45,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:11,859 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:11,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:11,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:11,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:11,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:17:12,506 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:17:12,507 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:17:13,090 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:17:14,159 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:17:14,159 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:17,144 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:17,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:17,176 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:17,177 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:17:17,177 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:17:17,854 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:17:17,855 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:17:18,455 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:17:18,619 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:17:18,619 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20128824476650564,
  "recall": 0.120598166907863,
  "score": 0.15082956259426847,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.69it/s]Extractor Predicting: 17it [00:10,  1.79it/s]Extractor Predicting: 18it [00:11,  1.83it/s]Extractor Predicting: 19it [00:11,  1.85it/s]Extractor Predicting: 20it [00:12,  1.88it/s]Extractor Predicting: 21it [00:12,  1.92it/s]Extractor Predicting: 22it [00:13,  1.86it/s]Extractor Predicting: 23it [00:14,  1.87it/s]Extractor Predicting: 24it [00:14,  1.86it/s]Extractor Predicting: 25it [00:15,  1.86it/s]Extractor Predicting: 26it [00:15,  1.90it/s]Extractor Predicting: 27it [00:16,  1.86it/s]Extractor Predicting: 28it [00:16,  1.86it/s]Extractor Predicting: 29it [00:17,  1.87it/s]Extractor Predicting: 30it [00:17,  1.91it/s]Extractor Predicting: 31it [00:18,  1.93it/s]Extractor Predicting: 32it [00:18,  1.92it/s]Extractor Predicting: 33it [00:19,  1.93it/s]Extractor Predicting: 34it [00:19,  1.90it/s]Extractor Predicting: 35it [00:20,  1.91it/s]Extractor Predicting: 36it [00:20,  1.86it/s]Extractor Predicting: 37it [00:21,  1.83it/s]Extractor Predicting: 38it [00:21,  1.89it/s]Extractor Predicting: 39it [00:22,  1.89it/s]Extractor Predicting: 40it [00:23,  1.86it/s]Extractor Predicting: 41it [00:23,  1.87it/s]Extractor Predicting: 42it [00:24,  1.91it/s]Extractor Predicting: 43it [00:24,  1.87it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:26,  1.56it/s]Extractor Predicting: 47it [00:27,  1.54it/s]Extractor Predicting: 48it [00:28,  1.52it/s]Extractor Predicting: 49it [00:28,  1.52it/s]Extractor Predicting: 50it [00:29,  1.50it/s]Extractor Predicting: 51it [00:30,  1.48it/s]Extractor Predicting: 52it [00:30,  1.47it/s]Extractor Predicting: 53it [00:31,  1.46it/s]Extractor Predicting: 54it [00:32,  1.45it/s]Extractor Predicting: 55it [00:32,  1.48it/s]Extractor Predicting: 56it [00:33,  1.46it/s]Extractor Predicting: 57it [00:34,  1.44it/s]Extractor Predicting: 58it [00:34,  1.46it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 59it [00:35,  1.67it/s]
[INFO|configuration_utils.py:515] 2023-08-29 10:17:56,326 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:17:56,327 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:17:56,418 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:17:56,419 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 10:17:56,472 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:18:13,169 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 10:18:13,199 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 10:18:13,396 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:18:13,397 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:18:13,463 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:18:13,512 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6709621993127147,
  "recall": 0.25250565793727775,
  "score": 0.3669250645994832,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 10:18:13,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:14,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:15,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:15,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:16,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:16,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:17,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:17,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:18,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:19,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:19,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:20,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:21,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:21,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:22,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:22,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:23,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:23,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:24,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:11<01:41, 11.22s/it][WARNING|generation_utils.py:914] 2023-08-29 10:18:25,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:25,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:26,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:26,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:27,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:28,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:28,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:29,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:29,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:30,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:31,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:31,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:32,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:32,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:33,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:34,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:34,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:35,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:35,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:36,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:36,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:37,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:24<01:37, 12.16s/it][WARNING|generation_utils.py:914] 2023-08-29 10:18:37,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:38,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:39,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:39,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:40,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:41,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:41,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:42,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:43,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:43,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:44,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:44,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:45,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:45,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:46,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:46,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:47,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:48,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:48,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:49,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:49,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:50,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:50,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:37<01:28, 12.70s/it][WARNING|generation_utils.py:914] 2023-08-29 10:18:51,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:51,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:52,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:53,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:53,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:54,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:54,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:55,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:56,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:56,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:57,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:58,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:18:59,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:00,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:00,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:01,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:02,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:02,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:03,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:03,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:50<01:17, 12.92s/it][WARNING|generation_utils.py:914] 2023-08-29 10:19:04,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:05,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:05,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:06,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:06,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:07,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:07,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:08,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:09,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:09,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:10,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:10,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:11,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:12,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:12,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:13,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:13,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:14,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:14,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:15,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:15,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:16,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:02<01:03, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-29 10:19:16,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:17,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:18,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:18,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:19,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:19,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:20,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:21,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:21,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:22,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:22,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:24,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:24,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:25,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:26,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:26,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:27,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:27,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:28,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:29,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:16<00:51, 12.83s/it][WARNING|generation_utils.py:914] 2023-08-29 10:19:29,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:30,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:30,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:31,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:31,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:32,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:33,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:33,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:34,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:34,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:35,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:36,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:36,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:37,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:37,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:38,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:38,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:39,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:39,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:40,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:41,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:27<00:37, 12.54s/it][WARNING|generation_utils.py:914] 2023-08-29 10:19:41,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:42,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:42,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:43,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:44,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:44,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:45,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:45,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:46,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:46,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:47,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:47,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:48,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:48,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:49,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:49,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:50,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:50,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:51,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:51,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:38<00:23, 11.84s/it][WARNING|generation_utils.py:914] 2023-08-29 10:19:52,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:52,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:53,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:53,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:54,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:54,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:55,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:55,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:56,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:56,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:57,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:57,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:58,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:58,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:19:59,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:00,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:00,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:01,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:02,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:48<00:11, 11.43s/it][WARNING|generation_utils.py:914] 2023-08-29 10:20:02,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:03,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:03,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:04,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:04,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:05,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:06,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:06,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:07,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:07,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:08,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:09,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:09,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:10,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:10,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:11,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:12,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:12,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:13,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:13,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 10:20:14,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:01<00:00, 11.71s/it]Generating: 100%|██████████| 10/10 [02:01<00:00, 12.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:21,876 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:21,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:21,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:21,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:21,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:20:22,546 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:20:22,547 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:20:23,159 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:20:24,252 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:20:24,252 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:27,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:27,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:27,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:27,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:20:27,554 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:20:28,228 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:20:28,230 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:20:28,863 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:20:29,041 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:20:29,041 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 285, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 568, 'raw': 576}
{'target': 600, 'success': 600, 'raw': 608}
{'prompt': 'Relation : given name .', 'success_rate': 0.9868421052631579, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 615, 'raw': 768}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.80078125, 'errors': {'', "('Major League Baseball', 'lowest point', '', 'This was the third time that a Major League Baseball team won it all and had won three straight at home on April 29 , 2005 .')", 'not enough values to unpack (expected 2, got 1)', "('North American League', 'lowest point', '', 'It moved to the top of the North American League .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Roman consort', 'mother', '', 'She was one of the young women of Rome who married a Roman consort ( d.')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8934659090909091, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : genre .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.9211309523809523, 'errors': {'', "('A list of', 'is a list of', '', 'A list of .')", 'not enough values to unpack (expected 2, got 1)', "('alphabetical lists', 'is a list of', '', 'It comprises the largest and most prominent of the listed alphabetical lists , which are of various genres .')", "('list', 'is a list of', '', 'It is the largest list of .')", "('countries', 'is a list of', '', 'It consists of all of the countries of the world with the most common population .')", "('list', 'is a list of', '', 'It is the successor to the list of .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.959375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 7166
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7266, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.54it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.63it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:07,  1.55it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:12,  1.61it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:13,  1.58it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.58it/s]Extractor Estimating: 26it [00:16,  1.66it/s]Extractor Estimating: 27it [00:16,  1.69it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.64it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:19,  1.63it/s]Extractor Estimating: 32it [00:19,  1.68it/s]Extractor Estimating: 33it [00:20,  1.78it/s]Extractor Estimating: 34it [00:21,  1.80it/s]Extractor Estimating: 35it [00:22,  1.32it/s]Extractor Estimating: 36it [00:22,  1.44it/s]Extractor Estimating: 37it [00:23,  1.60it/s]Extractor Estimating: 38it [00:23,  1.67it/s]Extractor Estimating: 39it [00:24,  1.67it/s]Extractor Estimating: 40it [00:24,  1.70it/s]Extractor Estimating: 41it [00:25,  1.71it/s]Extractor Estimating: 42it [00:26,  1.73it/s]Extractor Estimating: 43it [00:26,  1.79it/s]Extractor Estimating: 44it [00:27,  1.87it/s]Extractor Estimating: 45it [00:27,  1.83it/s]Extractor Estimating: 46it [00:28,  1.83it/s]Extractor Estimating: 47it [00:28,  1.81it/s]Extractor Estimating: 48it [00:29,  1.82it/s]Extractor Estimating: 49it [00:29,  1.86it/s]Extractor Estimating: 50it [00:30,  1.87it/s]Extractor Estimating: 51it [00:30,  1.85it/s]Extractor Estimating: 52it [00:31,  1.76it/s]Extractor Estimating: 53it [00:32,  1.74it/s]Extractor Estimating: 54it [00:32,  1.62it/s]Extractor Estimating: 55it [00:33,  1.66it/s]Extractor Estimating: 56it [00:33,  1.68it/s]Extractor Estimating: 57it [00:34,  1.63it/s]Extractor Estimating: 58it [00:35,  1.66it/s]Extractor Estimating: 59it [00:35,  1.68it/s]Extractor Estimating: 60it [00:36,  1.69it/s]Extractor Estimating: 61it [00:36,  1.71it/s]Extractor Estimating: 62it [00:37,  1.76it/s]Extractor Estimating: 63it [00:38,  1.73it/s]Extractor Estimating: 64it [00:38,  1.55it/s]Extractor Estimating: 65it [00:39,  1.55it/s]Extractor Estimating: 66it [00:40,  1.59it/s]Extractor Estimating: 67it [00:40,  1.59it/s]Extractor Estimating: 68it [00:41,  1.59it/s]Extractor Estimating: 69it [00:41,  1.61it/s]Extractor Estimating: 70it [00:42,  1.63it/s]Extractor Estimating: 71it [00:43,  1.56it/s]Extractor Estimating: 72it [00:43,  1.64it/s]Extractor Estimating: 73it [00:44,  1.62it/s]Extractor Estimating: 74it [00:44,  1.68it/s]Extractor Estimating: 75it [00:45,  1.67it/s]Extractor Estimating: 76it [00:46,  1.66it/s]Extractor Estimating: 77it [00:46,  1.66it/s]Extractor Estimating: 78it [00:47,  1.65it/s]Extractor Estimating: 79it [00:48,  1.59it/s]Extractor Estimating: 80it [00:48,  1.63it/s]Extractor Estimating: 81it [00:49,  1.66it/s]Extractor Estimating: 82it [00:49,  1.69it/s]Extractor Estimating: 83it [00:50,  1.69it/s]Extractor Estimating: 84it [00:50,  1.72it/s]Extractor Estimating: 85it [00:51,  1.71it/s]Extractor Estimating: 86it [00:52,  1.69it/s]Extractor Estimating: 87it [00:52,  1.66it/s]Extractor Estimating: 88it [00:53,  1.63it/s]Extractor Estimating: 89it [00:53,  1.70it/s]Extractor Estimating: 90it [00:54,  1.72it/s]Extractor Estimating: 91it [00:55,  1.67it/s]Extractor Estimating: 92it [00:55,  1.68it/s]Extractor Estimating: 93it [00:56,  1.68it/s]Extractor Estimating: 94it [00:56,  1.73it/s]Extractor Estimating: 95it [00:57,  1.72it/s]Extractor Estimating: 96it [00:58,  1.69it/s]Extractor Estimating: 97it [00:58,  1.71it/s]Extractor Estimating: 98it [00:59,  1.67it/s]Extractor Estimating: 99it [00:59,  1.67it/s]Extractor Estimating: 100it [01:00,  1.70it/s]Extractor Estimating: 101it [01:00,  1.79it/s]Extractor Estimating: 102it [01:01,  1.80it/s]Extractor Estimating: 103it [01:02,  1.85it/s]Extractor Estimating: 104it [01:02,  1.94it/s]Extractor Estimating: 105it [01:02,  1.96it/s]Extractor Estimating: 106it [01:03,  1.94it/s]Extractor Estimating: 107it [01:03,  1.95it/s]Extractor Estimating: 108it [01:04,  1.98it/s]Extractor Estimating: 109it [01:05,  1.95it/s]Extractor Estimating: 110it [01:05,  1.90it/s]Extractor Estimating: 111it [01:06,  1.91it/s]Extractor Estimating: 112it [01:06,  1.91it/s]Extractor Estimating: 113it [01:07,  1.94it/s]Extractor Estimating: 114it [01:07,  1.91it/s]Extractor Estimating: 115it [01:08,  1.91it/s]Extractor Estimating: 116it [01:08,  1.88it/s]Extractor Estimating: 117it [01:09,  1.87it/s]Extractor Estimating: 118it [01:09,  1.85it/s]Extractor Estimating: 119it [01:10,  1.79it/s]Extractor Estimating: 120it [01:10,  1.81it/s]Extractor Estimating: 121it [01:11,  1.82it/s]Extractor Estimating: 122it [01:11,  1.89it/s]Extractor Estimating: 123it [01:12,  1.95it/s]Extractor Estimating: 124it [01:13,  1.91it/s]Extractor Estimating: 125it [01:13,  1.89it/s]Extractor Estimating: 126it [01:14,  1.84it/s]Extractor Estimating: 127it [01:14,  1.75it/s]Extractor Estimating: 128it [01:15,  1.70it/s]Extractor Estimating: 129it [01:15,  1.76it/s]Extractor Estimating: 130it [01:16,  1.59it/s]Extractor Estimating: 131it [01:17,  1.60it/s]Extractor Estimating: 132it [01:17,  1.59it/s]Extractor Estimating: 133it [01:18,  1.51it/s]Extractor Estimating: 134it [01:19,  1.55it/s]Extractor Estimating: 135it [01:19,  1.55it/s]Extractor Estimating: 136it [01:20,  1.61it/s]Extractor Estimating: 137it [01:21,  1.58it/s]Extractor Estimating: 138it [01:21,  1.57it/s]Extractor Estimating: 139it [01:22,  1.54it/s]Extractor Estimating: 140it [01:23,  1.56it/s]Extractor Estimating: 141it [01:23,  1.49it/s]Extractor Estimating: 142it [01:24,  1.53it/s]Extractor Estimating: 143it [01:25,  1.28it/s]Extractor Estimating: 144it [01:26,  1.36it/s]Extractor Estimating: 145it [01:26,  1.46it/s]Extractor Estimating: 146it [01:27,  1.47it/s]Extractor Estimating: 147it [01:28,  1.53it/s]Extractor Estimating: 148it [01:28,  1.51it/s]Extractor Estimating: 149it [01:29,  1.51it/s]Extractor Estimating: 150it [01:29,  1.57it/s]Extractor Estimating: 151it [01:30,  1.69it/s]Extractor Estimating: 152it [01:30,  1.76it/s]Extractor Estimating: 153it [01:31,  1.82it/s]Extractor Estimating: 154it [01:32,  1.77it/s]Extractor Estimating: 155it [01:32,  1.87it/s]Extractor Estimating: 156it [01:33,  1.86it/s]Extractor Estimating: 157it [01:33,  1.86it/s]Extractor Estimating: 158it [01:34,  1.82it/s]Extractor Estimating: 159it [01:34,  1.85it/s]Extractor Estimating: 160it [01:35,  1.88it/s]Extractor Estimating: 161it [01:35,  1.88it/s]Extractor Estimating: 162it [01:36,  1.91it/s]Extractor Estimating: 163it [01:36,  1.88it/s]Extractor Estimating: 164it [01:37,  1.88it/s]Extractor Estimating: 165it [01:37,  1.89it/s]Extractor Estimating: 166it [01:38,  1.84it/s]Extractor Estimating: 167it [01:38,  1.96it/s]Extractor Estimating: 168it [01:39,  2.04it/s]Extractor Estimating: 169it [01:39,  2.08it/s]Extractor Estimating: 170it [01:40,  2.06it/s]Extractor Estimating: 171it [01:40,  2.11it/s]Extractor Estimating: 172it [01:41,  2.00it/s]Extractor Estimating: 173it [01:41,  1.89it/s]Extractor Estimating: 174it [01:42,  1.95it/s]Extractor Estimating: 175it [01:42,  1.96it/s]Extractor Estimating: 176it [01:43,  1.95it/s]Extractor Estimating: 177it [01:43,  1.97it/s]Extractor Estimating: 178it [01:44,  1.97it/s]Extractor Estimating: 179it [01:44,  1.97it/s]Extractor Estimating: 180it [01:45,  1.86it/s]Extractor Estimating: 181it [01:45,  1.87it/s]Extractor Estimating: 182it [01:46,  1.92it/s]Extractor Estimating: 183it [01:46,  1.92it/s]Extractor Estimating: 184it [01:47,  1.88it/s]Extractor Estimating: 185it [01:48,  1.95it/s]Extractor Estimating: 186it [01:48,  1.95it/s]Extractor Estimating: 187it [01:49,  1.94it/s]Extractor Estimating: 188it [01:49,  1.89it/s]Extractor Estimating: 189it [01:50,  1.93it/s]Extractor Estimating: 190it [01:50,  1.96it/s]Extractor Estimating: 191it [01:51,  2.00it/s]Extractor Estimating: 192it [01:51,  2.01it/s]Extractor Estimating: 193it [01:52,  2.02it/s]Extractor Estimating: 194it [01:52,  1.91it/s]Extractor Estimating: 195it [01:53,  1.88it/s]Extractor Estimating: 196it [01:53,  1.89it/s]Extractor Estimating: 197it [01:54,  1.82it/s]Extractor Estimating: 198it [01:54,  1.86it/s]Extractor Estimating: 199it [01:55,  1.91it/s]Extractor Estimating: 200it [01:55,  1.89it/s]Extractor Estimating: 201it [01:56,  1.92it/s]Extractor Estimating: 202it [01:56,  1.91it/s]Extractor Estimating: 203it [01:57,  1.86it/s]Extractor Estimating: 204it [01:57,  1.85it/s]Extractor Estimating: 205it [01:58,  1.86it/s]Extractor Estimating: 206it [01:59,  1.93it/s]Extractor Estimating: 207it [01:59,  1.91it/s]Extractor Estimating: 208it [02:00,  1.91it/s]Extractor Estimating: 209it [02:00,  1.96it/s]Extractor Estimating: 210it [02:01,  1.99it/s]Extractor Estimating: 211it [02:01,  1.92it/s]Extractor Estimating: 212it [02:02,  1.91it/s]Extractor Estimating: 213it [02:02,  1.94it/s]Extractor Estimating: 214it [02:03,  1.91it/s]Extractor Estimating: 215it [02:03,  1.88it/s]Extractor Estimating: 216it [02:04,  1.92it/s]Extractor Estimating: 217it [02:04,  1.91it/s]Extractor Estimating: 218it [02:05,  1.98it/s]Extractor Estimating: 219it [02:05,  1.90it/s]Extractor Estimating: 220it [02:06,  1.85it/s]Extractor Estimating: 221it [02:06,  1.83it/s]Extractor Estimating: 222it [02:07,  1.77it/s]Extractor Estimating: 223it [02:08,  1.79it/s]Extractor Estimating: 224it [02:08,  1.79it/s]Extractor Estimating: 225it [02:09,  1.76it/s]Extractor Estimating: 226it [02:09,  1.66it/s]Extractor Estimating: 227it [02:10,  1.68it/s]Extractor Estimating: 228it [02:11,  1.65it/s]Extractor Estimating: 229it [02:11,  1.68it/s]Extractor Estimating: 230it [02:12,  1.70it/s]Extractor Estimating: 231it [02:12,  1.68it/s]Extractor Estimating: 232it [02:13,  1.63it/s]Extractor Estimating: 233it [02:14,  1.67it/s]Extractor Estimating: 234it [02:14,  1.67it/s]Extractor Estimating: 235it [02:15,  1.67it/s]Extractor Estimating: 236it [02:15,  1.71it/s]Extractor Estimating: 237it [02:16,  1.72it/s]Extractor Estimating: 238it [02:17,  1.53it/s]Extractor Estimating: 239it [02:17,  1.61it/s]Extractor Estimating: 240it [02:18,  1.62it/s]Extractor Estimating: 241it [02:18,  1.64it/s]Extractor Estimating: 242it [02:19,  1.66it/s]Extractor Estimating: 243it [02:20,  1.62it/s]Extractor Estimating: 244it [02:20,  1.83it/s]Extractor Estimating: 244it [02:20,  1.74it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:10,937 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:10,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:10,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:10,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:10,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:23:11,736 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:23:11,737 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:23:12,080 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:23:13,246 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:23:13,246 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:15,545 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:15,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:15,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:15,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:23:15,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:23:16,610 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:23:16,611 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:23:17,309 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:23:17,619 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:23:17,619 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:27:22,523 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:27:22,778 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 3843 mean pseudo reward: 0.9828823750440402
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 16177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16277, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.988, loss:218.7014
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 39, avg_time 0.975, loss:215.3369
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 139, avg_time 0.955, loss:202.4499
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 78, avg_time 0.957, loss:189.5903
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 17, avg_time 0.961, loss:192.3955
>> valid entity prec:0.4874, rec:0.4745, f1:0.4809
>> valid relation prec:0.1909, rec:0.0982, f1:0.1297
>> valid relation with NER prec:0.1909, rec:0.0982, f1:0.1297
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 117, avg_time 2.340, loss:179.5423
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 56, avg_time 0.949, loss:176.0336
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 156, avg_time 0.959, loss:184.7495
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 95, avg_time 0.945, loss:174.4752
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 34, avg_time 0.968, loss:195.0315
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4873, rec:0.4373, f1:0.4610
>> valid relation prec:0.1835, rec:0.0724, f1:0.1038
>> valid relation with NER prec:0.1835, rec:0.0724, f1:0.1038
g_step 1100, step 134, avg_time 2.298, loss:175.5748
g_step 1200, step 73, avg_time 0.945, loss:177.7822
g_step 1300, step 12, avg_time 0.946, loss:167.9303
g_step 1400, step 112, avg_time 0.944, loss:162.1651
g_step 1500, step 51, avg_time 1.008, loss:160.9096
>> valid entity prec:0.4996, rec:0.4391, f1:0.4674
>> valid relation prec:0.1797, rec:0.1004, f1:0.1288
>> valid relation with NER prec:0.1797, rec:0.1004, f1:0.1288
g_step 1600, step 151, avg_time 2.268, loss:149.3718
g_step 1700, step 90, avg_time 0.951, loss:147.5709
g_step 1800, step 29, avg_time 0.930, loss:147.3230
g_step 1900, step 129, avg_time 0.952, loss:149.9347
g_step 2000, step 68, avg_time 0.997, loss:135.6022
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5168, rec:0.4313, f1:0.4702
>> valid relation prec:0.1839, rec:0.0848, f1:0.1161
>> valid relation with NER prec:0.1839, rec:0.0848, f1:0.1161
g_step 2100, step 7, avg_time 2.231, loss:136.5165
g_step 2200, step 107, avg_time 0.911, loss:132.0229
g_step 2300, step 46, avg_time 0.935, loss:131.4565
g_step 2400, step 146, avg_time 0.919, loss:126.6170
g_step 2500, step 85, avg_time 0.911, loss:117.1526
>> valid entity prec:0.5050, rec:0.4157, f1:0.4560
>> valid relation prec:0.1582, rec:0.0838, f1:0.1096
>> valid relation with NER prec:0.1582, rec:0.0838, f1:0.1096
g_step 2600, step 24, avg_time 2.233, loss:125.5407
g_step 2700, step 124, avg_time 0.931, loss:113.4389
g_step 2800, step 63, avg_time 0.925, loss:110.1206
g_step 2900, step 2, avg_time 0.917, loss:116.6053
g_step 3000, step 102, avg_time 0.930, loss:103.4955
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4689, rec:0.4309, f1:0.4491
>> valid relation prec:0.1433, rec:0.0825, f1:0.1047
>> valid relation with NER prec:0.1433, rec:0.0825, f1:0.1047
g_step 3100, step 41, avg_time 2.218, loss:103.1985
g_step 3200, step 141, avg_time 0.918, loss:104.6383
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:27:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:27:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-27-22_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:27:24 - WARNING - datasets.builder -   Using custom data configuration default-e5139e20e9dee335
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e5139e20e9dee335/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  3.85 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:27:26,983 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:27:26,985 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:27:26,985 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:27:26,986 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:27:27,090 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:27:27,144 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:27:27,732 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:27:31,012 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:27:31,067 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e5139e20e9dee335/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.59ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.48ba/s]100%|██████████| 4/4 [00:01<00:00,  4.03ba/s]100%|██████████| 4/4 [00:01<00:00,  3.73ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.11ba/s] 40%|████      | 2/5 [00:00<00:00,  3.85ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.34ba/s]100%|██████████| 5/5 [00:00<00:00,  5.08ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.05ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  6.49ba/s]100%|██████████| 4/4 [00:00<00:00,  6.58ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.81ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  5.22ba/s]100%|██████████| 5/5 [00:00<00:00,  6.27ba/s]
[INFO|trainer.py:414] 2023-08-29 11:27:37,324 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:27:37,578 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:27:37,578 >>   Num examples = 4000
[INFO|trainer.py:1149] 2023-08-29 11:27:37,578 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:27:37,578 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:27:37,578 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:27:37,578 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:27:37,578 >>   Total optimization steps = 310
  0%|          | 0/310 [00:00<?, ?it/s]  0%|          | 1/310 [00:00<01:32,  3.33it/s]  1%|          | 2/310 [00:00<01:31,  3.38it/s]  1%|          | 3/310 [00:00<01:30,  3.39it/s]  1%|▏         | 4/310 [00:01<01:30,  3.39it/s]  2%|▏         | 5/310 [00:01<01:29,  3.40it/s]  2%|▏         | 6/310 [00:01<01:29,  3.40it/s]  2%|▏         | 7/310 [00:02<01:29,  3.40it/s]  3%|▎         | 8/310 [00:02<01:28,  3.40it/s]  3%|▎         | 9/310 [00:02<01:28,  3.40it/s]  3%|▎         | 10/310 [00:02<01:28,  3.40it/s]  4%|▎         | 11/310 [00:03<01:27,  3.40it/s]  4%|▍         | 12/310 [00:03<01:34,  3.17it/s]  4%|▍         | 13/310 [00:03<01:31,  3.23it/s]  5%|▍         | 14/310 [00:04<01:30,  3.28it/s]  5%|▍         | 15/310 [00:04<01:28,  3.31it/s]  5%|▌         | 16/310 [00:04<01:28,  3.34it/s]  5%|▌         | 17/310 [00:05<01:27,  3.35it/s]  6%|▌         | 18/310 [00:05<01:26,  3.37it/s]  6%|▌         | 19/310 [00:05<01:26,  3.38it/s]  6%|▋         | 20/310 [00:05<01:25,  3.38it/s]  7%|▋         | 21/310 [00:06<01:25,  3.38it/s]  7%|▋         | 22/310 [00:06<01:25,  3.39it/s]  7%|▋         | 23/310 [00:06<01:24,  3.39it/s]  8%|▊         | 24/310 [00:07<01:24,  3.39it/s]  8%|▊         | 25/310 [00:07<01:24,  3.39it/s]  8%|▊         | 26/310 [00:07<01:23,  3.39it/s]  9%|▊         | 27/310 [00:08<01:23,  3.39it/s]  9%|▉         | 28/310 [00:08<01:23,  3.39it/s]  9%|▉         | 29/310 [00:08<01:29,  3.15it/s] 10%|▉         | 30/310 [00:08<01:26,  3.22it/s] 10%|█         | 31/310 [00:09<01:25,  3.27it/s] 10%|█         | 32/310 [00:09<01:24,  3.30it/s] 11%|█         | 33/310 [00:09<01:23,  3.33it/s] 11%|█         | 34/310 [00:10<01:22,  3.35it/s] 11%|█▏        | 35/310 [00:10<01:21,  3.36it/s] 12%|█▏        | 36/310 [00:10<01:21,  3.37it/s] 12%|█▏        | 37/310 [00:11<01:20,  3.38it/s] 12%|█▏        | 38/310 [00:11<01:20,  3.38it/s] 13%|█▎        | 39/310 [00:11<01:20,  3.38it/s] 13%|█▎        | 40/310 [00:11<01:19,  3.38it/s] 13%|█▎        | 41/310 [00:12<01:19,  3.38it/s] 14%|█▎        | 42/310 [00:12<01:19,  3.39it/s] 14%|█▍        | 43/310 [00:12<01:18,  3.38it/s] 14%|█▍        | 44/310 [00:13<01:18,  3.38it/s] 15%|█▍        | 45/310 [00:13<01:18,  3.38it/s] 15%|█▍        | 46/310 [00:13<01:20,  3.27it/s] 15%|█▌        | 47/310 [00:14<01:19,  3.30it/s] 15%|█▌        | 48/310 [00:14<01:18,  3.33it/s] 16%|█▌        | 49/310 [00:14<01:18,  3.34it/s] 16%|█▌        | 50/310 [00:14<01:17,  3.36it/s] 16%|█▋        | 51/310 [00:15<01:17,  3.36it/s] 17%|█▋        | 52/310 [00:15<01:16,  3.37it/s] 17%|█▋        | 53/310 [00:15<01:16,  3.37it/s] 17%|█▋        | 54/310 [00:16<01:15,  3.38it/s] 18%|█▊        | 55/310 [00:18<03:42,  1.15it/s] 18%|█▊        | 56/310 [00:18<02:57,  1.43it/s] 18%|█▊        | 57/310 [00:18<02:28,  1.70it/s] 19%|█▊        | 58/310 [00:19<02:05,  2.00it/s] 19%|█▉        | 59/310 [00:19<01:49,  2.28it/s] 19%|█▉        | 60/310 [00:19<01:38,  2.54it/s] 20%|█▉        | 61/310 [00:20<01:30,  2.76it/s] 20%|██        | 62/310 [00:20<01:24,  2.94it/s][INFO|trainer.py:2140] 2023-08-29 11:27:58,770 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:27:58,770 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:27:58,770 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.35it/s][A
  2%|▏         | 12/505 [00:00<00:09, 49.57it/s][A
  4%|▎         | 18/505 [00:00<00:10, 47.50it/s][A
  5%|▍         | 23/505 [00:00<00:10, 46.33it/s][A
  6%|▌         | 28/505 [00:00<00:10, 46.01it/s][A
  7%|▋         | 33/505 [00:00<00:10, 45.66it/s][A
  8%|▊         | 38/505 [00:00<00:10, 45.50it/s][A
  9%|▊         | 43/505 [00:00<00:10, 44.94it/s][A
 10%|▉         | 48/505 [00:01<00:10, 44.33it/s][A
 10%|█         | 53/505 [00:01<00:10, 43.20it/s][A
 11%|█▏        | 58/505 [00:01<00:10, 43.66it/s][A
 12%|█▏        | 63/505 [00:01<00:10, 44.14it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 44.45it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 44.63it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 44.79it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 44.69it/s][A
 17%|█▋        | 88/505 [00:01<00:09, 44.44it/s][A
 18%|█▊        | 93/505 [00:02<00:09, 44.12it/s][A
 19%|█▉        | 98/505 [00:02<00:09, 44.10it/s][A
 20%|██        | 103/505 [00:02<00:09, 44.28it/s][A
 21%|██▏       | 108/505 [00:02<00:09, 40.05it/s][A
 22%|██▏       | 113/505 [00:02<00:09, 41.48it/s][A
 23%|██▎       | 118/505 [00:02<00:09, 42.47it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 43.27it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 43.85it/s][A
 26%|██▋       | 133/505 [00:02<00:08, 44.05it/s][A
 27%|██▋       | 138/505 [00:03<00:08, 44.23it/s][A
 28%|██▊       | 143/505 [00:03<00:08, 44.27it/s][A
 29%|██▉       | 148/505 [00:03<00:08, 44.03it/s][A
 30%|███       | 153/505 [00:03<00:08, 43.95it/s][A
 31%|███▏      | 158/505 [00:03<00:07, 44.12it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 44.48it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 44.62it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 44.82it/s][A
 35%|███▌      | 178/505 [00:04<00:07, 43.13it/s][A
 36%|███▌      | 183/505 [00:04<00:07, 43.71it/s][A
 37%|███▋      | 188/505 [00:04<00:07, 43.83it/s][A
 38%|███▊      | 193/505 [00:04<00:07, 43.77it/s][A
 39%|███▉      | 198/505 [00:04<00:07, 43.83it/s][A
 40%|████      | 203/505 [00:04<00:06, 44.06it/s][A
 41%|████      | 208/505 [00:04<00:06, 44.28it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 44.50it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 44.42it/s][A
 44%|████▍     | 223/505 [00:05<00:06, 44.67it/s][A
 45%|████▌     | 228/505 [00:05<00:06, 44.65it/s][A
 46%|████▌     | 233/505 [00:05<00:06, 44.54it/s][A
 47%|████▋     | 238/505 [00:05<00:06, 44.34it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 44.27it/s][A
 49%|████▉     | 248/505 [00:07<00:31,  8.22it/s][A
 50%|████▉     | 252/505 [00:07<00:24, 10.23it/s][A
 51%|█████     | 257/505 [00:07<00:18, 13.49it/s][A
 52%|█████▏    | 261/505 [00:07<00:15, 15.86it/s][A
 53%|█████▎    | 266/505 [00:07<00:11, 20.08it/s][A
 54%|█████▎    | 271/505 [00:07<00:09, 24.32it/s][A
 55%|█████▍    | 276/505 [00:07<00:08, 28.37it/s][A
 56%|█████▌    | 281/505 [00:08<00:06, 32.02it/s][A
 57%|█████▋    | 286/505 [00:08<00:06, 34.96it/s][A
 58%|█████▊    | 291/505 [00:08<00:05, 37.26it/s][A
 59%|█████▊    | 296/505 [00:08<00:05, 39.20it/s][A
 60%|█████▉    | 301/505 [00:08<00:05, 39.30it/s][A
 61%|██████    | 306/505 [00:08<00:04, 40.89it/s][A
 62%|██████▏   | 311/505 [00:08<00:04, 42.02it/s][A
 63%|██████▎   | 316/505 [00:08<00:04, 42.79it/s][A
 64%|██████▎   | 321/505 [00:08<00:04, 43.48it/s][A
 65%|██████▍   | 326/505 [00:09<00:04, 43.83it/s][A
 66%|██████▌   | 331/505 [00:09<00:03, 44.05it/s][A
 67%|██████▋   | 336/505 [00:09<00:03, 44.14it/s][A
 68%|██████▊   | 341/505 [00:09<00:03, 43.88it/s][A
 69%|██████▊   | 346/505 [00:09<00:03, 44.06it/s][A
 70%|██████▉   | 351/505 [00:09<00:03, 38.81it/s][A
 70%|███████   | 356/505 [00:09<00:03, 40.51it/s][A
 71%|███████▏  | 361/505 [00:09<00:03, 41.75it/s][A
 72%|███████▏  | 366/505 [00:10<00:03, 42.78it/s][A
 73%|███████▎  | 371/505 [00:10<00:03, 43.43it/s][A
 74%|███████▍  | 376/505 [00:10<00:02, 43.93it/s][A
 75%|███████▌  | 381/505 [00:10<00:02, 44.07it/s][A
 76%|███████▋  | 386/505 [00:10<00:02, 44.27it/s][A
 77%|███████▋  | 391/505 [00:10<00:02, 43.90it/s][A
 78%|███████▊  | 396/505 [00:10<00:02, 43.69it/s][A
 79%|███████▉  | 401/505 [00:10<00:02, 43.98it/s][A
 80%|████████  | 406/505 [00:10<00:02, 44.35it/s][A
 81%|████████▏ | 411/505 [00:11<00:02, 44.51it/s][A
 82%|████████▏ | 416/505 [00:11<00:01, 44.72it/s][A
 83%|████████▎ | 421/505 [00:11<00:01, 44.81it/s][A
 84%|████████▍ | 426/505 [00:11<00:01, 42.95it/s][A
 85%|████████▌ | 431/505 [00:11<00:01, 43.48it/s][A
 86%|████████▋ | 436/505 [00:11<00:01, 43.57it/s][A
 87%|████████▋ | 441/505 [00:11<00:01, 43.67it/s][A
 88%|████████▊ | 446/505 [00:11<00:01, 43.96it/s][A
 89%|████████▉ | 451/505 [00:11<00:01, 44.26it/s][A
 90%|█████████ | 456/505 [00:12<00:01, 44.48it/s][A
 91%|█████████▏| 461/505 [00:12<00:00, 44.62it/s][A
 92%|█████████▏| 466/505 [00:12<00:00, 44.64it/s][A
 93%|█████████▎| 471/505 [00:12<00:00, 44.46it/s][A
 94%|█████████▍| 476/505 [00:12<00:00, 44.45it/s][A
 95%|█████████▌| 481/505 [00:12<00:00, 42.49it/s][A
 96%|█████████▌| 486/505 [00:12<00:00, 43.07it/s][A
 97%|█████████▋| 491/505 [00:12<00:00, 43.58it/s][A
 98%|█████████▊| 496/505 [00:12<00:00, 43.96it/s][A
 99%|█████████▉| 501/505 [00:13<00:00, 44.24it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:13<00:00, 44.24it/s][A 20%|██        | 62/310 [00:34<01:24,  2.94it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:28:12,181 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62
[INFO|configuration_utils.py:351] 2023-08-29 11:28:12,394 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:28:15,729 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:28:15,921 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:28:16,018 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62/special_tokens_map.json
 20%|██        | 63/310 [00:47<34:10,  8.30s/it] 21%|██        | 64/310 [00:47<24:13,  5.91s/it] 21%|██        | 65/310 [00:47<17:14,  4.22s/it] 21%|██▏       | 66/310 [00:48<12:22,  3.04s/it] 22%|██▏       | 67/310 [00:48<08:59,  2.22s/it] 22%|██▏       | 68/310 [00:48<06:37,  1.64s/it] 22%|██▏       | 69/310 [00:49<04:58,  1.24s/it] 23%|██▎       | 70/310 [00:49<03:49,  1.05it/s] 23%|██▎       | 71/310 [00:49<03:00,  1.32it/s] 23%|██▎       | 72/310 [00:49<02:27,  1.62it/s] 24%|██▎       | 73/310 [00:50<02:03,  1.92it/s] 24%|██▍       | 74/310 [00:50<01:46,  2.21it/s] 24%|██▍       | 75/310 [00:50<01:37,  2.41it/s] 25%|██▍       | 76/310 [00:51<01:28,  2.63it/s] 25%|██▍       | 77/310 [00:51<01:22,  2.82it/s] 25%|██▌       | 78/310 [00:51<01:17,  2.98it/s] 25%|██▌       | 79/310 [00:52<01:14,  3.09it/s] 26%|██▌       | 80/310 [00:52<01:12,  3.18it/s] 26%|██▌       | 81/310 [00:52<01:10,  3.24it/s] 26%|██▋       | 82/310 [00:52<01:09,  3.29it/s] 27%|██▋       | 83/310 [00:53<01:08,  3.32it/s] 27%|██▋       | 84/310 [00:53<01:07,  3.34it/s] 27%|██▋       | 85/310 [00:53<01:07,  3.35it/s] 28%|██▊       | 86/310 [00:54<01:09,  3.21it/s] 28%|██▊       | 87/310 [00:54<01:08,  3.26it/s] 28%|██▊       | 88/310 [00:54<01:07,  3.30it/s] 29%|██▊       | 89/310 [00:55<01:06,  3.33it/s] 29%|██▉       | 90/310 [00:55<01:05,  3.35it/s] 29%|██▉       | 91/310 [00:55<01:05,  3.36it/s] 30%|██▉       | 92/310 [00:55<01:04,  3.38it/s] 30%|███       | 93/310 [00:56<01:04,  3.38it/s] 30%|███       | 94/310 [00:56<01:03,  3.38it/s] 31%|███       | 95/310 [00:56<01:03,  3.39it/s] 31%|███       | 96/310 [00:57<01:03,  3.39it/s] 31%|███▏      | 97/310 [00:57<01:05,  3.25it/s] 32%|███▏      | 98/310 [00:57<01:04,  3.29it/s] 32%|███▏      | 99/310 [00:58<01:03,  3.32it/s] 32%|███▏      | 100/310 [00:58<01:02,  3.34it/s] 33%|███▎      | 101/310 [00:58<01:02,  3.35it/s] 33%|███▎      | 102/310 [00:58<01:01,  3.37it/s] 33%|███▎      | 103/310 [00:59<01:01,  3.38it/s] 34%|███▎      | 104/310 [00:59<01:00,  3.38it/s] 34%|███▍      | 105/310 [00:59<01:00,  3.39it/s] 34%|███▍      | 106/310 [01:00<01:00,  3.39it/s] 35%|███▍      | 107/310 [01:00<00:59,  3.39it/s] 35%|███▍      | 108/310 [01:00<01:04,  3.14it/s] 35%|███▌      | 109/310 [01:01<01:02,  3.21it/s] 35%|███▌      | 110/310 [01:01<01:01,  3.28it/s] 36%|███▌      | 111/310 [01:01<00:59,  3.32it/s] 36%|███▌      | 112/310 [01:01<00:58,  3.36it/s] 36%|███▋      | 113/310 [01:02<00:58,  3.39it/s] 37%|███▋      | 114/310 [01:02<00:57,  3.40it/s] 37%|███▋      | 115/310 [01:02<00:57,  3.41it/s] 37%|███▋      | 116/310 [01:03<00:56,  3.42it/s] 38%|███▊      | 117/310 [01:03<00:56,  3.43it/s] 38%|███▊      | 118/310 [01:03<00:55,  3.44it/s] 38%|███▊      | 119/310 [01:03<00:55,  3.44it/s] 39%|███▊      | 120/310 [01:04<00:55,  3.44it/s] 39%|███▉      | 121/310 [01:04<00:54,  3.44it/s] 39%|███▉      | 122/310 [01:04<00:58,  3.24it/s] 40%|███▉      | 123/310 [01:05<00:56,  3.30it/s] 40%|████      | 124/310 [01:05<00:55,  3.34it/s][INFO|trainer.py:2140] 2023-08-29 11:28:43,193 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:28:43,193 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:28:43,193 >>   Batch size = 8
{'eval_loss': 1.167448878288269, 'eval_runtime': 13.2607, 'eval_samples_per_second': 304.585, 'eval_steps_per_second': 38.083, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 57.73it/s][A
  2%|▏         | 12/505 [00:00<00:10, 49.10it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.45it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.48it/s][A
  5%|▌         | 27/505 [00:00<00:10, 46.04it/s][A
  6%|▋         | 32/505 [00:00<00:10, 45.65it/s][A
  7%|▋         | 37/505 [00:00<00:10, 45.33it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.86it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.38it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.46it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.64it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.80it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.89it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.76it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.51it/s][A
 18%|█▊        | 92/505 [00:02<00:10, 41.03it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 42.12it/s][A
 20%|██        | 102/505 [00:02<00:09, 42.86it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.46it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 43.96it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.30it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.46it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.34it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.06it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 43.93it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.03it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.26it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.44it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.67it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.79it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.77it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.56it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.34it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.24it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.20it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.29it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.49it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.60it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.78it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.73it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.54it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.36it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 43.64it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 43.90it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.06it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.30it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.53it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.65it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.61it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.48it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.24it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.19it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.30it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.40it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.49it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.65it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.70it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.70it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.53it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.32it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.21it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.30it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.35it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.53it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.61it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.63it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.66it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.42it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.34it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 42.47it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 43.22it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 43.61it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.93it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.20it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.35it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.49it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.38it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.03it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.16it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.29it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.48it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.53it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.59it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.64it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.58it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.47it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.21it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.21it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.28it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.44it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.54it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.63it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.59it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.58it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.35it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.24it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 42.98it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 43.48it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 43.48it/s][A 40%|████      | 124/310 [01:17<00:55,  3.34it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:28:54,851 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-29 11:28:55,117 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:28:58,987 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:28:59,194 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:28:59,279 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124/special_tokens_map.json
 40%|████      | 125/310 [01:29<23:19,  7.56s/it] 41%|████      | 126/310 [01:30<16:31,  5.39s/it] 41%|████      | 127/310 [01:30<11:46,  3.86s/it] 41%|████▏     | 128/310 [01:30<08:27,  2.79s/it] 42%|████▏     | 129/310 [01:31<06:09,  2.04s/it] 42%|████▏     | 130/310 [01:31<04:33,  1.52s/it] 42%|████▏     | 131/310 [01:31<03:25,  1.15s/it] 43%|████▎     | 132/310 [01:32<02:39,  1.12it/s] 43%|████▎     | 133/310 [01:32<02:06,  1.40it/s] 43%|████▎     | 134/310 [01:32<01:43,  1.70it/s] 44%|████▎     | 135/310 [01:32<01:27,  2.00it/s] 44%|████▍     | 136/310 [01:33<01:16,  2.29it/s] 44%|████▍     | 137/310 [01:33<01:09,  2.48it/s] 45%|████▍     | 138/310 [01:33<01:03,  2.70it/s] 45%|████▍     | 139/310 [01:34<00:59,  2.88it/s] 45%|████▌     | 140/310 [01:34<00:56,  3.02it/s] 45%|████▌     | 141/310 [01:34<00:54,  3.13it/s] 46%|████▌     | 142/310 [01:35<00:52,  3.20it/s] 46%|████▌     | 143/310 [01:35<00:51,  3.26it/s] 46%|████▋     | 144/310 [01:35<00:50,  3.30it/s] 47%|████▋     | 145/310 [01:35<00:49,  3.33it/s] 47%|████▋     | 146/310 [01:36<00:48,  3.35it/s] 47%|████▋     | 147/310 [01:36<00:48,  3.37it/s] 48%|████▊     | 148/310 [01:36<00:49,  3.25it/s] 48%|████▊     | 149/310 [01:37<00:48,  3.30it/s] 48%|████▊     | 150/310 [01:37<00:47,  3.35it/s] 49%|████▊     | 151/310 [01:37<00:47,  3.38it/s] 49%|████▉     | 152/310 [01:37<00:46,  3.40it/s] 49%|████▉     | 153/310 [01:38<00:45,  3.42it/s] 50%|████▉     | 154/310 [01:38<00:45,  3.43it/s] 50%|█████     | 155/310 [01:38<00:45,  3.44it/s] 50%|█████     | 156/310 [01:39<00:44,  3.44it/s] 51%|█████     | 157/310 [01:39<00:44,  3.45it/s] 51%|█████     | 158/310 [01:39<00:44,  3.45it/s] 51%|█████▏    | 159/310 [01:40<00:45,  3.34it/s] 52%|█████▏    | 160/310 [01:40<00:44,  3.37it/s] 52%|█████▏    | 161/310 [01:40<00:43,  3.40it/s] 52%|█████▏    | 162/310 [01:40<00:43,  3.42it/s] 53%|█████▎    | 163/310 [01:41<00:42,  3.43it/s] 53%|█████▎    | 164/310 [01:41<00:42,  3.44it/s] 53%|█████▎    | 165/310 [01:41<00:42,  3.44it/s] 54%|█████▎    | 166/310 [01:42<00:41,  3.45it/s] 54%|█████▍    | 167/310 [01:42<00:41,  3.45it/s] 54%|█████▍    | 168/310 [01:42<00:41,  3.45it/s] 55%|█████▍    | 169/310 [01:42<00:40,  3.45it/s] 55%|█████▍    | 170/310 [01:43<00:42,  3.28it/s] 55%|█████▌    | 171/310 [01:43<00:41,  3.33it/s] 55%|█████▌    | 172/310 [01:43<00:41,  3.36it/s] 56%|█████▌    | 173/310 [01:44<00:40,  3.39it/s] 56%|█████▌    | 174/310 [01:44<00:39,  3.41it/s] 56%|█████▋    | 175/310 [01:44<00:39,  3.42it/s] 57%|█████▋    | 176/310 [01:45<00:39,  3.43it/s] 57%|█████▋    | 177/310 [01:45<00:38,  3.44it/s] 57%|█████▋    | 178/310 [01:45<00:38,  3.44it/s] 58%|█████▊    | 179/310 [01:45<00:37,  3.45it/s] 58%|█████▊    | 180/310 [01:46<00:38,  3.35it/s] 58%|█████▊    | 181/310 [01:46<00:39,  3.26it/s] 59%|█████▊    | 182/310 [01:47<00:52,  2.43it/s] 59%|█████▉    | 183/310 [01:47<00:47,  2.67it/s] 59%|█████▉    | 184/310 [01:47<00:44,  2.86it/s] 60%|█████▉    | 185/310 [01:48<00:41,  3.02it/s] 60%|██████    | 186/310 [01:48<00:39,  3.14it/s][INFO|trainer.py:2140] 2023-08-29 11:29:26,071 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:29:26,071 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:29:26,071 >>   Batch size = 8
{'eval_loss': 1.1853374242782593, 'eval_runtime': 11.4543, 'eval_samples_per_second': 352.618, 'eval_steps_per_second': 44.088, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.38it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.62it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.70it/s][A
  4%|▍         | 22/505 [00:00<00:10, 45.67it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.08it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.90it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.74it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.56it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.63it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.75it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.73it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.79it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.58it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.49it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.50it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.39it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.42it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.53it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 42.35it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.14it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.67it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 43.90it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.06it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.13it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.25it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.29it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.07it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.19it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.24it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.57it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.66it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.62it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.58it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.47it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.46it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.23it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.27it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.43it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.55it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.62it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.63it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.64it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.61it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.40it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.32it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 43.69it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.06it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.24it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.39it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.47it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.45it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.45it/s][A
 53%|█████▎    | 267/505 [00:05<00:05, 44.38it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.15it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.36it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.43it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.60it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.10it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.73it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 43.96it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.11it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.07it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.37it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.35it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.39it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.52it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.55it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.40it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.29it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.26it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 42.07it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 42.92it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.52it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.89it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.17it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.31it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.20it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.20it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.01it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.13it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.41it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.56it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.57it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.71it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.60it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.46it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.27it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.12it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.25it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.38it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.52it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.62it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.66it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.53it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.33it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.25it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.17it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 33.84it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 33.84it/s][A 60%|██████    | 186/310 [02:00<00:39,  3.14it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:29:38,128 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186
[INFO|configuration_utils.py:351] 2023-08-29 11:29:38,576 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:29:43,182 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:29:43,541 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:29:43,728 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186/special_tokens_map.json
 60%|██████    | 187/310 [02:17<18:15,  8.91s/it] 61%|██████    | 188/310 [02:17<12:52,  6.33s/it] 61%|██████    | 189/310 [02:17<09:07,  4.52s/it] 61%|██████▏   | 190/310 [02:18<06:30,  3.25s/it] 62%|██████▏   | 191/310 [02:18<04:41,  2.37s/it] 62%|██████▏   | 192/310 [02:18<03:25,  1.74s/it] 62%|██████▏   | 193/310 [02:19<02:33,  1.31s/it] 63%|██████▎   | 194/310 [02:19<01:56,  1.00s/it] 63%|██████▎   | 195/310 [02:19<01:31,  1.26it/s] 63%|██████▎   | 196/310 [02:19<01:13,  1.56it/s] 64%|██████▎   | 197/310 [02:20<01:00,  1.86it/s] 64%|██████▍   | 198/310 [02:20<00:52,  2.15it/s] 64%|██████▍   | 199/310 [02:20<00:45,  2.42it/s] 65%|██████▍   | 200/310 [02:21<00:41,  2.65it/s] 65%|██████▍   | 201/310 [02:21<00:38,  2.83it/s] 65%|██████▌   | 202/310 [02:21<00:37,  2.89it/s] 65%|██████▌   | 203/310 [02:22<00:35,  3.02it/s] 66%|██████▌   | 204/310 [02:22<00:33,  3.13it/s] 66%|██████▌   | 205/310 [02:22<00:32,  3.20it/s] 66%|██████▋   | 206/310 [02:22<00:31,  3.26it/s] 67%|██████▋   | 207/310 [02:23<00:31,  3.30it/s] 67%|██████▋   | 208/310 [02:23<00:30,  3.33it/s] 67%|██████▋   | 209/310 [02:23<00:30,  3.34it/s] 68%|██████▊   | 210/310 [02:24<00:29,  3.36it/s] 68%|██████▊   | 211/310 [02:24<00:29,  3.37it/s] 68%|██████▊   | 212/310 [02:24<00:28,  3.38it/s] 69%|██████▊   | 213/310 [02:25<00:29,  3.30it/s] 69%|██████▉   | 214/310 [02:25<00:28,  3.33it/s] 69%|██████▉   | 215/310 [02:25<00:28,  3.35it/s] 70%|██████▉   | 216/310 [02:25<00:27,  3.37it/s] 70%|███████   | 217/310 [02:26<00:27,  3.38it/s] 70%|███████   | 218/310 [02:26<00:27,  3.38it/s] 71%|███████   | 219/310 [02:26<00:26,  3.39it/s] 71%|███████   | 220/310 [02:27<00:26,  3.39it/s] 71%|███████▏  | 221/310 [02:27<00:26,  3.39it/s] 72%|███████▏  | 222/310 [02:27<00:25,  3.39it/s] 72%|███████▏  | 223/310 [02:27<00:25,  3.39it/s] 72%|███████▏  | 224/310 [02:28<00:26,  3.23it/s] 73%|███████▎  | 225/310 [02:28<00:25,  3.28it/s] 73%|███████▎  | 226/310 [02:28<00:25,  3.31it/s] 73%|███████▎  | 227/310 [02:29<00:24,  3.34it/s] 74%|███████▎  | 228/310 [02:29<00:24,  3.35it/s] 74%|███████▍  | 229/310 [02:29<00:24,  3.37it/s] 74%|███████▍  | 230/310 [02:30<00:23,  3.38it/s] 75%|███████▍  | 231/310 [02:30<00:23,  3.38it/s] 75%|███████▍  | 232/310 [02:30<00:23,  3.38it/s] 75%|███████▌  | 233/310 [02:30<00:22,  3.39it/s] 75%|███████▌  | 234/310 [02:31<00:22,  3.39it/s] 76%|███████▌  | 235/310 [02:31<00:22,  3.32it/s] 76%|███████▌  | 236/310 [02:31<00:22,  3.35it/s] 76%|███████▋  | 237/310 [02:32<00:21,  3.38it/s] 77%|███████▋  | 238/310 [02:32<00:21,  3.39it/s] 77%|███████▋  | 239/310 [02:32<00:20,  3.41it/s] 77%|███████▋  | 240/310 [02:33<00:20,  3.42it/s] 78%|███████▊  | 241/310 [02:33<00:20,  3.43it/s] 78%|███████▊  | 242/310 [02:33<00:19,  3.44it/s] 78%|███████▊  | 243/310 [02:33<00:19,  3.45it/s] 79%|███████▊  | 244/310 [02:34<00:19,  3.45it/s] 79%|███████▉  | 245/310 [02:34<00:18,  3.45it/s] 79%|███████▉  | 246/310 [02:34<00:18,  3.37it/s] 80%|███████▉  | 247/310 [02:35<00:18,  3.40it/s] 80%|████████  | 248/310 [02:35<00:18,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 11:30:13,108 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:30:13,108 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:30:13,108 >>   Batch size = 8
{'eval_loss': 1.2060143947601318, 'eval_runtime': 11.5243, 'eval_samples_per_second': 350.477, 'eval_steps_per_second': 43.82, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.05it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.55it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.13it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.15it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.49it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.99it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.56it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.42it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.56it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.60it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.71it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.86it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.79it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.54it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.44it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.25it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.29it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.38it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 42.11it/s][A
 20%|██        | 102/505 [00:02<00:09, 42.95it/s][A
 21%|██        | 107/505 [00:02<00:09, 43.55it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.00it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.10it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.12it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.10it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 43.99it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.00it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.27it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.47it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.67it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.60it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.57it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.42it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.36it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.27it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.32it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.45it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.52it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.71it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.60it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.48it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.39it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.27it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.25it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 40.86it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 42.05it/s][A
 48%|████▊     | 242/505 [00:05<00:06, 42.87it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.46it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 43.89it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.18it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.28it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.19it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.91it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 43.86it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.58it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.60it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.70it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.79it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.71it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.46it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.15it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.11it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.25it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.45it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.61it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.76it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.68it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.60it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.39it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.21it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 41.69it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 42.73it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 43.39it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 43.83it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.20it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.27it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.25it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.17it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.87it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.03it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.22it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.50it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.61it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.77it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.67it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.45it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.27it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.14it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.09it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.20it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.50it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 41.24it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.28it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 43.82it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 43.85it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 43.86it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 21.46it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 21.46it/s][A 80%|████████  | 248/310 [02:47<00:18,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:30:25,162 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-29 11:30:25,397 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:30:29,713 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:30:29,957 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:30:30,087 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248/special_tokens_map.json
 80%|████████  | 249/310 [03:02<08:32,  8.40s/it] 81%|████████  | 250/310 [03:03<05:58,  5.98s/it] 81%|████████  | 251/310 [03:03<04:12,  4.28s/it] 81%|████████▏ | 252/310 [03:03<02:58,  3.08s/it] 82%|████████▏ | 253/310 [03:03<02:07,  2.25s/it] 82%|████████▏ | 254/310 [03:04<01:32,  1.66s/it] 82%|████████▏ | 255/310 [03:04<01:08,  1.25s/it] 83%|████████▎ | 256/310 [03:04<00:51,  1.04it/s] 83%|████████▎ | 257/310 [03:05<00:40,  1.32it/s] 83%|████████▎ | 258/310 [03:05<00:32,  1.62it/s] 84%|████████▎ | 259/310 [03:05<00:26,  1.92it/s] 84%|████████▍ | 260/310 [03:05<00:22,  2.22it/s] 84%|████████▍ | 261/310 [03:06<00:20,  2.40it/s] 85%|████████▍ | 262/310 [03:06<00:18,  2.64it/s] 85%|████████▍ | 263/310 [03:06<00:16,  2.84it/s] 85%|████████▌ | 264/310 [03:07<00:15,  3.00it/s] 85%|████████▌ | 265/310 [03:07<00:14,  3.12it/s] 86%|████████▌ | 266/310 [03:07<00:13,  3.21it/s] 86%|████████▌ | 267/310 [03:08<00:13,  3.28it/s] 86%|████████▋ | 268/310 [03:08<00:12,  3.33it/s] 87%|████████▋ | 269/310 [03:08<00:12,  3.37it/s] 87%|████████▋ | 270/310 [03:08<00:11,  3.39it/s] 87%|████████▋ | 271/310 [03:09<00:11,  3.41it/s] 88%|████████▊ | 272/310 [03:09<00:12,  3.16it/s] 88%|████████▊ | 273/310 [03:09<00:11,  3.24it/s] 88%|████████▊ | 274/310 [03:10<00:10,  3.30it/s] 89%|████████▊ | 275/310 [03:10<00:10,  3.35it/s] 89%|████████▉ | 276/310 [03:10<00:10,  3.37it/s] 89%|████████▉ | 277/310 [03:11<00:09,  3.40it/s] 90%|████████▉ | 278/310 [03:11<00:09,  3.41it/s] 90%|█████████ | 279/310 [03:11<00:09,  3.42it/s] 90%|█████████ | 280/310 [03:11<00:08,  3.43it/s] 91%|█████████ | 281/310 [03:12<00:08,  3.44it/s] 91%|█████████ | 282/310 [03:12<00:08,  3.44it/s] 91%|█████████▏| 283/310 [03:12<00:08,  3.31it/s] 92%|█████████▏| 284/310 [03:13<00:07,  3.35it/s] 92%|█████████▏| 285/310 [03:13<00:07,  3.38it/s] 92%|█████████▏| 286/310 [03:13<00:07,  3.40it/s] 93%|█████████▎| 287/310 [03:13<00:06,  3.41it/s] 93%|█████████▎| 288/310 [03:14<00:06,  3.42it/s] 93%|█████████▎| 289/310 [03:14<00:06,  3.43it/s] 94%|█████████▎| 290/310 [03:14<00:05,  3.44it/s] 94%|█████████▍| 291/310 [03:15<00:05,  3.44it/s] 94%|█████████▍| 292/310 [03:15<00:05,  3.44it/s] 95%|█████████▍| 293/310 [03:15<00:04,  3.44it/s] 95%|█████████▍| 294/310 [03:16<00:04,  3.36it/s] 95%|█████████▌| 295/310 [03:16<00:04,  3.38it/s] 95%|█████████▌| 296/310 [03:16<00:04,  3.40it/s] 96%|█████████▌| 297/310 [03:16<00:03,  3.41it/s] 96%|█████████▌| 298/310 [03:17<00:03,  3.42it/s] 96%|█████████▋| 299/310 [03:17<00:03,  3.42it/s] 97%|█████████▋| 300/310 [03:17<00:02,  3.43it/s] 97%|█████████▋| 301/310 [03:18<00:02,  3.44it/s] 97%|█████████▋| 302/310 [03:18<00:02,  3.44it/s] 98%|█████████▊| 303/310 [03:18<00:02,  3.44it/s] 98%|█████████▊| 304/310 [03:18<00:01,  3.45it/s] 98%|█████████▊| 305/310 [03:19<00:01,  3.34it/s] 99%|█████████▊| 306/310 [03:19<00:01,  3.37it/s] 99%|█████████▉| 307/310 [03:19<00:00,  3.39it/s] 99%|█████████▉| 308/310 [03:20<00:00,  3.41it/s]100%|█████████▉| 309/310 [03:20<00:00,  3.42it/s]100%|██████████| 310/310 [03:20<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 11:30:58,259 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:30:58,259 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:30:58,259 >>   Batch size = 8
{'eval_loss': 1.2215969562530518, 'eval_runtime': 11.8454, 'eval_samples_per_second': 340.975, 'eval_steps_per_second': 42.632, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.11it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.70it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.15it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.25it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.60it/s][A
  6%|▋         | 32/505 [00:00<00:10, 45.03it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.60it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.42it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.52it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.63it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.80it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.80it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.79it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.67it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.46it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.31it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.26it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.34it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 43.06it/s][A
 20%|██        | 102/505 [00:02<00:09, 43.65it/s][A
 21%|██        | 107/505 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.30it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.35it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.22it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.19it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.20it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.29it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.44it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.52it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.66it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.71it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.55it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.45it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.31it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.23it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.36it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.42it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.62it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.71it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.61it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.52it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.44it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.33it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.28it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 42.08it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 42.86it/s][A
 48%|████▊     | 242/505 [00:05<00:06, 43.42it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 43.93it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.20it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.31it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.31it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.18it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 43.97it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.00it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.24it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.37it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.36it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.62it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.61it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.51it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.27it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.13it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.24it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.42it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.60it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.72it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.67it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.53it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.37it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.22it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 39.39it/s][A
 74%|███████▎  | 372/505 [00:08<00:03, 40.89it/s][A
 75%|███████▍  | 377/505 [00:08<00:03, 42.15it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 42.98it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 43.60it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.02it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.09it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.11it/s][A
 81%|████████  | 407/505 [00:09<00:02, 43.82it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 43.70it/s][A
 83%|████████▎ | 417/505 [00:09<00:02, 43.89it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.16it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.47it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.62it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 44.75it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 44.71it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.34it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.18it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.08it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.13it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.27it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.42it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.56it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.73it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.65it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.39it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.20it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 42.77it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 42.77it/s][A100%|██████████| 310/310 [03:32<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:31:09,785 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310
[INFO|configuration_utils.py:351] 2023-08-29 11:31:10,034 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:31:13,722 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:31:13,926 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:31:14,030 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:31:24,975 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:31:25,005 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62 (score: 1.167448878288269).
                                                 100%|██████████| 310/310 [03:59<00:00,  3.43it/s]100%|██████████| 310/310 [03:59<00:00,  1.29it/s]
[INFO|trainer.py:1894] 2023-08-29 11:31:37,239 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 11:31:37,530 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:31:43,052 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:31:43,675 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:31:43,860 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:31:44,804 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   train_loss               =     0.3479
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   train_runtime            = 0:03:59.55
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   train_samples            =       4000
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   train_samples_per_second =     83.489
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:44,805 >>   train_steps_per_second   =      1.294
{'eval_loss': 1.2252570390701294, 'eval_runtime': 11.4445, 'eval_samples_per_second': 352.919, 'eval_steps_per_second': 44.126, 'epoch': 4.99}
{'train_runtime': 239.5531, 'train_samples_per_second': 83.489, 'train_steps_per_second': 1.294, 'train_loss': 0.3478740569083921, 'epoch': 4.99}
08/29/2023 11:31:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:31:45,458 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:31:45,458 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 11:31:45,458 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.43it/s]  2%|▏         | 12/505 [00:00<00:10, 49.19it/s]  3%|▎         | 17/505 [00:00<00:10, 47.47it/s]  4%|▍         | 22/505 [00:00<00:10, 46.54it/s]  5%|▌         | 27/505 [00:00<00:10, 46.12it/s]  6%|▋         | 32/505 [00:00<00:10, 45.83it/s]  7%|▋         | 37/505 [00:00<00:12, 38.08it/s]  8%|▊         | 42/505 [00:00<00:11, 40.11it/s]  9%|▉         | 47/505 [00:01<00:11, 41.57it/s] 10%|█         | 52/505 [00:01<00:10, 42.60it/s] 11%|█▏        | 57/505 [00:01<00:10, 43.34it/s] 12%|█▏        | 62/505 [00:01<00:10, 43.88it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.16it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.31it/s] 15%|█▌        | 77/505 [00:01<00:09, 44.06it/s] 16%|█▌        | 82/505 [00:01<00:09, 43.88it/s] 17%|█▋        | 87/505 [00:01<00:09, 43.91it/s] 18%|█▊        | 92/505 [00:02<00:09, 44.19it/s] 19%|█▉        | 97/505 [00:02<00:09, 44.45it/s] 20%|██        | 102/505 [00:02<00:09, 44.64it/s] 21%|██        | 107/505 [00:02<00:08, 44.73it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.81it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.67it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.45it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.22it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.17it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.28it/s] 28%|██▊       | 142/505 [00:03<00:08, 44.54it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.63it/s] 30%|███       | 152/505 [00:03<00:07, 44.77it/s] 31%|███       | 157/505 [00:03<00:07, 44.79it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.70it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.46it/s] 34%|███▍      | 172/505 [00:03<00:08, 39.04it/s] 35%|███▌      | 177/505 [00:04<00:08, 40.70it/s] 36%|███▌      | 182/505 [00:04<00:07, 41.90it/s] 37%|███▋      | 187/505 [00:04<00:07, 42.78it/s] 38%|███▊      | 192/505 [00:04<00:07, 43.45it/s] 39%|███▉      | 197/505 [00:04<00:07, 43.95it/s] 40%|████      | 202/505 [00:04<00:06, 44.27it/s] 41%|████      | 207/505 [00:04<00:06, 44.30it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.09it/s] 43%|████▎     | 217/505 [00:04<00:06, 43.85it/s] 44%|████▍     | 222/505 [00:05<00:06, 43.92it/s] 45%|████▍     | 227/505 [00:05<00:06, 44.10it/s] 46%|████▌     | 232/505 [00:05<00:06, 44.33it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.56it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.76it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.82it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.69it/s] 51%|█████     | 257/505 [00:05<00:05, 44.40it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.18it/s] 53%|█████▎    | 267/505 [00:06<00:05, 44.18it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.21it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.31it/s] 56%|█████▌    | 282/505 [00:06<00:05, 44.54it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.69it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.82it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.65it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.49it/s] 61%|██████    | 307/505 [00:06<00:04, 43.36it/s] 62%|██████▏   | 312/505 [00:07<00:04, 43.62it/s] 63%|██████▎   | 317/505 [00:07<00:04, 43.85it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.04it/s] 65%|██████▍   | 327/505 [00:07<00:04, 44.29it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.48it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.63it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.66it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.36it/s] 70%|██████▉   | 352/505 [00:07<00:03, 44.24it/s] 71%|███████   | 357/505 [00:08<00:03, 44.31it/s] 72%|███████▏  | 362/505 [00:08<00:03, 44.36it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.38it/s] 74%|███████▎  | 372/505 [00:08<00:02, 44.47it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.67it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.75it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.54it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.38it/s] 79%|███████▊  | 397/505 [00:09<00:02, 44.25it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.26it/s] 81%|████████  | 407/505 [00:09<00:02, 44.30it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.37it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.57it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.69it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.68it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.59it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.37it/s] 88%|████████▊ | 442/505 [00:10<00:01, 41.74it/s] 89%|████████▊ | 447/505 [00:10<00:01, 42.69it/s] 90%|████████▉ | 452/505 [00:10<00:01, 43.31it/s] 90%|█████████ | 457/505 [00:10<00:01, 43.67it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.03it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.27it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.37it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.31it/s] 95%|█████████▌| 482/505 [00:10<00:00, 43.96it/s] 96%|█████████▋| 487/505 [00:11<00:00, 43.96it/s] 97%|█████████▋| 492/505 [00:11<00:00, 44.21it/s] 98%|█████████▊| 497/505 [00:11<00:00, 44.43it/s] 99%|█████████▉| 502/505 [00:11<00:00, 44.54it/s]100%|██████████| 505/505 [00:11<00:00, 44.05it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:31:56,939 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   eval_loss               =     1.1674
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   eval_runtime            = 0:00:11.48
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   eval_samples_per_second =    351.806
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   eval_steps_per_second   =     43.987
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:31:56,939 >>   perplexity              =     3.2138
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:08,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:08,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:08,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:08,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:08,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:32:09,379 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:32:09,381 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:32:10,003 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:32:11,174 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:32:11,174 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:14,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:14,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:14,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:14,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:32:14,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:32:15,116 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:32:15,117 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:32:15,751 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:32:16,019 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:32:16,019 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-62
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-248
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-186
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/checkpoint-310
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:24,  1.63it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.67it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.65it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:38,  1.62it/s]Extractor Predicting: 62it [00:39,  1.62it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:42,  1.60it/s]Extractor Predicting: 68it [00:43,  1.55it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:45,  1.42it/s]Extractor Predicting: 72it [00:45,  1.42it/s]Extractor Predicting: 73it [00:46,  1.42it/s]Extractor Predicting: 74it [00:47,  1.40it/s]Extractor Predicting: 75it [00:48,  1.44it/s]Extractor Predicting: 76it [00:48,  1.48it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:49,  1.48it/s]Extractor Predicting: 79it [00:50,  1.50it/s]Extractor Predicting: 80it [00:51,  1.51it/s]Extractor Predicting: 81it [00:51,  1.52it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.51it/s]Extractor Predicting: 84it [00:53,  1.53it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:55,  1.58it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:57,  1.56it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.61it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:00,  1.56it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:02,  1.53it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.54it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.52it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:06,  1.56it/s]Extractor Predicting: 105it [01:07,  1.56it/s]Extractor Predicting: 106it [01:07,  1.59it/s]Extractor Predicting: 107it [01:08,  1.59it/s]Extractor Predicting: 108it [01:09,  1.59it/s]Extractor Predicting: 109it [01:09,  1.60it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:11,  1.60it/s]Extractor Predicting: 113it [01:12,  1.58it/s]Extractor Predicting: 114it [01:13,  1.56it/s]Extractor Predicting: 115it [01:13,  1.58it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:14,  1.59it/s]Extractor Predicting: 118it [01:15,  1.53it/s]Extractor Predicting: 119it [01:16,  1.58it/s]Extractor Predicting: 120it [01:16,  1.57it/s]Extractor Predicting: 121it [01:17,  1.55it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:18,  1.59it/s]Extractor Predicting: 124it [01:19,  1.59it/s]Extractor Predicting: 125it [01:20,  1.43it/s]Extractor Predicting: 126it [01:20,  1.49it/s]Extractor Predicting: 127it [01:21,  1.45it/s]Extractor Predicting: 128it [01:22,  1.44it/s]Extractor Predicting: 129it [01:22,  1.45it/s]Extractor Predicting: 130it [01:23,  1.42it/s]Extractor Predicting: 131it [01:24,  1.40it/s]Extractor Predicting: 132it [01:25,  1.41it/s]Extractor Predicting: 133it [01:25,  1.43it/s]Extractor Predicting: 134it [01:26,  1.48it/s]Extractor Predicting: 135it [01:27,  1.46it/s]Extractor Predicting: 136it [01:27,  1.47it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:29,  1.46it/s]Extractor Predicting: 139it [01:29,  1.47it/s]Extractor Predicting: 140it [01:30,  1.44it/s]Extractor Predicting: 141it [01:31,  1.33it/s]Extractor Predicting: 142it [01:32,  1.38it/s]Extractor Predicting: 143it [01:32,  1.40it/s]Extractor Predicting: 144it [01:33,  1.43it/s]Extractor Predicting: 145it [01:34,  1.39it/s]Extractor Predicting: 146it [01:34,  1.41it/s]Extractor Predicting: 147it [01:35,  1.44it/s]Extractor Predicting: 148it [01:36,  1.40it/s]Extractor Predicting: 149it [01:36,  1.46it/s]Extractor Predicting: 149it [01:37,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:07,621 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:07,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:07,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:07,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:07,671 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:34:08,650 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:34:08,651 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:34:09,376 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:34:10,463 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:34:10,463 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:13,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:13,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:13,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:13,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:34:13,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:34:14,439 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:34:14,440 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:34:15,067 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:34:15,285 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:34:15,286 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2309924550203134,
  "recall": 0.09853924238672938,
  "score": 0.13814647691773688,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.72it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:08,  1.80it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:09,  1.83it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:12,  1.76it/s]Extractor Predicting: 22it [00:12,  1.78it/s]Extractor Predicting: 23it [00:13,  1.80it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.79it/s]Extractor Predicting: 26it [00:15,  1.75it/s]Extractor Predicting: 27it [00:15,  1.81it/s]Extractor Predicting: 28it [00:16,  1.83it/s]Extractor Predicting: 29it [00:16,  1.81it/s]Extractor Predicting: 30it [00:17,  1.81it/s]Extractor Predicting: 31it [00:17,  1.81it/s]Extractor Predicting: 32it [00:18,  1.81it/s]Extractor Predicting: 33it [00:18,  1.76it/s]Extractor Predicting: 34it [00:19,  1.73it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:20,  1.77it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:22,  1.73it/s]Extractor Predicting: 40it [00:22,  1.70it/s]Extractor Predicting: 41it [00:23,  1.68it/s]Extractor Predicting: 42it [00:24,  1.72it/s]Extractor Predicting: 43it [00:24,  1.73it/s]Extractor Predicting: 44it [00:25,  1.73it/s]Extractor Predicting: 45it [00:26,  1.59it/s]Extractor Predicting: 46it [00:26,  1.58it/s]Extractor Predicting: 47it [00:27,  1.61it/s]Extractor Predicting: 48it [00:27,  1.60it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.67it/s]Extractor Predicting: 51it [00:29,  1.72it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:30,  1.70it/s]Extractor Predicting: 54it [00:31,  1.65it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:32,  1.61it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:33,  1.66it/s]Extractor Predicting: 59it [00:34,  1.60it/s]Extractor Predicting: 60it [00:35,  1.63it/s]Extractor Predicting: 61it [00:35,  1.64it/s]Extractor Predicting: 62it [00:36,  1.68it/s]Extractor Predicting: 63it [00:36,  1.66it/s]Extractor Predicting: 64it [00:37,  1.68it/s]Extractor Predicting: 65it [00:38,  1.71it/s]Extractor Predicting: 66it [00:38,  1.71it/s]Extractor Predicting: 67it [00:39,  1.70it/s]Extractor Predicting: 68it [00:39,  1.70it/s]Extractor Predicting: 69it [00:40,  1.67it/s]Extractor Predicting: 70it [00:41,  1.64it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:42,  1.65it/s]Extractor Predicting: 74it [00:43,  1.66it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:44,  1.66it/s]Extractor Predicting: 77it [00:45,  1.66it/s]Extractor Predicting: 78it [00:45,  1.61it/s]Extractor Predicting: 79it [00:46,  1.51it/s]Extractor Predicting: 80it [00:47,  1.59it/s]Extractor Predicting: 81it [00:47,  1.58it/s]Extractor Predicting: 82it [00:48,  1.57it/s]Extractor Predicting: 83it [00:49,  1.59it/s]Extractor Predicting: 84it [00:49,  1.62it/s]Extractor Predicting: 85it [00:50,  1.61it/s]Extractor Predicting: 86it [00:51,  1.59it/s]Extractor Predicting: 87it [00:51,  1.63it/s]Extractor Predicting: 88it [00:52,  1.65it/s]Extractor Predicting: 89it [00:52,  1.70it/s]Extractor Predicting: 90it [00:53,  1.69it/s]Extractor Predicting: 91it [00:53,  1.68it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:55,  1.71it/s]Extractor Predicting: 94it [00:55,  1.72it/s]Extractor Predicting: 95it [00:56,  1.70it/s]Extractor Predicting: 96it [00:56,  1.74it/s]Extractor Predicting: 97it [00:57,  1.76it/s]Extractor Predicting: 98it [00:57,  1.73it/s]Extractor Predicting: 99it [00:58,  1.70it/s]Extractor Predicting: 100it [00:59,  1.73it/s]Extractor Predicting: 101it [00:59,  1.72it/s]Extractor Predicting: 102it [01:00,  1.68it/s]Extractor Predicting: 103it [01:00,  1.67it/s]Extractor Predicting: 104it [01:01,  1.71it/s]Extractor Predicting: 105it [01:02,  1.66it/s]Extractor Predicting: 106it [01:02,  1.68it/s]Extractor Predicting: 107it [01:03,  1.69it/s]Extractor Predicting: 108it [01:03,  1.65it/s]Extractor Predicting: 109it [01:04,  1.65it/s]Extractor Predicting: 110it [01:05,  1.66it/s]Extractor Predicting: 111it [01:05,  1.66it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:06,  1.69it/s]Extractor Predicting: 114it [01:07,  1.69it/s]Extractor Predicting: 115it [01:08,  1.69it/s]Extractor Predicting: 116it [01:08,  1.70it/s]Extractor Predicting: 117it [01:09,  1.72it/s]Extractor Predicting: 118it [01:09,  1.77it/s]Extractor Predicting: 119it [01:10,  1.77it/s]Extractor Predicting: 120it [01:10,  1.77it/s]Extractor Predicting: 121it [01:11,  1.77it/s]Extractor Predicting: 122it [01:12,  1.73it/s]Extractor Predicting: 123it [01:12,  1.71it/s]Extractor Predicting: 124it [01:13,  1.74it/s]Extractor Predicting: 125it [01:13,  1.73it/s]Extractor Predicting: 126it [01:14,  1.71it/s]Extractor Predicting: 127it [01:15,  1.68it/s]Extractor Predicting: 128it [01:15,  1.63it/s]Extractor Predicting: 129it [01:16,  1.62it/s]Extractor Predicting: 130it [01:16,  1.61it/s]Extractor Predicting: 131it [01:17,  1.59it/s]Extractor Predicting: 132it [01:18,  1.61it/s]Extractor Predicting: 133it [01:18,  1.63it/s]Extractor Predicting: 134it [01:19,  1.65it/s]Extractor Predicting: 135it [01:20,  1.60it/s]Extractor Predicting: 136it [01:20,  1.59it/s]Extractor Predicting: 137it [01:21,  1.58it/s]Extractor Predicting: 138it [01:21,  1.61it/s]Extractor Predicting: 139it [01:22,  1.59it/s]Extractor Predicting: 140it [01:23,  1.57it/s]Extractor Predicting: 141it [01:23,  1.58it/s]Extractor Predicting: 142it [01:24,  1.58it/s]Extractor Predicting: 143it [01:25,  1.61it/s]Extractor Predicting: 144it [01:25,  1.58it/s]Extractor Predicting: 145it [01:26,  1.58it/s]Extractor Predicting: 146it [01:27,  1.59it/s]Extractor Predicting: 147it [01:27,  1.60it/s]Extractor Predicting: 148it [01:28,  1.61it/s]Extractor Predicting: 149it [01:28,  1.60it/s]Extractor Predicting: 150it [01:29,  1.61it/s]Extractor Predicting: 151it [01:30,  1.60it/s]Extractor Predicting: 152it [01:30,  1.61it/s]Extractor Predicting: 153it [01:31,  1.59it/s]Extractor Predicting: 154it [01:32,  1.57it/s]Extractor Predicting: 155it [01:32,  1.59it/s]Extractor Predicting: 156it [01:33,  1.65it/s]Extractor Predicting: 157it [01:33,  1.71it/s]Extractor Predicting: 158it [01:34,  1.71it/s]Extractor Predicting: 159it [01:34,  1.72it/s]Extractor Predicting: 160it [01:35,  1.69it/s]Extractor Predicting: 161it [01:36,  1.74it/s]Extractor Predicting: 162it [01:36,  1.70it/s]Extractor Predicting: 163it [01:37,  1.69it/s]Extractor Predicting: 164it [01:37,  1.71it/s]Extractor Predicting: 165it [01:38,  1.59it/s]Extractor Predicting: 166it [01:39,  1.62it/s]Extractor Predicting: 167it [01:39,  1.71it/s]Extractor Predicting: 168it [01:40,  1.75it/s]Extractor Predicting: 169it [01:40,  1.80it/s]Extractor Predicting: 170it [01:41,  1.75it/s]Extractor Predicting: 171it [01:41,  1.69it/s]Extractor Predicting: 172it [01:42,  1.64it/s]Extractor Predicting: 173it [01:43,  1.69it/s]Extractor Predicting: 173it [01:43,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:08,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:08,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:08,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:08,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:08,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:36:09,301 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:36:09,302 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:09,770 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:36:10,857 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:36:10,857 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:12,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:12,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:12,411 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:12,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:36:12,412 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:36:12,936 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:36:12,937 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:36:13,307 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:36:13,559 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:36:13,559 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.19362084456424078,
  "recall": 0.10395561987457791,
  "score": 0.13527934714375392,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.85it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:10,  1.73it/s]Extractor Predicting: 17it [00:10,  1.83it/s]Extractor Predicting: 18it [00:11,  1.83it/s]Extractor Predicting: 19it [00:11,  1.84it/s]Extractor Predicting: 20it [00:12,  1.89it/s]Extractor Predicting: 21it [00:12,  1.93it/s]Extractor Predicting: 22it [00:13,  1.90it/s]Extractor Predicting: 23it [00:13,  1.90it/s]Extractor Predicting: 24it [00:14,  1.87it/s]Extractor Predicting: 25it [00:14,  1.89it/s]Extractor Predicting: 26it [00:15,  1.92it/s]Extractor Predicting: 27it [00:15,  1.89it/s]Extractor Predicting: 28it [00:16,  1.91it/s]Extractor Predicting: 29it [00:16,  1.92it/s]Extractor Predicting: 30it [00:17,  1.94it/s]Extractor Predicting: 31it [00:17,  1.97it/s]Extractor Predicting: 32it [00:18,  1.96it/s]Extractor Predicting: 33it [00:18,  1.97it/s]Extractor Predicting: 34it [00:19,  1.96it/s]Extractor Predicting: 35it [00:20,  1.97it/s]Extractor Predicting: 36it [00:20,  1.90it/s]Extractor Predicting: 37it [00:21,  1.86it/s]Extractor Predicting: 38it [00:21,  1.93it/s]Extractor Predicting: 39it [00:22,  1.92it/s]Extractor Predicting: 40it [00:22,  1.93it/s]Extractor Predicting: 41it [00:23,  1.78it/s]Extractor Predicting: 42it [00:23,  1.85it/s]Extractor Predicting: 43it [00:24,  1.84it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.63it/s]Extractor Predicting: 46it [00:26,  1.57it/s]Extractor Predicting: 47it [00:27,  1.55it/s]Extractor Predicting: 48it [00:27,  1.53it/s]Extractor Predicting: 49it [00:28,  1.54it/s]Extractor Predicting: 50it [00:29,  1.52it/s]Extractor Predicting: 51it [00:29,  1.52it/s]Extractor Predicting: 52it [00:30,  1.50it/s]Extractor Predicting: 53it [00:31,  1.49it/s]Extractor Predicting: 54it [00:31,  1.49it/s]Extractor Predicting: 55it [00:32,  1.51it/s]Extractor Predicting: 56it [00:33,  1.49it/s]Extractor Predicting: 57it [00:33,  1.47it/s]Extractor Predicting: 58it [00:34,  1.48it/s]Extractor Predicting: 59it [00:34,  1.76it/s]Extractor Predicting: 59it [00:34,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-29 11:36:51,218 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:36:51,272 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:36:51,368 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:36:51,370 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 11:36:51,412 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:37:05,108 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 11:37:05,145 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 11:37:05,479 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:37:05,480 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:37:05,577 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:37:05,636 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.72018779342723,
  "recall": 0.2479793081150986,
  "score": 0.3689273689273689,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 11:37:06,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:06,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:07,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:08,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:08,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:09,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:09,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:10,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:10,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:11,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:11,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:12,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:12,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:13,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:13,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:14,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:14,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:15,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:15,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:10<01:31, 10.20s/it][WARNING|generation_utils.py:914] 2023-08-29 11:37:16,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:16,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:17,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:17,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:18,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:19,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:19,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:20,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:20,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:21,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:21,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:22,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:22,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:23,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:23,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:24,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:24,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:25,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:25,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:26,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:26,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:21<01:25, 10.68s/it][WARNING|generation_utils.py:914] 2023-08-29 11:37:27,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:27,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:28,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:28,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:29,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:29,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:30,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:30,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:31,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:31,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:32,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:33,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:33,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:34,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:35,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:35,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:36,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:36,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:37,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:37,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:32<01:15, 10.74s/it][WARNING|generation_utils.py:914] 2023-08-29 11:37:38,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:38,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:39,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:39,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:40,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:41,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:41,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:42,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:43,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:43,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:44,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:44,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:45,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:45,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:47,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:47,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:48,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:48,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:49,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:49,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:44<01:08, 11.41s/it][WARNING|generation_utils.py:914] 2023-08-29 11:37:50,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:51,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:51,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:52,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:52,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:53,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:53,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:53,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:54,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:54,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:55,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:55,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:56,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:56,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:57,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:57,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:58,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:58,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:59,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:37:59,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:00,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [00:54<00:54, 10.98s/it][WARNING|generation_utils.py:914] 2023-08-29 11:38:00,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:01,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:02,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:02,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:03,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:03,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:04,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:04,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:05,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:06,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:06,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:07,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:07,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:08,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:09,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:10,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:10,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:11,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:11,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:06<00:44, 11.23s/it][WARNING|generation_utils.py:914] 2023-08-29 11:38:12,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:12,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:13,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:13,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:14,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:14,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:15,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:15,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:16,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:17,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:17,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:17,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:18,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:19,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:19,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:19,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:20,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:20,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:21,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:21,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:22,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:16<00:32, 10.94s/it][WARNING|generation_utils.py:914] 2023-08-29 11:38:22,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:23,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:23,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:24,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:24,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:25,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:25,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:26,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:26,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:26,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:27,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:28,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:29,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:29,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:30,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:30,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:30,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:25<00:20, 10.15s/it][WARNING|generation_utils.py:914] 2023-08-29 11:38:31,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:31,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:32,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:32,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:33,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:33,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:34,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:34,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:35,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:35,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:36,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:36,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:37,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:37,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:38,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:38,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:39,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:39,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:40,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:40,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [01:34<00:10, 10.03s/it][WARNING|generation_utils.py:914] 2023-08-29 11:38:41,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:41,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:42,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:42,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:43,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:43,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:44,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:44,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:45,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:45,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:46,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:46,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:47,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:47,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:48,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:48,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:49,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:49,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:50,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:38:50,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [01:45<00:00, 10.08s/it]Generating: 100%|██████████| 10/10 [01:45<00:00, 10.52s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:38:59,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:38:59,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:38:59,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:38:59,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:38:59,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:39:00,844 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:39:00,845 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:39:01,531 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:39:02,757 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:39:02,820 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:05,945 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:05,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:05,948 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:05,948 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:39:05,948 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:39:06,736 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:39:06,775 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:39:07,409 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:39:07,656 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:39:07,656 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 411, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 474, 'raw': 480}
{'target': 600, 'success': 506, 'raw': 512}
{'target': 600, 'success': 538, 'raw': 544}
{'target': 600, 'success': 570, 'raw': 576}
{'target': 600, 'success': 602, 'raw': 608}
{'prompt': 'Relation : given name .', 'success_rate': 0.9901315789473685, 'errors': {'', "('Carl D. Friesen', 'given name', '', 'Carl D. Friesen ( born May 2 , 1982 ) is an American retired professional ice hockey defenceman who played with the Vancouver Canucks from 1992 to 1999 .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : languages spoken, written or signed .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.9181547619047619, 'errors': {'', "('Pittsburgh Pirates', 'lowest point', '', 'He played a total of 29 games for the Pittsburgh Pirates , and had a career average of 19 . 3 WAR .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : genre .', 'success_rate': 0.9375, 'errors': {'', "('The Last of the Champs', 'genre', '', 'It features the same set of songs as The Last of the Champs and is a sequel to their 2014 album , A Year in the Life .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : is a list of .', 'success_rate': 0.9285714285714286, 'errors': {'', "('list of', 'is a list of', '', 'is a list of .')", 'not enough values to unpack (expected 2, got 1)', "('list', 'is a list of', '', 'It is the second largest list of .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 352, 'raw': 352}
{'target': 600, 'success': 384, 'raw': 384}
{'target': 600, 'success': 416, 'raw': 416}
{'target': 600, 'success': 448, 'raw': 448}
{'target': 600, 'success': 480, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 542, 'raw': 544}
{'target': 600, 'success': 574, 'raw': 576}
{'target': 600, 'success': 606, 'raw': 608}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9967105263157895, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.9546875, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 6281
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6381, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.65it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.67it/s]Extractor Estimating: 7it [00:04,  1.63it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.68it/s]Extractor Estimating: 10it [00:06,  1.69it/s]Extractor Estimating: 11it [00:06,  1.73it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.69it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.73it/s]Extractor Estimating: 18it [00:10,  1.73it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:11,  1.71it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.74it/s]Extractor Estimating: 23it [00:13,  1.75it/s]Extractor Estimating: 24it [00:14,  1.74it/s]Extractor Estimating: 25it [00:14,  1.71it/s]Extractor Estimating: 26it [00:15,  1.68it/s]Extractor Estimating: 27it [00:15,  1.75it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:16,  1.86it/s]Extractor Estimating: 30it [00:17,  1.94it/s]Extractor Estimating: 31it [00:17,  1.92it/s]Extractor Estimating: 32it [00:18,  1.89it/s]Extractor Estimating: 33it [00:18,  1.99it/s]Extractor Estimating: 34it [00:19,  2.00it/s]Extractor Estimating: 35it [00:19,  1.99it/s]Extractor Estimating: 36it [00:20,  2.01it/s]Extractor Estimating: 37it [00:20,  1.92it/s]Extractor Estimating: 38it [00:21,  1.93it/s]Extractor Estimating: 39it [00:22,  1.95it/s]Extractor Estimating: 40it [00:22,  1.89it/s]Extractor Estimating: 41it [00:23,  1.89it/s]Extractor Estimating: 42it [00:23,  1.96it/s]Extractor Estimating: 43it [00:24,  1.96it/s]Extractor Estimating: 44it [00:24,  1.95it/s]Extractor Estimating: 45it [00:25,  1.94it/s]Extractor Estimating: 46it [00:25,  1.93it/s]Extractor Estimating: 47it [00:26,  1.94it/s]Extractor Estimating: 48it [00:26,  2.02it/s]Extractor Estimating: 49it [00:27,  1.99it/s]Extractor Estimating: 50it [00:27,  2.02it/s]Extractor Estimating: 51it [00:28,  1.94it/s]Extractor Estimating: 52it [00:28,  1.84it/s]Extractor Estimating: 53it [00:29,  1.82it/s]Extractor Estimating: 54it [00:29,  1.83it/s]Extractor Estimating: 55it [00:30,  1.83it/s]Extractor Estimating: 56it [00:31,  1.74it/s]Extractor Estimating: 57it [00:31,  1.65it/s]Extractor Estimating: 58it [00:32,  1.64it/s]Extractor Estimating: 59it [00:32,  1.68it/s]Extractor Estimating: 60it [00:33,  1.77it/s]Extractor Estimating: 61it [00:33,  1.79it/s]Extractor Estimating: 62it [00:34,  1.81it/s]Extractor Estimating: 63it [00:35,  1.80it/s]Extractor Estimating: 64it [00:35,  1.79it/s]Extractor Estimating: 65it [00:36,  1.86it/s]Extractor Estimating: 66it [00:36,  1.86it/s]Extractor Estimating: 67it [00:37,  1.76it/s]Extractor Estimating: 68it [00:37,  1.75it/s]Extractor Estimating: 69it [00:38,  1.74it/s]Extractor Estimating: 70it [00:39,  1.69it/s]Extractor Estimating: 71it [00:39,  1.72it/s]Extractor Estimating: 72it [00:40,  1.77it/s]Extractor Estimating: 73it [00:40,  1.81it/s]Extractor Estimating: 74it [00:41,  1.77it/s]Extractor Estimating: 75it [00:41,  1.80it/s]Extractor Estimating: 76it [00:42,  1.76it/s]Extractor Estimating: 77it [00:42,  1.80it/s]Extractor Estimating: 78it [00:43,  1.80it/s]Extractor Estimating: 79it [00:44,  1.77it/s]Extractor Estimating: 80it [00:44,  1.81it/s]Extractor Estimating: 81it [00:45,  1.78it/s]Extractor Estimating: 82it [00:45,  1.70it/s]Extractor Estimating: 83it [00:46,  1.74it/s]Extractor Estimating: 84it [00:46,  1.74it/s]Extractor Estimating: 85it [00:47,  1.72it/s]Extractor Estimating: 86it [00:48,  1.75it/s]Extractor Estimating: 87it [00:48,  1.78it/s]Extractor Estimating: 88it [00:49,  1.76it/s]Extractor Estimating: 89it [00:49,  1.80it/s]Extractor Estimating: 90it [00:50,  1.77it/s]Extractor Estimating: 91it [00:51,  1.62it/s]Extractor Estimating: 92it [00:51,  1.63it/s]Extractor Estimating: 93it [00:52,  1.63it/s]Extractor Estimating: 94it [00:52,  1.66it/s]Extractor Estimating: 95it [00:53,  1.71it/s]Extractor Estimating: 96it [00:54,  1.68it/s]Extractor Estimating: 97it [00:54,  1.75it/s]Extractor Estimating: 98it [00:55,  1.72it/s]Extractor Estimating: 99it [00:55,  1.75it/s]Extractor Estimating: 100it [00:56,  1.73it/s]Extractor Estimating: 101it [00:56,  1.89it/s]Extractor Estimating: 102it [00:57,  1.98it/s]Extractor Estimating: 103it [00:57,  2.00it/s]Extractor Estimating: 104it [00:58,  1.98it/s]Extractor Estimating: 105it [00:58,  2.03it/s]Extractor Estimating: 106it [00:59,  2.05it/s]Extractor Estimating: 107it [00:59,  2.04it/s]Extractor Estimating: 108it [01:00,  2.02it/s]Extractor Estimating: 109it [01:00,  2.06it/s]Extractor Estimating: 110it [01:01,  2.02it/s]Extractor Estimating: 111it [01:01,  1.94it/s]Extractor Estimating: 112it [01:02,  1.99it/s]Extractor Estimating: 113it [01:02,  2.02it/s]Extractor Estimating: 114it [01:03,  1.98it/s]Extractor Estimating: 115it [01:03,  2.00it/s]Extractor Estimating: 116it [01:04,  2.06it/s]Extractor Estimating: 117it [01:04,  2.11it/s]Extractor Estimating: 118it [01:04,  2.16it/s]Extractor Estimating: 119it [01:05,  2.15it/s]Extractor Estimating: 120it [01:05,  2.11it/s]Extractor Estimating: 121it [01:06,  2.09it/s]Extractor Estimating: 122it [01:06,  2.14it/s]Extractor Estimating: 123it [01:07,  2.12it/s]Extractor Estimating: 124it [01:07,  2.17it/s]Extractor Estimating: 125it [01:08,  2.10it/s]Extractor Estimating: 126it [01:08,  1.94it/s]Extractor Estimating: 127it [01:09,  1.80it/s]Extractor Estimating: 128it [01:10,  1.75it/s]Extractor Estimating: 129it [01:10,  1.71it/s]Extractor Estimating: 130it [01:11,  1.71it/s]Extractor Estimating: 131it [01:11,  1.70it/s]Extractor Estimating: 132it [01:12,  1.64it/s]Extractor Estimating: 133it [01:13,  1.64it/s]Extractor Estimating: 134it [01:13,  1.67it/s]Extractor Estimating: 135it [01:14,  1.64it/s]Extractor Estimating: 136it [01:14,  1.68it/s]Extractor Estimating: 137it [01:15,  1.61it/s]Extractor Estimating: 138it [01:16,  1.66it/s]Extractor Estimating: 139it [01:16,  1.62it/s]Extractor Estimating: 140it [01:32,  5.01s/it]Extractor Estimating: 141it [01:32,  3.72s/it]Extractor Estimating: 142it [01:33,  2.80s/it]Extractor Estimating: 143it [01:34,  2.14s/it]Extractor Estimating: 144it [01:34,  1.68s/it]Extractor Estimating: 145it [01:35,  1.38s/it]Extractor Estimating: 146it [01:36,  1.16s/it]Extractor Estimating: 147it [01:36,  1.01it/s]Extractor Estimating: 148it [01:37,  1.15it/s]Extractor Estimating: 149it [01:37,  1.27it/s]Extractor Estimating: 150it [01:38,  1.40it/s]Extractor Estimating: 151it [01:38,  1.56it/s]Extractor Estimating: 152it [01:39,  1.64it/s]Extractor Estimating: 153it [01:39,  1.73it/s]Extractor Estimating: 154it [01:40,  1.82it/s]Extractor Estimating: 155it [01:40,  1.85it/s]Extractor Estimating: 156it [01:41,  1.87it/s]Extractor Estimating: 157it [01:41,  1.94it/s]Extractor Estimating: 158it [01:42,  1.86it/s]Extractor Estimating: 159it [01:42,  1.93it/s]Extractor Estimating: 160it [01:43,  1.96it/s]Extractor Estimating: 161it [01:43,  2.03it/s]Extractor Estimating: 162it [01:44,  2.03it/s]Extractor Estimating: 163it [01:44,  2.03it/s]Extractor Estimating: 164it [01:45,  2.06it/s]Extractor Estimating: 165it [01:45,  2.04it/s]Extractor Estimating: 166it [01:46,  1.95it/s]Extractor Estimating: 167it [01:46,  2.06it/s]Extractor Estimating: 168it [01:47,  2.11it/s]Extractor Estimating: 169it [01:47,  2.06it/s]Extractor Estimating: 170it [01:48,  2.13it/s]Extractor Estimating: 171it [01:48,  2.12it/s]Extractor Estimating: 172it [01:49,  2.12it/s]Extractor Estimating: 173it [01:49,  2.13it/s]Extractor Estimating: 174it [01:50,  2.14it/s]Extractor Estimating: 175it [01:50,  2.00it/s]Extractor Estimating: 176it [01:51,  1.95it/s]Extractor Estimating: 177it [01:51,  1.79it/s]Extractor Estimating: 178it [01:52,  1.76it/s]Extractor Estimating: 179it [01:52,  1.80it/s]Extractor Estimating: 180it [01:53,  1.87it/s]Extractor Estimating: 181it [01:53,  1.92it/s]Extractor Estimating: 182it [01:54,  1.91it/s]Extractor Estimating: 183it [01:54,  1.98it/s]Extractor Estimating: 184it [01:55,  1.95it/s]Extractor Estimating: 185it [01:55,  1.95it/s]Extractor Estimating: 186it [01:56,  1.99it/s]Extractor Estimating: 187it [01:56,  1.96it/s]Extractor Estimating: 188it [01:57,  1.96it/s]Extractor Estimating: 189it [01:57,  2.01it/s]Extractor Estimating: 190it [01:58,  2.01it/s]Extractor Estimating: 191it [01:58,  2.04it/s]Extractor Estimating: 192it [01:59,  2.05it/s]Extractor Estimating: 193it [01:59,  2.01it/s]Extractor Estimating: 194it [02:00,  1.97it/s]Extractor Estimating: 195it [02:01,  1.93it/s]Extractor Estimating: 196it [02:01,  1.99it/s]Extractor Estimating: 197it [02:01,  2.05it/s]Extractor Estimating: 198it [02:02,  1.88it/s]Extractor Estimating: 199it [02:03,  1.92it/s]Extractor Estimating: 200it [02:03,  1.93it/s]Extractor Estimating: 201it [02:04,  1.91it/s]Extractor Estimating: 202it [02:04,  1.95it/s]Extractor Estimating: 203it [02:05,  1.97it/s]Extractor Estimating: 204it [02:05,  1.98it/s]Extractor Estimating: 205it [02:06,  2.07it/s]Extractor Estimating: 206it [02:06,  1.98it/s]Extractor Estimating: 207it [02:07,  1.90it/s]Extractor Estimating: 208it [02:07,  1.93it/s]Extractor Estimating: 209it [02:08,  2.01it/s]Extractor Estimating: 210it [02:08,  2.01it/s]Extractor Estimating: 211it [02:09,  1.99it/s]Extractor Estimating: 212it [02:09,  1.97it/s]Extractor Estimating: 213it [02:10,  1.86it/s]Extractor Estimating: 214it [02:10,  1.82it/s]Extractor Estimating: 215it [02:11,  1.82it/s]Extractor Estimating: 216it [02:11,  1.87it/s]Extractor Estimating: 217it [02:12,  1.90it/s]Extractor Estimating: 218it [02:12,  1.86it/s]Extractor Estimating: 219it [02:13,  1.76it/s]Extractor Estimating: 220it [02:14,  1.78it/s]Extractor Estimating: 221it [02:14,  1.78it/s]Extractor Estimating: 222it [02:15,  1.79it/s]Extractor Estimating: 223it [02:15,  1.78it/s]Extractor Estimating: 224it [02:16,  1.84it/s]Extractor Estimating: 225it [02:16,  1.74it/s]Extractor Estimating: 226it [02:17,  1.82it/s]Extractor Estimating: 227it [02:18,  1.81it/s]Extractor Estimating: 228it [02:18,  1.79it/s]Extractor Estimating: 229it [02:19,  1.79it/s]Extractor Estimating: 230it [02:19,  1.82it/s]Extractor Estimating: 231it [02:20,  1.80it/s]Extractor Estimating: 232it [02:20,  1.87it/s]Extractor Estimating: 233it [02:21,  1.88it/s]Extractor Estimating: 234it [02:21,  1.82it/s]Extractor Estimating: 235it [02:22,  1.82it/s]Extractor Estimating: 236it [02:22,  1.85it/s]Extractor Estimating: 236it [02:22,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:48,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:48,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:48,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:48,856 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:48,856 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:41:49,414 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:41:49,415 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:49,746 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:50,882 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:50,882 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:52,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:52,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:52,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:52,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:41:52,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:41:52,943 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:41:52,944 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:41:53,241 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:41:53,449 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:41:53,450 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 12:55:32,819 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 12:55:33,405 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 4661 mean pseudo reward: 0.9665431303706531
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl'}
train vocab size: 16148
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16248, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16248, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.907, loss:307.0240
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 5, avg_time 0.883, loss:286.1383
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 105, avg_time 0.901, loss:267.3861
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 10, avg_time 0.893, loss:257.6032
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 110, avg_time 0.896, loss:247.7741
>> valid entity prec:0.4854, rec:0.4167, f1:0.4485
>> valid relation prec:0.1841, rec:0.0940, f1:0.1244
>> valid relation with NER prec:0.1841, rec:0.0940, f1:0.1244
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 15, avg_time 2.199, loss:267.6489
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 115, avg_time 0.895, loss:247.5734
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 20, avg_time 0.894, loss:257.7335
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 120, avg_time 0.908, loss:251.2398
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 25, avg_time 0.897, loss:258.3745
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4839, rec:0.3579, f1:0.4115
>> valid relation prec:0.1676, rec:0.0669, f1:0.0957
>> valid relation with NER prec:0.1676, rec:0.0669, f1:0.0957
g_step 1100, step 125, avg_time 2.186, loss:256.5482
g_step 1200, step 30, avg_time 0.887, loss:230.4809
g_step 1300, step 130, avg_time 0.910, loss:243.1507
g_step 1400, step 35, avg_time 0.907, loss:235.6647
g_step 1500, step 135, avg_time 0.893, loss:229.1936
>> valid entity prec:0.4599, rec:0.4030, f1:0.4295
>> valid relation prec:0.1536, rec:0.0734, f1:0.0993
>> valid relation with NER prec:0.1536, rec:0.0734, f1:0.0993
g_step 1600, step 40, avg_time 2.195, loss:223.0861
g_step 1700, step 140, avg_time 0.899, loss:221.0619
g_step 1800, step 45, avg_time 0.892, loss:207.3375
g_step 1900, step 145, avg_time 0.897, loss:216.2274
g_step 2000, step 50, avg_time 0.894, loss:200.3292
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4678, rec:0.4270, f1:0.4465
>> valid relation prec:0.1716, rec:0.0811, f1:0.1101
>> valid relation with NER prec:0.1716, rec:0.0811, f1:0.1101
g_step 2100, step 150, avg_time 2.201, loss:198.7966
g_step 2200, step 55, avg_time 0.913, loss:182.7585
g_step 2300, step 155, avg_time 0.901, loss:198.8743
g_step 2400, step 60, avg_time 0.898, loss:184.9010
g_step 2500, step 160, avg_time 0.909, loss:166.9246
>> valid entity prec:0.4492, rec:0.4881, f1:0.4679
>> valid relation prec:0.1612, rec:0.0855, f1:0.1118
>> valid relation with NER prec:0.1612, rec:0.0855, f1:0.1118
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 65, avg_time 2.213, loss:184.9283
g_step 2700, step 165, avg_time 0.905, loss:183.0120
g_step 2800, step 70, avg_time 0.903, loss:159.2337
g_step 2900, step 170, avg_time 0.896, loss:175.0213
g_step 3000, step 75, avg_time 0.894, loss:168.3285
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4643, rec:0.4695, f1:0.4669
>> valid relation prec:0.1552, rec:0.0992, f1:0.1210
>> valid relation with NER prec:0.1552, rec:0.0992, f1:0.1210
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 175, avg_time 2.212, loss:175.5484
g_step 3200, step 80, avg_time 0.898, loss:147.0437
g_step 3300, step 180, avg_time 0.893, loss:163.7394
g_step 3400, step 85, avg_time 0.888, loss:139.1028
g_step 3500, step 185, avg_time 0.906, loss:156.6310
>> valid entity prec:0.4490, rec:0.3507, f1:0.3938
>> valid relation prec:0.1377, rec:0.0600, f1:0.0836
>> valid relation with NER prec:0.1377, rec:0.0600, f1:0.0836
g_step 3600, step 90, avg_time 2.184, loss:147.7681
g_step 3700, step 190, avg_time 0.889, loss:142.6445
g_step 3800, step 95, avg_time 0.889, loss:133.9545
g_step 3900, step 195, avg_time 0.898, loss:142.2502
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 12:55:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 12:55:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_12-55-32_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 12:55:34 - WARNING - datasets.builder -   Using custom data configuration default-3b9667c943058781
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3b9667c943058781/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 12:55:37,596 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:55:37,597 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:55:37,598 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:55:37,599 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:55:37,728 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:55:37,797 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 12:55:38,420 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:55:41,683 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 12:55:41,734 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3b9667c943058781/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.23ba/s] 40%|████      | 2/5 [00:00<00:00,  3.39ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.06ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.49ba/s]100%|██████████| 5/5 [00:01<00:00,  4.02ba/s]100%|██████████| 5/5 [00:01<00:00,  3.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.46ba/s] 40%|████      | 2/5 [00:00<00:00,  3.42ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.89ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.15ba/s]100%|██████████| 5/5 [00:01<00:00,  4.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.62ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.15ba/s]100%|██████████| 5/5 [00:00<00:00,  8.63ba/s]100%|██████████| 5/5 [00:00<00:00,  7.73ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.50ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.01ba/s]100%|██████████| 5/5 [00:00<00:00,  8.82ba/s]
[INFO|trainer.py:414] 2023-08-29 12:55:47,997 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 12:55:48,259 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 12:55:48,259 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-29 12:55:48,259 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 12:55:48,259 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 12:55:48,259 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 12:55:48,259 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 12:55:48,259 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:56,  3.34it/s]  1%|          | 2/390 [00:00<01:53,  3.41it/s]  1%|          | 3/390 [00:00<01:52,  3.44it/s]  1%|          | 4/390 [00:01<01:52,  3.43it/s]  1%|▏         | 5/390 [00:01<01:52,  3.42it/s]  2%|▏         | 6/390 [00:01<01:52,  3.42it/s]  2%|▏         | 7/390 [00:02<01:51,  3.42it/s]  2%|▏         | 8/390 [00:02<01:51,  3.42it/s]  2%|▏         | 9/390 [00:02<01:51,  3.41it/s]  3%|▎         | 10/390 [00:02<01:51,  3.42it/s]  3%|▎         | 11/390 [00:03<01:54,  3.32it/s]  3%|▎         | 12/390 [00:03<01:52,  3.35it/s]  3%|▎         | 13/390 [00:03<01:52,  3.36it/s]  4%|▎         | 14/390 [00:04<01:51,  3.38it/s]  4%|▍         | 15/390 [00:04<01:50,  3.38it/s]  4%|▍         | 16/390 [00:04<01:50,  3.39it/s]  4%|▍         | 17/390 [00:05<01:49,  3.39it/s]  5%|▍         | 18/390 [00:05<01:49,  3.40it/s]  5%|▍         | 19/390 [00:05<01:49,  3.40it/s]  5%|▌         | 20/390 [00:05<01:48,  3.40it/s]  5%|▌         | 21/390 [00:06<01:48,  3.40it/s]  6%|▌         | 22/390 [00:06<01:54,  3.22it/s]  6%|▌         | 23/390 [00:06<01:51,  3.28it/s]  6%|▌         | 24/390 [00:07<01:50,  3.31it/s]  6%|▋         | 25/390 [00:07<01:49,  3.34it/s]  7%|▋         | 26/390 [00:07<01:48,  3.36it/s]  7%|▋         | 27/390 [00:07<01:47,  3.38it/s]  7%|▋         | 28/390 [00:08<01:47,  3.38it/s]  7%|▋         | 29/390 [00:08<01:46,  3.39it/s]  8%|▊         | 30/390 [00:08<01:46,  3.39it/s]  8%|▊         | 31/390 [00:09<01:45,  3.40it/s]  8%|▊         | 32/390 [00:09<01:45,  3.40it/s]  8%|▊         | 33/390 [00:09<01:49,  3.26it/s]  9%|▊         | 34/390 [00:10<01:47,  3.30it/s]  9%|▉         | 35/390 [00:10<01:46,  3.33it/s]  9%|▉         | 36/390 [00:10<01:45,  3.35it/s]  9%|▉         | 37/390 [00:10<01:44,  3.37it/s] 10%|▉         | 38/390 [00:11<01:44,  3.38it/s] 10%|█         | 39/390 [00:11<01:43,  3.39it/s] 10%|█         | 40/390 [00:11<01:43,  3.39it/s] 11%|█         | 41/390 [00:12<01:42,  3.40it/s] 11%|█         | 42/390 [00:12<01:42,  3.40it/s] 11%|█         | 43/390 [00:12<01:42,  3.40it/s] 11%|█▏        | 44/390 [00:13<01:44,  3.31it/s] 12%|█▏        | 45/390 [00:13<01:42,  3.35it/s] 12%|█▏        | 46/390 [00:13<01:41,  3.38it/s] 12%|█▏        | 47/390 [00:13<01:40,  3.40it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.42it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.43it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 51/390 [00:15<01:38,  3.44it/s] 13%|█▎        | 52/390 [00:15<01:37,  3.45it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.45it/s] 14%|█▍        | 54/390 [00:15<01:37,  3.44it/s] 14%|█▍        | 55/390 [00:16<01:41,  3.31it/s] 14%|█▍        | 56/390 [00:16<01:39,  3.36it/s] 15%|█▍        | 57/390 [00:16<01:38,  3.39it/s] 15%|█▍        | 58/390 [00:17<01:37,  3.40it/s] 15%|█▌        | 59/390 [00:17<01:36,  3.41it/s] 15%|█▌        | 60/390 [00:17<01:36,  3.43it/s] 16%|█▌        | 61/390 [00:18<01:35,  3.43it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.44it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.44it/s] 16%|█▋        | 64/390 [00:18<01:34,  3.44it/s] 17%|█▋        | 65/390 [00:19<01:34,  3.45it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.45it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.45it/s] 17%|█▋        | 68/390 [00:20<01:33,  3.45it/s] 18%|█▊        | 69/390 [00:20<01:33,  3.45it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.45it/s] 18%|█▊        | 71/390 [00:20<01:35,  3.33it/s] 18%|█▊        | 72/390 [00:21<01:34,  3.36it/s] 19%|█▊        | 73/390 [00:21<01:33,  3.39it/s] 19%|█▉        | 74/390 [00:21<01:32,  3.41it/s] 19%|█▉        | 75/390 [00:22<01:32,  3.42it/s] 19%|█▉        | 76/390 [00:22<01:31,  3.43it/s] 20%|█▉        | 77/390 [00:22<01:31,  3.44it/s] 20%|██        | 78/390 [00:22<01:30,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 12:56:12,046 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:56:12,095 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 12:56:12,095 >>   Batch size = 8

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 58.06it/s][A
  2%|▏         | 12/505 [00:00<00:09, 49.46it/s][A
  4%|▎         | 18/505 [00:00<00:10, 47.34it/s][A
  5%|▍         | 23/505 [00:00<00:10, 46.53it/s][A
  6%|▌         | 28/505 [00:00<00:10, 46.09it/s][A
  7%|▋         | 33/505 [00:00<00:10, 45.80it/s][A
  8%|▊         | 38/505 [00:00<00:10, 45.22it/s][A
  9%|▊         | 43/505 [00:00<00:10, 44.68it/s][A
 10%|▉         | 48/505 [00:01<00:10, 44.39it/s][A
 10%|█         | 53/505 [00:01<00:10, 44.37it/s][A
 11%|█▏        | 58/505 [00:01<00:10, 44.42it/s][A
 12%|█▏        | 63/505 [00:01<00:09, 44.65it/s][A
 13%|█▎        | 68/505 [00:01<00:09, 44.76it/s][A
 14%|█▍        | 73/505 [00:01<00:09, 44.85it/s][A
 15%|█▌        | 78/505 [00:01<00:09, 44.97it/s][A
 16%|█▋        | 83/505 [00:01<00:09, 44.80it/s][A
 17%|█▋        | 88/505 [00:01<00:09, 44.43it/s][A
 18%|█▊        | 93/505 [00:02<00:09, 44.23it/s][A
 19%|█▉        | 98/505 [00:02<00:09, 44.19it/s][A
 20%|██        | 103/505 [00:02<00:09, 44.25it/s][A
 21%|██▏       | 108/505 [00:02<00:08, 44.49it/s][A
 22%|██▏       | 113/505 [00:02<00:08, 44.65it/s][A
 23%|██▎       | 118/505 [00:02<00:08, 44.82it/s][A
 24%|██▍       | 123/505 [00:02<00:08, 44.85it/s][A
 25%|██▌       | 128/505 [00:02<00:08, 44.57it/s][A
 26%|██▋       | 133/505 [00:03<00:08, 44.41it/s][A
 27%|██▋       | 138/505 [00:03<00:09, 38.64it/s][A
 28%|██▊       | 143/505 [00:03<00:08, 40.39it/s][A
 29%|██▉       | 148/505 [00:03<00:08, 41.68it/s][A
 30%|███       | 153/505 [00:03<00:08, 42.66it/s][A
 31%|███▏      | 158/505 [00:03<00:08, 43.28it/s][A
 32%|███▏      | 163/505 [00:03<00:07, 43.86it/s][A
 33%|███▎      | 168/505 [00:03<00:07, 44.20it/s][A
 34%|███▍      | 173/505 [00:03<00:07, 44.28it/s][A
 35%|███▌      | 178/505 [00:04<00:07, 43.99it/s][A
 36%|███▌      | 183/505 [00:04<00:07, 43.75it/s][A
 37%|███▋      | 188/505 [00:04<00:07, 43.92it/s][A
 38%|███▊      | 193/505 [00:04<00:07, 44.15it/s][A
 39%|███▉      | 198/505 [00:04<00:06, 44.46it/s][A
 40%|████      | 203/505 [00:04<00:06, 44.60it/s][A
 41%|████      | 208/505 [00:04<00:06, 44.66it/s][A
 42%|████▏     | 213/505 [00:04<00:06, 44.80it/s][A
 43%|████▎     | 218/505 [00:04<00:06, 44.70it/s][A
 44%|████▍     | 223/505 [00:05<00:06, 44.35it/s][A
 45%|████▌     | 228/505 [00:05<00:06, 44.14it/s][A
 46%|████▌     | 233/505 [00:05<00:06, 43.99it/s][A
 47%|████▋     | 238/505 [00:05<00:06, 44.24it/s][A
 48%|████▊     | 243/505 [00:05<00:05, 44.51it/s][A
 49%|████▉     | 248/505 [00:05<00:05, 44.65it/s][A
 50%|█████     | 253/505 [00:05<00:05, 44.78it/s][A
 51%|█████     | 258/505 [00:05<00:05, 44.86it/s][A
 52%|█████▏    | 263/505 [00:05<00:05, 44.75it/s][A
 53%|█████▎    | 268/505 [00:06<00:05, 44.48it/s][A
 54%|█████▍    | 273/505 [00:06<00:05, 42.89it/s][A
 55%|█████▌    | 278/505 [00:06<00:05, 43.37it/s][A
 56%|█████▌    | 283/505 [00:06<00:05, 43.71it/s][A
 57%|█████▋    | 288/505 [00:06<00:04, 43.98it/s][A
 58%|█████▊    | 293/505 [00:06<00:04, 44.20it/s][A
 59%|█████▉    | 298/505 [00:06<00:04, 44.48it/s][A
 60%|██████    | 303/505 [00:06<00:04, 44.63it/s][A
 61%|██████    | 308/505 [00:06<00:04, 44.53it/s][A
 62%|██████▏   | 313/505 [00:07<00:04, 44.26it/s][A
 63%|██████▎   | 318/505 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 323/505 [00:07<00:04, 44.26it/s][A
 65%|██████▍   | 328/505 [00:07<00:03, 44.40it/s][A
 66%|██████▌   | 333/505 [00:07<00:03, 44.50it/s][A
 67%|██████▋   | 338/505 [00:07<00:03, 44.58it/s][A
 68%|██████▊   | 343/505 [00:07<00:03, 44.60it/s][A
 69%|██████▉   | 348/505 [00:07<00:03, 44.75it/s][A
 70%|██████▉   | 353/505 [00:07<00:03, 44.57it/s][A
 71%|███████   | 358/505 [00:08<00:03, 44.34it/s][A
 72%|███████▏  | 363/505 [00:08<00:03, 44.32it/s][A
 73%|███████▎  | 368/505 [00:08<00:03, 44.38it/s][A
 74%|███████▍  | 373/505 [00:08<00:02, 44.29it/s][A
 75%|███████▍  | 378/505 [00:08<00:02, 44.51it/s][A
 76%|███████▌  | 383/505 [00:08<00:02, 44.63it/s][A
 77%|███████▋  | 388/505 [00:08<00:02, 44.74it/s][A
 78%|███████▊  | 393/505 [00:08<00:02, 44.77it/s][A
 79%|███████▉  | 398/505 [00:08<00:02, 44.48it/s][A
 80%|███████▉  | 403/505 [00:09<00:02, 44.44it/s][A
 81%|████████  | 408/505 [00:09<00:02, 34.80it/s][A
 82%|████████▏ | 413/505 [00:09<00:02, 37.33it/s][A
 83%|████████▎ | 418/505 [00:09<00:02, 39.32it/s][A
 84%|████████▍ | 423/505 [00:09<00:02, 40.80it/s][A
 85%|████████▍ | 428/505 [00:09<00:01, 41.99it/s][A
 86%|████████▌ | 433/505 [00:09<00:01, 42.87it/s][A
 87%|████████▋ | 438/505 [00:09<00:01, 43.50it/s][A
 88%|████████▊ | 443/505 [00:10<00:01, 43.71it/s][A
 89%|████████▊ | 448/505 [00:10<00:01, 43.52it/s][A
 90%|████████▉ | 453/505 [00:10<00:01, 43.61it/s][A
 91%|█████████ | 458/505 [00:10<00:01, 43.89it/s][A
 92%|█████████▏| 463/505 [00:10<00:00, 44.11it/s][A
 93%|█████████▎| 468/505 [00:10<00:00, 44.32it/s][A
 94%|█████████▎| 473/505 [00:10<00:00, 44.58it/s][A
 95%|█████████▍| 478/505 [00:10<00:00, 44.56it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 44.80it/s][A
 97%|█████████▋| 488/505 [00:11<00:00, 44.63it/s][A
 98%|█████████▊| 493/505 [00:11<00:00, 44.27it/s][A
 99%|█████████▊| 498/505 [00:11<00:00, 44.15it/s][A
100%|█████████▉| 503/505 [00:11<00:00, 44.26it/s][A
                                                 [A                                                
100%|██████████| 505/505 [00:11<00:00, 44.26it/s][A 20%|██        | 78/390 [00:35<01:30,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:56:23,801 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-29 12:56:24,037 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:56:28,165 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:56:28,350 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:56:28,422 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [00:49<41:49,  8.07s/it] 21%|██        | 80/390 [00:49<29:37,  5.73s/it] 21%|██        | 81/390 [00:49<21:07,  4.10s/it] 21%|██        | 82/390 [00:50<15:11,  2.96s/it] 21%|██▏       | 83/390 [00:50<11:02,  2.16s/it] 22%|██▏       | 84/390 [00:50<08:09,  1.60s/it] 22%|██▏       | 85/390 [00:50<06:08,  1.21s/it] 22%|██▏       | 86/390 [00:51<04:43,  1.07it/s] 22%|██▏       | 87/390 [00:51<03:49,  1.32it/s] 23%|██▎       | 88/390 [00:51<03:07,  1.61it/s] 23%|██▎       | 89/390 [00:52<02:37,  1.92it/s] 23%|██▎       | 90/390 [00:52<02:16,  2.21it/s] 23%|██▎       | 91/390 [00:52<02:01,  2.46it/s] 24%|██▎       | 92/390 [00:53<01:51,  2.68it/s] 24%|██▍       | 93/390 [00:53<01:43,  2.87it/s] 24%|██▍       | 94/390 [00:53<01:38,  3.01it/s] 24%|██▍       | 95/390 [00:53<01:34,  3.12it/s] 25%|██▍       | 96/390 [00:54<01:31,  3.20it/s] 25%|██▍       | 97/390 [00:54<01:37,  3.01it/s] 25%|██▌       | 98/390 [00:54<01:33,  3.12it/s] 25%|██▌       | 99/390 [00:55<01:30,  3.20it/s] 26%|██▌       | 100/390 [00:55<01:28,  3.26it/s] 26%|██▌       | 101/390 [00:55<01:27,  3.30it/s] 26%|██▌       | 102/390 [00:56<01:26,  3.33it/s] 26%|██▋       | 103/390 [00:56<01:25,  3.35it/s] 27%|██▋       | 104/390 [00:56<01:24,  3.37it/s] 27%|██▋       | 105/390 [00:56<01:24,  3.38it/s] 27%|██▋       | 106/390 [00:57<01:23,  3.39it/s] 27%|██▋       | 107/390 [00:57<01:30,  3.13it/s] 28%|██▊       | 108/390 [00:57<01:27,  3.21it/s] 28%|██▊       | 109/390 [00:58<01:26,  3.26it/s] 28%|██▊       | 110/390 [00:58<01:24,  3.31it/s] 28%|██▊       | 111/390 [00:58<01:23,  3.34it/s] 29%|██▊       | 112/390 [00:59<01:22,  3.35it/s] 29%|██▉       | 113/390 [00:59<01:22,  3.37it/s] 29%|██▉       | 114/390 [00:59<01:21,  3.38it/s] 29%|██▉       | 115/390 [00:59<01:21,  3.38it/s] 30%|██▉       | 116/390 [01:00<01:20,  3.39it/s] 30%|███       | 117/390 [01:00<01:26,  3.15it/s] 30%|███       | 118/390 [01:00<01:24,  3.22it/s] 31%|███       | 119/390 [01:01<01:22,  3.27it/s] 31%|███       | 120/390 [01:01<01:21,  3.31it/s] 31%|███       | 121/390 [01:01<01:20,  3.33it/s] 31%|███▏      | 122/390 [01:02<01:20,  3.35it/s] 32%|███▏      | 123/390 [01:02<01:19,  3.36it/s] 32%|███▏      | 124/390 [01:02<01:18,  3.38it/s] 32%|███▏      | 125/390 [01:03<01:18,  3.38it/s] 32%|███▏      | 126/390 [01:03<01:18,  3.38it/s] 33%|███▎      | 127/390 [01:03<01:23,  3.17it/s] 33%|███▎      | 128/390 [01:03<01:20,  3.23it/s] 33%|███▎      | 129/390 [01:04<01:19,  3.28it/s] 33%|███▎      | 130/390 [01:04<01:18,  3.32it/s] 34%|███▎      | 131/390 [01:04<01:17,  3.34it/s] 34%|███▍      | 132/390 [01:05<01:16,  3.36it/s] 34%|███▍      | 133/390 [01:05<01:16,  3.37it/s] 34%|███▍      | 134/390 [01:05<01:15,  3.37it/s] 35%|███▍      | 135/390 [01:06<01:15,  3.38it/s] 35%|███▍      | 136/390 [01:06<01:14,  3.39it/s] 35%|███▌      | 137/390 [01:06<01:17,  3.25it/s] 35%|███▌      | 138/390 [01:06<01:16,  3.29it/s] 36%|███▌      | 139/390 [01:07<01:15,  3.32it/s] 36%|███▌      | 140/390 [01:07<01:14,  3.35it/s] 36%|███▌      | 141/390 [01:07<01:14,  3.36it/s] 36%|███▋      | 142/390 [01:08<01:13,  3.37it/s] 37%|███▋      | 143/390 [01:08<01:13,  3.38it/s] 37%|███▋      | 144/390 [01:08<01:12,  3.38it/s] 37%|███▋      | 145/390 [01:09<01:12,  3.39it/s] 37%|███▋      | 146/390 [01:09<01:12,  3.38it/s] 38%|███▊      | 147/390 [01:09<01:11,  3.39it/s] 38%|███▊      | 148/390 [01:09<01:13,  3.31it/s] 38%|███▊      | 149/390 [01:10<01:12,  3.34it/s] 38%|███▊      | 150/390 [01:10<01:11,  3.35it/s] 39%|███▊      | 151/390 [01:10<01:11,  3.36it/s] 39%|███▉      | 152/390 [01:11<01:10,  3.37it/s] 39%|███▉      | 153/390 [01:11<01:10,  3.38it/s] 39%|███▉      | 154/390 [01:11<01:09,  3.38it/s] 40%|███▉      | 155/390 [01:11<01:09,  3.39it/s] 40%|████      | 156/390 [01:12<01:08,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 12:57:00,584 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:57:00,585 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 12:57:00,585 >>   Batch size = 8
{'eval_loss': 1.1915907859802246, 'eval_runtime': 11.5559, 'eval_samples_per_second': 349.519, 'eval_steps_per_second': 43.701, 'epoch': 0.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 55.57it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.55it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.13it/s][A
  4%|▍         | 22/505 [00:00<00:11, 43.59it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.03it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.05it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.15it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.03it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.18it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.35it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.54it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.48it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.61it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.62it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.59it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.45it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.37it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.40it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.50it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.64it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.65it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.61it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.61it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.52it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.49it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.34it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.37it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.50it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.62it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.70it/s][A
 31%|███       | 157/505 [00:03<00:08, 43.16it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.51it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.89it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.07it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.07it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.24it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.34it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.37it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.39it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.50it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.53it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.52it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.42it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.36it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.43it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.52it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.48it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.57it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.55it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.52it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.55it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.45it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.31it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.46it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.46it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.57it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.54it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 43.35it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 43.85it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 44.07it/s][A
 61%|██████    | 307/505 [00:06<00:04, 44.15it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 44.29it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.37it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.33it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.45it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.39it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.39it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.40it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.41it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.43it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.48it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.48it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.40it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.43it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.45it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.44it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.34it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.28it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.46it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.47it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.60it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.56it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.54it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.42it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.37it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 42.64it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.30it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.71it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.98it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.02it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.26it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.31it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.25it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.10it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.29it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.33it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.54it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.58it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.57it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.56it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.56it/s][A 40%|████      | 156/390 [01:23<01:08,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:57:12,127 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 12:57:12,292 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:57:16,175 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:57:16,503 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:57:16,612 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [01:38<31:30,  8.11s/it] 41%|████      | 158/390 [01:38<22:21,  5.78s/it] 41%|████      | 159/390 [01:39<15:54,  4.13s/it] 41%|████      | 160/390 [01:39<11:25,  2.98s/it] 41%|████▏     | 161/390 [01:39<08:17,  2.17s/it] 42%|████▏     | 162/390 [01:40<06:06,  1.61s/it] 42%|████▏     | 163/390 [01:40<04:35,  1.21s/it] 42%|████▏     | 164/390 [01:40<03:31,  1.07it/s] 42%|████▏     | 165/390 [01:41<02:46,  1.35it/s] 43%|████▎     | 166/390 [01:41<02:15,  1.65it/s] 43%|████▎     | 167/390 [01:41<01:54,  1.95it/s] 43%|████▎     | 168/390 [01:41<01:38,  2.25it/s] 43%|████▎     | 169/390 [01:42<01:30,  2.45it/s] 44%|████▎     | 170/390 [01:42<01:21,  2.69it/s] 44%|████▍     | 171/390 [01:42<01:16,  2.88it/s] 44%|████▍     | 172/390 [01:43<01:11,  3.03it/s] 44%|████▍     | 173/390 [01:43<01:08,  3.15it/s] 45%|████▍     | 174/390 [01:43<01:06,  3.24it/s] 45%|████▍     | 175/390 [01:43<01:05,  3.30it/s] 45%|████▌     | 176/390 [01:44<01:03,  3.35it/s] 45%|████▌     | 177/390 [01:44<01:02,  3.38it/s] 46%|████▌     | 178/390 [01:44<01:02,  3.41it/s] 46%|████▌     | 179/390 [01:45<01:01,  3.42it/s] 46%|████▌     | 180/390 [01:45<01:03,  3.29it/s] 46%|████▋     | 181/390 [01:45<01:02,  3.34it/s] 47%|████▋     | 182/390 [01:45<01:01,  3.38it/s] 47%|████▋     | 183/390 [01:46<01:00,  3.40it/s] 47%|████▋     | 184/390 [01:46<01:00,  3.42it/s] 47%|████▋     | 185/390 [01:46<00:59,  3.42it/s] 48%|████▊     | 186/390 [01:47<00:59,  3.43it/s] 48%|████▊     | 187/390 [01:47<00:59,  3.44it/s] 48%|████▊     | 188/390 [01:47<00:58,  3.45it/s] 48%|████▊     | 189/390 [01:48<00:59,  3.38it/s] 49%|████▊     | 190/390 [01:48<00:58,  3.40it/s] 49%|████▉     | 191/390 [01:48<01:19,  2.52it/s] 49%|████▉     | 192/390 [01:49<01:12,  2.74it/s] 49%|████▉     | 193/390 [01:49<01:07,  2.92it/s] 50%|████▉     | 194/390 [01:49<01:04,  3.06it/s] 50%|█████     | 195/390 [01:50<01:01,  3.17it/s] 50%|█████     | 196/390 [01:50<00:59,  3.25it/s] 51%|█████     | 197/390 [01:50<00:58,  3.31it/s] 51%|█████     | 198/390 [01:50<00:57,  3.34it/s] 51%|█████     | 199/390 [01:51<00:56,  3.38it/s] 51%|█████▏    | 200/390 [01:51<00:55,  3.39it/s] 52%|█████▏    | 201/390 [01:51<00:55,  3.41it/s] 52%|█████▏    | 202/390 [01:52<00:54,  3.42it/s] 52%|█████▏    | 203/390 [01:52<00:54,  3.43it/s] 52%|█████▏    | 204/390 [01:52<00:54,  3.43it/s] 53%|█████▎    | 205/390 [01:53<00:57,  3.22it/s] 53%|█████▎    | 206/390 [01:53<00:56,  3.28it/s] 53%|█████▎    | 207/390 [01:53<00:54,  3.34it/s] 53%|█████▎    | 208/390 [01:53<00:54,  3.37it/s] 54%|█████▎    | 209/390 [01:54<00:53,  3.39it/s] 54%|█████▍    | 210/390 [01:54<00:52,  3.40it/s] 54%|█████▍    | 211/390 [01:54<00:52,  3.42it/s] 54%|█████▍    | 212/390 [01:55<00:51,  3.43it/s] 55%|█████▍    | 213/390 [01:55<00:51,  3.43it/s] 55%|█████▍    | 214/390 [01:55<00:51,  3.44it/s] 55%|█████▌    | 215/390 [01:55<00:50,  3.44it/s] 55%|█████▌    | 216/390 [01:56<00:54,  3.18it/s] 56%|█████▌    | 217/390 [01:56<00:53,  3.26it/s] 56%|█████▌    | 218/390 [01:56<00:51,  3.32it/s] 56%|█████▌    | 219/390 [01:57<00:51,  3.35it/s] 56%|█████▋    | 220/390 [01:57<00:50,  3.38it/s] 57%|█████▋    | 221/390 [01:57<00:49,  3.40it/s] 57%|█████▋    | 222/390 [01:58<00:49,  3.42it/s] 57%|█████▋    | 223/390 [01:58<00:48,  3.42it/s] 57%|█████▋    | 224/390 [01:58<00:48,  3.43it/s] 58%|█████▊    | 225/390 [01:58<00:48,  3.43it/s] 58%|█████▊    | 226/390 [01:59<00:47,  3.44it/s] 58%|█████▊    | 227/390 [01:59<00:49,  3.26it/s] 58%|█████▊    | 228/390 [01:59<00:48,  3.32it/s] 59%|█████▊    | 229/390 [02:00<00:47,  3.36it/s] 59%|█████▉    | 230/390 [02:00<00:47,  3.38it/s] 59%|█████▉    | 231/390 [02:00<00:46,  3.40it/s] 59%|█████▉    | 232/390 [02:01<00:46,  3.41it/s] 60%|█████▉    | 233/390 [02:01<00:45,  3.42it/s] 60%|██████    | 234/390 [02:01<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 12:57:49,934 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:57:49,934 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 12:57:49,935 >>   Batch size = 8
{'eval_loss': 1.2080938816070557, 'eval_runtime': 11.4044, 'eval_samples_per_second': 354.161, 'eval_steps_per_second': 44.281, 'epoch': 1.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.05it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.55it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.95it/s][A
  4%|▍         | 22/505 [00:00<00:10, 46.14it/s][A
  5%|▌         | 27/505 [00:00<00:10, 45.42it/s][A
  6%|▋         | 32/505 [00:00<00:11, 41.78it/s][A
  7%|▋         | 37/505 [00:00<00:10, 42.67it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.21it/s][A
  9%|▉         | 47/505 [00:01<00:10, 43.68it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.06it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.33it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.55it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.48it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.12it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.12it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.27it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.35it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.42it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.57it/s][A
 20%|██        | 102/505 [00:02<00:08, 44.79it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.78it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.58it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.34it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.18it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.28it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.40it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.48it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.63it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.66it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.76it/s][A
 31%|███       | 157/505 [00:03<00:07, 44.56it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 44.28it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 43.89it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.00it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.24it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.51it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.61it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.58it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.58it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.43it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.29it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.29it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.31it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.53it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.67it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.62it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.63it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.58it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.46it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.34it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.13it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.24it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.42it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.62it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.76it/s][A
 56%|█████▌    | 282/505 [00:06<00:04, 44.74it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.61it/s][A
 58%|█████▊    | 292/505 [00:06<00:04, 44.50it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 44.38it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 42.80it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.35it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.74it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.09it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.32it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.40it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 44.36it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.24it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.04it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.15it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.34it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.51it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.69it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.59it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.64it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.54it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.22it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.20it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.30it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.34it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.50it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.59it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.65it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.71it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.44it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 44.29it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 44.26it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 42.56it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.31it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 43.75it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.07it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.27it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.30it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 44.25it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 44.18it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 43.82it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.14it/s][A
 96%|█████████▋| 487/505 [00:10<00:00, 44.35it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.60it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.70it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.67it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.67it/s][A 60%|██████    | 234/390 [02:13<00:45,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:58:01,426 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 12:58:01,592 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:58:05,658 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:58:05,868 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:58:05,960 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [02:27<20:44,  8.03s/it] 61%|██████    | 236/390 [02:28<14:40,  5.72s/it] 61%|██████    | 237/390 [02:28<10:25,  4.09s/it] 61%|██████    | 238/390 [02:28<07:28,  2.95s/it] 61%|██████▏   | 239/390 [02:28<05:25,  2.15s/it] 62%|██████▏   | 240/390 [02:29<03:59,  1.60s/it] 62%|██████▏   | 241/390 [02:29<02:59,  1.20s/it] 62%|██████▏   | 242/390 [02:29<02:17,  1.07it/s] 62%|██████▏   | 243/390 [02:30<01:48,  1.35it/s] 63%|██████▎   | 244/390 [02:30<01:28,  1.65it/s] 63%|██████▎   | 245/390 [02:30<01:14,  1.95it/s] 63%|██████▎   | 246/390 [02:30<01:04,  2.24it/s] 63%|██████▎   | 247/390 [02:31<00:58,  2.45it/s] 64%|██████▎   | 248/390 [02:31<00:53,  2.67it/s] 64%|██████▍   | 249/390 [02:31<00:49,  2.85it/s] 64%|██████▍   | 250/390 [02:32<00:46,  3.00it/s] 64%|██████▍   | 251/390 [02:32<00:44,  3.11it/s] 65%|██████▍   | 252/390 [02:32<00:43,  3.19it/s] 65%|██████▍   | 253/390 [02:33<00:42,  3.25it/s] 65%|██████▌   | 254/390 [02:33<00:41,  3.30it/s] 65%|██████▌   | 255/390 [02:33<00:40,  3.33it/s] 66%|██████▌   | 256/390 [02:33<00:40,  3.35it/s] 66%|██████▌   | 257/390 [02:34<00:39,  3.36it/s] 66%|██████▌   | 258/390 [02:34<00:40,  3.30it/s] 66%|██████▋   | 259/390 [02:34<00:39,  3.33it/s] 67%|██████▋   | 260/390 [02:35<00:38,  3.35it/s] 67%|██████▋   | 261/390 [02:35<00:38,  3.36it/s] 67%|██████▋   | 262/390 [02:35<00:37,  3.37it/s] 67%|██████▋   | 263/390 [02:36<00:37,  3.38it/s] 68%|██████▊   | 264/390 [02:36<00:37,  3.38it/s] 68%|██████▊   | 265/390 [02:36<00:36,  3.39it/s] 68%|██████▊   | 266/390 [02:36<00:36,  3.39it/s] 68%|██████▊   | 267/390 [02:37<00:36,  3.39it/s] 69%|██████▊   | 268/390 [02:37<00:35,  3.39it/s] 69%|██████▉   | 269/390 [02:37<00:36,  3.30it/s] 69%|██████▉   | 270/390 [02:38<00:36,  3.33it/s] 69%|██████▉   | 271/390 [02:38<00:35,  3.35it/s] 70%|██████▉   | 272/390 [02:38<00:35,  3.36it/s] 70%|███████   | 273/390 [02:39<00:34,  3.37it/s] 70%|███████   | 274/390 [02:39<00:34,  3.38it/s] 71%|███████   | 275/390 [02:39<00:33,  3.38it/s] 71%|███████   | 276/390 [02:39<00:33,  3.39it/s] 71%|███████   | 277/390 [02:40<00:33,  3.39it/s] 71%|███████▏  | 278/390 [02:40<00:33,  3.39it/s] 72%|███████▏  | 279/390 [02:40<00:32,  3.40it/s] 72%|███████▏  | 280/390 [02:41<00:33,  3.29it/s] 72%|███████▏  | 281/390 [02:41<00:32,  3.32it/s] 72%|███████▏  | 282/390 [02:41<00:32,  3.34it/s] 73%|███████▎  | 283/390 [02:41<00:31,  3.36it/s] 73%|███████▎  | 284/390 [02:42<00:31,  3.37it/s] 73%|███████▎  | 285/390 [02:42<00:31,  3.38it/s] 73%|███████▎  | 286/390 [02:42<00:30,  3.38it/s] 74%|███████▎  | 287/390 [02:43<00:30,  3.39it/s] 74%|███████▍  | 288/390 [02:43<00:30,  3.40it/s] 74%|███████▍  | 289/390 [02:43<00:29,  3.41it/s] 74%|███████▍  | 290/390 [02:44<00:29,  3.42it/s] 75%|███████▍  | 291/390 [02:44<00:30,  3.28it/s] 75%|███████▍  | 292/390 [02:44<00:29,  3.33it/s] 75%|███████▌  | 293/390 [02:44<00:28,  3.36it/s] 75%|███████▌  | 294/390 [02:45<00:28,  3.40it/s] 76%|███████▌  | 295/390 [02:45<00:27,  3.41it/s] 76%|███████▌  | 296/390 [02:45<00:27,  3.43it/s] 76%|███████▌  | 297/390 [02:46<00:27,  3.43it/s] 76%|███████▋  | 298/390 [02:46<00:26,  3.44it/s] 77%|███████▋  | 299/390 [02:46<00:26,  3.44it/s] 77%|███████▋  | 300/390 [02:46<00:26,  3.45it/s] 77%|███████▋  | 301/390 [02:47<00:25,  3.45it/s] 77%|███████▋  | 302/390 [02:47<00:26,  3.37it/s] 78%|███████▊  | 303/390 [02:47<00:25,  3.39it/s] 78%|███████▊  | 304/390 [02:48<00:25,  3.37it/s] 78%|███████▊  | 305/390 [02:48<00:25,  3.39it/s] 78%|███████▊  | 306/390 [02:48<00:29,  2.88it/s] 79%|███████▊  | 307/390 [02:49<00:27,  2.97it/s] 79%|███████▉  | 308/390 [02:49<00:26,  3.10it/s] 79%|███████▉  | 309/390 [02:49<00:25,  3.20it/s] 79%|███████▉  | 310/390 [02:50<00:24,  3.27it/s] 80%|███████▉  | 311/390 [02:50<00:23,  3.33it/s] 80%|████████  | 312/390 [02:50<00:24,  3.21it/s][INFO|trainer.py:2140] 2023-08-29 12:58:39,026 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:58:39,026 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 12:58:39,026 >>   Batch size = 8
{'eval_loss': 1.2279167175292969, 'eval_runtime': 11.4116, 'eval_samples_per_second': 353.94, 'eval_steps_per_second': 44.253, 'epoch': 2.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.49it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.91it/s][A
  3%|▎         | 17/505 [00:00<00:10, 47.15it/s][A
  4%|▍         | 22/505 [00:00<00:10, 44.33it/s][A
  5%|▌         | 27/505 [00:00<00:10, 44.35it/s][A
  6%|▋         | 32/505 [00:00<00:10, 44.10it/s][A
  7%|▋         | 37/505 [00:00<00:10, 44.02it/s][A
  8%|▊         | 42/505 [00:00<00:10, 43.96it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.13it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.39it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.54it/s][A
 12%|█▏        | 62/505 [00:01<00:09, 44.64it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.78it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.79it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.43it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.29it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.23it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.30it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.49it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.53it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.52it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.69it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.64it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.51it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.46it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.25it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.30it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.45it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.47it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.65it/s][A
 31%|███       | 157/505 [00:03<00:09, 38.07it/s][A
 32%|███▏      | 162/505 [00:03<00:08, 39.91it/s][A
 33%|███▎      | 167/505 [00:03<00:08, 41.33it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 42.39it/s][A
 35%|███▌      | 177/505 [00:04<00:07, 43.16it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 43.75it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.24it/s][A
 39%|███▉      | 197/505 [00:04<00:07, 43.92it/s][A
 40%|████      | 202/505 [00:04<00:06, 43.74it/s][A
 41%|████      | 207/505 [00:04<00:06, 43.80it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.00it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.27it/s][A
 44%|████▍     | 222/505 [00:05<00:06, 44.54it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.73it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.85it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.63it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.39it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.12it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.04it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.14it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.41it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.52it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.73it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.79it/s][A
 56%|█████▌    | 282/505 [00:06<00:04, 44.77it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.45it/s][A
 58%|█████▊    | 292/505 [00:06<00:05, 40.52it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 41.84it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 42.64it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.36it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.85it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.12it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.35it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.17it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.90it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 43.86it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.12it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.32it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.51it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.69it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.80it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.73it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.43it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.08it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.05it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.17it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.35it/s][A
 79%|███████▊  | 397/505 [00:09<00:02, 44.57it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.71it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.74it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.76it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.48it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.23it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 41.33it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 42.38it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.14it/s][A
 88%|████████▊ | 442/505 [00:10<00:01, 43.70it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 44.11it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 44.38it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 44.38it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 44.04it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.85it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.95it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 44.15it/s][A
 95%|█████████▌| 482/505 [00:10<00:00, 44.44it/s][A
 96%|█████████▋| 487/505 [00:11<00:00, 44.58it/s][A
 97%|█████████▋| 492/505 [00:11<00:00, 44.76it/s][A
 98%|█████████▊| 497/505 [00:11<00:00, 44.84it/s][A
 99%|█████████▉| 502/505 [00:11<00:00, 44.63it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 44.63it/s][A 80%|████████  | 312/390 [03:02<00:24,  3.21it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:58:50,783 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 12:58:50,999 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:58:54,873 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:58:55,047 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:58:55,115 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [03:14<09:28,  7.39s/it] 81%|████████  | 314/390 [03:14<06:40,  5.27s/it] 81%|████████  | 315/390 [03:15<04:43,  3.77s/it] 81%|████████  | 316/390 [03:15<03:22,  2.73s/it] 81%|████████▏ | 317/390 [03:15<02:25,  2.00s/it] 82%|████████▏ | 318/390 [03:16<01:47,  1.49s/it] 82%|████████▏ | 319/390 [03:16<01:20,  1.13s/it] 82%|████████▏ | 320/390 [03:16<01:01,  1.14it/s] 82%|████████▏ | 321/390 [03:16<00:48,  1.42it/s] 83%|████████▎ | 322/390 [03:17<00:39,  1.73it/s] 83%|████████▎ | 323/390 [03:17<00:33,  2.02it/s] 83%|████████▎ | 324/390 [03:17<00:28,  2.31it/s] 83%|████████▎ | 325/390 [03:18<00:26,  2.48it/s] 84%|████████▎ | 326/390 [03:18<00:23,  2.70it/s] 84%|████████▍ | 327/390 [03:18<00:21,  2.88it/s] 84%|████████▍ | 328/390 [03:19<00:20,  3.02it/s] 84%|████████▍ | 329/390 [03:19<00:19,  3.12it/s] 85%|████████▍ | 330/390 [03:19<00:18,  3.20it/s] 85%|████████▍ | 331/390 [03:19<00:18,  3.26it/s] 85%|████████▌ | 332/390 [03:20<00:17,  3.30it/s] 85%|████████▌ | 333/390 [03:20<00:17,  3.33it/s] 86%|████████▌ | 334/390 [03:20<00:16,  3.35it/s] 86%|████████▌ | 335/390 [03:21<00:16,  3.37it/s] 86%|████████▌ | 336/390 [03:21<00:16,  3.27it/s] 86%|████████▋ | 337/390 [03:21<00:16,  3.31it/s] 87%|████████▋ | 338/390 [03:22<00:15,  3.34it/s] 87%|████████▋ | 339/390 [03:22<00:15,  3.35it/s] 87%|████████▋ | 340/390 [03:22<00:14,  3.37it/s] 87%|████████▋ | 341/390 [03:22<00:14,  3.38it/s] 88%|████████▊ | 342/390 [03:23<00:14,  3.38it/s] 88%|████████▊ | 343/390 [03:23<00:13,  3.39it/s] 88%|████████▊ | 344/390 [03:23<00:13,  3.39it/s] 88%|████████▊ | 345/390 [03:24<00:13,  3.39it/s] 89%|████████▊ | 346/390 [03:24<00:12,  3.40it/s] 89%|████████▉ | 347/390 [03:24<00:12,  3.40it/s] 89%|████████▉ | 348/390 [03:24<00:12,  3.40it/s] 89%|████████▉ | 349/390 [03:25<00:12,  3.27it/s] 90%|████████▉ | 350/390 [03:25<00:12,  3.31it/s] 90%|█████████ | 351/390 [03:25<00:11,  3.34it/s] 90%|█████████ | 352/390 [03:26<00:11,  3.36it/s] 91%|█████████ | 353/390 [03:26<00:10,  3.37it/s] 91%|█████████ | 354/390 [03:26<00:10,  3.38it/s] 91%|█████████ | 355/390 [03:27<00:10,  3.38it/s] 91%|█████████▏| 356/390 [03:27<00:10,  3.40it/s] 92%|█████████▏| 357/390 [03:27<00:09,  3.41it/s] 92%|█████████▏| 358/390 [03:27<00:09,  3.43it/s] 92%|█████████▏| 359/390 [03:28<00:09,  3.43it/s] 92%|█████████▏| 360/390 [03:28<00:09,  3.32it/s] 93%|█████████▎| 361/390 [03:28<00:08,  3.36it/s] 93%|█████████▎| 362/390 [03:29<00:08,  3.39it/s] 93%|█████████▎| 363/390 [03:29<00:07,  3.41it/s] 93%|█████████▎| 364/390 [03:29<00:07,  3.42it/s] 94%|█████████▎| 365/390 [03:30<00:07,  3.43it/s] 94%|█████████▍| 366/390 [03:30<00:06,  3.44it/s] 94%|█████████▍| 367/390 [03:30<00:06,  3.44it/s] 94%|█████████▍| 368/390 [03:30<00:06,  3.45it/s] 95%|█████████▍| 369/390 [03:31<00:06,  3.46it/s] 95%|█████████▍| 370/390 [03:31<00:05,  3.45it/s] 95%|█████████▌| 371/390 [03:31<00:05,  3.33it/s] 95%|█████████▌| 372/390 [03:32<00:05,  3.36it/s] 96%|█████████▌| 373/390 [03:32<00:05,  3.39it/s] 96%|█████████▌| 374/390 [03:32<00:04,  3.41it/s] 96%|█████████▌| 375/390 [03:32<00:04,  3.42it/s] 96%|█████████▋| 376/390 [03:33<00:04,  3.43it/s] 97%|█████████▋| 377/390 [03:33<00:03,  3.44it/s] 97%|█████████▋| 378/390 [03:33<00:03,  3.44it/s] 97%|█████████▋| 379/390 [03:34<00:03,  3.44it/s] 97%|█████████▋| 380/390 [03:34<00:02,  3.45it/s] 98%|█████████▊| 381/390 [03:34<00:02,  3.45it/s] 98%|█████████▊| 382/390 [03:35<00:02,  3.32it/s] 98%|█████████▊| 383/390 [03:35<00:02,  3.36it/s] 98%|█████████▊| 384/390 [03:35<00:01,  3.39it/s] 99%|█████████▊| 385/390 [03:35<00:01,  3.40it/s] 99%|█████████▉| 386/390 [03:36<00:01,  3.42it/s] 99%|█████████▉| 387/390 [03:36<00:00,  3.43it/s] 99%|█████████▉| 388/390 [03:36<00:00,  3.44it/s]100%|█████████▉| 389/390 [03:37<00:00,  3.44it/s]100%|██████████| 390/390 [03:37<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 12:59:25,602 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:59:25,602 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 12:59:25,602 >>   Batch size = 8
{'eval_loss': 1.2490793466567993, 'eval_runtime': 11.4861, 'eval_samples_per_second': 351.644, 'eval_steps_per_second': 43.966, 'epoch': 3.99}

  0%|          | 0/505 [00:00<?, ?it/s][A
  1%|          | 6/505 [00:00<00:08, 56.06it/s][A
  2%|▏         | 12/505 [00:00<00:10, 48.70it/s][A
  3%|▎         | 17/505 [00:00<00:10, 46.78it/s][A
  4%|▍         | 22/505 [00:00<00:11, 42.85it/s][A
  5%|▌         | 27/505 [00:00<00:10, 43.53it/s][A
  6%|▋         | 32/505 [00:00<00:10, 43.76it/s][A
  7%|▋         | 37/505 [00:00<00:10, 43.94it/s][A
  8%|▊         | 42/505 [00:00<00:10, 44.09it/s][A
  9%|▉         | 47/505 [00:01<00:10, 44.26it/s][A
 10%|█         | 52/505 [00:01<00:10, 44.51it/s][A
 11%|█▏        | 57/505 [00:01<00:10, 44.57it/s][A
 12%|█▏        | 62/505 [00:01<00:10, 44.29it/s][A
 13%|█▎        | 67/505 [00:01<00:09, 44.38it/s][A
 14%|█▍        | 72/505 [00:01<00:09, 44.41it/s][A
 15%|█▌        | 77/505 [00:01<00:09, 44.46it/s][A
 16%|█▌        | 82/505 [00:01<00:09, 44.45it/s][A
 17%|█▋        | 87/505 [00:01<00:09, 44.50it/s][A
 18%|█▊        | 92/505 [00:02<00:09, 44.59it/s][A
 19%|█▉        | 97/505 [00:02<00:09, 44.64it/s][A
 20%|██        | 102/505 [00:02<00:09, 44.55it/s][A
 21%|██        | 107/505 [00:02<00:08, 44.43it/s][A
 22%|██▏       | 112/505 [00:02<00:08, 44.36it/s][A
 23%|██▎       | 117/505 [00:02<00:08, 44.43it/s][A
 24%|██▍       | 122/505 [00:02<00:08, 44.43it/s][A
 25%|██▌       | 127/505 [00:02<00:08, 44.47it/s][A
 26%|██▌       | 132/505 [00:02<00:08, 44.52it/s][A
 27%|██▋       | 137/505 [00:03<00:08, 44.48it/s][A
 28%|██▊       | 142/505 [00:03<00:08, 44.59it/s][A
 29%|██▉       | 147/505 [00:03<00:08, 44.55it/s][A
 30%|███       | 152/505 [00:03<00:07, 44.41it/s][A
 31%|███       | 157/505 [00:03<00:07, 43.57it/s][A
 32%|███▏      | 162/505 [00:03<00:07, 43.90it/s][A
 33%|███▎      | 167/505 [00:03<00:07, 44.10it/s][A
 34%|███▍      | 172/505 [00:03<00:07, 44.21it/s][A
 35%|███▌      | 177/505 [00:03<00:07, 44.41it/s][A
 36%|███▌      | 182/505 [00:04<00:07, 44.42it/s][A
 37%|███▋      | 187/505 [00:04<00:07, 44.48it/s][A
 38%|███▊      | 192/505 [00:04<00:07, 44.49it/s][A
 39%|███▉      | 197/505 [00:04<00:06, 44.34it/s][A
 40%|████      | 202/505 [00:04<00:06, 44.39it/s][A
 41%|████      | 207/505 [00:04<00:06, 44.46it/s][A
 42%|████▏     | 212/505 [00:04<00:06, 44.44it/s][A
 43%|████▎     | 217/505 [00:04<00:06, 44.51it/s][A
 44%|████▍     | 222/505 [00:04<00:06, 44.57it/s][A
 45%|████▍     | 227/505 [00:05<00:06, 44.59it/s][A
 46%|████▌     | 232/505 [00:05<00:06, 44.54it/s][A
 47%|████▋     | 237/505 [00:05<00:06, 44.46it/s][A
 48%|████▊     | 242/505 [00:05<00:05, 44.41it/s][A
 49%|████▉     | 247/505 [00:05<00:05, 44.27it/s][A
 50%|████▉     | 252/505 [00:05<00:05, 44.43it/s][A
 51%|█████     | 257/505 [00:05<00:05, 44.39it/s][A
 52%|█████▏    | 262/505 [00:05<00:05, 44.38it/s][A
 53%|█████▎    | 267/505 [00:06<00:05, 44.51it/s][A
 54%|█████▍    | 272/505 [00:06<00:05, 44.56it/s][A
 55%|█████▍    | 277/505 [00:06<00:05, 44.49it/s][A
 56%|█████▌    | 282/505 [00:06<00:05, 44.45it/s][A
 57%|█████▋    | 287/505 [00:06<00:04, 44.41it/s][A
 58%|█████▊    | 292/505 [00:06<00:05, 40.96it/s][A
 59%|█████▉    | 297/505 [00:06<00:04, 42.16it/s][A
 60%|█████▉    | 302/505 [00:06<00:04, 42.93it/s][A
 61%|██████    | 307/505 [00:06<00:04, 43.45it/s][A
 62%|██████▏   | 312/505 [00:07<00:04, 43.92it/s][A
 63%|██████▎   | 317/505 [00:07<00:04, 44.21it/s][A
 64%|██████▍   | 322/505 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 327/505 [00:07<00:04, 44.12it/s][A
 66%|██████▌   | 332/505 [00:07<00:03, 43.93it/s][A
 67%|██████▋   | 337/505 [00:07<00:03, 44.02it/s][A
 68%|██████▊   | 342/505 [00:07<00:03, 44.24it/s][A
 69%|██████▊   | 347/505 [00:07<00:03, 44.42it/s][A
 70%|██████▉   | 352/505 [00:07<00:03, 44.59it/s][A
 71%|███████   | 357/505 [00:08<00:03, 44.59it/s][A
 72%|███████▏  | 362/505 [00:08<00:03, 44.67it/s][A
 73%|███████▎  | 367/505 [00:08<00:03, 44.71it/s][A
 74%|███████▎  | 372/505 [00:08<00:02, 44.40it/s][A
 75%|███████▍  | 377/505 [00:08<00:02, 44.12it/s][A
 76%|███████▌  | 382/505 [00:08<00:02, 44.15it/s][A
 77%|███████▋  | 387/505 [00:08<00:02, 44.30it/s][A
 78%|███████▊  | 392/505 [00:08<00:02, 44.39it/s][A
 79%|███████▊  | 397/505 [00:08<00:02, 44.56it/s][A
 80%|███████▉  | 402/505 [00:09<00:02, 44.65it/s][A
 81%|████████  | 407/505 [00:09<00:02, 44.83it/s][A
 82%|████████▏ | 412/505 [00:09<00:02, 44.69it/s][A
 83%|████████▎ | 417/505 [00:09<00:01, 44.40it/s][A
 84%|████████▎ | 422/505 [00:09<00:01, 44.25it/s][A
 85%|████████▍ | 427/505 [00:09<00:01, 41.79it/s][A
 86%|████████▌ | 432/505 [00:09<00:01, 42.73it/s][A
 87%|████████▋ | 437/505 [00:09<00:01, 43.32it/s][A
 88%|████████▊ | 442/505 [00:09<00:01, 43.82it/s][A
 89%|████████▊ | 447/505 [00:10<00:01, 41.78it/s][A
 90%|████████▉ | 452/505 [00:10<00:01, 42.86it/s][A
 90%|█████████ | 457/505 [00:10<00:01, 43.40it/s][A
 91%|█████████▏| 462/505 [00:10<00:00, 43.59it/s][A
 92%|█████████▏| 467/505 [00:10<00:00, 43.53it/s][A
 93%|█████████▎| 472/505 [00:10<00:00, 43.80it/s][A
 94%|█████████▍| 477/505 [00:10<00:00, 38.58it/s][A
 96%|█████████▌| 483/505 [00:10<00:00, 41.94it/s][A
 97%|█████████▋| 488/505 [00:11<00:00, 42.69it/s][A
 98%|█████████▊| 493/505 [00:11<00:00, 43.36it/s][A
 99%|█████████▊| 498/505 [00:11<00:00, 43.88it/s][A
100%|█████████▉| 503/505 [00:11<00:00, 23.43it/s][A
                                                 [A                                                 
100%|██████████| 505/505 [00:11<00:00, 23.43it/s][A100%|██████████| 390/390 [03:49<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:59:37,569 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-29 12:59:37,778 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:59:41,498 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:59:41,642 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:59:41,747 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 12:59:56,389 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 12:59:56,468 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78 (score: 1.1915907859802246).
                                                 100%|██████████| 390/390 [04:27<00:00,  3.45it/s]100%|██████████| 390/390 [04:27<00:00,  1.46it/s]
[INFO|trainer.py:1894] 2023-08-29 13:00:16,306 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 13:00:16,621 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:00:21,331 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:00:21,568 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:00:21,690 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:00:22,368 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,368 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,368 >>   train_loss               =     0.3159
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,368 >>   train_runtime            = 0:04:27.91
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,368 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,369 >>   train_samples_per_second =     93.313
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:22,369 >>   train_steps_per_second   =      1.456
{'eval_loss': 1.2539530992507935, 'eval_runtime': 11.8115, 'eval_samples_per_second': 341.955, 'eval_steps_per_second': 42.755, 'epoch': 4.99}
{'train_runtime': 267.9167, 'train_samples_per_second': 93.313, 'train_steps_per_second': 1.456, 'train_loss': 0.31588719685872396, 'epoch': 4.99}
08/29/2023 13:00:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 13:00:22,685 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:00:22,685 >>   Num examples = 4039
[INFO|trainer.py:2145] 2023-08-29 13:00:22,685 >>   Batch size = 8
  0%|          | 0/505 [00:00<?, ?it/s]  1%|          | 6/505 [00:00<00:09, 55.34it/s]  2%|▏         | 12/505 [00:00<00:09, 49.37it/s]  3%|▎         | 17/505 [00:00<00:10, 47.68it/s]  4%|▍         | 22/505 [00:00<00:10, 46.75it/s]  5%|▌         | 27/505 [00:00<00:10, 46.32it/s]  6%|▋         | 32/505 [00:00<00:10, 46.01it/s]  7%|▋         | 37/505 [00:00<00:10, 45.77it/s]  8%|▊         | 42/505 [00:00<00:10, 45.35it/s]  9%|▉         | 47/505 [00:01<00:10, 44.68it/s] 10%|█         | 52/505 [00:01<00:10, 44.43it/s] 11%|█▏        | 57/505 [00:01<00:10, 44.52it/s] 12%|█▏        | 62/505 [00:01<00:09, 44.70it/s] 13%|█▎        | 67/505 [00:01<00:09, 44.63it/s] 14%|█▍        | 72/505 [00:01<00:09, 44.98it/s] 15%|█▌        | 77/505 [00:01<00:09, 45.12it/s] 16%|█▌        | 82/505 [00:01<00:10, 41.94it/s] 17%|█▋        | 87/505 [00:01<00:09, 42.85it/s] 18%|█▊        | 92/505 [00:02<00:09, 43.25it/s] 19%|█▉        | 97/505 [00:02<00:09, 43.46it/s] 20%|██        | 102/505 [00:02<00:09, 43.87it/s] 21%|██        | 107/505 [00:02<00:08, 44.27it/s] 22%|██▏       | 112/505 [00:02<00:08, 44.55it/s] 23%|██▎       | 117/505 [00:02<00:08, 44.77it/s] 24%|██▍       | 122/505 [00:02<00:08, 44.48it/s] 25%|██▌       | 127/505 [00:02<00:08, 44.65it/s] 26%|██▌       | 132/505 [00:02<00:08, 44.69it/s] 27%|██▋       | 137/505 [00:03<00:08, 44.58it/s] 28%|██▊       | 142/505 [00:03<00:08, 44.51it/s] 29%|██▉       | 147/505 [00:03<00:08, 44.53it/s] 30%|███       | 152/505 [00:03<00:07, 44.75it/s] 31%|███       | 157/505 [00:03<00:07, 44.92it/s] 32%|███▏      | 162/505 [00:03<00:07, 44.87it/s] 33%|███▎      | 167/505 [00:03<00:07, 44.70it/s] 34%|███▍      | 172/505 [00:03<00:07, 44.66it/s] 35%|███▌      | 177/505 [00:03<00:07, 44.70it/s] 36%|███▌      | 182/505 [00:04<00:07, 44.59it/s] 37%|███▋      | 187/505 [00:04<00:07, 44.56it/s] 38%|███▊      | 192/505 [00:04<00:07, 44.65it/s] 39%|███▉      | 197/505 [00:04<00:06, 44.79it/s] 40%|████      | 202/505 [00:04<00:06, 44.90it/s] 41%|████      | 207/505 [00:04<00:06, 44.81it/s] 42%|████▏     | 212/505 [00:04<00:06, 44.70it/s] 43%|████▎     | 217/505 [00:04<00:06, 42.61it/s] 44%|████▍     | 222/505 [00:04<00:06, 43.31it/s] 45%|████▍     | 227/505 [00:05<00:06, 43.70it/s] 46%|████▌     | 232/505 [00:05<00:06, 43.92it/s] 47%|████▋     | 237/505 [00:05<00:06, 44.16it/s] 48%|████▊     | 242/505 [00:05<00:05, 44.43it/s] 49%|████▉     | 247/505 [00:05<00:05, 44.63it/s] 50%|████▉     | 252/505 [00:05<00:05, 44.71it/s] 51%|█████     | 257/505 [00:05<00:05, 44.40it/s] 52%|█████▏    | 262/505 [00:05<00:05, 44.46it/s] 53%|█████▎    | 267/505 [00:05<00:05, 44.60it/s] 54%|█████▍    | 272/505 [00:06<00:05, 44.60it/s] 55%|█████▍    | 277/505 [00:06<00:05, 44.62it/s] 56%|█████▌    | 282/505 [00:06<00:04, 44.63it/s] 57%|█████▋    | 287/505 [00:06<00:04, 44.72it/s] 58%|█████▊    | 292/505 [00:06<00:04, 44.78it/s] 59%|█████▉    | 297/505 [00:06<00:04, 44.66it/s] 60%|█████▉    | 302/505 [00:06<00:04, 44.57it/s] 61%|██████    | 307/505 [00:06<00:04, 44.54it/s] 62%|██████▏   | 312/505 [00:06<00:04, 44.67it/s] 63%|██████▎   | 317/505 [00:07<00:04, 44.64it/s] 64%|██████▍   | 322/505 [00:07<00:04, 44.65it/s] 65%|██████▍   | 327/505 [00:07<00:03, 44.63it/s] 66%|██████▌   | 332/505 [00:07<00:03, 44.66it/s] 67%|██████▋   | 337/505 [00:07<00:03, 44.79it/s] 68%|██████▊   | 342/505 [00:07<00:03, 44.65it/s] 69%|██████▊   | 347/505 [00:07<00:03, 44.64it/s] 70%|██████▉   | 352/505 [00:07<00:03, 43.02it/s] 71%|███████   | 357/505 [00:08<00:03, 43.63it/s] 72%|███████▏  | 362/505 [00:08<00:03, 43.98it/s] 73%|███████▎  | 367/505 [00:08<00:03, 44.12it/s] 74%|███████▎  | 372/505 [00:08<00:03, 44.30it/s] 75%|███████▍  | 377/505 [00:08<00:02, 44.44it/s] 76%|███████▌  | 382/505 [00:08<00:02, 44.55it/s] 77%|███████▋  | 387/505 [00:08<00:02, 44.53it/s] 78%|███████▊  | 392/505 [00:08<00:02, 44.28it/s] 79%|███████▊  | 397/505 [00:08<00:02, 44.49it/s] 80%|███████▉  | 402/505 [00:09<00:02, 44.67it/s] 81%|████████  | 407/505 [00:09<00:02, 44.66it/s] 82%|████████▏ | 412/505 [00:09<00:02, 44.67it/s] 83%|████████▎ | 417/505 [00:09<00:01, 44.67it/s] 84%|████████▎ | 422/505 [00:09<00:01, 44.75it/s] 85%|████████▍ | 427/505 [00:09<00:01, 44.73it/s] 86%|████████▌ | 432/505 [00:09<00:01, 44.55it/s] 87%|████████▋ | 437/505 [00:09<00:01, 44.43it/s] 88%|████████▊ | 442/505 [00:09<00:01, 44.45it/s] 89%|████████▊ | 447/505 [00:10<00:01, 44.59it/s] 90%|████████▉ | 452/505 [00:10<00:01, 44.66it/s] 90%|█████████ | 457/505 [00:10<00:01, 44.71it/s] 91%|█████████▏| 462/505 [00:10<00:00, 44.69it/s] 92%|█████████▏| 467/505 [00:10<00:00, 44.69it/s] 93%|█████████▎| 472/505 [00:10<00:00, 44.62it/s] 94%|█████████▍| 477/505 [00:10<00:00, 44.52it/s] 95%|█████████▌| 482/505 [00:10<00:00, 44.50it/s] 96%|█████████▋| 487/505 [00:10<00:00, 42.34it/s] 97%|█████████▋| 492/505 [00:11<00:00, 43.12it/s] 98%|█████████▊| 497/505 [00:11<00:00, 43.70it/s] 99%|█████████▉| 502/505 [00:11<00:00, 43.99it/s]100%|██████████| 505/505 [00:11<00:00, 44.47it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:00:34,058 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   eval_loss               =     1.1916
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   eval_runtime            = 0:00:11.37
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   eval_samples            =       4039
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   eval_samples_per_second =    355.133
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   eval_steps_per_second   =     44.403
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:00:34,059 >>   perplexity              =     3.2923
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:43,684 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:43,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:43,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:43,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:43,687 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:00:43,995 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:00:43,996 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:00:44,295 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:00:45,355 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:00:45,451 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:47,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:47,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:47,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:47,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:00:47,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:00:48,095 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:00:48,096 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:00:48,938 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:00:49,278 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:00:49,278 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/dev.jsonl', 'labels': ['given name', 'languages spoken, written or signed', 'lowest point', 'mother', 'mouth of the watercourse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13430
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13530, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:20,  1.58it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.60it/s]Extractor Predicting: 37it [00:23,  1.64it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:25,  1.66it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:27,  1.59it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:28,  1.63it/s]Extractor Predicting: 45it [00:28,  1.65it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:30,  1.59it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.57it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:32,  1.64it/s]Extractor Predicting: 52it [00:33,  1.66it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.66it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:35,  1.64it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:38,  1.65it/s]Extractor Predicting: 62it [00:39,  1.62it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.59it/s]Extractor Predicting: 66it [00:41,  1.60it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.56it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:44,  1.56it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:45,  1.50it/s]Extractor Predicting: 73it [00:46,  1.51it/s]Extractor Predicting: 74it [00:47,  1.37it/s]Extractor Predicting: 75it [00:48,  1.42it/s]Extractor Predicting: 76it [00:48,  1.46it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:49,  1.50it/s]Extractor Predicting: 79it [00:50,  1.51it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:51,  1.51it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.53it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:54,  1.53it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.59it/s]Extractor Predicting: 89it [00:57,  1.58it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.57it/s]Extractor Predicting: 92it [00:58,  1.59it/s]Extractor Predicting: 93it [00:59,  1.59it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:00,  1.56it/s]Extractor Predicting: 96it [01:01,  1.54it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:04,  1.54it/s]Extractor Predicting: 101it [01:04,  1.50it/s]Extractor Predicting: 102it [01:05,  1.51it/s]Extractor Predicting: 103it [01:06,  1.53it/s]Extractor Predicting: 104it [01:06,  1.57it/s]Extractor Predicting: 105it [01:07,  1.57it/s]Extractor Predicting: 106it [01:07,  1.58it/s]Extractor Predicting: 107it [01:08,  1.59it/s]Extractor Predicting: 108it [01:09,  1.61it/s]Extractor Predicting: 109it [01:09,  1.61it/s]Extractor Predicting: 110it [01:10,  1.59it/s]Extractor Predicting: 111it [01:11,  1.54it/s]Extractor Predicting: 112it [01:11,  1.59it/s]Extractor Predicting: 113it [01:12,  1.58it/s]Extractor Predicting: 114it [01:13,  1.56it/s]Extractor Predicting: 115it [01:13,  1.61it/s]Extractor Predicting: 116it [01:14,  1.59it/s]Extractor Predicting: 117it [01:14,  1.60it/s]Extractor Predicting: 118it [01:15,  1.54it/s]Extractor Predicting: 119it [01:16,  1.59it/s]Extractor Predicting: 120it [01:16,  1.59it/s]Extractor Predicting: 121it [01:17,  1.55it/s]Extractor Predicting: 122it [01:18,  1.56it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.60it/s]Extractor Predicting: 125it [01:20,  1.47it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:21,  1.48it/s]Extractor Predicting: 128it [01:22,  1.44it/s]Extractor Predicting: 129it [01:22,  1.46it/s]Extractor Predicting: 130it [01:23,  1.47it/s]Extractor Predicting: 131it [01:24,  1.43it/s]Extractor Predicting: 132it [01:24,  1.44it/s]Extractor Predicting: 133it [01:25,  1.43it/s]Extractor Predicting: 134it [01:26,  1.48it/s]Extractor Predicting: 135it [01:26,  1.48it/s]Extractor Predicting: 136it [01:27,  1.49it/s]Extractor Predicting: 137it [01:28,  1.51it/s]Extractor Predicting: 138it [01:28,  1.45it/s]Extractor Predicting: 139it [01:29,  1.46it/s]Extractor Predicting: 140it [01:30,  1.46it/s]Extractor Predicting: 141it [01:31,  1.47it/s]Extractor Predicting: 142it [01:31,  1.48it/s]Extractor Predicting: 143it [01:32,  1.46it/s]Extractor Predicting: 144it [01:33,  1.48it/s]Extractor Predicting: 145it [01:33,  1.46it/s]Extractor Predicting: 146it [01:34,  1.37it/s]Extractor Predicting: 147it [01:35,  1.40it/s]Extractor Predicting: 148it [01:36,  1.37it/s]Extractor Predicting: 149it [01:36,  1.43it/s]Extractor Predicting: 149it [01:36,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:39,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:39,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:39,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:39,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:39,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:02:39,810 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:02:39,811 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:02:40,102 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:02:41,144 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:02:41,144 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:42,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:42,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:42,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:42,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:02:42,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:02:42,880 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:02:42,882 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:02:43,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:02:43,296 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:02:43,296 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.18407731247123793,
  "recall": 0.09903441445902451,
  "score": 0.128783000643915,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 12675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:08,  1.79it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.80it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.73it/s]Extractor Predicting: 21it [00:12,  1.73it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:13,  1.79it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.80it/s]Extractor Predicting: 28it [00:16,  1.81it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:17,  1.77it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:18,  1.80it/s]Extractor Predicting: 33it [00:19,  1.74it/s]Extractor Predicting: 34it [00:19,  1.72it/s]Extractor Predicting: 35it [00:20,  1.70it/s]Extractor Predicting: 36it [00:20,  1.73it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.70it/s]Extractor Predicting: 39it [00:22,  1.71it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.69it/s]Extractor Predicting: 43it [00:24,  1.72it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:26,  1.58it/s]Extractor Predicting: 46it [00:26,  1.57it/s]Extractor Predicting: 47it [00:27,  1.58it/s]Extractor Predicting: 48it [00:28,  1.62it/s]Extractor Predicting: 49it [00:28,  1.66it/s]Extractor Predicting: 50it [00:29,  1.68it/s]Extractor Predicting: 51it [00:29,  1.73it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:30,  1.67it/s]Extractor Predicting: 54it [00:31,  1.69it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:32,  1.63it/s]Extractor Predicting: 57it [00:33,  1.64it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:34,  1.64it/s]Extractor Predicting: 60it [00:35,  1.65it/s]Extractor Predicting: 61it [00:35,  1.66it/s]Extractor Predicting: 62it [00:36,  1.70it/s]Extractor Predicting: 63it [00:37,  1.67it/s]Extractor Predicting: 64it [00:37,  1.68it/s]Extractor Predicting: 65it [00:38,  1.71it/s]Extractor Predicting: 66it [00:38,  1.72it/s]Extractor Predicting: 67it [00:39,  1.70it/s]Extractor Predicting: 68it [00:39,  1.70it/s]Extractor Predicting: 69it [00:40,  1.67it/s]Extractor Predicting: 70it [00:41,  1.64it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:42,  1.66it/s]Extractor Predicting: 73it [00:43,  1.65it/s]Extractor Predicting: 74it [00:43,  1.66it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:44,  1.65it/s]Extractor Predicting: 77it [00:45,  1.67it/s]Extractor Predicting: 78it [00:46,  1.62it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:47,  1.68it/s]Extractor Predicting: 81it [00:47,  1.66it/s]Extractor Predicting: 82it [00:48,  1.64it/s]Extractor Predicting: 83it [00:49,  1.64it/s]Extractor Predicting: 84it [00:49,  1.66it/s]Extractor Predicting: 85it [00:50,  1.64it/s]Extractor Predicting: 86it [00:50,  1.63it/s]Extractor Predicting: 87it [00:51,  1.66it/s]Extractor Predicting: 88it [00:52,  1.53it/s]Extractor Predicting: 89it [00:52,  1.61it/s]Extractor Predicting: 90it [00:53,  1.62it/s]Extractor Predicting: 91it [00:54,  1.62it/s]Extractor Predicting: 92it [00:54,  1.64it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:55,  1.70it/s]Extractor Predicting: 95it [00:56,  1.69it/s]Extractor Predicting: 96it [00:56,  1.71it/s]Extractor Predicting: 97it [00:57,  1.75it/s]Extractor Predicting: 98it [00:58,  1.72it/s]Extractor Predicting: 99it [00:58,  1.71it/s]Extractor Predicting: 100it [00:59,  1.74it/s]Extractor Predicting: 101it [00:59,  1.73it/s]Extractor Predicting: 102it [01:00,  1.67it/s]Extractor Predicting: 103it [01:01,  1.67it/s]Extractor Predicting: 104it [01:01,  1.71it/s]Extractor Predicting: 105it [01:02,  1.66it/s]Extractor Predicting: 106it [01:02,  1.69it/s]Extractor Predicting: 107it [01:03,  1.70it/s]Extractor Predicting: 108it [01:04,  1.66it/s]Extractor Predicting: 109it [01:04,  1.65it/s]Extractor Predicting: 110it [01:05,  1.64it/s]Extractor Predicting: 111it [01:05,  1.64it/s]Extractor Predicting: 112it [01:06,  1.65it/s]Extractor Predicting: 113it [01:07,  1.68it/s]Extractor Predicting: 114it [01:07,  1.68it/s]Extractor Predicting: 115it [01:08,  1.69it/s]Extractor Predicting: 116it [01:08,  1.67it/s]Extractor Predicting: 117it [01:09,  1.68it/s]Extractor Predicting: 118it [01:09,  1.73it/s]Extractor Predicting: 119it [01:10,  1.76it/s]Extractor Predicting: 120it [01:11,  1.76it/s]Extractor Predicting: 121it [01:11,  1.76it/s]Extractor Predicting: 122it [01:12,  1.71it/s]Extractor Predicting: 123it [01:12,  1.70it/s]Extractor Predicting: 124it [01:13,  1.72it/s]Extractor Predicting: 125it [01:13,  1.72it/s]Extractor Predicting: 126it [01:14,  1.70it/s]Extractor Predicting: 127it [01:15,  1.68it/s]Extractor Predicting: 128it [01:15,  1.63it/s]Extractor Predicting: 129it [01:16,  1.61it/s]Extractor Predicting: 130it [01:17,  1.60it/s]Extractor Predicting: 131it [01:17,  1.58it/s]Extractor Predicting: 132it [01:18,  1.60it/s]Extractor Predicting: 133it [01:19,  1.61it/s]Extractor Predicting: 134it [01:19,  1.65it/s]Extractor Predicting: 135it [01:20,  1.59it/s]Extractor Predicting: 136it [01:20,  1.59it/s]Extractor Predicting: 137it [01:21,  1.59it/s]Extractor Predicting: 138it [01:22,  1.60it/s]Extractor Predicting: 139it [01:22,  1.60it/s]Extractor Predicting: 140it [01:23,  1.57it/s]Extractor Predicting: 141it [01:24,  1.57it/s]Extractor Predicting: 142it [01:24,  1.58it/s]Extractor Predicting: 143it [01:25,  1.60it/s]Extractor Predicting: 144it [01:25,  1.59it/s]Extractor Predicting: 145it [01:26,  1.59it/s]Extractor Predicting: 146it [01:27,  1.60it/s]Extractor Predicting: 147it [01:27,  1.61it/s]Extractor Predicting: 148it [01:28,  1.60it/s]Extractor Predicting: 149it [01:28,  1.64it/s]Extractor Predicting: 150it [01:29,  1.63it/s]Extractor Predicting: 151it [01:30,  1.62it/s]Extractor Predicting: 152it [01:30,  1.62it/s]Extractor Predicting: 153it [01:31,  1.58it/s]Extractor Predicting: 154it [01:32,  1.58it/s]Extractor Predicting: 155it [01:32,  1.60it/s]Extractor Predicting: 156it [01:33,  1.65it/s]Extractor Predicting: 157it [01:33,  1.72it/s]Extractor Predicting: 158it [01:34,  1.73it/s]Extractor Predicting: 159it [01:34,  1.73it/s]Extractor Predicting: 160it [01:35,  1.71it/s]Extractor Predicting: 161it [01:36,  1.76it/s]Extractor Predicting: 162it [01:36,  1.72it/s]Extractor Predicting: 163it [01:37,  1.70it/s]Extractor Predicting: 164it [01:37,  1.73it/s]Extractor Predicting: 165it [01:38,  1.77it/s]Extractor Predicting: 166it [01:39,  1.74it/s]Extractor Predicting: 167it [01:39,  1.81it/s]Extractor Predicting: 168it [01:40,  1.82it/s]Extractor Predicting: 169it [01:40,  1.86it/s]Extractor Predicting: 170it [01:41,  1.80it/s]Extractor Predicting: 171it [01:41,  1.73it/s]Extractor Predicting: 172it [01:42,  1.67it/s]Extractor Predicting: 173it [01:43,  1.72it/s]Extractor Predicting: 173it [01:43,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:35,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:35,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:35,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:35,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:35,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:04:35,923 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:04:35,924 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:04:36,213 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:04:37,316 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:04:37,316 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:39,529 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:39,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:39,554 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:39,554 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:04:39,554 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:04:40,033 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:04:40,034 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:04:40,741 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:04:40,965 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:04:40,966 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.1936619718309859,
  "recall": 0.11939218523878437,
  "score": 0.1477170993733214,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 4332
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 4432, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.84it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.45it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.58it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.72it/s]Extractor Predicting: 17it [00:10,  1.82it/s]Extractor Predicting: 18it [00:11,  1.86it/s]Extractor Predicting: 19it [00:11,  1.88it/s]Extractor Predicting: 20it [00:12,  1.90it/s]Extractor Predicting: 21it [00:12,  1.90it/s]Extractor Predicting: 22it [00:13,  1.88it/s]Extractor Predicting: 23it [00:13,  1.89it/s]Extractor Predicting: 24it [00:14,  1.88it/s]Extractor Predicting: 25it [00:14,  1.89it/s]Extractor Predicting: 26it [00:15,  1.92it/s]Extractor Predicting: 27it [00:15,  1.86it/s]Extractor Predicting: 28it [00:16,  1.89it/s]Extractor Predicting: 29it [00:16,  1.90it/s]Extractor Predicting: 30it [00:17,  1.95it/s]Extractor Predicting: 31it [00:17,  1.98it/s]Extractor Predicting: 32it [00:18,  1.97it/s]Extractor Predicting: 33it [00:18,  1.93it/s]Extractor Predicting: 34it [00:19,  1.94it/s]Extractor Predicting: 35it [00:19,  1.95it/s]Extractor Predicting: 36it [00:20,  1.91it/s]Extractor Predicting: 37it [00:21,  1.88it/s]Extractor Predicting: 38it [00:21,  1.94it/s]Extractor Predicting: 39it [00:22,  1.92it/s]Extractor Predicting: 40it [00:22,  1.93it/s]Extractor Predicting: 41it [00:23,  1.93it/s]Extractor Predicting: 42it [00:23,  1.97it/s]Extractor Predicting: 43it [00:24,  1.94it/s]Extractor Predicting: 44it [00:24,  1.79it/s]Extractor Predicting: 45it [00:25,  1.68it/s]Extractor Predicting: 46it [00:26,  1.61it/s]Extractor Predicting: 47it [00:26,  1.58it/s]Extractor Predicting: 48it [00:27,  1.56it/s]Extractor Predicting: 49it [00:28,  1.56it/s]Extractor Predicting: 50it [00:28,  1.54it/s]Extractor Predicting: 51it [00:29,  1.54it/s]Extractor Predicting: 52it [00:30,  1.53it/s]Extractor Predicting: 53it [00:30,  1.52it/s]Extractor Predicting: 54it [00:31,  1.51it/s]Extractor Predicting: 55it [00:32,  1.52it/s]Extractor Predicting: 56it [00:32,  1.41it/s]Extractor Predicting: 57it [00:33,  1.43it/s]Extractor Predicting: 58it [00:34,  1.46it/s]Extractor Predicting: 59it [00:34,  1.70it/s]Extractor Predicting: 59it [00:34,  1.70it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6791187739463601,
  "recall": 0.22922728742321372,
  "score": 0.34276045443558134,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/', 'labels': ['genre', 'is a list of', 'located on astronomical body', 'manufacturer', 'member of'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
